{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TweetsCNN(object):\n",
    "    \"\"\"\n",
    "    Creating a CNN model for Twitter Sentiment analysis \n",
    "    Input: Word embedding \n",
    "    Input > Convolutonal > Max-Pooling > Softmax Layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "    self, sequence_length, num_classes, vocab_size,\n",
    "    embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        \n",
    "        # Defining Placeholders for X,y and dropout \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout = tf.placeholder(tf.float32, name=\"dropout\")\n",
    "        \n",
    "        # L2 regularization loss\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Define Embedding Layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # Initiate a word vector\n",
    "            WV = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size, embedding_size], -1.0,1.0))\n",
    "            # Initiate embedding layer\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(WV, self.input_x)\n",
    "            # Expand dimension to make the input 4-dimensional\n",
    "            # [None, sequence_length, embedding_size, 1]\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "       \n",
    "        # Convolution and Max-Pooling Layers\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                \n",
    "                # Add Activation function\n",
    "                activation_func = tf.nn.relu(tf.nn.bias_add(conv,b), name=\"relu\")\n",
    "                \n",
    "                # Add Max-pooling layer\n",
    "                pooling = tf.nn.max_pool(\n",
    "                    activation_func,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides = [1,1,1,1],\n",
    "                    padding = 'VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooling)\n",
    "                \n",
    "        # Combine all the pooled feature\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # Add Dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout)\n",
    "        \n",
    "        # Predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = \"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        # Define Loss Function: mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            \n",
    "        # Define Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            \n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_sample_percentage =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "embedding_dim = 128\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_model = pd.read_csv(\"C:/Users/Ung Lik Teng/Desktop/CodenData/Machine Learning/NLP/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>miss new moon trailer</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>omg alreadi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cri dentist sinc supos...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>think mi bf cheat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0            7               0               0                    1   \n",
       "1            6               0               0                    0   \n",
       "2            6               0               0                    0   \n",
       "3           25               0               0                    0   \n",
       "4            9               0               0                    0   \n",
       "\n",
       "   count_excl_quest_marks  count_urls  Sentiment  \\\n",
       "0                       0           0          0   \n",
       "1                       0           0          0   \n",
       "2                       0           0          1   \n",
       "3                       0           0          0   \n",
       "4                       3           0          0   \n",
       "\n",
       "                                          clean_text  clean_count_words  \n",
       "0                                     sad apl friend                  3  \n",
       "1                              miss new moon trailer                  4  \n",
       "2                                        omg alreadi                  2  \n",
       "3  omgaga im sooo im gunna cri dentist sinc supos...                 13  \n",
       "4                                  think mi bf cheat                  4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = df_model.clean_text\n",
    "y = df_model.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if (y[i] == 1):\n",
    "        labels.append([1,0])\n",
    "    else:\n",
    "        labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-9f8d486d5062>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Ung Lik Teng\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Ung Lik Teng\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y_train)))\n",
    "x_train, x_dev = x_train[:dev_sample_index], x_train[dev_sample_index:]\n",
    "y_train, y_dev = y_train[:dev_sample_index], y_train[dev_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42915"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Function\n",
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        cnn.input_x: x_batch,\n",
    "        cnn.input_y: y_batch,\n",
    "        cnn.dropout: dropout_keep_prob\n",
    "    }\n",
    "            \n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "def dev_step(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "\n",
    "    feed_dict = {\n",
    "        cnn.input_x: x_batch,\n",
    "        cnn.input_y: y_batch,\n",
    "        cnn.dropout: 1.0\n",
    "    }\n",
    "\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/hist is illegal; using embedding/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/sparsity is illegal; using embedding/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\n",
      "\n",
      "2018-05-23T03:17:29.964931: step 1, loss 1.66312, acc 0.484375\n",
      "2018-05-23T03:17:30.307016: step 2, loss 1.70709, acc 0.46875\n",
      "2018-05-23T03:17:30.737864: step 3, loss 1.52377, acc 0.53125\n",
      "2018-05-23T03:17:31.176690: step 4, loss 1.74567, acc 0.578125\n",
      "2018-05-23T03:17:31.560662: step 5, loss 1.34328, acc 0.625\n",
      "2018-05-23T03:17:31.889782: step 6, loss 1.41803, acc 0.640625\n",
      "2018-05-23T03:17:32.201948: step 7, loss 1.78503, acc 0.421875\n",
      "2018-05-23T03:17:32.538047: step 8, loss 1.82617, acc 0.5\n",
      "2018-05-23T03:17:32.964905: step 9, loss 1.356, acc 0.625\n",
      "2018-05-23T03:17:33.347881: step 10, loss 1.73607, acc 0.546875\n",
      "2018-05-23T03:17:33.661043: step 11, loss 1.81835, acc 0.453125\n",
      "2018-05-23T03:17:33.990163: step 12, loss 1.16934, acc 0.640625\n",
      "2018-05-23T03:17:34.310308: step 13, loss 1.74542, acc 0.5\n",
      "2018-05-23T03:17:34.615489: step 14, loss 2.04425, acc 0.4375\n",
      "2018-05-23T03:17:34.925068: step 15, loss 1.72721, acc 0.484375\n",
      "2018-05-23T03:17:35.230254: step 16, loss 1.41807, acc 0.578125\n",
      "2018-05-23T03:17:35.543416: step 17, loss 1.67099, acc 0.484375\n",
      "2018-05-23T03:17:35.852588: step 18, loss 1.24858, acc 0.5\n",
      "2018-05-23T03:17:36.158768: step 19, loss 1.31135, acc 0.5\n",
      "2018-05-23T03:17:36.472929: step 20, loss 1.08065, acc 0.578125\n",
      "2018-05-23T03:17:36.784096: step 21, loss 1.64495, acc 0.515625\n",
      "2018-05-23T03:17:37.095265: step 22, loss 1.30168, acc 0.53125\n",
      "2018-05-23T03:17:37.426377: step 23, loss 1.18516, acc 0.59375\n",
      "2018-05-23T03:17:37.799380: step 24, loss 1.31677, acc 0.515625\n",
      "2018-05-23T03:17:38.109550: step 25, loss 1.22097, acc 0.59375\n",
      "2018-05-23T03:17:38.426701: step 26, loss 1.24589, acc 0.515625\n",
      "2018-05-23T03:17:38.736873: step 27, loss 1.05482, acc 0.65625\n",
      "2018-05-23T03:17:39.067991: step 28, loss 1.34277, acc 0.578125\n",
      "2018-05-23T03:17:39.382146: step 29, loss 1.92081, acc 0.5\n",
      "2018-05-23T03:17:39.701292: step 30, loss 0.917633, acc 0.609375\n",
      "2018-05-23T03:17:40.017447: step 31, loss 1.8861, acc 0.46875\n",
      "2018-05-23T03:17:40.390447: step 32, loss 1.06288, acc 0.65625\n",
      "2018-05-23T03:17:40.705606: step 33, loss 1.1084, acc 0.5625\n",
      "2018-05-23T03:17:41.066639: step 34, loss 1.37432, acc 0.546875\n",
      "2018-05-23T03:17:41.384788: step 35, loss 1.23856, acc 0.59375\n",
      "2018-05-23T03:17:41.706927: step 36, loss 1.1795, acc 0.546875\n",
      "2018-05-23T03:17:42.008120: step 37, loss 1.62703, acc 0.421875\n",
      "2018-05-23T03:17:42.373143: step 38, loss 1.32804, acc 0.484375\n",
      "2018-05-23T03:17:42.688302: step 39, loss 1.12393, acc 0.640625\n",
      "2018-05-23T03:17:43.026398: step 40, loss 1.56364, acc 0.453125\n",
      "2018-05-23T03:17:43.355516: step 41, loss 1.20387, acc 0.5\n",
      "2018-05-23T03:17:43.669676: step 42, loss 1.73457, acc 0.46875\n",
      "2018-05-23T03:17:44.028716: step 43, loss 0.973006, acc 0.515625\n",
      "2018-05-23T03:17:44.341877: step 44, loss 1.0083, acc 0.546875\n",
      "2018-05-23T03:17:44.687951: step 45, loss 1.50544, acc 0.515625\n",
      "2018-05-23T03:17:45.008097: step 46, loss 1.57121, acc 0.515625\n",
      "2018-05-23T03:17:45.322254: step 47, loss 1.36597, acc 0.5\n",
      "2018-05-23T03:17:45.634420: step 48, loss 1.00963, acc 0.609375\n",
      "2018-05-23T03:17:45.970521: step 49, loss 1.26715, acc 0.578125\n",
      "2018-05-23T03:17:46.286674: step 50, loss 1.25172, acc 0.53125\n",
      "2018-05-23T03:17:46.606818: step 51, loss 0.939179, acc 0.5625\n",
      "2018-05-23T03:17:46.939927: step 52, loss 0.984899, acc 0.59375\n",
      "2018-05-23T03:17:47.253089: step 53, loss 1.18981, acc 0.625\n",
      "2018-05-23T03:17:47.552289: step 54, loss 1.1114, acc 0.578125\n",
      "2018-05-23T03:17:47.867446: step 55, loss 1.20377, acc 0.609375\n",
      "2018-05-23T03:17:48.191578: step 56, loss 1.43991, acc 0.515625\n",
      "2018-05-23T03:17:48.583530: step 57, loss 1.07919, acc 0.578125\n",
      "2018-05-23T03:17:48.895697: step 58, loss 1.18403, acc 0.546875\n",
      "2018-05-23T03:17:49.210851: step 59, loss 1.17411, acc 0.4375\n",
      "2018-05-23T03:17:49.517032: step 60, loss 1.61967, acc 0.453125\n",
      "2018-05-23T03:17:49.841165: step 61, loss 1.07197, acc 0.546875\n",
      "2018-05-23T03:17:50.239101: step 62, loss 1.03139, acc 0.546875\n",
      "2018-05-23T03:17:50.633047: step 63, loss 1.05338, acc 0.609375\n",
      "2018-05-23T03:17:51.011037: step 64, loss 1.12899, acc 0.453125\n",
      "2018-05-23T03:17:51.506712: step 65, loss 0.897434, acc 0.640625\n",
      "2018-05-23T03:17:51.987425: step 66, loss 1.00873, acc 0.671875\n",
      "2018-05-23T03:17:52.297595: step 67, loss 1.06089, acc 0.609375\n",
      "2018-05-23T03:17:52.755371: step 68, loss 0.934722, acc 0.578125\n",
      "2018-05-23T03:17:53.193199: step 69, loss 0.936044, acc 0.515625\n",
      "2018-05-23T03:17:53.526306: step 70, loss 1.26494, acc 0.546875\n",
      "2018-05-23T03:17:53.929231: step 71, loss 1.05548, acc 0.59375\n",
      "2018-05-23T03:17:54.323178: step 72, loss 1.29909, acc 0.40625\n",
      "2018-05-23T03:17:54.671245: step 73, loss 1.13527, acc 0.578125\n",
      "2018-05-23T03:17:54.983408: step 74, loss 0.9976, acc 0.578125\n",
      "2018-05-23T03:17:55.289590: step 75, loss 1.29789, acc 0.53125\n",
      "2018-05-23T03:17:55.590784: step 76, loss 0.886438, acc 0.5625\n",
      "2018-05-23T03:17:55.907936: step 77, loss 0.923159, acc 0.59375\n",
      "2018-05-23T03:17:56.243039: step 78, loss 0.958028, acc 0.59375\n",
      "2018-05-23T03:17:56.543236: step 79, loss 1.17614, acc 0.4375\n",
      "2018-05-23T03:17:56.872355: step 80, loss 1.39246, acc 0.4375\n",
      "2018-05-23T03:17:57.261316: step 81, loss 0.90256, acc 0.578125\n",
      "2018-05-23T03:17:57.565501: step 82, loss 1.19037, acc 0.546875\n",
      "2018-05-23T03:17:57.881656: step 83, loss 1.06723, acc 0.53125\n",
      "2018-05-23T03:17:58.190828: step 84, loss 1.29228, acc 0.484375\n",
      "2018-05-23T03:17:58.500001: step 85, loss 1.01197, acc 0.5625\n",
      "2018-05-23T03:17:58.804188: step 86, loss 1.13348, acc 0.484375\n",
      "2018-05-23T03:17:59.103389: step 87, loss 1.03267, acc 0.5625\n",
      "2018-05-23T03:17:59.423530: step 88, loss 1.57679, acc 0.546875\n",
      "2018-05-23T03:17:59.738690: step 89, loss 1.32196, acc 0.5625\n",
      "2018-05-23T03:18:00.058831: step 90, loss 1.01438, acc 0.640625\n",
      "2018-05-23T03:18:00.372990: step 91, loss 0.971569, acc 0.625\n",
      "2018-05-23T03:18:00.785888: step 92, loss 1.24326, acc 0.578125\n",
      "2018-05-23T03:18:01.114008: step 93, loss 1.38499, acc 0.578125\n",
      "2018-05-23T03:18:01.449116: step 94, loss 1.00983, acc 0.578125\n",
      "2018-05-23T03:18:01.795186: step 95, loss 0.999937, acc 0.609375\n",
      "2018-05-23T03:18:02.102364: step 96, loss 0.934878, acc 0.515625\n",
      "2018-05-23T03:18:02.414532: step 97, loss 1.12842, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:18:02.751635: step 98, loss 1.04558, acc 0.546875\n",
      "2018-05-23T03:18:03.113659: step 99, loss 1.26794, acc 0.484375\n",
      "2018-05-23T03:18:03.424828: step 100, loss 1.17625, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:18:08.016544: step 100, loss 0.691035, acc 0.612659\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-100\n",
      "\n",
      "2018-05-23T03:18:09.496583: step 101, loss 1.41617, acc 0.46875\n",
      "2018-05-23T03:18:09.809747: step 102, loss 0.819972, acc 0.65625\n",
      "2018-05-23T03:18:10.144851: step 103, loss 1.10914, acc 0.578125\n",
      "2018-05-23T03:18:10.466987: step 104, loss 1.13755, acc 0.59375\n",
      "2018-05-23T03:18:10.813063: step 105, loss 1.19524, acc 0.53125\n",
      "2018-05-23T03:18:11.137197: step 106, loss 1.13801, acc 0.515625\n",
      "2018-05-23T03:18:11.447365: step 107, loss 1.02198, acc 0.640625\n",
      "2018-05-23T03:18:11.754542: step 108, loss 1.18514, acc 0.640625\n",
      "2018-05-23T03:18:12.060724: step 109, loss 1.0365, acc 0.6875\n",
      "2018-05-23T03:18:12.371891: step 110, loss 1.05016, acc 0.609375\n",
      "2018-05-23T03:18:12.683061: step 111, loss 1.37574, acc 0.421875\n",
      "2018-05-23T03:18:13.048082: step 112, loss 0.914338, acc 0.578125\n",
      "2018-05-23T03:18:13.353266: step 113, loss 0.947116, acc 0.546875\n",
      "2018-05-23T03:18:13.736241: step 114, loss 0.993034, acc 0.546875\n",
      "2018-05-23T03:18:14.050402: step 115, loss 1.07961, acc 0.59375\n",
      "2018-05-23T03:18:14.410438: step 116, loss 0.711919, acc 0.546875\n",
      "2018-05-23T03:18:14.731580: step 117, loss 1.04356, acc 0.578125\n",
      "2018-05-23T03:18:15.047735: step 118, loss 1.17981, acc 0.46875\n",
      "2018-05-23T03:18:15.399792: step 119, loss 0.789873, acc 0.640625\n",
      "2018-05-23T03:18:15.707966: step 120, loss 0.976753, acc 0.5\n",
      "2018-05-23T03:18:16.072818: step 121, loss 0.875893, acc 0.59375\n",
      "2018-05-23T03:18:16.380994: step 122, loss 0.80406, acc 0.59375\n",
      "2018-05-23T03:18:16.698145: step 123, loss 0.800574, acc 0.65625\n",
      "2018-05-23T03:18:17.003329: step 124, loss 0.941112, acc 0.5625\n",
      "2018-05-23T03:18:17.313501: step 125, loss 0.717835, acc 0.71875\n",
      "2018-05-23T03:18:17.619681: step 126, loss 1.05621, acc 0.625\n",
      "2018-05-23T03:18:17.928853: step 127, loss 1.16214, acc 0.515625\n",
      "2018-05-23T03:18:18.311829: step 128, loss 0.982648, acc 0.609375\n",
      "2018-05-23T03:18:18.724725: step 129, loss 0.977227, acc 0.640625\n",
      "2018-05-23T03:18:19.136622: step 130, loss 1.15416, acc 0.484375\n",
      "2018-05-23T03:18:19.550516: step 131, loss 1.45067, acc 0.53125\n",
      "2018-05-23T03:18:19.961416: step 132, loss 0.919418, acc 0.609375\n",
      "2018-05-23T03:18:20.372317: step 133, loss 1.10327, acc 0.453125\n",
      "2018-05-23T03:18:20.757286: step 134, loss 0.972573, acc 0.546875\n",
      "2018-05-23T03:18:21.073441: step 135, loss 1.16561, acc 0.484375\n",
      "2018-05-23T03:18:21.401565: step 136, loss 1.16586, acc 0.5\n",
      "2018-05-23T03:18:21.710737: step 137, loss 1.04529, acc 0.53125\n",
      "2018-05-23T03:18:22.012927: step 138, loss 0.942314, acc 0.578125\n",
      "2018-05-23T03:18:22.329081: step 139, loss 0.771138, acc 0.59375\n",
      "2018-05-23T03:18:22.629279: step 140, loss 0.898184, acc 0.625\n",
      "2018-05-23T03:18:22.931473: step 141, loss 1.11915, acc 0.5625\n",
      "2018-05-23T03:18:23.257598: step 142, loss 0.943036, acc 0.5625\n",
      "2018-05-23T03:18:23.561784: step 143, loss 0.808677, acc 0.640625\n",
      "2018-05-23T03:18:23.876942: step 144, loss 0.847832, acc 0.6875\n",
      "2018-05-23T03:18:24.184119: step 145, loss 0.953299, acc 0.53125\n",
      "2018-05-23T03:18:24.497281: step 146, loss 0.930504, acc 0.671875\n",
      "2018-05-23T03:18:24.800471: step 147, loss 0.822622, acc 0.671875\n",
      "2018-05-23T03:18:25.107649: step 148, loss 1.11428, acc 0.5625\n",
      "2018-05-23T03:18:25.422806: step 149, loss 0.940554, acc 0.625\n",
      "2018-05-23T03:18:25.729985: step 150, loss 0.799824, acc 0.671875\n",
      "2018-05-23T03:18:26.031223: step 151, loss 1.09669, acc 0.546875\n",
      "2018-05-23T03:18:26.336407: step 152, loss 0.710058, acc 0.6875\n",
      "2018-05-23T03:18:26.639595: step 153, loss 1.08712, acc 0.625\n",
      "2018-05-23T03:18:26.948770: step 154, loss 1.05136, acc 0.53125\n",
      "2018-05-23T03:18:27.253951: step 155, loss 0.867846, acc 0.59375\n",
      "2018-05-23T03:18:27.564123: step 156, loss 0.74867, acc 0.59375\n",
      "2018-05-23T03:18:27.892245: step 157, loss 0.965878, acc 0.5\n",
      "2018-05-23T03:18:28.197427: step 158, loss 1.07807, acc 0.5\n",
      "2018-05-23T03:18:28.502612: step 159, loss 0.959348, acc 0.578125\n",
      "2018-05-23T03:18:28.806798: step 160, loss 0.710037, acc 0.65625\n",
      "2018-05-23T03:18:29.111982: step 161, loss 0.853649, acc 0.625\n",
      "2018-05-23T03:18:29.412178: step 162, loss 1.01368, acc 0.625\n",
      "2018-05-23T03:18:29.729330: step 163, loss 0.784739, acc 0.609375\n",
      "2018-05-23T03:18:30.029526: step 164, loss 0.826175, acc 0.5625\n",
      "2018-05-23T03:18:30.332716: step 165, loss 0.874034, acc 0.609375\n",
      "2018-05-23T03:18:30.635906: step 166, loss 0.812146, acc 0.5625\n",
      "2018-05-23T03:18:30.933112: step 167, loss 0.932237, acc 0.546875\n",
      "2018-05-23T03:18:31.242283: step 168, loss 0.975519, acc 0.546875\n",
      "2018-05-23T03:18:31.548465: step 169, loss 0.913294, acc 0.59375\n",
      "2018-05-23T03:18:31.851653: step 170, loss 0.921387, acc 0.640625\n",
      "2018-05-23T03:18:32.153844: step 171, loss 0.806652, acc 0.703125\n",
      "2018-05-23T03:18:32.459028: step 172, loss 0.971331, acc 0.515625\n",
      "2018-05-23T03:18:32.775185: step 173, loss 0.908058, acc 0.53125\n",
      "2018-05-23T03:18:33.076375: step 174, loss 1.02069, acc 0.5625\n",
      "2018-05-23T03:18:33.383555: step 175, loss 0.774345, acc 0.609375\n",
      "2018-05-23T03:18:33.691732: step 176, loss 0.890336, acc 0.625\n",
      "2018-05-23T03:18:33.992924: step 177, loss 1.01355, acc 0.59375\n",
      "2018-05-23T03:18:34.304092: step 178, loss 0.849359, acc 0.640625\n",
      "2018-05-23T03:18:34.618252: step 179, loss 0.809301, acc 0.59375\n",
      "2018-05-23T03:18:34.936401: step 180, loss 1.05646, acc 0.453125\n",
      "2018-05-23T03:18:35.234602: step 181, loss 0.966171, acc 0.5625\n",
      "2018-05-23T03:18:35.532805: step 182, loss 0.868032, acc 0.609375\n",
      "2018-05-23T03:18:35.845967: step 183, loss 0.961423, acc 0.5625\n",
      "2018-05-23T03:18:36.155140: step 184, loss 0.693796, acc 0.671875\n",
      "2018-05-23T03:18:36.454340: step 185, loss 0.726705, acc 0.640625\n",
      "2018-05-23T03:18:36.768500: step 186, loss 0.994558, acc 0.46875\n",
      "2018-05-23T03:18:37.069393: step 187, loss 0.836366, acc 0.65625\n",
      "2018-05-23T03:18:37.381561: step 188, loss 0.758315, acc 0.65625\n",
      "2018-05-23T03:18:37.687740: step 189, loss 1.14255, acc 0.53125\n",
      "2018-05-23T03:18:37.985941: step 190, loss 0.882485, acc 0.578125\n",
      "2018-05-23T03:18:38.286138: step 191, loss 0.819507, acc 0.609375\n",
      "2018-05-23T03:18:38.584341: step 192, loss 0.977845, acc 0.578125\n",
      "2018-05-23T03:18:38.894509: step 193, loss 0.813854, acc 0.609375\n",
      "2018-05-23T03:18:39.189720: step 194, loss 0.703656, acc 0.703125\n",
      "2018-05-23T03:18:39.493943: step 195, loss 1.05567, acc 0.640625\n",
      "2018-05-23T03:18:39.819038: step 196, loss 0.886249, acc 0.515625\n",
      "2018-05-23T03:18:40.122240: step 197, loss 0.619299, acc 0.734375\n",
      "2018-05-23T03:18:40.437382: step 198, loss 0.792485, acc 0.625\n",
      "2018-05-23T03:18:40.749330: step 199, loss 0.818341, acc 0.625\n",
      "2018-05-23T03:18:41.047531: step 200, loss 0.925329, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:18:45.039852: step 200, loss 0.670016, acc 0.624804\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-200\n",
      "\n",
      "2018-05-23T03:18:46.030302: step 201, loss 0.887494, acc 0.578125\n",
      "2018-05-23T03:18:46.443197: step 202, loss 0.92274, acc 0.578125\n",
      "2018-05-23T03:18:46.749379: step 203, loss 1.08493, acc 0.578125\n",
      "2018-05-23T03:18:47.057554: step 204, loss 0.84338, acc 0.578125\n",
      "2018-05-23T03:18:47.362737: step 205, loss 0.887151, acc 0.546875\n",
      "2018-05-23T03:18:47.661937: step 206, loss 0.957217, acc 0.625\n",
      "2018-05-23T03:18:47.994049: step 207, loss 0.855549, acc 0.625\n",
      "2018-05-23T03:18:48.297238: step 208, loss 0.887004, acc 0.578125\n",
      "2018-05-23T03:18:48.602422: step 209, loss 0.778452, acc 0.671875\n",
      "2018-05-23T03:18:48.901623: step 210, loss 1.33102, acc 0.515625\n",
      "2018-05-23T03:18:49.213785: step 211, loss 0.866177, acc 0.640625\n",
      "2018-05-23T03:18:49.513983: step 212, loss 0.860167, acc 0.578125\n",
      "2018-05-23T03:18:49.843102: step 213, loss 0.930061, acc 0.609375\n",
      "2018-05-23T03:18:50.203140: step 214, loss 0.779149, acc 0.65625\n",
      "2018-05-23T03:18:50.560184: step 215, loss 0.733561, acc 0.671875\n",
      "2018-05-23T03:18:50.923215: step 216, loss 0.994374, acc 0.53125\n",
      "2018-05-23T03:18:51.341095: step 217, loss 0.887101, acc 0.609375\n",
      "2018-05-23T03:18:51.705121: step 218, loss 0.780011, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:18:52.066155: step 219, loss 0.92082, acc 0.5625\n",
      "2018-05-23T03:18:52.476058: step 220, loss 0.847682, acc 0.625\n",
      "2018-05-23T03:18:52.840085: step 221, loss 0.804633, acc 0.5625\n",
      "2018-05-23T03:18:53.205108: step 222, loss 0.872319, acc 0.53125\n",
      "2018-05-23T03:18:53.564151: step 223, loss 0.72386, acc 0.640625\n",
      "2018-05-23T03:18:53.926180: step 224, loss 0.609817, acc 0.640625\n",
      "2018-05-23T03:18:54.282227: step 225, loss 0.778719, acc 0.59375\n",
      "2018-05-23T03:18:54.651239: step 226, loss 0.611077, acc 0.65625\n",
      "2018-05-23T03:18:55.011277: step 227, loss 0.784278, acc 0.625\n",
      "2018-05-23T03:18:55.375303: step 228, loss 0.80617, acc 0.53125\n",
      "2018-05-23T03:18:55.731351: step 229, loss 0.880079, acc 0.53125\n",
      "2018-05-23T03:18:56.082445: step 230, loss 0.94188, acc 0.625\n",
      "2018-05-23T03:18:56.443904: step 231, loss 0.88872, acc 0.5625\n",
      "2018-05-23T03:18:56.814942: step 232, loss 0.949715, acc 0.578125\n",
      "2018-05-23T03:18:57.171956: step 233, loss 0.850037, acc 0.59375\n",
      "2018-05-23T03:18:57.529996: step 234, loss 0.926903, acc 0.515625\n",
      "2018-05-23T03:18:57.995752: step 235, loss 0.910895, acc 0.5625\n",
      "2018-05-23T03:18:58.346813: step 236, loss 0.922885, acc 0.5625\n",
      "2018-05-23T03:18:58.708844: step 237, loss 0.788176, acc 0.578125\n",
      "2018-05-23T03:18:59.063893: step 238, loss 0.851174, acc 0.578125\n",
      "2018-05-23T03:18:59.422932: step 239, loss 0.961328, acc 0.453125\n",
      "2018-05-23T03:18:59.797931: step 240, loss 0.718809, acc 0.65625\n",
      "2018-05-23T03:19:00.161956: step 241, loss 0.893537, acc 0.546875\n",
      "2018-05-23T03:19:00.521993: step 242, loss 0.81711, acc 0.609375\n",
      "2018-05-23T03:19:00.888014: step 243, loss 0.949158, acc 0.609375\n",
      "2018-05-23T03:19:01.243067: step 244, loss 0.93205, acc 0.59375\n",
      "2018-05-23T03:19:01.627036: step 245, loss 0.99684, acc 0.5625\n",
      "2018-05-23T03:19:01.983084: step 246, loss 0.830847, acc 0.640625\n",
      "2018-05-23T03:19:02.356086: step 247, loss 0.890062, acc 0.484375\n",
      "2018-05-23T03:19:02.725100: step 248, loss 0.826902, acc 0.609375\n",
      "2018-05-23T03:19:03.075164: step 249, loss 0.722679, acc 0.578125\n",
      "2018-05-23T03:19:03.423231: step 250, loss 0.606899, acc 0.734375\n",
      "2018-05-23T03:19:03.785262: step 251, loss 0.759847, acc 0.625\n",
      "2018-05-23T03:19:04.140313: step 252, loss 0.809134, acc 0.71875\n",
      "2018-05-23T03:19:04.495911: step 253, loss 0.800183, acc 0.609375\n",
      "2018-05-23T03:19:04.862930: step 254, loss 0.758908, acc 0.625\n",
      "2018-05-23T03:19:05.218977: step 255, loss 0.678206, acc 0.640625\n",
      "2018-05-23T03:19:05.576025: step 256, loss 0.952565, acc 0.59375\n",
      "2018-05-23T03:19:05.944038: step 257, loss 0.748664, acc 0.59375\n",
      "2018-05-23T03:19:06.296098: step 258, loss 1.03482, acc 0.5625\n",
      "2018-05-23T03:19:06.649155: step 259, loss 0.732777, acc 0.578125\n",
      "2018-05-23T03:19:07.013178: step 260, loss 0.753708, acc 0.609375\n",
      "2018-05-23T03:19:07.370223: step 261, loss 0.515589, acc 0.734375\n",
      "2018-05-23T03:19:07.731257: step 262, loss 0.831868, acc 0.5625\n",
      "2018-05-23T03:19:08.093288: step 263, loss 0.881406, acc 0.578125\n",
      "2018-05-23T03:19:08.445346: step 264, loss 0.734037, acc 0.640625\n",
      "2018-05-23T03:19:08.804389: step 265, loss 0.907028, acc 0.46875\n",
      "2018-05-23T03:19:09.159437: step 266, loss 0.830742, acc 0.53125\n",
      "2018-05-23T03:19:09.515484: step 267, loss 0.771197, acc 0.59375\n",
      "2018-05-23T03:19:09.875521: step 268, loss 0.881415, acc 0.59375\n",
      "2018-05-23T03:19:10.235557: step 269, loss 0.705702, acc 0.671875\n",
      "2018-05-23T03:19:10.600582: step 270, loss 0.689191, acc 0.65625\n",
      "2018-05-23T03:19:10.956628: step 271, loss 0.720878, acc 0.65625\n",
      "2018-05-23T03:19:11.308686: step 272, loss 0.71787, acc 0.625\n",
      "2018-05-23T03:19:11.653765: step 273, loss 0.965828, acc 0.53125\n",
      "2018-05-23T03:19:11.999837: step 274, loss 0.661621, acc 0.671875\n",
      "2018-05-23T03:19:12.345911: step 275, loss 0.713818, acc 0.640625\n",
      "2018-05-23T03:19:12.689991: step 276, loss 0.857569, acc 0.625\n",
      "2018-05-23T03:19:13.052025: step 277, loss 1.02372, acc 0.578125\n",
      "2018-05-23T03:19:13.451952: step 278, loss 0.851969, acc 0.578125\n",
      "2018-05-23T03:19:13.800023: step 279, loss 0.818119, acc 0.578125\n",
      "2018-05-23T03:19:14.145099: step 280, loss 0.760621, acc 0.625\n",
      "2018-05-23T03:19:14.499151: step 281, loss 0.708772, acc 0.59375\n",
      "2018-05-23T03:19:14.895092: step 282, loss 0.806795, acc 0.609375\n",
      "2018-05-23T03:19:15.258122: step 283, loss 0.95687, acc 0.546875\n",
      "2018-05-23T03:19:15.604198: step 284, loss 0.570728, acc 0.71875\n",
      "2018-05-23T03:19:15.964232: step 285, loss 0.793342, acc 0.609375\n",
      "2018-05-23T03:19:16.334247: step 286, loss 0.764909, acc 0.640625\n",
      "2018-05-23T03:19:16.734174: step 287, loss 0.904157, acc 0.59375\n",
      "2018-05-23T03:19:17.367478: step 288, loss 0.659832, acc 0.671875\n",
      "2018-05-23T03:19:18.081569: step 289, loss 0.652846, acc 0.6875\n",
      "2018-05-23T03:19:18.778703: step 290, loss 0.901568, acc 0.609375\n",
      "2018-05-23T03:19:19.249445: step 291, loss 0.64049, acc 0.71875\n",
      "2018-05-23T03:19:19.740132: step 292, loss 0.85696, acc 0.578125\n",
      "2018-05-23T03:19:20.100167: step 293, loss 0.68352, acc 0.6875\n",
      "2018-05-23T03:19:20.814258: step 294, loss 0.837646, acc 0.515625\n",
      "2018-05-23T03:19:21.354812: step 295, loss 1.03782, acc 0.484375\n",
      "2018-05-23T03:19:21.765712: step 296, loss 0.79028, acc 0.578125\n",
      "2018-05-23T03:19:22.124754: step 297, loss 0.894441, acc 0.578125\n",
      "2018-05-23T03:19:22.468832: step 298, loss 0.582428, acc 0.734375\n",
      "2018-05-23T03:19:22.885716: step 299, loss 0.762254, acc 0.546875\n",
      "2018-05-23T03:19:23.230793: step 300, loss 0.641812, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:19:28.648114: step 300, loss 0.630848, acc 0.647235\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-300\n",
      "\n",
      "2018-05-23T03:19:30.161866: step 301, loss 0.643599, acc 0.75\n",
      "2018-05-23T03:19:30.565072: step 302, loss 0.729522, acc 0.609375\n",
      "2018-05-23T03:19:30.986946: step 303, loss 0.690534, acc 0.59375\n",
      "2018-05-23T03:19:31.408815: step 304, loss 0.622039, acc 0.640625\n",
      "2018-05-23T03:19:31.771844: step 305, loss 0.768084, acc 0.640625\n",
      "2018-05-23T03:19:32.132878: step 306, loss 0.767848, acc 0.609375\n",
      "2018-05-23T03:19:32.495908: step 307, loss 0.682612, acc 0.703125\n",
      "2018-05-23T03:19:32.863901: step 308, loss 0.75425, acc 0.640625\n",
      "2018-05-23T03:19:33.263830: step 309, loss 0.638263, acc 0.671875\n",
      "2018-05-23T03:19:33.607908: step 310, loss 0.753621, acc 0.5625\n",
      "2018-05-23T03:19:33.961963: step 311, loss 0.762345, acc 0.609375\n",
      "2018-05-23T03:19:34.340949: step 312, loss 0.777966, acc 0.640625\n",
      "2018-05-23T03:19:34.793736: step 313, loss 0.656499, acc 0.640625\n",
      "2018-05-23T03:19:35.226580: step 314, loss 0.755201, acc 0.640625\n",
      "2018-05-23T03:19:35.580632: step 315, loss 0.798149, acc 0.578125\n",
      "2018-05-23T03:19:36.022452: step 316, loss 0.759717, acc 0.625\n",
      "2018-05-23T03:19:36.425373: step 317, loss 0.68654, acc 0.625\n",
      "2018-05-23T03:19:36.806354: step 318, loss 0.817687, acc 0.53125\n",
      "2018-05-23T03:19:37.184379: step 319, loss 0.792919, acc 0.640625\n",
      "2018-05-23T03:19:37.556346: step 320, loss 0.66532, acc 0.6875\n",
      "2018-05-23T03:19:37.898431: step 321, loss 0.716221, acc 0.59375\n",
      "2018-05-23T03:19:38.395104: step 322, loss 0.744435, acc 0.609375\n",
      "2018-05-23T03:19:38.910723: step 323, loss 0.836444, acc 0.53125\n",
      "2018-05-23T03:19:39.439309: step 324, loss 0.602754, acc 0.671875\n",
      "2018-05-23T03:19:39.897085: step 325, loss 0.774115, acc 0.59375\n",
      "2018-05-23T03:19:40.460578: step 326, loss 0.737578, acc 0.640625\n",
      "2018-05-23T03:19:41.055985: step 327, loss 0.785953, acc 0.625\n",
      "2018-05-23T03:19:41.673333: step 328, loss 0.814032, acc 0.5\n",
      "2018-05-23T03:19:42.191945: step 329, loss 0.586053, acc 0.703125\n",
      "2018-05-23T03:19:42.639748: step 330, loss 0.744072, acc 0.609375\n",
      "2018-05-23T03:19:43.290008: step 331, loss 0.700171, acc 0.640625\n",
      "2018-05-23T03:19:43.880428: step 332, loss 0.678758, acc 0.65625\n",
      "2018-05-23T03:19:44.424972: step 333, loss 0.70623, acc 0.640625\n",
      "2018-05-23T03:19:44.928626: step 334, loss 0.73146, acc 0.6875\n",
      "2018-05-23T03:19:45.308608: step 335, loss 0.616474, acc 0.703125\n",
      "2018-05-23T03:19:45.662662: step 336, loss 0.586158, acc 0.71875\n",
      "2018-05-23T03:19:46.016714: step 337, loss 0.825579, acc 0.484375\n",
      "2018-05-23T03:19:46.354808: step 338, loss 0.789582, acc 0.484375\n",
      "2018-05-23T03:19:46.695897: step 339, loss 0.652645, acc 0.6875\n",
      "2018-05-23T03:19:47.049949: step 340, loss 0.704694, acc 0.65625\n",
      "2018-05-23T03:19:47.395026: step 341, loss 0.670669, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:19:47.743094: step 342, loss 0.802172, acc 0.59375\n",
      "2018-05-23T03:19:48.090166: step 343, loss 0.787828, acc 0.625\n",
      "2018-05-23T03:19:48.428262: step 344, loss 0.675425, acc 0.65625\n",
      "2018-05-23T03:19:48.765364: step 345, loss 0.805906, acc 0.59375\n",
      "2018-05-23T03:19:49.115424: step 346, loss 0.732399, acc 0.625\n",
      "2018-05-23T03:19:49.454517: step 347, loss 0.617774, acc 0.640625\n",
      "2018-05-23T03:19:49.792487: step 348, loss 0.529488, acc 0.71875\n",
      "2018-05-23T03:19:50.137942: step 349, loss 0.720834, acc 0.65625\n",
      "2018-05-23T03:19:50.478033: step 350, loss 0.831136, acc 0.578125\n",
      "2018-05-23T03:19:50.818123: step 351, loss 0.612047, acc 0.625\n",
      "2018-05-23T03:19:51.170181: step 352, loss 0.596891, acc 0.671875\n",
      "2018-05-23T03:19:51.509275: step 353, loss 0.793701, acc 0.546875\n",
      "2018-05-23T03:19:51.844614: step 354, loss 0.646373, acc 0.6875\n",
      "2018-05-23T03:19:52.195672: step 355, loss 0.75662, acc 0.578125\n",
      "2018-05-23T03:19:52.529778: step 356, loss 0.610515, acc 0.703125\n",
      "2018-05-23T03:19:52.871863: step 357, loss 0.797029, acc 0.59375\n",
      "2018-05-23T03:19:53.225321: step 358, loss 0.625398, acc 0.671875\n",
      "2018-05-23T03:19:53.562417: step 359, loss 0.573456, acc 0.6875\n",
      "2018-05-23T03:19:53.904504: step 360, loss 0.643207, acc 0.625\n",
      "2018-05-23T03:19:54.245594: step 361, loss 0.654026, acc 0.609375\n",
      "2018-05-23T03:19:54.622581: step 362, loss 0.633412, acc 0.640625\n",
      "2018-05-23T03:19:55.008548: step 363, loss 0.824888, acc 0.515625\n",
      "2018-05-23T03:19:55.444382: step 364, loss 0.798183, acc 0.5625\n",
      "2018-05-23T03:19:55.856282: step 365, loss 0.891004, acc 0.546875\n",
      "2018-05-23T03:19:56.241252: step 366, loss 0.767172, acc 0.546875\n",
      "2018-05-23T03:19:56.627218: step 367, loss 0.686653, acc 0.59375\n",
      "2018-05-23T03:19:57.047096: step 368, loss 0.589713, acc 0.671875\n",
      "2018-05-23T03:19:57.446028: step 369, loss 0.791342, acc 0.5625\n",
      "2018-05-23T03:19:57.830998: step 370, loss 0.71226, acc 0.640625\n",
      "2018-05-23T03:19:58.377540: step 371, loss 0.84015, acc 0.515625\n",
      "2018-05-23T03:19:59.014832: step 372, loss 0.538094, acc 0.671875\n",
      "2018-05-23T03:19:59.516489: step 373, loss 0.586516, acc 0.734375\n",
      "2018-05-23T03:20:00.010168: step 374, loss 0.758416, acc 0.546875\n",
      "2018-05-23T03:20:00.503849: step 375, loss 0.769287, acc 0.640625\n",
      "2018-05-23T03:20:01.434358: step 376, loss 0.669419, acc 0.578125\n",
      "2018-05-23T03:20:02.625982: step 377, loss 0.682878, acc 0.6875\n",
      "2018-05-23T03:20:03.268298: step 378, loss 0.623975, acc 0.65625\n",
      "2018-05-23T03:20:04.339072: step 379, loss 0.857194, acc 0.578125\n",
      "2018-05-23T03:20:04.829760: step 380, loss 0.676101, acc 0.59375\n",
      "2018-05-23T03:20:05.377295: step 381, loss 0.839666, acc 0.515625\n",
      "2018-05-23T03:20:05.781214: step 382, loss 0.746061, acc 0.546875\n",
      "2018-05-23T03:20:06.146238: step 383, loss 0.681584, acc 0.5625\n",
      "2018-05-23T03:20:06.516246: step 384, loss 0.578474, acc 0.703125\n",
      "2018-05-23T03:20:06.982999: step 385, loss 0.651061, acc 0.671875\n",
      "2018-05-23T03:20:07.837375: step 386, loss 0.698999, acc 0.5625\n",
      "2018-05-23T03:20:08.370948: step 387, loss 0.655696, acc 0.625\n",
      "2018-05-23T03:20:08.926460: step 388, loss 0.780241, acc 0.546875\n",
      "2018-05-23T03:20:09.374262: step 389, loss 0.629774, acc 0.640625\n",
      "2018-05-23T03:20:09.878915: step 390, loss 0.799094, acc 0.578125\n",
      "2018-05-23T03:20:10.288817: step 391, loss 0.741528, acc 0.546875\n",
      "2018-05-23T03:20:10.839342: step 392, loss 0.689202, acc 0.640625\n",
      "2018-05-23T03:20:11.230301: step 393, loss 0.755426, acc 0.59375\n",
      "2018-05-23T03:20:11.870266: step 394, loss 0.720266, acc 0.59375\n",
      "2018-05-23T03:20:12.350979: step 395, loss 0.693298, acc 0.640625\n",
      "2018-05-23T03:20:12.770856: step 396, loss 0.562223, acc 0.71875\n",
      "2018-05-23T03:20:13.389203: step 397, loss 0.651049, acc 0.59375\n",
      "2018-05-23T03:20:13.776167: step 398, loss 0.69427, acc 0.609375\n",
      "2018-05-23T03:20:14.141190: step 399, loss 0.660282, acc 0.671875\n",
      "2018-05-23T03:20:14.518182: step 400, loss 0.865677, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:20:19.890896: step 400, loss 0.600041, acc 0.662666\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-400\n",
      "\n",
      "2018-05-23T03:20:21.181987: step 401, loss 0.643576, acc 0.609375\n",
      "2018-05-23T03:20:21.631816: step 402, loss 0.595285, acc 0.703125\n",
      "2018-05-23T03:20:21.997839: step 403, loss 0.716405, acc 0.609375\n",
      "2018-05-23T03:20:22.423697: step 404, loss 0.688734, acc 0.640625\n",
      "2018-05-23T03:20:22.990184: step 405, loss 0.733602, acc 0.65625\n",
      "2018-05-23T03:20:23.448956: step 406, loss 0.715434, acc 0.5625\n",
      "2018-05-23T03:20:23.830932: step 407, loss 0.744886, acc 0.515625\n",
      "2018-05-23T03:20:24.193965: step 408, loss 0.708006, acc 0.546875\n",
      "2018-05-23T03:20:24.732520: step 409, loss 0.576651, acc 0.703125\n",
      "2018-05-23T03:20:25.136440: step 410, loss 0.487617, acc 0.765625\n",
      "2018-05-23T03:20:25.636611: step 411, loss 0.563879, acc 0.671875\n",
      "2018-05-23T03:20:26.080423: step 412, loss 0.642297, acc 0.609375\n",
      "2018-05-23T03:20:26.566136: step 413, loss 0.577726, acc 0.671875\n",
      "2018-05-23T03:20:27.078940: step 414, loss 0.662503, acc 0.6875\n",
      "2018-05-23T03:20:27.515774: step 415, loss 0.599098, acc 0.671875\n",
      "2018-05-23T03:20:27.992496: step 416, loss 0.560523, acc 0.6875\n",
      "2018-05-23T03:20:28.363503: step 417, loss 0.687455, acc 0.640625\n",
      "2018-05-23T03:20:28.730524: step 418, loss 0.710153, acc 0.53125\n",
      "2018-05-23T03:20:29.214228: step 419, loss 0.712784, acc 0.546875\n",
      "2018-05-23T03:20:29.597235: step 420, loss 0.722656, acc 0.640625\n",
      "2018-05-23T03:20:30.064981: step 421, loss 0.734611, acc 0.578125\n",
      "2018-05-23T03:20:30.521873: step 422, loss 0.70866, acc 0.65625\n",
      "2018-05-23T03:20:30.971931: step 423, loss 0.647094, acc 0.671875\n",
      "2018-05-23T03:20:31.540908: step 424, loss 0.827109, acc 0.5\n",
      "2018-05-23T03:20:31.943828: step 425, loss 0.807213, acc 0.5\n",
      "2018-05-23T03:20:32.539788: step 426, loss 0.611122, acc 0.640625\n",
      "2018-05-23T03:20:33.153655: step 427, loss 0.614486, acc 0.703125\n",
      "2018-05-23T03:20:33.722143: step 428, loss 0.826999, acc 0.609375\n",
      "2018-05-23T03:20:34.357352: step 429, loss 0.698364, acc 0.671875\n",
      "2018-05-23T03:20:35.160302: step 430, loss 0.673315, acc 0.515625\n",
      "2018-05-23T03:20:35.648034: step 431, loss 0.615117, acc 0.671875\n",
      "2018-05-23T03:20:36.032008: step 432, loss 0.550947, acc 0.625\n",
      "2018-05-23T03:20:36.409000: step 433, loss 0.687249, acc 0.5625\n",
      "2018-05-23T03:20:36.801949: step 434, loss 0.718685, acc 0.609375\n",
      "2018-05-23T03:20:37.285085: step 435, loss 0.72304, acc 0.625\n",
      "2018-05-23T03:20:37.669057: step 436, loss 0.714238, acc 0.609375\n",
      "2018-05-23T03:20:38.061013: step 437, loss 0.67318, acc 0.65625\n",
      "2018-05-23T03:20:38.486869: step 438, loss 0.798301, acc 0.53125\n",
      "2018-05-23T03:20:38.860871: step 439, loss 0.758201, acc 0.53125\n",
      "2018-05-23T03:20:39.284735: step 440, loss 0.715524, acc 0.640625\n",
      "2018-05-23T03:20:39.662725: step 441, loss 0.654879, acc 0.640625\n",
      "2018-05-23T03:20:40.130510: step 442, loss 0.730365, acc 0.578125\n",
      "2018-05-23T03:20:40.692009: step 443, loss 0.680238, acc 0.59375\n",
      "2018-05-23T03:20:41.755468: step 444, loss 0.732419, acc 0.65625\n",
      "2018-05-23T03:20:42.192300: step 445, loss 0.539893, acc 0.734375\n",
      "2018-05-23T03:20:42.580261: step 446, loss 0.686936, acc 0.609375\n",
      "2018-05-23T03:20:42.989168: step 447, loss 0.654952, acc 0.6875\n",
      "2018-05-23T03:20:43.435974: step 448, loss 0.666793, acc 0.53125\n",
      "2018-05-23T03:20:43.806622: step 449, loss 0.584251, acc 0.734375\n",
      "2018-05-23T03:20:44.203558: step 450, loss 0.720697, acc 0.578125\n",
      "2018-05-23T03:20:44.571574: step 451, loss 0.732411, acc 0.5\n",
      "2018-05-23T03:20:45.004416: step 452, loss 0.632249, acc 0.625\n",
      "2018-05-23T03:20:45.437258: step 453, loss 0.565958, acc 0.734375\n",
      "2018-05-23T03:20:45.801287: step 454, loss 0.671394, acc 0.578125\n",
      "2018-05-23T03:20:46.158330: step 455, loss 0.630374, acc 0.671875\n",
      "2018-05-23T03:20:46.517370: step 456, loss 0.736652, acc 0.625\n",
      "2018-05-23T03:20:46.886384: step 457, loss 0.655563, acc 0.65625\n",
      "2018-05-23T03:20:47.292297: step 458, loss 0.733147, acc 0.53125\n",
      "2018-05-23T03:20:47.655326: step 459, loss 0.707533, acc 0.625\n",
      "2018-05-23T03:20:48.025335: step 460, loss 0.622631, acc 0.671875\n",
      "2018-05-23T03:20:48.946438: step 461, loss 0.735307, acc 0.625\n",
      "2018-05-23T03:20:49.357339: step 462, loss 0.541944, acc 0.703125\n",
      "2018-05-23T03:20:49.747296: step 463, loss 0.543298, acc 0.703125\n",
      "2018-05-23T03:20:50.204074: step 464, loss 0.611044, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:20:50.578072: step 465, loss 0.594059, acc 0.71875\n",
      "2018-05-23T03:20:50.986981: step 466, loss 0.666206, acc 0.640625\n",
      "2018-05-23T03:20:51.353998: step 467, loss 0.612662, acc 0.65625\n",
      "2018-05-23T03:20:51.796812: step 468, loss 0.554856, acc 0.71875\n",
      "2018-05-23T03:20:52.177795: step 469, loss 0.67261, acc 0.609375\n",
      "2018-05-23T03:20:52.861996: step 470, loss 0.830802, acc 0.546875\n",
      "2018-05-23T03:20:53.402060: step 471, loss 0.683199, acc 0.6875\n",
      "2018-05-23T03:20:53.830915: step 472, loss 0.660532, acc 0.625\n",
      "2018-05-23T03:20:54.498151: step 473, loss 0.592731, acc 0.6875\n",
      "2018-05-23T03:20:55.178331: step 474, loss 0.560212, acc 0.703125\n",
      "2018-05-23T03:20:55.782739: step 475, loss 0.748338, acc 0.5625\n",
      "2018-05-23T03:20:56.597070: step 476, loss 0.689716, acc 0.59375\n",
      "2018-05-23T03:20:56.990020: step 477, loss 0.641317, acc 0.609375\n",
      "2018-05-23T03:20:57.370002: step 478, loss 0.572192, acc 0.640625\n",
      "2018-05-23T03:20:57.782899: step 479, loss 0.589117, acc 0.640625\n",
      "2018-05-23T03:20:58.415206: step 480, loss 0.746753, acc 0.609375\n",
      "2018-05-23T03:20:58.835085: step 481, loss 0.573337, acc 0.6875\n",
      "2018-05-23T03:20:59.274907: step 482, loss 0.543284, acc 0.734375\n",
      "2018-05-23T03:20:59.710740: step 483, loss 0.751965, acc 0.640625\n",
      "2018-05-23T03:21:00.146574: step 484, loss 0.646114, acc 0.65625\n",
      "2018-05-23T03:21:00.567450: step 485, loss 0.623153, acc 0.59375\n",
      "2018-05-23T03:21:01.033203: step 486, loss 0.706183, acc 0.65625\n",
      "2018-05-23T03:21:01.502946: step 487, loss 0.616829, acc 0.578125\n",
      "2018-05-23T03:21:01.973696: step 488, loss 0.723644, acc 0.609375\n",
      "2018-05-23T03:21:02.515243: step 489, loss 0.714952, acc 0.59375\n",
      "2018-05-23T03:21:03.030859: step 490, loss 0.567755, acc 0.703125\n",
      "2018-05-23T03:21:03.466693: step 491, loss 0.645435, acc 0.65625\n",
      "2018-05-23T03:21:03.852662: step 492, loss 0.693896, acc 0.59375\n",
      "2018-05-23T03:21:04.260569: step 493, loss 0.656159, acc 0.609375\n",
      "2018-05-23T03:21:04.700393: step 494, loss 0.673687, acc 0.625\n",
      "2018-05-23T03:21:05.196068: step 495, loss 0.717409, acc 0.5625\n",
      "2018-05-23T03:21:05.671795: step 496, loss 0.623249, acc 0.671875\n",
      "2018-05-23T03:21:06.093666: step 497, loss 0.61181, acc 0.734375\n",
      "2018-05-23T03:21:06.526540: step 498, loss 0.683094, acc 0.671875\n",
      "2018-05-23T03:21:06.907522: step 499, loss 0.578747, acc 0.65625\n",
      "2018-05-23T03:21:07.279524: step 500, loss 0.733749, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:21:12.692047: step 500, loss 0.613385, acc 0.648093\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-500\n",
      "\n",
      "2018-05-23T03:21:14.322685: step 501, loss 0.625003, acc 0.6875\n",
      "2018-05-23T03:21:14.749542: step 502, loss 0.724419, acc 0.59375\n",
      "2018-05-23T03:21:15.126534: step 503, loss 0.74117, acc 0.625\n",
      "2018-05-23T03:21:15.489562: step 504, loss 0.623375, acc 0.578125\n",
      "2018-05-23T03:21:15.864559: step 505, loss 0.621561, acc 0.65625\n",
      "2018-05-23T03:21:16.224597: step 506, loss 0.614402, acc 0.6875\n",
      "2018-05-23T03:21:16.624528: step 507, loss 0.618609, acc 0.703125\n",
      "2018-05-23T03:21:16.997530: step 508, loss 0.735061, acc 0.578125\n",
      "2018-05-23T03:21:17.377512: step 509, loss 0.776271, acc 0.53125\n",
      "2018-05-23T03:21:17.739544: step 510, loss 0.597691, acc 0.6875\n",
      "2018-05-23T03:21:18.135484: step 511, loss 0.600415, acc 0.6875\n",
      "2018-05-23T03:21:18.539404: step 512, loss 0.606652, acc 0.6875\n",
      "2018-05-23T03:21:18.899440: step 513, loss 0.624346, acc 0.6875\n",
      "2018-05-23T03:21:19.263467: step 514, loss 0.63703, acc 0.671875\n",
      "2018-05-23T03:21:19.626496: step 515, loss 0.632736, acc 0.640625\n",
      "2018-05-23T03:21:19.996504: step 516, loss 0.617577, acc 0.6875\n",
      "2018-05-23T03:21:20.370505: step 517, loss 0.729731, acc 0.59375\n",
      "2018-05-23T03:21:20.737564: step 518, loss 0.631432, acc 0.640625\n",
      "2018-05-23T03:21:21.096564: step 519, loss 0.686544, acc 0.640625\n",
      "2018-05-23T03:21:21.457595: step 520, loss 0.652132, acc 0.640625\n",
      "2018-05-23T03:21:21.851052: step 521, loss 0.597517, acc 0.671875\n",
      "2018-05-23T03:21:22.243006: step 522, loss 0.613189, acc 0.65625\n",
      "2018-05-23T03:21:22.643931: step 523, loss 0.728317, acc 0.53125\n",
      "2018-05-23T03:21:23.011946: step 524, loss 0.644942, acc 0.703125\n",
      "2018-05-23T03:21:23.373977: step 525, loss 0.492437, acc 0.75\n",
      "2018-05-23T03:21:23.738004: step 526, loss 0.626455, acc 0.625\n",
      "2018-05-23T03:21:24.097045: step 527, loss 0.662852, acc 0.609375\n",
      "2018-05-23T03:21:24.493981: step 528, loss 0.669976, acc 0.625\n",
      "2018-05-23T03:21:24.865987: step 529, loss 0.643017, acc 0.671875\n",
      "2018-05-23T03:21:25.245970: step 530, loss 0.617022, acc 0.6875\n",
      "2018-05-23T03:21:25.599027: step 531, loss 0.662875, acc 0.59375\n",
      "2018-05-23T03:21:25.973024: step 532, loss 0.737293, acc 0.578125\n",
      "2018-05-23T03:21:26.478673: step 533, loss 0.622853, acc 0.65625\n",
      "2018-05-23T03:21:26.916500: step 534, loss 0.715931, acc 0.53125\n",
      "2018-05-23T03:21:27.304464: step 535, loss 0.644954, acc 0.671875\n",
      "2018-05-23T03:21:27.822079: step 536, loss 0.689063, acc 0.59375\n",
      "2018-05-23T03:21:28.251931: step 537, loss 0.603733, acc 0.671875\n",
      "2018-05-23T03:21:28.652855: step 538, loss 0.608644, acc 0.71875\n",
      "2018-05-23T03:21:29.023864: step 539, loss 0.718595, acc 0.625\n",
      "2018-05-23T03:21:29.393873: step 540, loss 0.660533, acc 0.59375\n",
      "2018-05-23T03:21:29.755907: step 541, loss 0.714742, acc 0.625\n",
      "2018-05-23T03:21:30.136889: step 542, loss 0.596723, acc 0.703125\n",
      "2018-05-23T03:21:30.526842: step 543, loss 0.565104, acc 0.765625\n",
      "2018-05-23T03:21:30.899846: step 544, loss 0.652101, acc 0.703125\n",
      "2018-05-23T03:21:31.271853: step 545, loss 0.672447, acc 0.640625\n",
      "2018-05-23T03:21:31.652830: step 546, loss 0.599434, acc 0.6875\n",
      "2018-05-23T03:21:32.012869: step 547, loss 0.545176, acc 0.671875\n",
      "2018-05-23T03:21:32.417785: step 548, loss 0.680978, acc 0.625\n",
      "2018-05-23T03:21:32.782807: step 549, loss 0.548689, acc 0.75\n",
      "2018-05-23T03:21:33.145837: step 550, loss 0.799515, acc 0.5625\n",
      "2018-05-23T03:21:33.514849: step 551, loss 0.645542, acc 0.609375\n",
      "2018-05-23T03:21:33.888849: step 552, loss 0.587169, acc 0.671875\n",
      "2018-05-23T03:21:34.243898: step 553, loss 0.619933, acc 0.640625\n",
      "2018-05-23T03:21:34.658790: step 554, loss 0.7182, acc 0.484375\n",
      "2018-05-23T03:21:35.103600: step 555, loss 0.530218, acc 0.734375\n",
      "2018-05-23T03:21:35.487572: step 556, loss 0.550278, acc 0.71875\n",
      "2018-05-23T03:21:36.019149: step 557, loss 0.545428, acc 0.71875\n",
      "2018-05-23T03:21:36.441020: step 558, loss 0.569393, acc 0.65625\n",
      "2018-05-23T03:21:36.822003: step 559, loss 0.654814, acc 0.640625\n",
      "2018-05-23T03:21:37.201986: step 560, loss 0.747603, acc 0.5625\n",
      "2018-05-23T03:21:37.596930: step 561, loss 0.590446, acc 0.703125\n",
      "2018-05-23T03:21:37.994865: step 562, loss 0.665467, acc 0.578125\n",
      "2018-05-23T03:21:38.375845: step 563, loss 0.851592, acc 0.46875\n",
      "2018-05-23T03:21:38.751840: step 564, loss 0.6696, acc 0.59375\n",
      "2018-05-23T03:21:39.124843: step 565, loss 0.638825, acc 0.71875\n",
      "2018-05-23T03:21:39.497843: step 566, loss 0.640101, acc 0.609375\n",
      "2018-05-23T03:21:39.870880: step 567, loss 0.645174, acc 0.578125\n",
      "2018-05-23T03:21:40.229885: step 568, loss 0.616469, acc 0.625\n",
      "2018-05-23T03:21:40.629815: step 569, loss 0.452803, acc 0.78125\n",
      "2018-05-23T03:21:40.987900: step 570, loss 0.75941, acc 0.59375\n",
      "2018-05-23T03:21:41.345941: step 571, loss 0.679398, acc 0.609375\n",
      "2018-05-23T03:21:41.717949: step 572, loss 0.71877, acc 0.59375\n",
      "2018-05-23T03:21:42.082969: step 573, loss 0.690253, acc 0.546875\n",
      "2018-05-23T03:21:42.459959: step 574, loss 0.623617, acc 0.578125\n",
      "2018-05-23T03:21:42.825980: step 575, loss 0.648258, acc 0.71875\n",
      "2018-05-23T03:21:43.227938: step 576, loss 0.617575, acc 0.640625\n",
      "2018-05-23T03:21:43.596917: step 577, loss 0.645066, acc 0.65625\n",
      "2018-05-23T03:21:43.977898: step 578, loss 0.60003, acc 0.625\n",
      "2018-05-23T03:21:44.340928: step 579, loss 0.647398, acc 0.65625\n",
      "2018-05-23T03:21:44.737865: step 580, loss 0.677991, acc 0.578125\n",
      "2018-05-23T03:21:45.127824: step 581, loss 0.707071, acc 0.625\n",
      "2018-05-23T03:21:45.499828: step 582, loss 0.610355, acc 0.640625\n",
      "2018-05-23T03:21:45.870836: step 583, loss 0.69633, acc 0.609375\n",
      "2018-05-23T03:21:46.236856: step 584, loss 0.697116, acc 0.6875\n",
      "2018-05-23T03:21:46.642772: step 585, loss 0.700777, acc 0.65625\n",
      "2018-05-23T03:21:47.014774: step 586, loss 0.720868, acc 0.5625\n",
      "2018-05-23T03:21:47.397749: step 587, loss 0.645326, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:21:47.777795: step 588, loss 0.690583, acc 0.59375\n",
      "2018-05-23T03:21:48.152760: step 589, loss 0.587816, acc 0.703125\n",
      "2018-05-23T03:21:48.623498: step 590, loss 0.684714, acc 0.59375\n",
      "2018-05-23T03:21:49.025424: step 591, loss 0.669406, acc 0.5625\n",
      "2018-05-23T03:21:49.409349: step 592, loss 0.60584, acc 0.703125\n",
      "2018-05-23T03:21:49.797311: step 593, loss 0.66496, acc 0.625\n",
      "2018-05-23T03:21:50.175301: step 594, loss 0.556971, acc 0.671875\n",
      "2018-05-23T03:21:50.577224: step 595, loss 0.542582, acc 0.75\n",
      "2018-05-23T03:21:50.972168: step 596, loss 0.606733, acc 0.65625\n",
      "2018-05-23T03:21:51.347164: step 597, loss 0.545998, acc 0.71875\n",
      "2018-05-23T03:21:51.712190: step 598, loss 0.678318, acc 0.625\n",
      "2018-05-23T03:21:52.072227: step 599, loss 0.62171, acc 0.609375\n",
      "2018-05-23T03:21:52.464176: step 600, loss 0.690419, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:21:57.558167: step 600, loss 0.595377, acc 0.667524\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-600\n",
      "\n",
      "2018-05-23T03:21:59.064475: step 601, loss 0.625245, acc 0.59375\n",
      "2018-05-23T03:21:59.452436: step 602, loss 0.593136, acc 0.671875\n",
      "2018-05-23T03:21:59.845385: step 603, loss 0.642786, acc 0.671875\n",
      "2018-05-23T03:22:00.229356: step 604, loss 0.723384, acc 0.546875\n",
      "2018-05-23T03:22:00.652226: step 605, loss 0.686403, acc 0.640625\n",
      "2018-05-23T03:22:01.027223: step 606, loss 0.627795, acc 0.671875\n",
      "2018-05-23T03:22:01.426156: step 607, loss 0.574227, acc 0.6875\n",
      "2018-05-23T03:22:01.823094: step 608, loss 0.541582, acc 0.6875\n",
      "2018-05-23T03:22:02.211056: step 609, loss 0.598767, acc 0.6875\n",
      "2018-05-23T03:22:02.648884: step 610, loss 0.598609, acc 0.640625\n",
      "2018-05-23T03:22:03.010918: step 611, loss 0.600884, acc 0.6875\n",
      "2018-05-23T03:22:03.376936: step 612, loss 0.629637, acc 0.65625\n",
      "2018-05-23T03:22:03.756920: step 613, loss 0.619192, acc 0.671875\n",
      "2018-05-23T03:22:04.123950: step 614, loss 0.667512, acc 0.640625\n",
      "2018-05-23T03:22:04.516887: step 615, loss 0.717833, acc 0.609375\n",
      "2018-05-23T03:22:04.905848: step 616, loss 0.526841, acc 0.734375\n",
      "2018-05-23T03:22:05.280847: step 617, loss 0.634588, acc 0.625\n",
      "2018-05-23T03:22:05.648858: step 618, loss 0.555976, acc 0.75\n",
      "2018-05-23T03:22:06.015877: step 619, loss 0.644765, acc 0.640625\n",
      "2018-05-23T03:22:06.374916: step 620, loss 0.652434, acc 0.609375\n",
      "2018-05-23T03:22:06.787814: step 621, loss 0.699132, acc 0.5625\n",
      "2018-05-23T03:22:07.183752: step 622, loss 0.680453, acc 0.578125\n",
      "2018-05-23T03:22:07.541794: step 623, loss 0.699248, acc 0.625\n",
      "2018-05-23T03:22:07.920780: step 624, loss 0.576257, acc 0.703125\n",
      "2018-05-23T03:22:08.276828: step 625, loss 0.60578, acc 0.75\n",
      "2018-05-23T03:22:08.684738: step 626, loss 0.749631, acc 0.578125\n",
      "2018-05-23T03:22:09.065719: step 627, loss 0.667646, acc 0.578125\n",
      "2018-05-23T03:22:09.428778: step 628, loss 0.664668, acc 0.578125\n",
      "2018-05-23T03:22:09.791810: step 629, loss 0.624363, acc 0.625\n",
      "2018-05-23T03:22:10.168799: step 630, loss 0.701882, acc 0.625\n",
      "2018-05-23T03:22:10.568729: step 631, loss 0.666135, acc 0.65625\n",
      "2018-05-23T03:22:10.937743: step 632, loss 0.618611, acc 0.6875\n",
      "2018-05-23T03:22:11.310745: step 633, loss 0.634665, acc 0.6875\n",
      "2018-05-23T03:22:11.671778: step 634, loss 0.618806, acc 0.640625\n",
      "2018-05-23T03:22:12.084674: step 635, loss 0.599646, acc 0.671875\n",
      "2018-05-23T03:22:12.568380: step 636, loss 0.734561, acc 0.546875\n",
      "2018-05-23T03:22:13.481972: step 637, loss 0.643976, acc 0.671875\n",
      "2018-05-23T03:22:14.239621: step 638, loss 0.708491, acc 0.609375\n",
      "2018-05-23T03:22:15.044467: step 639, loss 0.651785, acc 0.734375\n",
      "2018-05-23T03:22:15.862174: step 640, loss 0.703144, acc 0.578125\n",
      "2018-05-23T03:22:16.441623: step 641, loss 0.61485, acc 0.609375\n",
      "2018-05-23T03:22:17.107840: step 642, loss 0.645223, acc 0.640625\n",
      "2018-05-23T03:22:17.791547: step 643, loss 0.561474, acc 0.71875\n",
      "2018-05-23T03:22:18.485693: step 644, loss 0.647146, acc 0.65625\n",
      "2018-05-23T03:22:19.098053: step 645, loss 0.632563, acc 0.640625\n",
      "2018-05-23T03:22:19.770256: step 646, loss 0.56428, acc 0.703125\n",
      "2018-05-23T03:22:20.245982: step 647, loss 0.636775, acc 0.609375\n",
      "2018-05-23T03:22:20.745645: step 648, loss 0.664186, acc 0.65625\n",
      "2018-05-23T03:22:21.162530: step 649, loss 0.62704, acc 0.703125\n",
      "2018-05-23T03:22:21.574429: step 650, loss 0.642821, acc 0.609375\n",
      "2018-05-23T03:22:22.052150: step 651, loss 0.690018, acc 0.640625\n",
      "2018-05-23T03:22:22.453079: step 652, loss 0.607739, acc 0.65625\n",
      "2018-05-23T03:22:22.911849: step 653, loss 0.607352, acc 0.625\n",
      "2018-05-23T03:22:23.309786: step 654, loss 0.614371, acc 0.65625\n",
      "2018-05-23T03:22:23.713706: step 655, loss 0.730569, acc 0.578125\n",
      "2018-05-23T03:22:24.122611: step 656, loss 0.567884, acc 0.734375\n",
      "2018-05-23T03:22:24.536504: step 657, loss 0.630978, acc 0.640625\n",
      "2018-05-23T03:22:24.970344: step 658, loss 0.615767, acc 0.625\n",
      "2018-05-23T03:22:25.420140: step 659, loss 0.595169, acc 0.625\n",
      "2018-05-23T03:22:25.830044: step 660, loss 0.693882, acc 0.625\n",
      "2018-05-23T03:22:26.220998: step 661, loss 0.542096, acc 0.609375\n",
      "2018-05-23T03:22:26.673786: step 662, loss 0.57811, acc 0.65625\n",
      "2018-05-23T03:22:27.034821: step 663, loss 0.605273, acc 0.640625\n",
      "2018-05-23T03:22:27.390867: step 664, loss 0.555367, acc 0.6875\n",
      "2018-05-23T03:22:27.737978: step 665, loss 0.637844, acc 0.640625\n",
      "2018-05-23T03:22:28.085010: step 666, loss 0.613487, acc 0.609375\n",
      "2018-05-23T03:22:28.432115: step 667, loss 0.650255, acc 0.640625\n",
      "2018-05-23T03:22:28.794114: step 668, loss 0.60282, acc 0.703125\n",
      "2018-05-23T03:22:29.125227: step 669, loss 0.593702, acc 0.75\n",
      "2018-05-23T03:22:29.484267: step 670, loss 0.683296, acc 0.578125\n",
      "2018-05-23T03:22:29.879211: step 671, loss 0.631392, acc 0.640625\n",
      "2018-05-23T03:22:30.266174: step 672, loss 0.615966, acc 0.625\n",
      "2018-05-23T03:22:30.618233: step 673, loss 0.616148, acc 0.703125\n",
      "2018-05-23T03:22:30.954334: step 674, loss 0.624915, acc 0.625\n",
      "2018-05-23T03:22:31.301405: step 675, loss 0.69996, acc 0.515625\n",
      "2018-05-23T03:22:31.653463: step 676, loss 0.66525, acc 0.59375\n",
      "2018-05-23T03:22:31.988567: step 677, loss 0.509884, acc 0.75\n",
      "2018-05-23T03:22:32.337633: step 678, loss 0.695141, acc 0.578125\n",
      "2018-05-23T03:22:32.692684: step 679, loss 0.597182, acc 0.703125\n",
      "2018-05-23T03:22:33.049728: step 680, loss 0.600552, acc 0.671875\n",
      "2018-05-23T03:22:33.390818: step 681, loss 0.583437, acc 0.71875\n",
      "2018-05-23T03:22:33.736890: step 682, loss 0.541076, acc 0.6875\n",
      "2018-05-23T03:22:34.082968: step 683, loss 0.749004, acc 0.5625\n",
      "2018-05-23T03:22:34.429040: step 684, loss 0.727199, acc 0.578125\n",
      "2018-05-23T03:22:34.777109: step 685, loss 0.590028, acc 0.671875\n",
      "2018-05-23T03:22:35.123184: step 686, loss 0.564513, acc 0.765625\n",
      "2018-05-23T03:22:35.468258: step 687, loss 0.57159, acc 0.734375\n",
      "2018-05-23T03:22:35.828296: step 688, loss 0.673488, acc 0.625\n",
      "2018-05-23T03:22:36.180353: step 689, loss 0.70065, acc 0.5625\n",
      "2018-05-23T03:22:36.533409: step 690, loss 0.611622, acc 0.703125\n",
      "2018-05-23T03:22:36.877489: step 691, loss 0.556982, acc 0.734375\n",
      "2018-05-23T03:22:37.314320: step 692, loss 0.626738, acc 0.671875\n",
      "2018-05-23T03:22:37.678347: step 693, loss 0.67944, acc 0.59375\n",
      "2018-05-23T03:22:38.086261: step 694, loss 0.735363, acc 0.578125\n",
      "2018-05-23T03:22:38.523086: step 695, loss 0.585399, acc 0.671875\n",
      "2018-05-23T03:22:38.892101: step 696, loss 0.612897, acc 0.6875\n",
      "2018-05-23T03:22:39.245158: step 697, loss 0.57372, acc 0.71875\n",
      "2018-05-23T03:22:39.622146: step 698, loss 0.695997, acc 0.625\n",
      "2018-05-23T03:22:39.963234: step 699, loss 0.704305, acc 0.59375\n",
      "2018-05-23T03:22:40.300926: step 700, loss 0.573034, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:22:44.959463: step 700, loss 0.593997, acc 0.670667\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-700\n",
      "\n",
      "2018-05-23T03:22:46.285915: step 701, loss 0.612494, acc 0.796875\n",
      "2018-05-23T03:22:46.654928: step 702, loss 0.661582, acc 0.65625\n",
      "2018-05-23T03:22:47.069819: step 703, loss 0.582071, acc 0.671875\n",
      "2018-05-23T03:22:47.438830: step 704, loss 0.67489, acc 0.5625\n",
      "2018-05-23T03:22:47.783908: step 705, loss 0.5602, acc 0.75\n",
      "2018-05-23T03:22:48.132973: step 706, loss 0.643629, acc 0.71875\n",
      "2018-05-23T03:22:48.467106: step 707, loss 0.700527, acc 0.546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:22:48.821159: step 708, loss 0.570583, acc 0.671875\n",
      "2018-05-23T03:22:49.248015: step 709, loss 0.511447, acc 0.6875\n",
      "2018-05-23T03:22:49.598078: step 710, loss 0.601122, acc 0.65625\n",
      "2018-05-23T03:22:49.982052: step 711, loss 0.654458, acc 0.625\n",
      "2018-05-23T03:22:50.394946: step 712, loss 0.581605, acc 0.734375\n",
      "2018-05-23T03:22:50.756978: step 713, loss 0.540239, acc 0.71875\n",
      "2018-05-23T03:22:51.116019: step 714, loss 0.642136, acc 0.71875\n",
      "2018-05-23T03:22:51.461095: step 715, loss 0.552801, acc 0.734375\n",
      "2018-05-23T03:22:51.797196: step 716, loss 0.770198, acc 0.53125\n",
      "2018-05-23T03:22:52.150252: step 717, loss 0.599533, acc 0.640625\n",
      "2018-05-23T03:22:52.514278: step 718, loss 0.674891, acc 0.609375\n",
      "2018-05-23T03:22:52.867334: step 719, loss 0.591945, acc 0.703125\n",
      "2018-05-23T03:22:53.216401: step 720, loss 0.656352, acc 0.625\n",
      "2018-05-23T03:22:53.564468: step 721, loss 0.67066, acc 0.65625\n",
      "2018-05-23T03:22:53.971379: step 722, loss 0.494559, acc 0.75\n",
      "2018-05-23T03:22:54.361337: step 723, loss 0.670738, acc 0.65625\n",
      "2018-05-23T03:22:54.735337: step 724, loss 0.654826, acc 0.59375\n",
      "2018-05-23T03:22:55.114323: step 725, loss 0.5143, acc 0.75\n",
      "2018-05-23T03:22:55.467380: step 726, loss 0.666257, acc 0.640625\n",
      "2018-05-23T03:22:55.810460: step 727, loss 0.680379, acc 0.546875\n",
      "2018-05-23T03:22:56.160524: step 728, loss 0.671196, acc 0.59375\n",
      "2018-05-23T03:22:56.495629: step 729, loss 0.56762, acc 0.71875\n",
      "2018-05-23T03:22:56.838710: step 730, loss 0.66479, acc 0.609375\n",
      "2018-05-23T03:22:57.209717: step 731, loss 0.638155, acc 0.625\n",
      "2018-05-23T03:22:57.671481: step 732, loss 0.674473, acc 0.59375\n",
      "2018-05-23T03:22:58.154190: step 733, loss 0.661596, acc 0.71875\n",
      "2018-05-23T03:22:58.625929: step 734, loss 0.668498, acc 0.640625\n",
      "2018-05-23T03:22:59.152044: step 735, loss 0.617415, acc 0.703125\n",
      "2018-05-23T03:22:59.763009: step 736, loss 0.687454, acc 0.640625\n",
      "2018-05-23T03:23:00.363919: step 737, loss 0.577198, acc 0.6875\n",
      "2018-05-23T03:23:00.817706: step 738, loss 0.612146, acc 0.671875\n",
      "2018-05-23T03:23:01.214155: step 739, loss 0.615339, acc 0.65625\n",
      "2018-05-23T03:23:01.654023: step 740, loss 0.614771, acc 0.640625\n",
      "2018-05-23T03:23:02.127828: step 741, loss 0.608702, acc 0.703125\n",
      "2018-05-23T03:23:02.568746: step 742, loss 0.581033, acc 0.671875\n",
      "2018-05-23T03:23:02.989626: step 743, loss 0.629551, acc 0.71875\n",
      "2018-05-23T03:23:03.369609: step 744, loss 0.507307, acc 0.75\n",
      "2018-05-23T03:23:03.768542: step 745, loss 0.608663, acc 0.671875\n",
      "2018-05-23T03:23:04.215347: step 746, loss 0.660222, acc 0.640625\n",
      "2018-05-23T03:23:04.638216: step 747, loss 0.661059, acc 0.671875\n",
      "2018-05-23T03:23:05.032161: step 748, loss 0.477251, acc 0.8125\n",
      "2018-05-23T03:23:05.573714: step 749, loss 0.558749, acc 0.671875\n",
      "2018-05-23T03:23:06.027501: step 750, loss 0.567782, acc 0.71875\n",
      "2018-05-23T03:23:06.422444: step 751, loss 0.607438, acc 0.6875\n",
      "2018-05-23T03:23:06.881215: step 752, loss 0.618138, acc 0.6875\n",
      "2018-05-23T03:23:07.258207: step 753, loss 0.574582, acc 0.671875\n",
      "2018-05-23T03:23:07.642183: step 754, loss 0.589242, acc 0.703125\n",
      "2018-05-23T03:23:08.046101: step 755, loss 0.546388, acc 0.71875\n",
      "2018-05-23T03:23:08.419101: step 756, loss 0.660228, acc 0.625\n",
      "2018-05-23T03:23:08.871891: step 757, loss 0.54981, acc 0.703125\n",
      "2018-05-23T03:23:09.256860: step 758, loss 0.609382, acc 0.671875\n",
      "2018-05-23T03:23:09.644822: step 759, loss 0.61944, acc 0.59375\n",
      "2018-05-23T03:23:10.116560: step 760, loss 0.60227, acc 0.640625\n",
      "2018-05-23T03:23:10.506517: step 761, loss 0.590252, acc 0.6875\n",
      "2018-05-23T03:23:10.926396: step 762, loss 0.657161, acc 0.609375\n",
      "2018-05-23T03:23:11.308373: step 763, loss 0.711842, acc 0.546875\n",
      "2018-05-23T03:23:11.699326: step 764, loss 0.571241, acc 0.734375\n",
      "2018-05-23T03:23:12.149123: step 765, loss 0.662887, acc 0.625\n",
      "2018-05-23T03:23:12.548056: step 766, loss 0.722994, acc 0.546875\n",
      "2018-05-23T03:23:12.943996: step 767, loss 0.682597, acc 0.59375\n",
      "2018-05-23T03:23:13.325975: step 768, loss 0.565065, acc 0.6875\n",
      "2018-05-23T03:23:13.701971: step 769, loss 0.636282, acc 0.71875\n",
      "2018-05-23T03:23:14.112869: step 770, loss 0.65997, acc 0.609375\n",
      "2018-05-23T03:23:14.488864: step 771, loss 0.553055, acc 0.703125\n",
      "2018-05-23T03:23:14.875856: step 772, loss 0.630933, acc 0.65625\n",
      "2018-05-23T03:23:15.260825: step 773, loss 0.613555, acc 0.609375\n",
      "2018-05-23T03:23:15.650784: step 774, loss 0.603448, acc 0.71875\n",
      "2018-05-23T03:23:16.057694: step 775, loss 0.600821, acc 0.65625\n",
      "2018-05-23T03:23:16.431692: step 776, loss 0.611317, acc 0.6875\n",
      "2018-05-23T03:23:16.824643: step 777, loss 0.572814, acc 0.734375\n",
      "2018-05-23T03:23:17.246514: step 778, loss 0.571927, acc 0.71875\n",
      "2018-05-23T03:23:17.640459: step 779, loss 0.624842, acc 0.609375\n",
      "2018-05-23T03:23:18.059375: step 780, loss 0.736175, acc 0.578125\n",
      "2018-05-23T03:23:18.470276: step 781, loss 0.637604, acc 0.640625\n",
      "2018-05-23T03:23:18.930044: step 782, loss 0.530965, acc 0.734375\n",
      "2018-05-23T03:23:19.316014: step 783, loss 0.658569, acc 0.671875\n",
      "2018-05-23T03:23:19.991238: step 784, loss 0.626063, acc 0.71875\n",
      "2018-05-23T03:23:20.399146: step 785, loss 0.59274, acc 0.6875\n",
      "2018-05-23T03:23:20.789103: step 786, loss 0.597792, acc 0.734375\n",
      "2018-05-23T03:23:21.170085: step 787, loss 0.657779, acc 0.546875\n",
      "2018-05-23T03:23:21.560043: step 788, loss 0.687144, acc 0.5625\n",
      "2018-05-23T03:23:21.962964: step 789, loss 0.557176, acc 0.734375\n",
      "2018-05-23T03:23:22.361896: step 790, loss 0.597469, acc 0.640625\n",
      "2018-05-23T03:23:22.765818: step 791, loss 0.673099, acc 0.515625\n",
      "2018-05-23T03:23:23.156770: step 792, loss 0.546156, acc 0.765625\n",
      "2018-05-23T03:23:23.544731: step 793, loss 0.642703, acc 0.578125\n",
      "2018-05-23T03:23:23.965606: step 794, loss 0.600533, acc 0.671875\n",
      "2018-05-23T03:23:24.362543: step 795, loss 0.61468, acc 0.625\n",
      "2018-05-23T03:23:24.746517: step 796, loss 0.619105, acc 0.703125\n",
      "2018-05-23T03:23:25.120517: step 797, loss 0.555919, acc 0.71875\n",
      "2018-05-23T03:23:25.507511: step 798, loss 0.629368, acc 0.65625\n",
      "2018-05-23T03:23:25.889489: step 799, loss 0.575169, acc 0.671875\n",
      "2018-05-23T03:23:26.295403: step 800, loss 0.568363, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:23:31.621223: step 800, loss 0.577886, acc 0.688384\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-800\n",
      "\n",
      "2018-05-23T03:23:33.394479: step 801, loss 0.653431, acc 0.671875\n",
      "2018-05-23T03:23:33.986895: step 802, loss 0.673694, acc 0.59375\n",
      "2018-05-23T03:23:34.577315: step 803, loss 0.625195, acc 0.75\n",
      "2018-05-23T03:23:35.077976: step 804, loss 0.683573, acc 0.609375\n",
      "2018-05-23T03:23:35.477906: step 805, loss 0.594189, acc 0.609375\n",
      "2018-05-23T03:23:35.855894: step 806, loss 0.563491, acc 0.71875\n",
      "2018-05-23T03:23:36.240865: step 807, loss 0.538749, acc 0.671875\n",
      "2018-05-23T03:23:36.680688: step 808, loss 0.745919, acc 0.53125\n",
      "2018-05-23T03:23:37.099567: step 809, loss 0.61109, acc 0.640625\n",
      "2018-05-23T03:23:37.482543: step 810, loss 0.64464, acc 0.640625\n",
      "2018-05-23T03:23:37.899429: step 811, loss 0.529249, acc 0.75\n",
      "2018-05-23T03:23:38.327282: step 812, loss 0.589115, acc 0.71875\n",
      "2018-05-23T03:23:38.764114: step 813, loss 0.634758, acc 0.65625\n",
      "2018-05-23T03:23:39.364509: step 814, loss 0.614714, acc 0.625\n",
      "2018-05-23T03:23:39.768428: step 815, loss 0.596937, acc 0.65625\n",
      "2018-05-23T03:23:40.178333: step 816, loss 0.587876, acc 0.734375\n",
      "2018-05-23T03:23:40.541361: step 817, loss 0.615927, acc 0.640625\n",
      "2018-05-23T03:23:40.895412: step 818, loss 0.672304, acc 0.59375\n",
      "2018-05-23T03:23:41.233509: step 819, loss 0.663227, acc 0.609375\n",
      "2018-05-23T03:23:41.571604: step 820, loss 0.63146, acc 0.640625\n",
      "2018-05-23T03:23:41.944606: step 821, loss 0.601673, acc 0.609375\n",
      "2018-05-23T03:23:42.304642: step 822, loss 0.614087, acc 0.625\n",
      "2018-05-23T03:23:42.796328: step 823, loss 0.697039, acc 0.640625\n",
      "2018-05-23T03:23:43.371789: step 824, loss 0.62759, acc 0.671875\n",
      "2018-05-23T03:23:43.907355: step 825, loss 0.470505, acc 0.71875\n",
      "2018-05-23T03:23:44.404031: step 826, loss 0.563717, acc 0.6875\n",
      "2018-05-23T03:23:44.834875: step 827, loss 0.718077, acc 0.5625\n",
      "2018-05-23T03:23:45.201894: step 828, loss 0.660653, acc 0.703125\n",
      "2018-05-23T03:23:45.551955: step 829, loss 0.652213, acc 0.625\n",
      "2018-05-23T03:23:45.905011: step 830, loss 0.501289, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:23:46.356804: step 831, loss 0.615269, acc 0.640625\n",
      "2018-05-23T03:23:46.814578: step 832, loss 0.511138, acc 0.75\n",
      "2018-05-23T03:23:47.221491: step 833, loss 0.614337, acc 0.671875\n",
      "2018-05-23T03:23:47.627403: step 834, loss 0.531877, acc 0.796875\n",
      "2018-05-23T03:23:48.237771: step 835, loss 0.653183, acc 0.59375\n",
      "2018-05-23T03:23:48.863097: step 836, loss 0.554465, acc 0.734375\n",
      "2018-05-23T03:23:49.366751: step 837, loss 0.643518, acc 0.65625\n",
      "2018-05-23T03:23:49.876386: step 838, loss 0.631552, acc 0.6875\n",
      "2018-05-23T03:23:50.374055: step 839, loss 0.648947, acc 0.5625\n",
      "2018-05-23T03:23:50.923147: step 840, loss 0.690652, acc 0.5625\n",
      "2018-05-23T03:23:51.579391: step 841, loss 0.6017, acc 0.640625\n",
      "2018-05-23T03:23:52.070082: step 842, loss 0.624259, acc 0.65625\n",
      "2018-05-23T03:23:52.510899: step 843, loss 0.537019, acc 0.6875\n",
      "2018-05-23T03:23:52.869939: step 844, loss 0.583426, acc 0.65625\n",
      "2018-05-23T03:23:53.316746: step 845, loss 0.583302, acc 0.703125\n",
      "2018-05-23T03:23:53.675784: step 846, loss 0.533976, acc 0.734375\n",
      "2018-05-23T03:23:54.053772: step 847, loss 0.649113, acc 0.671875\n",
      "2018-05-23T03:23:54.563409: step 848, loss 0.596627, acc 0.6875\n",
      "2018-05-23T03:23:54.992261: step 849, loss 0.588281, acc 0.65625\n",
      "2018-05-23T03:23:55.338339: step 850, loss 0.766816, acc 0.5625\n",
      "2018-05-23T03:23:55.674472: step 851, loss 0.59557, acc 0.65625\n",
      "2018-05-23T03:23:56.035473: step 852, loss 0.582994, acc 0.625\n",
      "2018-05-23T03:23:56.409470: step 853, loss 0.579309, acc 0.65625\n",
      "2018-05-23T03:23:57.154477: step 854, loss 0.648718, acc 0.640625\n",
      "2018-05-23T03:23:57.942371: step 855, loss 0.519388, acc 0.71875\n",
      "2018-05-23T03:23:58.479932: step 856, loss 0.618768, acc 0.5625\n",
      "2018-05-23T03:23:58.967628: step 857, loss 0.492802, acc 0.75\n",
      "2018-05-23T03:23:59.437371: step 858, loss 0.547904, acc 0.734375\n",
      "2018-05-23T03:23:59.839295: step 859, loss 0.619217, acc 0.59375\n",
      "2018-05-23T03:24:00.348933: step 860, loss 0.637713, acc 0.59375\n",
      "2018-05-23T03:24:00.830644: step 861, loss 0.571956, acc 0.6875\n",
      "2018-05-23T03:24:01.237555: step 862, loss 0.638086, acc 0.671875\n",
      "2018-05-23T03:24:01.673390: step 863, loss 0.680255, acc 0.578125\n",
      "2018-05-23T03:24:02.097254: step 864, loss 0.583519, acc 0.671875\n",
      "2018-05-23T03:24:02.483223: step 865, loss 0.552867, acc 0.71875\n",
      "2018-05-23T03:24:02.832288: step 866, loss 0.692191, acc 0.5625\n",
      "2018-05-23T03:24:03.291061: step 867, loss 0.634285, acc 0.640625\n",
      "2018-05-23T03:24:03.653092: step 868, loss 0.569978, acc 0.734375\n",
      "2018-05-23T03:24:04.024099: step 869, loss 0.526544, acc 0.734375\n",
      "2018-05-23T03:24:04.575625: step 870, loss 0.62402, acc 0.609375\n",
      "2018-05-23T03:24:05.056338: step 871, loss 0.653184, acc 0.625\n",
      "2018-05-23T03:24:05.457266: step 872, loss 0.641033, acc 0.625\n",
      "2018-05-23T03:24:05.869164: step 873, loss 0.557057, acc 0.6875\n",
      "2018-05-23T03:24:06.253138: step 874, loss 0.585483, acc 0.6875\n",
      "2018-05-23T03:24:06.716896: step 875, loss 0.64879, acc 0.6875\n",
      "2018-05-23T03:24:07.079925: step 876, loss 0.580516, acc 0.671875\n",
      "2018-05-23T03:24:07.429989: step 877, loss 0.607238, acc 0.6875\n",
      "2018-05-23T03:24:07.794017: step 878, loss 0.591418, acc 0.765625\n",
      "2018-05-23T03:24:08.137097: step 879, loss 0.608853, acc 0.671875\n",
      "2018-05-23T03:24:08.484168: step 880, loss 0.559027, acc 0.71875\n",
      "2018-05-23T03:24:08.876124: step 881, loss 0.617043, acc 0.6875\n",
      "2018-05-23T03:24:09.289015: step 882, loss 0.605505, acc 0.6875\n",
      "2018-05-23T03:24:09.880434: step 883, loss 0.546471, acc 0.703125\n",
      "2018-05-23T03:24:10.336214: step 884, loss 0.584607, acc 0.6875\n",
      "2018-05-23T03:24:10.736143: step 885, loss 0.61688, acc 0.640625\n",
      "2018-05-23T03:24:11.130091: step 886, loss 0.574701, acc 0.671875\n",
      "2018-05-23T03:24:11.581881: step 887, loss 0.580912, acc 0.734375\n",
      "2018-05-23T03:24:12.022703: step 888, loss 0.66833, acc 0.625\n",
      "2018-05-23T03:24:12.462526: step 889, loss 0.756075, acc 0.5\n",
      "2018-05-23T03:24:12.860461: step 890, loss 0.592867, acc 0.640625\n",
      "2018-05-23T03:24:13.343170: step 891, loss 0.684821, acc 0.5625\n",
      "2018-05-23T03:24:13.773019: step 892, loss 0.600364, acc 0.703125\n",
      "2018-05-23T03:24:14.167963: step 893, loss 0.573709, acc 0.65625\n",
      "2018-05-23T03:24:14.590833: step 894, loss 0.535642, acc 0.703125\n",
      "2018-05-23T03:24:15.053594: step 895, loss 0.614204, acc 0.734375\n",
      "2018-05-23T03:24:15.457514: step 896, loss 0.654388, acc 0.671875\n",
      "2018-05-23T03:24:15.860435: step 897, loss 0.520041, acc 0.765625\n",
      "2018-05-23T03:24:16.238424: step 898, loss 0.628587, acc 0.625\n",
      "2018-05-23T03:24:16.635364: step 899, loss 0.59267, acc 0.703125\n",
      "2018-05-23T03:24:17.063219: step 900, loss 0.525609, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:24:21.943163: step 900, loss 0.570528, acc 0.691813\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-900\n",
      "\n",
      "2018-05-23T03:24:23.649597: step 901, loss 0.554954, acc 0.703125\n",
      "2018-05-23T03:24:24.042546: step 902, loss 0.536528, acc 0.703125\n",
      "2018-05-23T03:24:24.428514: step 903, loss 0.662984, acc 0.65625\n",
      "2018-05-23T03:24:24.851383: step 904, loss 0.704525, acc 0.578125\n",
      "2018-05-23T03:24:25.311153: step 905, loss 0.641674, acc 0.640625\n",
      "2018-05-23T03:24:25.708091: step 906, loss 0.6991, acc 0.65625\n",
      "2018-05-23T03:24:26.116998: step 907, loss 0.554861, acc 0.734375\n",
      "2018-05-23T03:24:26.486010: step 908, loss 0.545831, acc 0.71875\n",
      "2018-05-23T03:24:26.854026: step 909, loss 0.64364, acc 0.703125\n",
      "2018-05-23T03:24:27.237998: step 910, loss 0.58987, acc 0.71875\n",
      "2018-05-23T03:24:27.641918: step 911, loss 0.495647, acc 0.796875\n",
      "2018-05-23T03:24:28.013922: step 912, loss 0.733309, acc 0.546875\n",
      "2018-05-23T03:24:28.432803: step 913, loss 0.751524, acc 0.59375\n",
      "2018-05-23T03:24:28.817773: step 914, loss 0.63834, acc 0.59375\n",
      "2018-05-23T03:24:29.224684: step 915, loss 0.697769, acc 0.484375\n",
      "2018-05-23T03:24:29.679469: step 916, loss 0.65739, acc 0.671875\n",
      "2018-05-23T03:24:30.138240: step 917, loss 0.613869, acc 0.671875\n",
      "2018-05-23T03:24:30.543156: step 918, loss 0.588492, acc 0.671875\n",
      "2018-05-23T03:24:30.989961: step 919, loss 0.643073, acc 0.578125\n",
      "2018-05-23T03:24:31.380915: step 920, loss 0.61254, acc 0.671875\n",
      "2018-05-23T03:24:31.769875: step 921, loss 0.633888, acc 0.65625\n",
      "2018-05-23T03:24:32.186759: step 922, loss 0.60691, acc 0.6875\n",
      "2018-05-23T03:24:32.609629: step 923, loss 0.626118, acc 0.6875\n",
      "2018-05-23T03:24:33.098321: step 924, loss 0.634354, acc 0.65625\n",
      "2018-05-23T03:24:33.564082: step 925, loss 0.618817, acc 0.640625\n",
      "2018-05-23T03:24:34.055760: step 926, loss 0.579332, acc 0.6875\n",
      "2018-05-23T03:24:34.475637: step 927, loss 0.673194, acc 0.625\n",
      "2018-05-23T03:24:35.004223: step 928, loss 0.665488, acc 0.5625\n",
      "2018-05-23T03:24:35.551758: step 929, loss 0.637195, acc 0.625\n",
      "2018-05-23T03:24:36.067378: step 930, loss 0.548052, acc 0.671875\n",
      "2018-05-23T03:24:36.583996: step 931, loss 0.650272, acc 0.546875\n",
      "2018-05-23T03:24:37.106598: step 932, loss 0.592829, acc 0.6875\n",
      "2018-05-23T03:24:37.665104: step 933, loss 0.709608, acc 0.671875\n",
      "2018-05-23T03:24:38.228597: step 934, loss 0.531192, acc 0.765625\n",
      "2018-05-23T03:24:38.713299: step 935, loss 0.592111, acc 0.625\n",
      "2018-05-23T03:24:39.200996: step 936, loss 0.678792, acc 0.609375\n",
      "2018-05-23T03:24:39.619875: step 937, loss 0.599929, acc 0.6875\n",
      "2018-05-23T03:24:40.080642: step 938, loss 0.466593, acc 0.78125\n",
      "2018-05-23T03:24:40.630172: step 939, loss 0.606967, acc 0.59375\n",
      "2018-05-23T03:24:41.143798: step 940, loss 0.609681, acc 0.6875\n",
      "2018-05-23T03:24:41.723248: step 941, loss 0.525902, acc 0.734375\n",
      "2018-05-23T03:24:42.174042: step 942, loss 0.559019, acc 0.65625\n",
      "2018-05-23T03:24:42.707615: step 943, loss 0.539883, acc 0.75\n",
      "2018-05-23T03:24:43.354884: step 944, loss 0.598365, acc 0.65625\n",
      "2018-05-23T03:24:43.967244: step 945, loss 0.538994, acc 0.765625\n",
      "2018-05-23T03:24:44.416043: step 946, loss 0.752908, acc 0.546875\n",
      "2018-05-23T03:24:44.866837: step 947, loss 0.559122, acc 0.75\n",
      "2018-05-23T03:24:45.353536: step 948, loss 0.555946, acc 0.703125\n",
      "2018-05-23T03:24:45.758452: step 949, loss 0.532728, acc 0.734375\n",
      "2018-05-23T03:24:46.394751: step 950, loss 0.578961, acc 0.703125\n",
      "2018-05-23T03:24:46.870477: step 951, loss 0.712661, acc 0.53125\n",
      "2018-05-23T03:24:47.271404: step 952, loss 0.550041, acc 0.75\n",
      "2018-05-23T03:24:47.641415: step 953, loss 0.657462, acc 0.703125\n",
      "2018-05-23T03:24:48.010429: step 954, loss 0.581605, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:24:48.675649: step 955, loss 0.560262, acc 0.75\n",
      "2018-05-23T03:24:49.335884: step 956, loss 0.460662, acc 0.75\n",
      "2018-05-23T03:24:50.009083: step 957, loss 0.65686, acc 0.671875\n",
      "2018-05-23T03:24:50.702236: step 958, loss 0.592535, acc 0.6875\n",
      "2018-05-23T03:24:51.315587: step 959, loss 0.606507, acc 0.671875\n",
      "2018-05-23T03:24:51.921965: step 960, loss 0.623654, acc 0.65625\n",
      "2018-05-23T03:24:52.408662: step 961, loss 0.560034, acc 0.734375\n",
      "2018-05-23T03:24:52.887381: step 962, loss 0.510887, acc 0.75\n",
      "2018-05-23T03:24:53.425941: step 963, loss 0.703573, acc 0.5625\n",
      "2018-05-23T03:24:53.929595: step 964, loss 0.750847, acc 0.59375\n",
      "2018-05-23T03:24:54.371412: step 965, loss 0.628433, acc 0.671875\n",
      "2018-05-23T03:24:54.766354: step 966, loss 0.720389, acc 0.640625\n",
      "2018-05-23T03:24:55.327853: step 967, loss 0.666687, acc 0.640625\n",
      "2018-05-23T03:24:55.836493: step 968, loss 0.550497, acc 0.765625\n",
      "2018-05-23T03:24:56.541607: step 969, loss 0.633092, acc 0.625\n",
      "2018-05-23T03:24:57.242731: step 970, loss 0.578607, acc 0.625\n",
      "2018-05-23T03:24:57.646650: step 971, loss 0.610113, acc 0.71875\n",
      "2018-05-23T03:24:58.088467: step 972, loss 0.615119, acc 0.65625\n",
      "2018-05-23T03:24:58.446509: step 973, loss 0.493991, acc 0.71875\n",
      "2018-05-23T03:24:58.846440: step 974, loss 0.565277, acc 0.71875\n",
      "2018-05-23T03:24:59.215452: step 975, loss 0.713017, acc 0.703125\n",
      "2018-05-23T03:24:59.570504: step 976, loss 0.599163, acc 0.71875\n",
      "2018-05-23T03:25:00.147960: step 977, loss 0.626713, acc 0.6875\n",
      "2018-05-23T03:25:00.921888: step 978, loss 0.503094, acc 0.734375\n",
      "2018-05-23T03:25:01.499343: step 979, loss 0.457596, acc 0.78125\n",
      "2018-05-23T03:25:02.069816: step 980, loss 0.604317, acc 0.703125\n",
      "2018-05-23T03:25:02.645277: step 981, loss 0.584435, acc 0.71875\n",
      "2018-05-23T03:25:03.196802: step 982, loss 0.618512, acc 0.640625\n",
      "2018-05-23T03:25:03.764283: step 983, loss 0.61228, acc 0.703125\n",
      "2018-05-23T03:25:04.386618: step 984, loss 0.575646, acc 0.671875\n",
      "2018-05-23T03:25:04.857360: step 985, loss 0.560718, acc 0.705882\n",
      "2018-05-23T03:25:05.370985: step 986, loss 0.603826, acc 0.6875\n",
      "2018-05-23T03:25:05.892589: step 987, loss 0.536293, acc 0.71875\n",
      "2018-05-23T03:25:06.397240: step 988, loss 0.585568, acc 0.6875\n",
      "2018-05-23T03:25:07.053485: step 989, loss 0.475762, acc 0.765625\n",
      "2018-05-23T03:25:07.537190: step 990, loss 0.456473, acc 0.796875\n",
      "2018-05-23T03:25:07.917174: step 991, loss 0.589114, acc 0.734375\n",
      "2018-05-23T03:25:08.288182: step 992, loss 0.58066, acc 0.734375\n",
      "2018-05-23T03:25:08.731994: step 993, loss 0.571865, acc 0.6875\n",
      "2018-05-23T03:25:09.111978: step 994, loss 0.717751, acc 0.609375\n",
      "2018-05-23T03:25:09.524872: step 995, loss 0.601831, acc 0.6875\n",
      "2018-05-23T03:25:09.892889: step 996, loss 0.611247, acc 0.71875\n",
      "2018-05-23T03:25:10.255917: step 997, loss 0.581408, acc 0.6875\n",
      "2018-05-23T03:25:10.618947: step 998, loss 0.626843, acc 0.65625\n",
      "2018-05-23T03:25:10.977986: step 999, loss 0.544006, acc 0.75\n",
      "2018-05-23T03:25:11.372929: step 1000, loss 0.49897, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:25:16.142170: step 1000, loss 0.572251, acc 0.689241\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1000\n",
      "\n",
      "2018-05-23T03:25:18.024136: step 1001, loss 0.521141, acc 0.734375\n",
      "2018-05-23T03:25:18.462961: step 1002, loss 0.618976, acc 0.625\n",
      "2018-05-23T03:25:18.891815: step 1003, loss 0.591197, acc 0.65625\n",
      "2018-05-23T03:25:19.344603: step 1004, loss 0.603682, acc 0.671875\n",
      "2018-05-23T03:25:19.836288: step 1005, loss 0.623445, acc 0.71875\n",
      "2018-05-23T03:25:20.309028: step 1006, loss 0.49138, acc 0.765625\n",
      "2018-05-23T03:25:20.786746: step 1007, loss 0.537465, acc 0.671875\n",
      "2018-05-23T03:25:21.245518: step 1008, loss 0.659771, acc 0.609375\n",
      "2018-05-23T03:25:21.691326: step 1009, loss 0.542167, acc 0.703125\n",
      "2018-05-23T03:25:22.104234: step 1010, loss 0.612358, acc 0.671875\n",
      "2018-05-23T03:25:22.521105: step 1011, loss 0.514676, acc 0.703125\n",
      "2018-05-23T03:25:22.956939: step 1012, loss 0.535514, acc 0.75\n",
      "2018-05-23T03:25:23.369836: step 1013, loss 0.609555, acc 0.75\n",
      "2018-05-23T03:25:23.774752: step 1014, loss 0.527045, acc 0.75\n",
      "2018-05-23T03:25:24.190640: step 1015, loss 0.566974, acc 0.71875\n",
      "2018-05-23T03:25:24.611513: step 1016, loss 0.649227, acc 0.640625\n",
      "2018-05-23T03:25:25.044356: step 1017, loss 0.6334, acc 0.640625\n",
      "2018-05-23T03:25:25.465229: step 1018, loss 0.475421, acc 0.8125\n",
      "2018-05-23T03:25:25.939960: step 1019, loss 0.480957, acc 0.828125\n",
      "2018-05-23T03:25:26.398733: step 1020, loss 0.577888, acc 0.65625\n",
      "2018-05-23T03:25:27.016080: step 1021, loss 0.589719, acc 0.625\n",
      "2018-05-23T03:25:27.616474: step 1022, loss 0.529729, acc 0.78125\n",
      "2018-05-23T03:25:28.144063: step 1023, loss 0.538493, acc 0.703125\n",
      "2018-05-23T03:25:28.653700: step 1024, loss 0.470417, acc 0.796875\n",
      "2018-05-23T03:25:29.222179: step 1025, loss 0.517361, acc 0.71875\n",
      "2018-05-23T03:25:29.744781: step 1026, loss 0.527226, acc 0.796875\n",
      "2018-05-23T03:25:30.309270: step 1027, loss 0.506241, acc 0.78125\n",
      "2018-05-23T03:25:30.871767: step 1028, loss 0.565485, acc 0.703125\n",
      "2018-05-23T03:25:31.340513: step 1029, loss 0.483302, acc 0.734375\n",
      "2018-05-23T03:25:31.777343: step 1030, loss 0.644676, acc 0.65625\n",
      "2018-05-23T03:25:32.217167: step 1031, loss 0.544316, acc 0.78125\n",
      "2018-05-23T03:25:32.687907: step 1032, loss 0.45663, acc 0.71875\n",
      "2018-05-23T03:25:33.133716: step 1033, loss 0.577929, acc 0.671875\n",
      "2018-05-23T03:25:33.659309: step 1034, loss 0.563139, acc 0.640625\n",
      "2018-05-23T03:25:34.278652: step 1035, loss 0.557574, acc 0.6875\n",
      "2018-05-23T03:25:34.805243: step 1036, loss 0.551511, acc 0.65625\n",
      "2018-05-23T03:25:35.268004: step 1037, loss 0.539399, acc 0.671875\n",
      "2018-05-23T03:25:35.756698: step 1038, loss 0.657244, acc 0.609375\n",
      "2018-05-23T03:25:36.241400: step 1039, loss 0.545008, acc 0.75\n",
      "2018-05-23T03:25:36.704164: step 1040, loss 0.533919, acc 0.71875\n",
      "2018-05-23T03:25:37.190862: step 1041, loss 0.599939, acc 0.671875\n",
      "2018-05-23T03:25:37.909938: step 1042, loss 0.523068, acc 0.71875\n",
      "2018-05-23T03:25:38.501357: step 1043, loss 0.575322, acc 0.734375\n",
      "2018-05-23T03:25:39.003014: step 1044, loss 0.727838, acc 0.640625\n",
      "2018-05-23T03:25:39.467770: step 1045, loss 0.490853, acc 0.78125\n",
      "2018-05-23T03:25:39.933523: step 1046, loss 0.536357, acc 0.734375\n",
      "2018-05-23T03:25:40.354398: step 1047, loss 0.562698, acc 0.75\n",
      "2018-05-23T03:25:40.761310: step 1048, loss 0.604602, acc 0.671875\n",
      "2018-05-23T03:25:41.173208: step 1049, loss 0.705953, acc 0.609375\n",
      "2018-05-23T03:25:41.597074: step 1050, loss 0.495249, acc 0.765625\n",
      "2018-05-23T03:25:42.019943: step 1051, loss 0.530824, acc 0.734375\n",
      "2018-05-23T03:25:42.438822: step 1052, loss 0.600819, acc 0.75\n",
      "2018-05-23T03:25:42.850720: step 1053, loss 0.616781, acc 0.640625\n",
      "2018-05-23T03:25:43.302512: step 1054, loss 0.546549, acc 0.703125\n",
      "2018-05-23T03:25:43.722388: step 1055, loss 0.559646, acc 0.6875\n",
      "2018-05-23T03:25:44.120323: step 1056, loss 0.492492, acc 0.75\n",
      "2018-05-23T03:25:44.546185: step 1057, loss 0.61653, acc 0.640625\n",
      "2018-05-23T03:25:44.952098: step 1058, loss 0.601041, acc 0.6875\n",
      "2018-05-23T03:25:45.374967: step 1059, loss 0.471389, acc 0.8125\n",
      "2018-05-23T03:25:45.784871: step 1060, loss 0.633478, acc 0.671875\n",
      "2018-05-23T03:25:46.177819: step 1061, loss 0.478152, acc 0.765625\n",
      "2018-05-23T03:25:46.598693: step 1062, loss 0.573339, acc 0.65625\n",
      "2018-05-23T03:25:47.000618: step 1063, loss 0.561401, acc 0.765625\n",
      "2018-05-23T03:25:47.385591: step 1064, loss 0.707967, acc 0.671875\n",
      "2018-05-23T03:25:47.751611: step 1065, loss 0.523585, acc 0.75\n",
      "2018-05-23T03:25:48.093693: step 1066, loss 0.553391, acc 0.734375\n",
      "2018-05-23T03:25:48.517561: step 1067, loss 0.612062, acc 0.671875\n",
      "2018-05-23T03:25:48.903527: step 1068, loss 0.687404, acc 0.578125\n",
      "2018-05-23T03:25:49.485970: step 1069, loss 0.439317, acc 0.78125\n",
      "2018-05-23T03:25:49.894876: step 1070, loss 0.571688, acc 0.65625\n",
      "2018-05-23T03:25:50.317745: step 1071, loss 0.676939, acc 0.671875\n",
      "2018-05-23T03:25:50.707701: step 1072, loss 0.501484, acc 0.71875\n",
      "2018-05-23T03:25:51.155502: step 1073, loss 0.527002, acc 0.75\n",
      "2018-05-23T03:25:51.570393: step 1074, loss 0.550999, acc 0.78125\n",
      "2018-05-23T03:25:52.050110: step 1075, loss 0.507484, acc 0.75\n",
      "2018-05-23T03:25:52.710344: step 1076, loss 0.51567, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:25:53.352625: step 1077, loss 0.472382, acc 0.796875\n",
      "2018-05-23T03:25:53.878219: step 1078, loss 0.577652, acc 0.71875\n",
      "2018-05-23T03:25:54.397829: step 1079, loss 0.523521, acc 0.71875\n",
      "2018-05-23T03:25:54.966309: step 1080, loss 0.612838, acc 0.65625\n",
      "2018-05-23T03:25:55.534788: step 1081, loss 0.663844, acc 0.71875\n",
      "2018-05-23T03:25:56.142164: step 1082, loss 0.502477, acc 0.75\n",
      "2018-05-23T03:25:56.701666: step 1083, loss 0.428032, acc 0.75\n",
      "2018-05-23T03:25:57.160440: step 1084, loss 0.476209, acc 0.75\n",
      "2018-05-23T03:25:57.618215: step 1085, loss 0.682301, acc 0.65625\n",
      "2018-05-23T03:25:58.029114: step 1086, loss 0.637287, acc 0.671875\n",
      "2018-05-23T03:25:58.517842: step 1087, loss 0.630698, acc 0.640625\n",
      "2018-05-23T03:25:59.113215: step 1088, loss 0.58241, acc 0.703125\n",
      "2018-05-23T03:25:59.668730: step 1089, loss 0.486616, acc 0.75\n",
      "2018-05-23T03:26:00.136476: step 1090, loss 0.563926, acc 0.703125\n",
      "2018-05-23T03:26:00.521447: step 1091, loss 0.433138, acc 0.84375\n",
      "2018-05-23T03:26:00.898440: step 1092, loss 0.530736, acc 0.75\n",
      "2018-05-23T03:26:01.351230: step 1093, loss 0.620015, acc 0.640625\n",
      "2018-05-23T03:26:01.725227: step 1094, loss 0.604702, acc 0.703125\n",
      "2018-05-23T03:26:02.103215: step 1095, loss 0.670551, acc 0.609375\n",
      "2018-05-23T03:26:02.547029: step 1096, loss 0.524223, acc 0.6875\n",
      "2018-05-23T03:26:02.926017: step 1097, loss 0.590917, acc 0.734375\n",
      "2018-05-23T03:26:03.281066: step 1098, loss 0.464055, acc 0.765625\n",
      "2018-05-23T03:26:03.652073: step 1099, loss 0.634736, acc 0.671875\n",
      "2018-05-23T03:26:04.053996: step 1100, loss 0.638734, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:26:11.317566: step 1100, loss 0.633131, acc 0.668953\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1100\n",
      "\n",
      "2018-05-23T03:26:13.513692: step 1101, loss 0.58382, acc 0.734375\n",
      "2018-05-23T03:26:14.144004: step 1102, loss 0.539911, acc 0.75\n",
      "2018-05-23T03:26:14.785290: step 1103, loss 0.602676, acc 0.734375\n",
      "2018-05-23T03:26:15.683886: step 1104, loss 0.609076, acc 0.6875\n",
      "2018-05-23T03:26:16.234412: step 1105, loss 0.455106, acc 0.71875\n",
      "2018-05-23T03:26:16.739062: step 1106, loss 0.623076, acc 0.65625\n",
      "2018-05-23T03:26:17.252689: step 1107, loss 0.608226, acc 0.703125\n",
      "2018-05-23T03:26:17.790250: step 1108, loss 0.516469, acc 0.75\n",
      "2018-05-23T03:26:18.259993: step 1109, loss 0.636685, acc 0.65625\n",
      "2018-05-23T03:26:18.750680: step 1110, loss 0.58814, acc 0.671875\n",
      "2018-05-23T03:26:19.220423: step 1111, loss 0.609456, acc 0.6875\n",
      "2018-05-23T03:26:19.680194: step 1112, loss 0.662087, acc 0.6875\n",
      "2018-05-23T03:26:20.134977: step 1113, loss 0.514796, acc 0.734375\n",
      "2018-05-23T03:26:20.627661: step 1114, loss 0.587883, acc 0.671875\n",
      "2018-05-23T03:26:21.125328: step 1115, loss 0.523993, acc 0.703125\n",
      "2018-05-23T03:26:21.764618: step 1116, loss 0.625562, acc 0.640625\n",
      "2018-05-23T03:26:22.556500: step 1117, loss 0.586843, acc 0.71875\n",
      "2018-05-23T03:26:23.154899: step 1118, loss 0.511403, acc 0.703125\n",
      "2018-05-23T03:26:23.678498: step 1119, loss 0.443747, acc 0.8125\n",
      "2018-05-23T03:26:24.176167: step 1120, loss 0.651826, acc 0.6875\n",
      "2018-05-23T03:26:24.778556: step 1121, loss 0.515309, acc 0.75\n",
      "2018-05-23T03:26:25.404880: step 1122, loss 0.528894, acc 0.71875\n",
      "2018-05-23T03:26:25.972362: step 1123, loss 0.687477, acc 0.578125\n",
      "2018-05-23T03:26:26.616638: step 1124, loss 0.532317, acc 0.625\n",
      "2018-05-23T03:26:27.228003: step 1125, loss 0.536551, acc 0.671875\n",
      "2018-05-23T03:26:27.825404: step 1126, loss 0.55971, acc 0.703125\n",
      "2018-05-23T03:26:28.601329: step 1127, loss 0.56193, acc 0.671875\n",
      "2018-05-23T03:26:29.181798: step 1128, loss 0.639768, acc 0.65625\n",
      "2018-05-23T03:26:29.752250: step 1129, loss 0.533614, acc 0.71875\n",
      "2018-05-23T03:26:30.238947: step 1130, loss 0.530301, acc 0.71875\n",
      "2018-05-23T03:26:30.710685: step 1131, loss 0.608732, acc 0.640625\n",
      "2018-05-23T03:26:31.181427: step 1132, loss 0.567401, acc 0.75\n",
      "2018-05-23T03:26:31.651169: step 1133, loss 0.504171, acc 0.75\n",
      "2018-05-23T03:26:32.154822: step 1134, loss 0.520844, acc 0.734375\n",
      "2018-05-23T03:26:32.625563: step 1135, loss 0.439803, acc 0.8125\n",
      "2018-05-23T03:26:33.140187: step 1136, loss 0.660838, acc 0.65625\n",
      "2018-05-23T03:26:33.647830: step 1137, loss 0.485844, acc 0.75\n",
      "2018-05-23T03:26:34.217305: step 1138, loss 0.537404, acc 0.734375\n",
      "2018-05-23T03:26:34.736914: step 1139, loss 0.49903, acc 0.78125\n",
      "2018-05-23T03:26:35.260514: step 1140, loss 0.636867, acc 0.703125\n",
      "2018-05-23T03:26:35.884844: step 1141, loss 0.570312, acc 0.734375\n",
      "2018-05-23T03:26:36.403456: step 1142, loss 0.455048, acc 0.78125\n",
      "2018-05-23T03:26:36.947002: step 1143, loss 0.558431, acc 0.734375\n",
      "2018-05-23T03:26:37.485562: step 1144, loss 0.509839, acc 0.734375\n",
      "2018-05-23T03:26:37.988217: step 1145, loss 0.590735, acc 0.625\n",
      "2018-05-23T03:26:38.501842: step 1146, loss 0.428364, acc 0.828125\n",
      "2018-05-23T03:26:38.993528: step 1147, loss 0.525316, acc 0.734375\n",
      "2018-05-23T03:26:39.508151: step 1148, loss 0.675816, acc 0.640625\n",
      "2018-05-23T03:26:39.991857: step 1149, loss 0.561735, acc 0.75\n",
      "2018-05-23T03:26:40.498501: step 1150, loss 0.524839, acc 0.640625\n",
      "2018-05-23T03:26:40.991184: step 1151, loss 0.592176, acc 0.75\n",
      "2018-05-23T03:26:41.492841: step 1152, loss 0.596359, acc 0.640625\n",
      "2018-05-23T03:26:41.995497: step 1153, loss 0.621444, acc 0.671875\n",
      "2018-05-23T03:26:42.471223: step 1154, loss 0.746523, acc 0.640625\n",
      "2018-05-23T03:26:42.969889: step 1155, loss 0.536464, acc 0.75\n",
      "2018-05-23T03:26:43.471547: step 1156, loss 0.518945, acc 0.734375\n",
      "2018-05-23T03:26:43.971211: step 1157, loss 0.60447, acc 0.671875\n",
      "2018-05-23T03:26:44.470874: step 1158, loss 0.55686, acc 0.734375\n",
      "2018-05-23T03:26:44.985498: step 1159, loss 0.492259, acc 0.765625\n",
      "2018-05-23T03:26:45.541012: step 1160, loss 0.649205, acc 0.640625\n",
      "2018-05-23T03:26:46.198253: step 1161, loss 0.549462, acc 0.703125\n",
      "2018-05-23T03:26:46.717864: step 1162, loss 0.528262, acc 0.703125\n",
      "2018-05-23T03:26:47.166662: step 1163, loss 0.71158, acc 0.625\n",
      "2018-05-23T03:26:47.621447: step 1164, loss 0.578759, acc 0.671875\n",
      "2018-05-23T03:26:48.074235: step 1165, loss 0.564522, acc 0.640625\n",
      "2018-05-23T03:26:48.514058: step 1166, loss 0.544324, acc 0.734375\n",
      "2018-05-23T03:26:48.981806: step 1167, loss 0.589563, acc 0.640625\n",
      "2018-05-23T03:26:49.437587: step 1168, loss 0.600696, acc 0.671875\n",
      "2018-05-23T03:26:49.948221: step 1169, loss 0.537867, acc 0.734375\n",
      "2018-05-23T03:26:50.448881: step 1170, loss 0.677445, acc 0.65625\n",
      "2018-05-23T03:26:50.936578: step 1171, loss 0.493025, acc 0.765625\n",
      "2018-05-23T03:26:51.480124: step 1172, loss 0.579229, acc 0.71875\n",
      "2018-05-23T03:26:52.064560: step 1173, loss 0.506668, acc 0.734375\n",
      "2018-05-23T03:26:52.573199: step 1174, loss 0.569031, acc 0.765625\n",
      "2018-05-23T03:26:53.093806: step 1175, loss 0.660003, acc 0.578125\n",
      "2018-05-23T03:26:53.566542: step 1176, loss 0.552441, acc 0.703125\n",
      "2018-05-23T03:26:54.096125: step 1177, loss 0.656325, acc 0.5625\n",
      "2018-05-23T03:26:54.579831: step 1178, loss 0.652316, acc 0.640625\n",
      "2018-05-23T03:26:55.082486: step 1179, loss 0.548832, acc 0.765625\n",
      "2018-05-23T03:26:55.608084: step 1180, loss 0.519373, acc 0.78125\n",
      "2018-05-23T03:26:56.216453: step 1181, loss 0.653071, acc 0.625\n",
      "2018-05-23T03:26:56.708138: step 1182, loss 0.48543, acc 0.765625\n",
      "2018-05-23T03:26:57.179876: step 1183, loss 0.466564, acc 0.75\n",
      "2018-05-23T03:26:57.664579: step 1184, loss 0.529389, acc 0.75\n",
      "2018-05-23T03:26:58.132327: step 1185, loss 0.595025, acc 0.734375\n",
      "2018-05-23T03:26:58.584119: step 1186, loss 0.568003, acc 0.734375\n",
      "2018-05-23T03:26:59.050870: step 1187, loss 0.513199, acc 0.734375\n",
      "2018-05-23T03:26:59.605387: step 1188, loss 0.782828, acc 0.53125\n",
      "2018-05-23T03:27:00.085103: step 1189, loss 0.455126, acc 0.796875\n",
      "2018-05-23T03:27:00.699460: step 1190, loss 0.533006, acc 0.671875\n",
      "2018-05-23T03:27:01.525251: step 1191, loss 0.534152, acc 0.765625\n",
      "2018-05-23T03:27:02.350044: step 1192, loss 0.504406, acc 0.765625\n",
      "2018-05-23T03:27:02.978363: step 1193, loss 0.530932, acc 0.78125\n",
      "2018-05-23T03:27:03.540859: step 1194, loss 0.547477, acc 0.703125\n",
      "2018-05-23T03:27:04.055482: step 1195, loss 0.538852, acc 0.703125\n",
      "2018-05-23T03:27:04.583070: step 1196, loss 0.541644, acc 0.703125\n",
      "2018-05-23T03:27:05.118638: step 1197, loss 0.606671, acc 0.671875\n",
      "2018-05-23T03:27:05.645229: step 1198, loss 0.443565, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:27:06.132924: step 1199, loss 0.60814, acc 0.6875\n",
      "2018-05-23T03:27:06.610646: step 1200, loss 0.555719, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:27:11.620245: step 1200, loss 0.549911, acc 0.710387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1200\n",
      "\n",
      "2018-05-23T03:27:13.376546: step 1201, loss 0.577239, acc 0.65625\n",
      "2018-05-23T03:27:13.843297: step 1202, loss 0.504985, acc 0.734375\n",
      "2018-05-23T03:27:14.353931: step 1203, loss 0.511925, acc 0.765625\n",
      "2018-05-23T03:27:14.820683: step 1204, loss 0.582524, acc 0.6875\n",
      "2018-05-23T03:27:15.278459: step 1205, loss 0.553008, acc 0.671875\n",
      "2018-05-23T03:27:15.745209: step 1206, loss 0.611797, acc 0.65625\n",
      "2018-05-23T03:27:16.205977: step 1207, loss 0.551933, acc 0.734375\n",
      "2018-05-23T03:27:16.646797: step 1208, loss 0.511627, acc 0.6875\n",
      "2018-05-23T03:27:17.091608: step 1209, loss 0.50364, acc 0.78125\n",
      "2018-05-23T03:27:17.561351: step 1210, loss 0.493581, acc 0.75\n",
      "2018-05-23T03:27:18.025110: step 1211, loss 0.472349, acc 0.765625\n",
      "2018-05-23T03:27:18.481888: step 1212, loss 0.412972, acc 0.84375\n",
      "2018-05-23T03:27:18.937669: step 1213, loss 0.55167, acc 0.75\n",
      "2018-05-23T03:27:19.361535: step 1214, loss 0.449719, acc 0.828125\n",
      "2018-05-23T03:27:19.783407: step 1215, loss 0.588585, acc 0.65625\n",
      "2018-05-23T03:27:20.195305: step 1216, loss 0.449729, acc 0.8125\n",
      "2018-05-23T03:27:20.609197: step 1217, loss 0.642734, acc 0.6875\n",
      "2018-05-23T03:27:21.030071: step 1218, loss 0.600137, acc 0.71875\n",
      "2018-05-23T03:27:21.503804: step 1219, loss 0.645205, acc 0.65625\n",
      "2018-05-23T03:27:21.922684: step 1220, loss 0.518187, acc 0.734375\n",
      "2018-05-23T03:27:22.370486: step 1221, loss 0.568375, acc 0.6875\n",
      "2018-05-23T03:27:22.800336: step 1222, loss 0.625463, acc 0.671875\n",
      "2018-05-23T03:27:23.233177: step 1223, loss 0.591425, acc 0.703125\n",
      "2018-05-23T03:27:23.642084: step 1224, loss 0.556664, acc 0.734375\n",
      "2018-05-23T03:27:24.055976: step 1225, loss 0.577611, acc 0.65625\n",
      "2018-05-23T03:27:24.461891: step 1226, loss 0.680092, acc 0.59375\n",
      "2018-05-23T03:27:24.850850: step 1227, loss 0.506428, acc 0.765625\n",
      "2018-05-23T03:27:25.252775: step 1228, loss 0.606311, acc 0.734375\n",
      "2018-05-23T03:27:25.694592: step 1229, loss 0.577499, acc 0.703125\n",
      "2018-05-23T03:27:26.200240: step 1230, loss 0.562086, acc 0.625\n",
      "2018-05-23T03:27:26.669983: step 1231, loss 0.537551, acc 0.703125\n",
      "2018-05-23T03:27:27.102825: step 1232, loss 0.576006, acc 0.703125\n",
      "2018-05-23T03:27:27.538660: step 1233, loss 0.545332, acc 0.75\n",
      "2018-05-23T03:27:28.020371: step 1234, loss 0.61205, acc 0.671875\n",
      "2018-05-23T03:27:28.461192: step 1235, loss 0.541035, acc 0.71875\n",
      "2018-05-23T03:27:28.939912: step 1236, loss 0.508597, acc 0.765625\n",
      "2018-05-23T03:27:29.394695: step 1237, loss 0.460565, acc 0.796875\n",
      "2018-05-23T03:27:29.990102: step 1238, loss 0.551697, acc 0.734375\n",
      "2018-05-23T03:27:30.579525: step 1239, loss 0.600489, acc 0.640625\n",
      "2018-05-23T03:27:31.222805: step 1240, loss 0.664222, acc 0.609375\n",
      "2018-05-23T03:27:32.032638: step 1241, loss 0.625104, acc 0.6875\n",
      "2018-05-23T03:27:32.629042: step 1242, loss 0.574235, acc 0.703125\n",
      "2018-05-23T03:27:33.250380: step 1243, loss 0.458548, acc 0.8125\n",
      "2018-05-23T03:27:33.789937: step 1244, loss 0.538478, acc 0.734375\n",
      "2018-05-23T03:27:34.306554: step 1245, loss 0.687785, acc 0.609375\n",
      "2018-05-23T03:27:34.802228: step 1246, loss 0.584083, acc 0.703125\n",
      "2018-05-23T03:27:35.325828: step 1247, loss 0.477641, acc 0.78125\n",
      "2018-05-23T03:27:35.830477: step 1248, loss 0.556538, acc 0.71875\n",
      "2018-05-23T03:27:36.306205: step 1249, loss 0.4721, acc 0.796875\n",
      "2018-05-23T03:27:36.773954: step 1250, loss 0.668288, acc 0.625\n",
      "2018-05-23T03:27:37.303537: step 1251, loss 0.562418, acc 0.703125\n",
      "2018-05-23T03:27:37.772283: step 1252, loss 0.500346, acc 0.734375\n",
      "2018-05-23T03:27:38.253994: step 1253, loss 0.466717, acc 0.8125\n",
      "2018-05-23T03:27:38.728724: step 1254, loss 0.576525, acc 0.703125\n",
      "2018-05-23T03:27:39.232376: step 1255, loss 0.528321, acc 0.75\n",
      "2018-05-23T03:27:39.717080: step 1256, loss 0.636071, acc 0.671875\n",
      "2018-05-23T03:27:40.182834: step 1257, loss 0.622995, acc 0.703125\n",
      "2018-05-23T03:27:40.648588: step 1258, loss 0.618583, acc 0.765625\n",
      "2018-05-23T03:27:41.107361: step 1259, loss 0.49755, acc 0.765625\n",
      "2018-05-23T03:27:41.560150: step 1260, loss 0.439771, acc 0.78125\n",
      "2018-05-23T03:27:42.012938: step 1261, loss 0.619111, acc 0.6875\n",
      "2018-05-23T03:27:42.458746: step 1262, loss 0.475655, acc 0.8125\n",
      "2018-05-23T03:27:42.910537: step 1263, loss 0.58068, acc 0.65625\n",
      "2018-05-23T03:27:43.392248: step 1264, loss 0.574103, acc 0.640625\n",
      "2018-05-23T03:27:43.874958: step 1265, loss 0.535486, acc 0.65625\n",
      "2018-05-23T03:27:44.337719: step 1266, loss 0.569882, acc 0.71875\n",
      "2018-05-23T03:27:44.812449: step 1267, loss 0.544379, acc 0.78125\n",
      "2018-05-23T03:27:45.289174: step 1268, loss 0.568903, acc 0.71875\n",
      "2018-05-23T03:27:45.800805: step 1269, loss 0.478332, acc 0.765625\n",
      "2018-05-23T03:27:46.385242: step 1270, loss 0.439784, acc 0.765625\n",
      "2018-05-23T03:27:46.911833: step 1271, loss 0.541174, acc 0.765625\n",
      "2018-05-23T03:27:47.386563: step 1272, loss 0.462373, acc 0.765625\n",
      "2018-05-23T03:27:47.903181: step 1273, loss 0.507147, acc 0.765625\n",
      "2018-05-23T03:27:48.434759: step 1274, loss 0.568545, acc 0.71875\n",
      "2018-05-23T03:27:48.960352: step 1275, loss 0.56213, acc 0.765625\n",
      "2018-05-23T03:27:49.439072: step 1276, loss 0.65386, acc 0.65625\n",
      "2018-05-23T03:27:50.015530: step 1277, loss 0.46431, acc 0.796875\n",
      "2018-05-23T03:27:50.659807: step 1278, loss 0.52539, acc 0.734375\n",
      "2018-05-23T03:27:51.135534: step 1279, loss 0.539691, acc 0.71875\n",
      "2018-05-23T03:27:51.609266: step 1280, loss 0.64612, acc 0.640625\n",
      "2018-05-23T03:27:52.166775: step 1281, loss 0.512502, acc 0.71875\n",
      "2018-05-23T03:27:52.640508: step 1282, loss 0.621308, acc 0.640625\n",
      "2018-05-23T03:27:53.121223: step 1283, loss 0.582135, acc 0.78125\n",
      "2018-05-23T03:27:53.721615: step 1284, loss 0.61363, acc 0.6875\n",
      "2018-05-23T03:27:54.186372: step 1285, loss 0.576726, acc 0.65625\n",
      "2018-05-23T03:27:54.664094: step 1286, loss 0.390618, acc 0.8125\n",
      "2018-05-23T03:27:55.159768: step 1287, loss 0.591776, acc 0.71875\n",
      "2018-05-23T03:27:55.630509: step 1288, loss 0.591177, acc 0.65625\n",
      "2018-05-23T03:27:56.110226: step 1289, loss 0.521901, acc 0.71875\n",
      "2018-05-23T03:27:56.653771: step 1290, loss 0.494122, acc 0.796875\n",
      "2018-05-23T03:27:57.218262: step 1291, loss 0.5123, acc 0.8125\n",
      "2018-05-23T03:27:57.727898: step 1292, loss 0.491664, acc 0.78125\n",
      "2018-05-23T03:27:58.209610: step 1293, loss 0.668141, acc 0.671875\n",
      "2018-05-23T03:27:58.702291: step 1294, loss 0.491718, acc 0.75\n",
      "2018-05-23T03:27:59.176024: step 1295, loss 0.581467, acc 0.703125\n",
      "2018-05-23T03:27:59.643772: step 1296, loss 0.551836, acc 0.65625\n",
      "2018-05-23T03:28:00.113516: step 1297, loss 0.512627, acc 0.75\n",
      "2018-05-23T03:28:00.647088: step 1298, loss 0.542356, acc 0.703125\n",
      "2018-05-23T03:28:01.257456: step 1299, loss 0.567725, acc 0.6875\n",
      "2018-05-23T03:28:01.789033: step 1300, loss 0.551103, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:28:06.863458: step 1300, loss 0.59033, acc 0.682383\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1300\n",
      "\n",
      "2018-05-23T03:28:08.937909: step 1301, loss 0.601803, acc 0.75\n",
      "2018-05-23T03:28:09.410644: step 1302, loss 0.589169, acc 0.625\n",
      "2018-05-23T03:28:09.908312: step 1303, loss 0.516928, acc 0.75\n",
      "2018-05-23T03:28:10.407976: step 1304, loss 0.594926, acc 0.71875\n",
      "2018-05-23T03:28:11.105111: step 1305, loss 0.573848, acc 0.703125\n",
      "2018-05-23T03:28:11.560892: step 1306, loss 0.498739, acc 0.734375\n",
      "2018-05-23T03:28:12.077510: step 1307, loss 0.567366, acc 0.75\n",
      "2018-05-23T03:28:12.554234: step 1308, loss 0.5698, acc 0.71875\n",
      "2018-05-23T03:28:13.025973: step 1309, loss 0.685584, acc 0.625\n",
      "2018-05-23T03:28:13.512670: step 1310, loss 0.620514, acc 0.703125\n",
      "2018-05-23T03:28:13.984409: step 1311, loss 0.611312, acc 0.71875\n",
      "2018-05-23T03:28:14.430216: step 1312, loss 0.467664, acc 0.78125\n",
      "2018-05-23T03:28:14.886994: step 1313, loss 0.610686, acc 0.703125\n",
      "2018-05-23T03:28:15.351750: step 1314, loss 0.544291, acc 0.71875\n",
      "2018-05-23T03:28:15.803542: step 1315, loss 0.73272, acc 0.625\n",
      "2018-05-23T03:28:16.290239: step 1316, loss 0.671628, acc 0.640625\n",
      "2018-05-23T03:28:16.739039: step 1317, loss 0.583108, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:28:17.210777: step 1318, loss 0.565225, acc 0.75\n",
      "2018-05-23T03:28:17.671544: step 1319, loss 0.402447, acc 0.8125\n",
      "2018-05-23T03:28:18.158243: step 1320, loss 0.574164, acc 0.640625\n",
      "2018-05-23T03:28:18.622999: step 1321, loss 0.606417, acc 0.703125\n",
      "2018-05-23T03:28:19.088754: step 1322, loss 0.672121, acc 0.71875\n",
      "2018-05-23T03:28:19.558497: step 1323, loss 0.545992, acc 0.6875\n",
      "2018-05-23T03:28:20.028239: step 1324, loss 0.61443, acc 0.703125\n",
      "2018-05-23T03:28:20.490005: step 1325, loss 0.523429, acc 0.71875\n",
      "2018-05-23T03:28:20.966730: step 1326, loss 0.508178, acc 0.75\n",
      "2018-05-23T03:28:21.437471: step 1327, loss 0.492241, acc 0.75\n",
      "2018-05-23T03:28:21.938131: step 1328, loss 0.687274, acc 0.609375\n",
      "2018-05-23T03:28:22.406876: step 1329, loss 0.632437, acc 0.71875\n",
      "2018-05-23T03:28:22.863654: step 1330, loss 0.458238, acc 0.765625\n",
      "2018-05-23T03:28:23.309462: step 1331, loss 0.493372, acc 0.75\n",
      "2018-05-23T03:28:23.761254: step 1332, loss 0.490594, acc 0.71875\n",
      "2018-05-23T03:28:24.205067: step 1333, loss 0.455637, acc 0.796875\n",
      "2018-05-23T03:28:24.640900: step 1334, loss 0.480751, acc 0.75\n",
      "2018-05-23T03:28:25.079727: step 1335, loss 0.568725, acc 0.71875\n",
      "2018-05-23T03:28:25.526532: step 1336, loss 0.496716, acc 0.734375\n",
      "2018-05-23T03:28:25.969347: step 1337, loss 0.656146, acc 0.640625\n",
      "2018-05-23T03:28:26.405181: step 1338, loss 0.612456, acc 0.65625\n",
      "2018-05-23T03:28:26.854976: step 1339, loss 0.610942, acc 0.6875\n",
      "2018-05-23T03:28:27.294801: step 1340, loss 0.495584, acc 0.734375\n",
      "2018-05-23T03:28:27.776512: step 1341, loss 0.524425, acc 0.734375\n",
      "2018-05-23T03:28:28.239274: step 1342, loss 0.544492, acc 0.6875\n",
      "2018-05-23T03:28:28.691066: step 1343, loss 0.472358, acc 0.75\n",
      "2018-05-23T03:28:29.144852: step 1344, loss 0.583687, acc 0.703125\n",
      "2018-05-23T03:28:29.603624: step 1345, loss 0.51382, acc 0.765625\n",
      "2018-05-23T03:28:30.066386: step 1346, loss 0.549909, acc 0.796875\n",
      "2018-05-23T03:28:30.547100: step 1347, loss 0.57127, acc 0.671875\n",
      "2018-05-23T03:28:31.005872: step 1348, loss 0.515782, acc 0.765625\n",
      "2018-05-23T03:28:31.462650: step 1349, loss 0.628892, acc 0.671875\n",
      "2018-05-23T03:28:31.926411: step 1350, loss 0.479731, acc 0.796875\n",
      "2018-05-23T03:28:32.415102: step 1351, loss 0.635271, acc 0.671875\n",
      "2018-05-23T03:28:32.899806: step 1352, loss 0.5574, acc 0.671875\n",
      "2018-05-23T03:28:33.363566: step 1353, loss 0.595243, acc 0.71875\n",
      "2018-05-23T03:28:33.916087: step 1354, loss 0.596805, acc 0.65625\n",
      "2018-05-23T03:28:34.380844: step 1355, loss 0.556282, acc 0.671875\n",
      "2018-05-23T03:28:34.871532: step 1356, loss 0.548135, acc 0.6875\n",
      "2018-05-23T03:28:35.317339: step 1357, loss 0.616133, acc 0.6875\n",
      "2018-05-23T03:28:35.810020: step 1358, loss 0.484314, acc 0.78125\n",
      "2018-05-23T03:28:36.263806: step 1359, loss 0.433343, acc 0.796875\n",
      "2018-05-23T03:28:36.721583: step 1360, loss 0.564787, acc 0.703125\n",
      "2018-05-23T03:28:37.221245: step 1361, loss 0.501234, acc 0.734375\n",
      "2018-05-23T03:28:37.638129: step 1362, loss 0.498838, acc 0.765625\n",
      "2018-05-23T03:28:38.068979: step 1363, loss 0.380912, acc 0.828125\n",
      "2018-05-23T03:28:38.512791: step 1364, loss 0.64318, acc 0.65625\n",
      "2018-05-23T03:28:38.996496: step 1365, loss 0.574565, acc 0.734375\n",
      "2018-05-23T03:28:39.474218: step 1366, loss 0.686271, acc 0.671875\n",
      "2018-05-23T03:28:39.937978: step 1367, loss 0.552657, acc 0.734375\n",
      "2018-05-23T03:28:40.417694: step 1368, loss 0.415862, acc 0.8125\n",
      "2018-05-23T03:28:40.880456: step 1369, loss 0.619384, acc 0.640625\n",
      "2018-05-23T03:28:41.429987: step 1370, loss 0.603422, acc 0.71875\n",
      "2018-05-23T03:28:41.916689: step 1371, loss 0.554842, acc 0.765625\n",
      "2018-05-23T03:28:42.458827: step 1372, loss 0.667159, acc 0.625\n",
      "2018-05-23T03:28:42.881983: step 1373, loss 0.573298, acc 0.703125\n",
      "2018-05-23T03:28:43.331581: step 1374, loss 0.648201, acc 0.65625\n",
      "2018-05-23T03:28:43.782603: step 1375, loss 0.496459, acc 0.78125\n",
      "2018-05-23T03:28:44.514317: step 1376, loss 0.638378, acc 0.609375\n",
      "2018-05-23T03:28:45.047891: step 1377, loss 0.566061, acc 0.75\n",
      "2018-05-23T03:28:45.518632: step 1378, loss 0.631649, acc 0.671875\n",
      "2018-05-23T03:28:45.904600: step 1379, loss 0.499069, acc 0.75\n",
      "2018-05-23T03:28:46.314501: step 1380, loss 0.546203, acc 0.734375\n",
      "2018-05-23T03:28:46.680523: step 1381, loss 0.653014, acc 0.65625\n",
      "2018-05-23T03:28:47.037568: step 1382, loss 0.442861, acc 0.828125\n",
      "2018-05-23T03:28:47.420542: step 1383, loss 0.656998, acc 0.65625\n",
      "2018-05-23T03:28:47.800526: step 1384, loss 0.705864, acc 0.640625\n",
      "2018-05-23T03:28:48.247333: step 1385, loss 0.609337, acc 0.734375\n",
      "2018-05-23T03:28:48.609362: step 1386, loss 0.450183, acc 0.78125\n",
      "2018-05-23T03:28:48.978375: step 1387, loss 0.457607, acc 0.828125\n",
      "2018-05-23T03:28:49.403240: step 1388, loss 0.506461, acc 0.796875\n",
      "2018-05-23T03:28:49.784219: step 1389, loss 0.524903, acc 0.6875\n",
      "2018-05-23T03:28:50.145254: step 1390, loss 0.674702, acc 0.625\n",
      "2018-05-23T03:28:50.538203: step 1391, loss 0.638849, acc 0.71875\n",
      "2018-05-23T03:28:50.907216: step 1392, loss 0.460303, acc 0.75\n",
      "2018-05-23T03:28:51.277227: step 1393, loss 0.501713, acc 0.75\n",
      "2018-05-23T03:28:51.643246: step 1394, loss 0.567845, acc 0.703125\n",
      "2018-05-23T03:28:52.015251: step 1395, loss 0.532141, acc 0.703125\n",
      "2018-05-23T03:28:52.381271: step 1396, loss 0.546639, acc 0.703125\n",
      "2018-05-23T03:28:52.744302: step 1397, loss 0.503732, acc 0.734375\n",
      "2018-05-23T03:28:53.104338: step 1398, loss 0.670095, acc 0.625\n",
      "2018-05-23T03:28:53.483324: step 1399, loss 0.552818, acc 0.703125\n",
      "2018-05-23T03:28:53.917163: step 1400, loss 0.54142, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:28:58.441061: step 1400, loss 0.546394, acc 0.714816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1400\n",
      "\n",
      "2018-05-23T03:28:59.991913: step 1401, loss 0.612728, acc 0.671875\n",
      "2018-05-23T03:29:00.479608: step 1402, loss 0.542503, acc 0.71875\n",
      "2018-05-23T03:29:01.005201: step 1403, loss 0.502858, acc 0.78125\n",
      "2018-05-23T03:29:01.392168: step 1404, loss 0.453639, acc 0.78125\n",
      "2018-05-23T03:29:01.759184: step 1405, loss 0.517125, acc 0.765625\n",
      "2018-05-23T03:29:02.164102: step 1406, loss 0.596864, acc 0.6875\n",
      "2018-05-23T03:29:02.587967: step 1407, loss 0.681221, acc 0.609375\n",
      "2018-05-23T03:29:03.000863: step 1408, loss 0.502561, acc 0.734375\n",
      "2018-05-23T03:29:03.386831: step 1409, loss 0.598028, acc 0.59375\n",
      "2018-05-23T03:29:03.770803: step 1410, loss 0.703566, acc 0.640625\n",
      "2018-05-23T03:29:04.185693: step 1411, loss 0.653621, acc 0.65625\n",
      "2018-05-23T03:29:04.550717: step 1412, loss 0.543105, acc 0.703125\n",
      "2018-05-23T03:29:04.937685: step 1413, loss 0.52617, acc 0.75\n",
      "2018-05-23T03:29:05.316668: step 1414, loss 0.569367, acc 0.671875\n",
      "2018-05-23T03:29:05.679696: step 1415, loss 0.575316, acc 0.703125\n",
      "2018-05-23T03:29:06.045717: step 1416, loss 0.629858, acc 0.640625\n",
      "2018-05-23T03:29:06.431684: step 1417, loss 0.505581, acc 0.765625\n",
      "2018-05-23T03:29:06.803689: step 1418, loss 0.561602, acc 0.734375\n",
      "2018-05-23T03:29:07.161731: step 1419, loss 0.56232, acc 0.671875\n",
      "2018-05-23T03:29:07.542713: step 1420, loss 0.639709, acc 0.734375\n",
      "2018-05-23T03:29:07.923694: step 1421, loss 0.501164, acc 0.765625\n",
      "2018-05-23T03:29:08.297694: step 1422, loss 0.535243, acc 0.75\n",
      "2018-05-23T03:29:08.656732: step 1423, loss 0.565556, acc 0.75\n",
      "2018-05-23T03:29:09.025745: step 1424, loss 0.550033, acc 0.71875\n",
      "2018-05-23T03:29:09.388773: step 1425, loss 0.54988, acc 0.703125\n",
      "2018-05-23T03:29:09.742826: step 1426, loss 0.622053, acc 0.65625\n",
      "2018-05-23T03:29:10.124804: step 1427, loss 0.51833, acc 0.75\n",
      "2018-05-23T03:29:10.528723: step 1428, loss 0.569137, acc 0.71875\n",
      "2018-05-23T03:29:10.889759: step 1429, loss 0.464448, acc 0.78125\n",
      "2018-05-23T03:29:11.251789: step 1430, loss 0.566832, acc 0.71875\n",
      "2018-05-23T03:29:11.639753: step 1431, loss 0.581295, acc 0.703125\n",
      "2018-05-23T03:29:11.996797: step 1432, loss 0.640154, acc 0.71875\n",
      "2018-05-23T03:29:12.417675: step 1433, loss 0.538797, acc 0.6875\n",
      "2018-05-23T03:29:12.810619: step 1434, loss 0.489732, acc 0.8125\n",
      "2018-05-23T03:29:13.166667: step 1435, loss 0.559, acc 0.8125\n",
      "2018-05-23T03:29:13.609482: step 1436, loss 0.624949, acc 0.6875\n",
      "2018-05-23T03:29:13.983481: step 1437, loss 0.484266, acc 0.828125\n",
      "2018-05-23T03:29:14.318588: step 1438, loss 0.606449, acc 0.65625\n",
      "2018-05-23T03:29:14.701560: step 1439, loss 0.520221, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:29:15.028687: step 1440, loss 0.643988, acc 0.59375\n",
      "2018-05-23T03:29:15.409669: step 1441, loss 0.593403, acc 0.625\n",
      "2018-05-23T03:29:15.764718: step 1442, loss 0.475807, acc 0.734375\n",
      "2018-05-23T03:29:16.093836: step 1443, loss 0.453304, acc 0.8125\n",
      "2018-05-23T03:29:16.418966: step 1444, loss 0.768785, acc 0.65625\n",
      "2018-05-23T03:29:16.756065: step 1445, loss 0.572006, acc 0.703125\n",
      "2018-05-23T03:29:17.125078: step 1446, loss 0.549861, acc 0.71875\n",
      "2018-05-23T03:29:17.570887: step 1447, loss 0.42739, acc 0.828125\n",
      "2018-05-23T03:29:17.954858: step 1448, loss 0.473931, acc 0.75\n",
      "2018-05-23T03:29:18.288964: step 1449, loss 0.517354, acc 0.734375\n",
      "2018-05-23T03:29:18.627060: step 1450, loss 0.542444, acc 0.6875\n",
      "2018-05-23T03:29:19.004052: step 1451, loss 0.671666, acc 0.625\n",
      "2018-05-23T03:29:19.337159: step 1452, loss 0.553827, acc 0.703125\n",
      "2018-05-23T03:29:19.676253: step 1453, loss 0.545654, acc 0.6875\n",
      "2018-05-23T03:29:20.010359: step 1454, loss 0.507155, acc 0.734375\n",
      "2018-05-23T03:29:20.368400: step 1455, loss 0.519805, acc 0.703125\n",
      "2018-05-23T03:29:20.826177: step 1456, loss 0.514744, acc 0.765625\n",
      "2018-05-23T03:29:21.170255: step 1457, loss 0.486605, acc 0.796875\n",
      "2018-05-23T03:29:21.671919: step 1458, loss 0.546061, acc 0.765625\n",
      "2018-05-23T03:29:22.016990: step 1459, loss 0.554835, acc 0.75\n",
      "2018-05-23T03:29:22.356085: step 1460, loss 0.520587, acc 0.71875\n",
      "2018-05-23T03:29:22.693181: step 1461, loss 0.536507, acc 0.734375\n",
      "2018-05-23T03:29:23.029285: step 1462, loss 0.632864, acc 0.578125\n",
      "2018-05-23T03:29:23.355410: step 1463, loss 0.699228, acc 0.578125\n",
      "2018-05-23T03:29:23.701484: step 1464, loss 0.581908, acc 0.71875\n",
      "2018-05-23T03:29:24.036588: step 1465, loss 0.513247, acc 0.75\n",
      "2018-05-23T03:29:24.377675: step 1466, loss 0.481615, acc 0.75\n",
      "2018-05-23T03:29:24.742698: step 1467, loss 0.582513, acc 0.734375\n",
      "2018-05-23T03:29:25.125677: step 1468, loss 0.630523, acc 0.703125\n",
      "2018-05-23T03:29:25.473778: step 1469, loss 0.564559, acc 0.6875\n",
      "2018-05-23T03:29:25.815828: step 1470, loss 0.553784, acc 0.65625\n",
      "2018-05-23T03:29:26.162901: step 1471, loss 0.513705, acc 0.765625\n",
      "2018-05-23T03:29:26.495013: step 1472, loss 0.55225, acc 0.71875\n",
      "2018-05-23T03:29:26.831111: step 1473, loss 0.624256, acc 0.625\n",
      "2018-05-23T03:29:27.169208: step 1474, loss 0.606674, acc 0.671875\n",
      "2018-05-23T03:29:27.501318: step 1475, loss 0.581013, acc 0.71875\n",
      "2018-05-23T03:29:27.829443: step 1476, loss 0.409219, acc 0.875\n",
      "2018-05-23T03:29:28.164544: step 1477, loss 0.53013, acc 0.71875\n",
      "2018-05-23T03:29:28.500647: step 1478, loss 0.563406, acc 0.65625\n",
      "2018-05-23T03:29:28.833754: step 1479, loss 0.602299, acc 0.75\n",
      "2018-05-23T03:29:29.164869: step 1480, loss 0.509555, acc 0.734375\n",
      "2018-05-23T03:29:29.494988: step 1481, loss 0.542738, acc 0.703125\n",
      "2018-05-23T03:29:29.825103: step 1482, loss 0.578301, acc 0.71875\n",
      "2018-05-23T03:29:30.157215: step 1483, loss 0.503959, acc 0.765625\n",
      "2018-05-23T03:29:30.518248: step 1484, loss 0.496036, acc 0.8125\n",
      "2018-05-23T03:29:30.850361: step 1485, loss 0.496737, acc 0.765625\n",
      "2018-05-23T03:29:31.192444: step 1486, loss 0.581533, acc 0.734375\n",
      "2018-05-23T03:29:31.525555: step 1487, loss 0.397841, acc 0.8125\n",
      "2018-05-23T03:29:31.856667: step 1488, loss 0.510747, acc 0.78125\n",
      "2018-05-23T03:29:32.195762: step 1489, loss 0.685145, acc 0.625\n",
      "2018-05-23T03:29:32.525877: step 1490, loss 0.488069, acc 0.734375\n",
      "2018-05-23T03:29:32.869957: step 1491, loss 0.465493, acc 0.796875\n",
      "2018-05-23T03:29:33.208053: step 1492, loss 0.50732, acc 0.703125\n",
      "2018-05-23T03:29:33.547146: step 1493, loss 0.48695, acc 0.765625\n",
      "2018-05-23T03:29:33.886239: step 1494, loss 0.637444, acc 0.671875\n",
      "2018-05-23T03:29:34.232313: step 1495, loss 0.523182, acc 0.703125\n",
      "2018-05-23T03:29:34.558440: step 1496, loss 0.562591, acc 0.703125\n",
      "2018-05-23T03:29:34.892548: step 1497, loss 0.584545, acc 0.703125\n",
      "2018-05-23T03:29:35.222665: step 1498, loss 0.563923, acc 0.703125\n",
      "2018-05-23T03:29:35.557766: step 1499, loss 0.473487, acc 0.796875\n",
      "2018-05-23T03:29:35.895864: step 1500, loss 0.500198, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:29:40.638178: step 1500, loss 0.540369, acc 0.720532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1500\n",
      "\n",
      "2018-05-23T03:29:41.884841: step 1501, loss 0.404042, acc 0.765625\n",
      "2018-05-23T03:29:42.374532: step 1502, loss 0.621441, acc 0.640625\n",
      "2018-05-23T03:29:42.737561: step 1503, loss 0.663874, acc 0.671875\n",
      "2018-05-23T03:29:43.079646: step 1504, loss 0.599074, acc 0.671875\n",
      "2018-05-23T03:29:43.433696: step 1505, loss 0.809943, acc 0.59375\n",
      "2018-05-23T03:29:43.769800: step 1506, loss 0.580114, acc 0.71875\n",
      "2018-05-23T03:29:44.098919: step 1507, loss 0.468439, acc 0.75\n",
      "2018-05-23T03:29:44.425045: step 1508, loss 0.49428, acc 0.75\n",
      "2018-05-23T03:29:44.757156: step 1509, loss 0.564971, acc 0.65625\n",
      "2018-05-23T03:29:45.089269: step 1510, loss 0.580321, acc 0.703125\n",
      "2018-05-23T03:29:45.474239: step 1511, loss 0.584779, acc 0.75\n",
      "2018-05-23T03:29:45.808345: step 1512, loss 0.535577, acc 0.765625\n",
      "2018-05-23T03:29:46.140456: step 1513, loss 0.529738, acc 0.71875\n",
      "2018-05-23T03:29:46.472567: step 1514, loss 0.671205, acc 0.59375\n",
      "2018-05-23T03:29:46.815652: step 1515, loss 0.552742, acc 0.75\n",
      "2018-05-23T03:29:47.146765: step 1516, loss 0.529346, acc 0.734375\n",
      "2018-05-23T03:29:47.478877: step 1517, loss 0.562085, acc 0.6875\n",
      "2018-05-23T03:29:47.819962: step 1518, loss 0.570087, acc 0.71875\n",
      "2018-05-23T03:29:48.144098: step 1519, loss 0.487878, acc 0.765625\n",
      "2018-05-23T03:29:48.483189: step 1520, loss 0.479759, acc 0.828125\n",
      "2018-05-23T03:29:48.816298: step 1521, loss 0.647144, acc 0.671875\n",
      "2018-05-23T03:29:49.148409: step 1522, loss 0.559469, acc 0.703125\n",
      "2018-05-23T03:29:49.493485: step 1523, loss 0.560769, acc 0.6875\n",
      "2018-05-23T03:29:49.822606: step 1524, loss 0.477107, acc 0.75\n",
      "2018-05-23T03:29:50.161699: step 1525, loss 0.530014, acc 0.6875\n",
      "2018-05-23T03:29:50.604516: step 1526, loss 0.479868, acc 0.765625\n",
      "2018-05-23T03:29:51.007435: step 1527, loss 0.540318, acc 0.703125\n",
      "2018-05-23T03:29:51.390412: step 1528, loss 0.440462, acc 0.796875\n",
      "2018-05-23T03:29:51.797323: step 1529, loss 0.561355, acc 0.6875\n",
      "2018-05-23T03:29:52.185285: step 1530, loss 0.437275, acc 0.875\n",
      "2018-05-23T03:29:52.545324: step 1531, loss 0.466261, acc 0.78125\n",
      "2018-05-23T03:29:52.941263: step 1532, loss 0.570427, acc 0.75\n",
      "2018-05-23T03:29:53.340197: step 1533, loss 0.574511, acc 0.703125\n",
      "2018-05-23T03:29:53.741122: step 1534, loss 0.521119, acc 0.703125\n",
      "2018-05-23T03:29:54.115123: step 1535, loss 0.586992, acc 0.703125\n",
      "2018-05-23T03:29:54.492115: step 1536, loss 0.750505, acc 0.578125\n",
      "2018-05-23T03:29:54.838188: step 1537, loss 0.563713, acc 0.703125\n",
      "2018-05-23T03:29:55.179277: step 1538, loss 0.665158, acc 0.59375\n",
      "2018-05-23T03:29:55.528342: step 1539, loss 0.528749, acc 0.71875\n",
      "2018-05-23T03:29:55.876412: step 1540, loss 0.525735, acc 0.71875\n",
      "2018-05-23T03:29:56.236447: step 1541, loss 0.640194, acc 0.609375\n",
      "2018-05-23T03:29:56.594490: step 1542, loss 0.483446, acc 0.796875\n",
      "2018-05-23T03:29:56.926600: step 1543, loss 0.508071, acc 0.71875\n",
      "2018-05-23T03:29:57.263698: step 1544, loss 0.5179, acc 0.765625\n",
      "2018-05-23T03:29:57.588831: step 1545, loss 0.585767, acc 0.6875\n",
      "2018-05-23T03:29:57.936900: step 1546, loss 0.62847, acc 0.671875\n",
      "2018-05-23T03:29:58.284967: step 1547, loss 0.540204, acc 0.703125\n",
      "2018-05-23T03:29:58.651984: step 1548, loss 0.710374, acc 0.625\n",
      "2018-05-23T03:29:58.992075: step 1549, loss 0.479976, acc 0.796875\n",
      "2018-05-23T03:29:59.396465: step 1550, loss 0.443781, acc 0.796875\n",
      "2018-05-23T03:29:59.786422: step 1551, loss 0.507476, acc 0.71875\n",
      "2018-05-23T03:30:00.204306: step 1552, loss 0.433655, acc 0.859375\n",
      "2018-05-23T03:30:00.586283: step 1553, loss 0.645535, acc 0.703125\n",
      "2018-05-23T03:30:01.055029: step 1554, loss 0.568426, acc 0.65625\n",
      "2018-05-23T03:30:01.546715: step 1555, loss 0.556877, acc 0.71875\n",
      "2018-05-23T03:30:02.076297: step 1556, loss 0.615205, acc 0.640625\n",
      "2018-05-23T03:30:02.943979: step 1557, loss 0.523949, acc 0.703125\n",
      "2018-05-23T03:30:03.387788: step 1558, loss 0.526122, acc 0.671875\n",
      "2018-05-23T03:30:03.755803: step 1559, loss 0.48812, acc 0.765625\n",
      "2018-05-23T03:30:04.134823: step 1560, loss 0.564098, acc 0.75\n",
      "2018-05-23T03:30:04.475878: step 1561, loss 0.621027, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:30:04.854864: step 1562, loss 0.710336, acc 0.625\n",
      "2018-05-23T03:30:05.191962: step 1563, loss 0.593057, acc 0.671875\n",
      "2018-05-23T03:30:05.534048: step 1564, loss 0.494238, acc 0.78125\n",
      "2018-05-23T03:30:05.989827: step 1565, loss 0.763622, acc 0.59375\n",
      "2018-05-23T03:30:06.399732: step 1566, loss 0.532065, acc 0.75\n",
      "2018-05-23T03:30:06.807640: step 1567, loss 0.575074, acc 0.703125\n",
      "2018-05-23T03:30:07.178647: step 1568, loss 0.666525, acc 0.6875\n",
      "2018-05-23T03:30:07.719201: step 1569, loss 0.547067, acc 0.71875\n",
      "2018-05-23T03:30:08.274714: step 1570, loss 0.548574, acc 0.796875\n",
      "2018-05-23T03:30:08.660682: step 1571, loss 0.593502, acc 0.671875\n",
      "2018-05-23T03:30:09.106489: step 1572, loss 0.661542, acc 0.640625\n",
      "2018-05-23T03:30:09.463536: step 1573, loss 0.572002, acc 0.625\n",
      "2018-05-23T03:30:09.835539: step 1574, loss 0.557573, acc 0.65625\n",
      "2018-05-23T03:30:10.219512: step 1575, loss 0.574256, acc 0.65625\n",
      "2018-05-23T03:30:10.587527: step 1576, loss 0.484054, acc 0.78125\n",
      "2018-05-23T03:30:10.953548: step 1577, loss 0.551191, acc 0.6875\n",
      "2018-05-23T03:30:11.382401: step 1578, loss 0.60231, acc 0.640625\n",
      "2018-05-23T03:30:11.850151: step 1579, loss 0.543768, acc 0.703125\n",
      "2018-05-23T03:30:12.441569: step 1580, loss 0.62788, acc 0.671875\n",
      "2018-05-23T03:30:12.887374: step 1581, loss 0.565986, acc 0.71875\n",
      "2018-05-23T03:30:13.318223: step 1582, loss 0.44172, acc 0.78125\n",
      "2018-05-23T03:30:13.672276: step 1583, loss 0.562741, acc 0.6875\n",
      "2018-05-23T03:30:14.051262: step 1584, loss 0.695919, acc 0.671875\n",
      "2018-05-23T03:30:14.436231: step 1585, loss 0.528604, acc 0.734375\n",
      "2018-05-23T03:30:14.779315: step 1586, loss 0.530241, acc 0.765625\n",
      "2018-05-23T03:30:15.199192: step 1587, loss 0.479726, acc 0.8125\n",
      "2018-05-23T03:30:15.537286: step 1588, loss 0.514165, acc 0.71875\n",
      "2018-05-23T03:30:15.891340: step 1589, loss 0.547328, acc 0.71875\n",
      "2018-05-23T03:30:16.305233: step 1590, loss 0.492445, acc 0.71875\n",
      "2018-05-23T03:30:16.694191: step 1591, loss 0.49041, acc 0.78125\n",
      "2018-05-23T03:30:17.039268: step 1592, loss 0.529831, acc 0.75\n",
      "2018-05-23T03:30:17.387337: step 1593, loss 0.4385, acc 0.796875\n",
      "2018-05-23T03:30:17.733411: step 1594, loss 0.594055, acc 0.59375\n",
      "2018-05-23T03:30:18.078488: step 1595, loss 0.506135, acc 0.71875\n",
      "2018-05-23T03:30:18.416584: step 1596, loss 0.566553, acc 0.6875\n",
      "2018-05-23T03:30:18.786593: step 1597, loss 0.54948, acc 0.703125\n",
      "2018-05-23T03:30:19.144635: step 1598, loss 0.547937, acc 0.65625\n",
      "2018-05-23T03:30:19.481734: step 1599, loss 0.561733, acc 0.71875\n",
      "2018-05-23T03:30:19.828805: step 1600, loss 0.575912, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:30:25.350035: step 1600, loss 0.560571, acc 0.706244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1600\n",
      "\n",
      "2018-05-23T03:30:27.246600: step 1601, loss 0.568279, acc 0.625\n",
      "2018-05-23T03:30:27.710358: step 1602, loss 0.626787, acc 0.703125\n",
      "2018-05-23T03:30:28.142202: step 1603, loss 0.592496, acc 0.71875\n",
      "2018-05-23T03:30:28.566069: step 1604, loss 0.419018, acc 0.84375\n",
      "2018-05-23T03:30:28.990932: step 1605, loss 0.486477, acc 0.796875\n",
      "2018-05-23T03:30:29.397843: step 1606, loss 0.435429, acc 0.78125\n",
      "2018-05-23T03:30:29.751896: step 1607, loss 0.481184, acc 0.71875\n",
      "2018-05-23T03:30:30.208674: step 1608, loss 0.594097, acc 0.65625\n",
      "2018-05-23T03:30:30.559736: step 1609, loss 0.584978, acc 0.671875\n",
      "2018-05-23T03:30:30.925757: step 1610, loss 0.543572, acc 0.75\n",
      "2018-05-23T03:30:31.301751: step 1611, loss 0.416608, acc 0.8125\n",
      "2018-05-23T03:30:31.691707: step 1612, loss 0.47105, acc 0.84375\n",
      "2018-05-23T03:30:32.059722: step 1613, loss 0.608065, acc 0.65625\n",
      "2018-05-23T03:30:32.490572: step 1614, loss 0.51033, acc 0.71875\n",
      "2018-05-23T03:30:33.042095: step 1615, loss 0.506926, acc 0.765625\n",
      "2018-05-23T03:30:33.469950: step 1616, loss 0.588358, acc 0.6875\n",
      "2018-05-23T03:30:33.867887: step 1617, loss 0.508519, acc 0.796875\n",
      "2018-05-23T03:30:34.233906: step 1618, loss 0.669811, acc 0.71875\n",
      "2018-05-23T03:30:34.602920: step 1619, loss 0.501405, acc 0.765625\n",
      "2018-05-23T03:30:35.030775: step 1620, loss 0.446609, acc 0.75\n",
      "2018-05-23T03:30:35.522461: step 1621, loss 0.733276, acc 0.578125\n",
      "2018-05-23T03:30:35.979237: step 1622, loss 0.51212, acc 0.75\n",
      "2018-05-23T03:30:36.450976: step 1623, loss 0.478404, acc 0.78125\n",
      "2018-05-23T03:30:36.838937: step 1624, loss 0.643979, acc 0.734375\n",
      "2018-05-23T03:30:37.333616: step 1625, loss 0.477787, acc 0.734375\n",
      "2018-05-23T03:30:37.718585: step 1626, loss 0.463259, acc 0.765625\n",
      "2018-05-23T03:30:38.104551: step 1627, loss 0.536765, acc 0.734375\n",
      "2018-05-23T03:30:38.475559: step 1628, loss 0.61887, acc 0.71875\n",
      "2018-05-23T03:30:38.838590: step 1629, loss 0.578555, acc 0.71875\n",
      "2018-05-23T03:30:39.198626: step 1630, loss 0.410837, acc 0.765625\n",
      "2018-05-23T03:30:39.571627: step 1631, loss 0.551041, acc 0.671875\n",
      "2018-05-23T03:30:39.931665: step 1632, loss 0.611776, acc 0.671875\n",
      "2018-05-23T03:30:40.404399: step 1633, loss 0.602791, acc 0.640625\n",
      "2018-05-23T03:30:41.110511: step 1634, loss 0.461631, acc 0.765625\n",
      "2018-05-23T03:30:41.561305: step 1635, loss 0.592456, acc 0.671875\n",
      "2018-05-23T03:30:42.015091: step 1636, loss 0.437722, acc 0.796875\n",
      "2018-05-23T03:30:42.567618: step 1637, loss 0.559522, acc 0.671875\n",
      "2018-05-23T03:30:43.043339: step 1638, loss 0.564851, acc 0.765625\n",
      "2018-05-23T03:30:43.469200: step 1639, loss 0.54016, acc 0.703125\n",
      "2018-05-23T03:30:43.896058: step 1640, loss 0.483711, acc 0.71875\n",
      "2018-05-23T03:30:44.299978: step 1641, loss 0.4091, acc 0.84375\n",
      "2018-05-23T03:30:45.090863: step 1642, loss 0.532861, acc 0.734375\n",
      "2018-05-23T03:30:45.751095: step 1643, loss 0.511222, acc 0.71875\n",
      "2018-05-23T03:30:46.401358: step 1644, loss 0.457195, acc 0.75\n",
      "2018-05-23T03:30:46.947894: step 1645, loss 0.424041, acc 0.875\n",
      "2018-05-23T03:30:47.477478: step 1646, loss 0.538423, acc 0.71875\n",
      "2018-05-23T03:30:48.076398: step 1647, loss 0.628701, acc 0.65625\n",
      "2018-05-23T03:30:48.525197: step 1648, loss 0.512653, acc 0.703125\n",
      "2018-05-23T03:30:48.989953: step 1649, loss 0.551437, acc 0.734375\n",
      "2018-05-23T03:30:49.422796: step 1650, loss 0.448593, acc 0.78125\n",
      "2018-05-23T03:30:49.912503: step 1651, loss 0.610783, acc 0.65625\n",
      "2018-05-23T03:30:50.468001: step 1652, loss 0.553963, acc 0.75\n",
      "2018-05-23T03:30:50.946723: step 1653, loss 0.462493, acc 0.765625\n",
      "2018-05-23T03:30:51.487273: step 1654, loss 0.51005, acc 0.75\n",
      "2018-05-23T03:30:51.947044: step 1655, loss 0.482692, acc 0.78125\n",
      "2018-05-23T03:30:52.365923: step 1656, loss 0.451822, acc 0.75\n",
      "2018-05-23T03:30:52.777821: step 1657, loss 0.542149, acc 0.75\n",
      "2018-05-23T03:30:53.207670: step 1658, loss 0.558692, acc 0.703125\n",
      "2018-05-23T03:30:53.630538: step 1659, loss 0.547485, acc 0.71875\n",
      "2018-05-23T03:30:54.029472: step 1660, loss 0.538276, acc 0.6875\n",
      "2018-05-23T03:30:54.424415: step 1661, loss 0.632112, acc 0.6875\n",
      "2018-05-23T03:30:54.968958: step 1662, loss 0.571231, acc 0.71875\n",
      "2018-05-23T03:30:55.557385: step 1663, loss 0.602094, acc 0.765625\n",
      "2018-05-23T03:30:56.174733: step 1664, loss 0.437329, acc 0.796875\n",
      "2018-05-23T03:30:56.776124: step 1665, loss 0.607564, acc 0.640625\n",
      "2018-05-23T03:30:57.349591: step 1666, loss 0.53267, acc 0.75\n",
      "2018-05-23T03:30:57.866208: step 1667, loss 0.496697, acc 0.75\n",
      "2018-05-23T03:30:58.294063: step 1668, loss 0.4944, acc 0.71875\n",
      "2018-05-23T03:30:58.841599: step 1669, loss 0.555857, acc 0.703125\n",
      "2018-05-23T03:30:59.300371: step 1670, loss 0.558671, acc 0.671875\n",
      "2018-05-23T03:30:59.896775: step 1671, loss 0.442727, acc 0.796875\n",
      "2018-05-23T03:31:00.347570: step 1672, loss 0.521056, acc 0.75\n",
      "2018-05-23T03:31:00.906392: step 1673, loss 0.619031, acc 0.703125\n",
      "2018-05-23T03:31:01.308318: step 1674, loss 0.542244, acc 0.734375\n",
      "2018-05-23T03:31:01.997727: step 1675, loss 0.587804, acc 0.6875\n",
      "2018-05-23T03:31:02.952173: step 1676, loss 0.546728, acc 0.71875\n",
      "2018-05-23T03:31:03.508685: step 1677, loss 0.503036, acc 0.78125\n",
      "2018-05-23T03:31:04.256683: step 1678, loss 0.535766, acc 0.734375\n",
      "2018-05-23T03:31:05.037594: step 1679, loss 0.593324, acc 0.640625\n",
      "2018-05-23T03:31:05.938185: step 1680, loss 0.656753, acc 0.65625\n",
      "2018-05-23T03:31:06.574484: step 1681, loss 0.581203, acc 0.71875\n",
      "2018-05-23T03:31:07.338439: step 1682, loss 0.555675, acc 0.671875\n",
      "2018-05-23T03:31:07.938833: step 1683, loss 0.462603, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:31:08.521273: step 1684, loss 0.499461, acc 0.828125\n",
      "2018-05-23T03:31:09.071802: step 1685, loss 0.407676, acc 0.78125\n",
      "2018-05-23T03:31:09.594404: step 1686, loss 0.588328, acc 0.71875\n",
      "2018-05-23T03:31:10.050184: step 1687, loss 0.491045, acc 0.765625\n",
      "2018-05-23T03:31:10.520925: step 1688, loss 0.584986, acc 0.703125\n",
      "2018-05-23T03:31:11.029566: step 1689, loss 0.545846, acc 0.65625\n",
      "2018-05-23T03:31:11.514267: step 1690, loss 0.464932, acc 0.765625\n",
      "2018-05-23T03:31:11.963066: step 1691, loss 0.481642, acc 0.734375\n",
      "2018-05-23T03:31:12.376959: step 1692, loss 0.568919, acc 0.734375\n",
      "2018-05-23T03:31:12.770906: step 1693, loss 0.416233, acc 0.859375\n",
      "2018-05-23T03:31:13.176819: step 1694, loss 0.434427, acc 0.796875\n",
      "2018-05-23T03:31:13.628610: step 1695, loss 0.467568, acc 0.765625\n",
      "2018-05-23T03:31:14.038514: step 1696, loss 0.500353, acc 0.8125\n",
      "2018-05-23T03:31:14.455399: step 1697, loss 0.518367, acc 0.71875\n",
      "2018-05-23T03:31:14.877270: step 1698, loss 0.412686, acc 0.828125\n",
      "2018-05-23T03:31:15.347013: step 1699, loss 0.508453, acc 0.71875\n",
      "2018-05-23T03:31:15.816757: step 1700, loss 0.691737, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:31:21.836653: step 1700, loss 0.539809, acc 0.721103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1700\n",
      "\n",
      "2018-05-23T03:31:23.623873: step 1701, loss 0.772237, acc 0.59375\n",
      "2018-05-23T03:31:24.081646: step 1702, loss 0.442921, acc 0.796875\n",
      "2018-05-23T03:31:24.656112: step 1703, loss 0.689333, acc 0.65625\n",
      "2018-05-23T03:31:25.081972: step 1704, loss 0.644071, acc 0.671875\n",
      "2018-05-23T03:31:25.536755: step 1705, loss 0.540407, acc 0.65625\n",
      "2018-05-23T03:31:26.035421: step 1706, loss 0.489512, acc 0.765625\n",
      "2018-05-23T03:31:26.598913: step 1707, loss 0.569033, acc 0.765625\n",
      "2018-05-23T03:31:27.094587: step 1708, loss 0.557632, acc 0.75\n",
      "2018-05-23T03:31:27.522442: step 1709, loss 0.513414, acc 0.671875\n",
      "2018-05-23T03:31:27.975232: step 1710, loss 0.48981, acc 0.734375\n",
      "2018-05-23T03:31:28.444974: step 1711, loss 0.493667, acc 0.75\n",
      "2018-05-23T03:31:28.914718: step 1712, loss 0.576673, acc 0.671875\n",
      "2018-05-23T03:31:29.317639: step 1713, loss 0.414296, acc 0.8125\n",
      "2018-05-23T03:31:29.808327: step 1714, loss 0.560657, acc 0.703125\n",
      "2018-05-23T03:31:30.361849: step 1715, loss 0.592552, acc 0.609375\n",
      "2018-05-23T03:31:31.140763: step 1716, loss 0.536691, acc 0.734375\n",
      "2018-05-23T03:31:32.134615: step 1717, loss 0.540896, acc 0.734375\n",
      "2018-05-23T03:31:32.800831: step 1718, loss 0.417791, acc 0.859375\n",
      "2018-05-23T03:31:33.283543: step 1719, loss 0.574809, acc 0.6875\n",
      "2018-05-23T03:31:33.748298: step 1720, loss 0.488961, acc 0.703125\n",
      "2018-05-23T03:31:34.166179: step 1721, loss 0.574688, acc 0.6875\n",
      "2018-05-23T03:31:34.571098: step 1722, loss 0.519731, acc 0.765625\n",
      "2018-05-23T03:31:34.957064: step 1723, loss 0.526927, acc 0.75\n",
      "2018-05-23T03:31:35.346023: step 1724, loss 0.562575, acc 0.703125\n",
      "2018-05-23T03:31:35.768891: step 1725, loss 0.389374, acc 0.765625\n",
      "2018-05-23T03:31:36.160842: step 1726, loss 0.497506, acc 0.734375\n",
      "2018-05-23T03:31:36.552794: step 1727, loss 0.483714, acc 0.78125\n",
      "2018-05-23T03:31:36.959706: step 1728, loss 0.406266, acc 0.828125\n",
      "2018-05-23T03:31:37.353652: step 1729, loss 0.612353, acc 0.65625\n",
      "2018-05-23T03:31:37.752585: step 1730, loss 0.568736, acc 0.71875\n",
      "2018-05-23T03:31:38.148525: step 1731, loss 0.632549, acc 0.6875\n",
      "2018-05-23T03:31:38.538482: step 1732, loss 0.510802, acc 0.75\n",
      "2018-05-23T03:31:38.940407: step 1733, loss 0.554662, acc 0.703125\n",
      "2018-05-23T03:31:39.332359: step 1734, loss 0.4715, acc 0.78125\n",
      "2018-05-23T03:31:39.754231: step 1735, loss 0.504328, acc 0.75\n",
      "2018-05-23T03:31:40.132218: step 1736, loss 0.500201, acc 0.75\n",
      "2018-05-23T03:31:40.521180: step 1737, loss 0.672576, acc 0.640625\n",
      "2018-05-23T03:31:40.919114: step 1738, loss 0.574358, acc 0.71875\n",
      "2018-05-23T03:31:41.305082: step 1739, loss 0.716954, acc 0.625\n",
      "2018-05-23T03:31:41.700026: step 1740, loss 0.641106, acc 0.671875\n",
      "2018-05-23T03:31:42.104942: step 1741, loss 0.502369, acc 0.765625\n",
      "2018-05-23T03:31:42.499885: step 1742, loss 0.504747, acc 0.765625\n",
      "2018-05-23T03:31:42.892835: step 1743, loss 0.630683, acc 0.65625\n",
      "2018-05-23T03:31:43.285782: step 1744, loss 0.56925, acc 0.671875\n",
      "2018-05-23T03:31:43.673744: step 1745, loss 0.549294, acc 0.6875\n",
      "2018-05-23T03:31:44.049739: step 1746, loss 0.500454, acc 0.796875\n",
      "2018-05-23T03:31:44.455654: step 1747, loss 0.539761, acc 0.734375\n",
      "2018-05-23T03:31:44.848603: step 1748, loss 0.526453, acc 0.765625\n",
      "2018-05-23T03:31:45.246539: step 1749, loss 0.394526, acc 0.8125\n",
      "2018-05-23T03:31:45.632504: step 1750, loss 0.424411, acc 0.828125\n",
      "2018-05-23T03:31:46.039417: step 1751, loss 0.669954, acc 0.640625\n",
      "2018-05-23T03:31:46.441341: step 1752, loss 0.54253, acc 0.6875\n",
      "2018-05-23T03:31:46.842269: step 1753, loss 0.527467, acc 0.703125\n",
      "2018-05-23T03:31:47.250178: step 1754, loss 0.548397, acc 0.75\n",
      "2018-05-23T03:31:47.665067: step 1755, loss 0.528066, acc 0.6875\n",
      "2018-05-23T03:31:48.073973: step 1756, loss 0.561603, acc 0.71875\n",
      "2018-05-23T03:31:48.459939: step 1757, loss 0.490665, acc 0.78125\n",
      "2018-05-23T03:31:48.858873: step 1758, loss 0.58557, acc 0.71875\n",
      "2018-05-23T03:31:49.247832: step 1759, loss 0.474958, acc 0.765625\n",
      "2018-05-23T03:31:49.636793: step 1760, loss 0.660347, acc 0.640625\n",
      "2018-05-23T03:31:50.046696: step 1761, loss 0.479524, acc 0.71875\n",
      "2018-05-23T03:31:50.446625: step 1762, loss 0.590594, acc 0.75\n",
      "2018-05-23T03:31:50.876476: step 1763, loss 0.48, acc 0.765625\n",
      "2018-05-23T03:31:51.313307: step 1764, loss 0.518231, acc 0.703125\n",
      "2018-05-23T03:31:51.702267: step 1765, loss 0.678699, acc 0.703125\n",
      "2018-05-23T03:31:52.101200: step 1766, loss 0.533071, acc 0.734375\n",
      "2018-05-23T03:31:52.497139: step 1767, loss 0.51359, acc 0.8125\n",
      "2018-05-23T03:31:52.892086: step 1768, loss 0.440157, acc 0.78125\n",
      "2018-05-23T03:31:53.293011: step 1769, loss 0.52382, acc 0.734375\n",
      "2018-05-23T03:31:53.688952: step 1770, loss 0.545862, acc 0.703125\n",
      "2018-05-23T03:31:54.097858: step 1771, loss 0.558094, acc 0.734375\n",
      "2018-05-23T03:31:54.498785: step 1772, loss 0.499998, acc 0.765625\n",
      "2018-05-23T03:31:54.895724: step 1773, loss 0.579186, acc 0.65625\n",
      "2018-05-23T03:31:55.285681: step 1774, loss 0.583885, acc 0.703125\n",
      "2018-05-23T03:31:55.680625: step 1775, loss 0.586887, acc 0.6875\n",
      "2018-05-23T03:31:56.094517: step 1776, loss 0.54636, acc 0.671875\n",
      "2018-05-23T03:31:56.490458: step 1777, loss 0.572446, acc 0.734375\n",
      "2018-05-23T03:31:56.894378: step 1778, loss 0.654143, acc 0.640625\n",
      "2018-05-23T03:31:57.293311: step 1779, loss 0.636747, acc 0.6875\n",
      "2018-05-23T03:31:57.692242: step 1780, loss 0.573351, acc 0.71875\n",
      "2018-05-23T03:31:58.078209: step 1781, loss 0.574602, acc 0.71875\n",
      "2018-05-23T03:31:58.537979: step 1782, loss 0.622535, acc 0.640625\n",
      "2018-05-23T03:31:58.937909: step 1783, loss 0.54673, acc 0.71875\n",
      "2018-05-23T03:31:59.328864: step 1784, loss 0.659474, acc 0.609375\n",
      "2018-05-23T03:31:59.715830: step 1785, loss 0.565474, acc 0.8125\n",
      "2018-05-23T03:32:00.123738: step 1786, loss 0.506249, acc 0.78125\n",
      "2018-05-23T03:32:00.563561: step 1787, loss 0.437151, acc 0.765625\n",
      "2018-05-23T03:32:01.025326: step 1788, loss 0.416578, acc 0.828125\n",
      "2018-05-23T03:32:01.480109: step 1789, loss 0.57541, acc 0.65625\n",
      "2018-05-23T03:32:01.951848: step 1790, loss 0.485702, acc 0.796875\n",
      "2018-05-23T03:32:02.387681: step 1791, loss 0.549123, acc 0.6875\n",
      "2018-05-23T03:32:02.813541: step 1792, loss 0.424012, acc 0.84375\n",
      "2018-05-23T03:32:03.211479: step 1793, loss 0.516765, acc 0.78125\n",
      "2018-05-23T03:32:03.603429: step 1794, loss 0.56175, acc 0.71875\n",
      "2018-05-23T03:32:03.967458: step 1795, loss 0.565302, acc 0.671875\n",
      "2018-05-23T03:32:04.405282: step 1796, loss 0.471471, acc 0.765625\n",
      "2018-05-23T03:32:04.766318: step 1797, loss 0.488999, acc 0.78125\n",
      "2018-05-23T03:32:05.132339: step 1798, loss 0.504259, acc 0.71875\n",
      "2018-05-23T03:32:05.483402: step 1799, loss 0.57828, acc 0.71875\n",
      "2018-05-23T03:32:05.835458: step 1800, loss 0.522072, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:32:10.489008: step 1800, loss 0.539145, acc 0.722532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1800\n",
      "\n",
      "2018-05-23T03:32:11.957080: step 1801, loss 0.575562, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:32:12.369976: step 1802, loss 0.474975, acc 0.734375\n",
      "2018-05-23T03:32:12.760930: step 1803, loss 0.549118, acc 0.703125\n",
      "2018-05-23T03:32:13.161857: step 1804, loss 0.466178, acc 0.765625\n",
      "2018-05-23T03:32:13.527881: step 1805, loss 0.563999, acc 0.71875\n",
      "2018-05-23T03:32:13.895895: step 1806, loss 0.431706, acc 0.796875\n",
      "2018-05-23T03:32:14.252939: step 1807, loss 0.483759, acc 0.78125\n",
      "2018-05-23T03:32:14.601009: step 1808, loss 0.464376, acc 0.734375\n",
      "2018-05-23T03:32:14.972017: step 1809, loss 0.529603, acc 0.78125\n",
      "2018-05-23T03:32:15.336043: step 1810, loss 0.637047, acc 0.65625\n",
      "2018-05-23T03:32:15.680120: step 1811, loss 0.702661, acc 0.6875\n",
      "2018-05-23T03:32:16.021209: step 1812, loss 0.44728, acc 0.796875\n",
      "2018-05-23T03:32:16.381246: step 1813, loss 0.522662, acc 0.6875\n",
      "2018-05-23T03:32:16.730313: step 1814, loss 0.57004, acc 0.71875\n",
      "2018-05-23T03:32:17.079379: step 1815, loss 0.604594, acc 0.703125\n",
      "2018-05-23T03:32:17.431436: step 1816, loss 0.416224, acc 0.859375\n",
      "2018-05-23T03:32:17.777510: step 1817, loss 0.472058, acc 0.78125\n",
      "2018-05-23T03:32:18.161216: step 1818, loss 0.5047, acc 0.671875\n",
      "2018-05-23T03:32:18.517263: step 1819, loss 0.45923, acc 0.875\n",
      "2018-05-23T03:32:18.866331: step 1820, loss 0.491579, acc 0.796875\n",
      "2018-05-23T03:32:19.222376: step 1821, loss 0.482576, acc 0.78125\n",
      "2018-05-23T03:32:19.583411: step 1822, loss 0.581989, acc 0.765625\n",
      "2018-05-23T03:32:19.933473: step 1823, loss 0.458061, acc 0.796875\n",
      "2018-05-23T03:32:20.298499: step 1824, loss 0.610807, acc 0.65625\n",
      "2018-05-23T03:32:20.686459: step 1825, loss 0.490271, acc 0.8125\n",
      "2018-05-23T03:32:21.035526: step 1826, loss 0.563924, acc 0.6875\n",
      "2018-05-23T03:32:21.374935: step 1827, loss 0.63746, acc 0.703125\n",
      "2018-05-23T03:32:21.734140: step 1828, loss 0.590681, acc 0.703125\n",
      "2018-05-23T03:32:22.089190: step 1829, loss 0.615138, acc 0.734375\n",
      "2018-05-23T03:32:22.504079: step 1830, loss 0.461822, acc 0.8125\n",
      "2018-05-23T03:32:22.859129: step 1831, loss 0.54076, acc 0.8125\n",
      "2018-05-23T03:32:23.212185: step 1832, loss 0.572037, acc 0.71875\n",
      "2018-05-23T03:32:23.566238: step 1833, loss 0.645551, acc 0.703125\n",
      "2018-05-23T03:32:23.921289: step 1834, loss 0.506107, acc 0.75\n",
      "2018-05-23T03:32:24.279330: step 1835, loss 0.529637, acc 0.765625\n",
      "2018-05-23T03:32:24.674274: step 1836, loss 0.456808, acc 0.75\n",
      "2018-05-23T03:32:25.030320: step 1837, loss 0.490234, acc 0.75\n",
      "2018-05-23T03:32:25.414301: step 1838, loss 0.575039, acc 0.671875\n",
      "2018-05-23T03:32:25.787297: step 1839, loss 0.60828, acc 0.703125\n",
      "2018-05-23T03:32:26.138357: step 1840, loss 0.618754, acc 0.703125\n",
      "2018-05-23T03:32:26.484431: step 1841, loss 0.47983, acc 0.71875\n",
      "2018-05-23T03:32:26.850453: step 1842, loss 0.603545, acc 0.65625\n",
      "2018-05-23T03:32:27.206499: step 1843, loss 0.566191, acc 0.671875\n",
      "2018-05-23T03:32:27.594462: step 1844, loss 0.500562, acc 0.8125\n",
      "2018-05-23T03:32:27.953500: step 1845, loss 0.446408, acc 0.796875\n",
      "2018-05-23T03:32:28.316530: step 1846, loss 0.467223, acc 0.8125\n",
      "2018-05-23T03:32:28.671580: step 1847, loss 0.643781, acc 0.671875\n",
      "2018-05-23T03:32:29.076533: step 1848, loss 0.613152, acc 0.75\n",
      "2018-05-23T03:32:29.421574: step 1849, loss 0.568663, acc 0.65625\n",
      "2018-05-23T03:32:29.822501: step 1850, loss 0.541332, acc 0.6875\n",
      "2018-05-23T03:32:30.191515: step 1851, loss 0.581127, acc 0.65625\n",
      "2018-05-23T03:32:30.553546: step 1852, loss 0.491465, acc 0.765625\n",
      "2018-05-23T03:32:30.907597: step 1853, loss 0.47176, acc 0.75\n",
      "2018-05-23T03:32:31.267634: step 1854, loss 0.501461, acc 0.796875\n",
      "2018-05-23T03:32:31.615704: step 1855, loss 0.528953, acc 0.75\n",
      "2018-05-23T03:32:31.964802: step 1856, loss 0.520346, acc 0.765625\n",
      "2018-05-23T03:32:32.316829: step 1857, loss 0.572247, acc 0.734375\n",
      "2018-05-23T03:32:32.671880: step 1858, loss 0.547131, acc 0.765625\n",
      "2018-05-23T03:32:33.024935: step 1859, loss 0.521192, acc 0.6875\n",
      "2018-05-23T03:32:33.386966: step 1860, loss 0.520657, acc 0.703125\n",
      "2018-05-23T03:32:33.740021: step 1861, loss 0.561686, acc 0.703125\n",
      "2018-05-23T03:32:34.084101: step 1862, loss 0.598706, acc 0.671875\n",
      "2018-05-23T03:32:34.435162: step 1863, loss 0.553559, acc 0.65625\n",
      "2018-05-23T03:32:34.786223: step 1864, loss 0.519454, acc 0.75\n",
      "2018-05-23T03:32:35.132295: step 1865, loss 0.574229, acc 0.625\n",
      "2018-05-23T03:32:35.480366: step 1866, loss 0.56307, acc 0.71875\n",
      "2018-05-23T03:32:35.837411: step 1867, loss 0.551722, acc 0.75\n",
      "2018-05-23T03:32:36.179494: step 1868, loss 0.531452, acc 0.703125\n",
      "2018-05-23T03:32:36.533549: step 1869, loss 0.546574, acc 0.734375\n",
      "2018-05-23T03:32:36.890592: step 1870, loss 0.65284, acc 0.71875\n",
      "2018-05-23T03:32:37.274564: step 1871, loss 0.557783, acc 0.65625\n",
      "2018-05-23T03:32:37.632609: step 1872, loss 0.563735, acc 0.765625\n",
      "2018-05-23T03:32:37.979678: step 1873, loss 0.467088, acc 0.78125\n",
      "2018-05-23T03:32:38.330742: step 1874, loss 0.509961, acc 0.71875\n",
      "2018-05-23T03:32:38.687784: step 1875, loss 0.531566, acc 0.765625\n",
      "2018-05-23T03:32:39.036851: step 1876, loss 0.526316, acc 0.71875\n",
      "2018-05-23T03:32:39.384920: step 1877, loss 0.418741, acc 0.796875\n",
      "2018-05-23T03:32:39.737278: step 1878, loss 0.497831, acc 0.703125\n",
      "2018-05-23T03:32:40.095321: step 1879, loss 0.613163, acc 0.671875\n",
      "2018-05-23T03:32:40.441393: step 1880, loss 0.474158, acc 0.78125\n",
      "2018-05-23T03:32:40.800435: step 1881, loss 0.482731, acc 0.75\n",
      "2018-05-23T03:32:41.141520: step 1882, loss 0.468376, acc 0.765625\n",
      "2018-05-23T03:32:41.491584: step 1883, loss 0.48841, acc 0.765625\n",
      "2018-05-23T03:32:41.850624: step 1884, loss 0.493675, acc 0.8125\n",
      "2018-05-23T03:32:42.197695: step 1885, loss 0.531542, acc 0.71875\n",
      "2018-05-23T03:32:42.541776: step 1886, loss 0.465538, acc 0.78125\n",
      "2018-05-23T03:32:42.892837: step 1887, loss 0.572089, acc 0.671875\n",
      "2018-05-23T03:32:43.247885: step 1888, loss 0.554619, acc 0.71875\n",
      "2018-05-23T03:32:43.640834: step 1889, loss 0.589793, acc 0.75\n",
      "2018-05-23T03:32:44.000873: step 1890, loss 0.470864, acc 0.734375\n",
      "2018-05-23T03:32:44.346979: step 1891, loss 0.5257, acc 0.671875\n",
      "2018-05-23T03:32:44.695016: step 1892, loss 0.515982, acc 0.765625\n",
      "2018-05-23T03:32:45.050064: step 1893, loss 0.481852, acc 0.796875\n",
      "2018-05-23T03:32:45.403119: step 1894, loss 0.55002, acc 0.71875\n",
      "2018-05-23T03:32:45.792079: step 1895, loss 0.532371, acc 0.71875\n",
      "2018-05-23T03:32:46.142145: step 1896, loss 0.444916, acc 0.78125\n",
      "2018-05-23T03:32:46.489217: step 1897, loss 0.492354, acc 0.78125\n",
      "2018-05-23T03:32:46.838284: step 1898, loss 0.539132, acc 0.8125\n",
      "2018-05-23T03:32:47.186349: step 1899, loss 0.62273, acc 0.65625\n",
      "2018-05-23T03:32:47.535417: step 1900, loss 0.489457, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:32:52.248808: step 1900, loss 0.529115, acc 0.731962\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-1900\n",
      "\n",
      "2018-05-23T03:32:53.467581: step 1901, loss 0.570746, acc 0.71875\n",
      "2018-05-23T03:32:53.898429: step 1902, loss 0.477152, acc 0.8125\n",
      "2018-05-23T03:32:54.317306: step 1903, loss 0.64928, acc 0.6875\n",
      "2018-05-23T03:32:54.683328: step 1904, loss 0.531549, acc 0.71875\n",
      "2018-05-23T03:32:55.032394: step 1905, loss 0.563846, acc 0.734375\n",
      "2018-05-23T03:32:55.378470: step 1906, loss 0.613851, acc 0.671875\n",
      "2018-05-23T03:32:55.732520: step 1907, loss 0.597358, acc 0.703125\n",
      "2018-05-23T03:32:56.081586: step 1908, loss 0.451251, acc 0.796875\n",
      "2018-05-23T03:32:56.434642: step 1909, loss 0.515128, acc 0.78125\n",
      "2018-05-23T03:32:56.781713: step 1910, loss 0.45388, acc 0.765625\n",
      "2018-05-23T03:32:57.127788: step 1911, loss 0.573839, acc 0.71875\n",
      "2018-05-23T03:32:57.481840: step 1912, loss 0.478277, acc 0.734375\n",
      "2018-05-23T03:32:57.835894: step 1913, loss 0.476732, acc 0.75\n",
      "2018-05-23T03:32:58.177647: step 1914, loss 0.56485, acc 0.71875\n",
      "2018-05-23T03:32:58.536652: step 1915, loss 0.527999, acc 0.71875\n",
      "2018-05-23T03:32:58.890704: step 1916, loss 0.488286, acc 0.71875\n",
      "2018-05-23T03:32:59.231792: step 1917, loss 0.514632, acc 0.765625\n",
      "2018-05-23T03:32:59.586844: step 1918, loss 0.516132, acc 0.75\n",
      "2018-05-23T03:32:59.932964: step 1919, loss 0.609866, acc 0.6875\n",
      "2018-05-23T03:33:00.316934: step 1920, loss 0.533498, acc 0.6875\n",
      "2018-05-23T03:33:00.668504: step 1921, loss 0.473885, acc 0.78125\n",
      "2018-05-23T03:33:01.018568: step 1922, loss 0.507369, acc 0.75\n",
      "2018-05-23T03:33:01.375614: step 1923, loss 0.538866, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:33:01.724680: step 1924, loss 0.539865, acc 0.8125\n",
      "2018-05-23T03:33:02.144556: step 1925, loss 0.596903, acc 0.640625\n",
      "2018-05-23T03:33:02.519555: step 1926, loss 0.469364, acc 0.71875\n",
      "2018-05-23T03:33:02.890560: step 1927, loss 0.44039, acc 0.84375\n",
      "2018-05-23T03:33:03.246610: step 1928, loss 0.593303, acc 0.671875\n",
      "2018-05-23T03:33:03.599663: step 1929, loss 0.609697, acc 0.71875\n",
      "2018-05-23T03:33:03.948732: step 1930, loss 0.581975, acc 0.6875\n",
      "2018-05-23T03:33:04.294803: step 1931, loss 0.465739, acc 0.734375\n",
      "2018-05-23T03:33:04.633896: step 1932, loss 0.497004, acc 0.71875\n",
      "2018-05-23T03:33:04.975986: step 1933, loss 0.475113, acc 0.75\n",
      "2018-05-23T03:33:05.321060: step 1934, loss 0.504442, acc 0.75\n",
      "2018-05-23T03:33:05.666135: step 1935, loss 0.506281, acc 0.75\n",
      "2018-05-23T03:33:06.011212: step 1936, loss 0.558026, acc 0.703125\n",
      "2018-05-23T03:33:06.396183: step 1937, loss 0.47918, acc 0.78125\n",
      "2018-05-23T03:33:06.743255: step 1938, loss 0.513973, acc 0.734375\n",
      "2018-05-23T03:33:07.090325: step 1939, loss 0.535893, acc 0.703125\n",
      "2018-05-23T03:33:07.439393: step 1940, loss 0.615051, acc 0.625\n",
      "2018-05-23T03:33:07.787463: step 1941, loss 0.524498, acc 0.78125\n",
      "2018-05-23T03:33:08.152485: step 1942, loss 0.509025, acc 0.734375\n",
      "2018-05-23T03:33:08.527481: step 1943, loss 0.480175, acc 0.75\n",
      "2018-05-23T03:33:08.877546: step 1944, loss 0.577334, acc 0.671875\n",
      "2018-05-23T03:33:09.222621: step 1945, loss 0.449882, acc 0.765625\n",
      "2018-05-23T03:33:09.610583: step 1946, loss 0.547253, acc 0.703125\n",
      "2018-05-23T03:33:09.961645: step 1947, loss 0.635707, acc 0.703125\n",
      "2018-05-23T03:33:10.322680: step 1948, loss 0.567933, acc 0.59375\n",
      "2018-05-23T03:33:10.686704: step 1949, loss 0.442242, acc 0.75\n",
      "2018-05-23T03:33:11.065692: step 1950, loss 0.551656, acc 0.671875\n",
      "2018-05-23T03:33:11.476590: step 1951, loss 0.546389, acc 0.703125\n",
      "2018-05-23T03:33:11.844609: step 1952, loss 0.625207, acc 0.671875\n",
      "2018-05-23T03:33:12.187688: step 1953, loss 0.5864, acc 0.734375\n",
      "2018-05-23T03:33:12.582632: step 1954, loss 0.638938, acc 0.640625\n",
      "2018-05-23T03:33:12.938682: step 1955, loss 0.509233, acc 0.78125\n",
      "2018-05-23T03:33:13.298717: step 1956, loss 0.470321, acc 0.765625\n",
      "2018-05-23T03:33:13.646786: step 1957, loss 0.478318, acc 0.734375\n",
      "2018-05-23T03:33:13.989870: step 1958, loss 0.597051, acc 0.6875\n",
      "2018-05-23T03:33:14.332953: step 1959, loss 0.607392, acc 0.703125\n",
      "2018-05-23T03:33:14.746843: step 1960, loss 0.558351, acc 0.71875\n",
      "2018-05-23T03:33:15.087932: step 1961, loss 0.561935, acc 0.734375\n",
      "2018-05-23T03:33:15.434004: step 1962, loss 0.475879, acc 0.84375\n",
      "2018-05-23T03:33:15.793044: step 1963, loss 0.638804, acc 0.703125\n",
      "2018-05-23T03:33:16.135129: step 1964, loss 0.618216, acc 0.734375\n",
      "2018-05-23T03:33:16.473225: step 1965, loss 0.514292, acc 0.6875\n",
      "2018-05-23T03:33:16.899085: step 1966, loss 0.526284, acc 0.734375\n",
      "2018-05-23T03:33:17.246156: step 1967, loss 0.591829, acc 0.6875\n",
      "2018-05-23T03:33:17.592232: step 1968, loss 0.540088, acc 0.703125\n",
      "2018-05-23T03:33:17.943293: step 1969, loss 0.648806, acc 0.671875\n",
      "2018-05-23T03:33:18.179659: step 1970, loss 0.552427, acc 0.705882\n",
      "2018-05-23T03:33:18.533713: step 1971, loss 0.519927, acc 0.671875\n",
      "2018-05-23T03:33:18.927659: step 1972, loss 0.425585, acc 0.859375\n",
      "2018-05-23T03:33:19.283706: step 1973, loss 0.51425, acc 0.6875\n",
      "2018-05-23T03:33:19.634766: step 1974, loss 0.461726, acc 0.75\n",
      "2018-05-23T03:33:19.991811: step 1975, loss 0.496845, acc 0.765625\n",
      "2018-05-23T03:33:20.333897: step 1976, loss 0.522881, acc 0.703125\n",
      "2018-05-23T03:33:20.689946: step 1977, loss 0.42818, acc 0.859375\n",
      "2018-05-23T03:33:21.086883: step 1978, loss 0.362415, acc 0.84375\n",
      "2018-05-23T03:33:21.433953: step 1979, loss 0.488663, acc 0.75\n",
      "2018-05-23T03:33:21.792994: step 1980, loss 0.550579, acc 0.75\n",
      "2018-05-23T03:33:22.142060: step 1981, loss 0.56767, acc 0.640625\n",
      "2018-05-23T03:33:22.492124: step 1982, loss 0.462055, acc 0.828125\n",
      "2018-05-23T03:33:22.849877: step 1983, loss 0.545213, acc 0.75\n",
      "2018-05-23T03:33:23.243825: step 1984, loss 0.487316, acc 0.78125\n",
      "2018-05-23T03:33:23.589899: step 1985, loss 0.617674, acc 0.75\n",
      "2018-05-23T03:33:23.945945: step 1986, loss 0.523424, acc 0.703125\n",
      "2018-05-23T03:33:24.291025: step 1987, loss 0.481998, acc 0.765625\n",
      "2018-05-23T03:33:24.634106: step 1988, loss 0.56478, acc 0.6875\n",
      "2018-05-23T03:33:24.980178: step 1989, loss 0.468734, acc 0.671875\n",
      "2018-05-23T03:33:25.368142: step 1990, loss 0.462422, acc 0.765625\n",
      "2018-05-23T03:33:25.808962: step 1991, loss 0.473269, acc 0.796875\n",
      "2018-05-23T03:33:26.298652: step 1992, loss 0.439125, acc 0.8125\n",
      "2018-05-23T03:33:26.783356: step 1993, loss 0.596449, acc 0.765625\n",
      "2018-05-23T03:33:27.229162: step 1994, loss 0.54141, acc 0.734375\n",
      "2018-05-23T03:33:27.683949: step 1995, loss 0.412704, acc 0.765625\n",
      "2018-05-23T03:33:28.186602: step 1996, loss 0.443807, acc 0.828125\n",
      "2018-05-23T03:33:28.572569: step 1997, loss 0.474046, acc 0.75\n",
      "2018-05-23T03:33:28.927621: step 1998, loss 0.583313, acc 0.734375\n",
      "2018-05-23T03:33:29.291646: step 1999, loss 0.541997, acc 0.734375\n",
      "2018-05-23T03:33:29.653677: step 2000, loss 0.433552, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:33:34.432891: step 2000, loss 0.526385, acc 0.735677\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2000\n",
      "\n",
      "2018-05-23T03:33:35.795653: step 2001, loss 0.489643, acc 0.71875\n",
      "2018-05-23T03:33:36.171648: step 2002, loss 0.439742, acc 0.8125\n",
      "2018-05-23T03:33:36.607481: step 2003, loss 0.508062, acc 0.78125\n",
      "2018-05-23T03:33:36.964526: step 2004, loss 0.427562, acc 0.765625\n",
      "2018-05-23T03:33:37.322569: step 2005, loss 0.42685, acc 0.8125\n",
      "2018-05-23T03:33:37.690586: step 2006, loss 0.599507, acc 0.703125\n",
      "2018-05-23T03:33:38.033666: step 2007, loss 0.402835, acc 0.84375\n",
      "2018-05-23T03:33:38.399689: step 2008, loss 0.41762, acc 0.828125\n",
      "2018-05-23T03:33:38.782662: step 2009, loss 0.607137, acc 0.703125\n",
      "2018-05-23T03:33:39.121756: step 2010, loss 0.499943, acc 0.796875\n",
      "2018-05-23T03:33:39.474810: step 2011, loss 0.538557, acc 0.78125\n",
      "2018-05-23T03:33:39.825871: step 2012, loss 0.435, acc 0.734375\n",
      "2018-05-23T03:33:40.174939: step 2013, loss 0.49407, acc 0.75\n",
      "2018-05-23T03:33:40.538966: step 2014, loss 0.444678, acc 0.828125\n",
      "2018-05-23T03:33:40.936900: step 2015, loss 0.557841, acc 0.703125\n",
      "2018-05-23T03:33:41.284968: step 2016, loss 0.628878, acc 0.671875\n",
      "2018-05-23T03:33:41.645005: step 2017, loss 0.503336, acc 0.71875\n",
      "2018-05-23T03:33:41.988090: step 2018, loss 0.466175, acc 0.71875\n",
      "2018-05-23T03:33:42.346133: step 2019, loss 0.330162, acc 0.859375\n",
      "2018-05-23T03:33:42.732096: step 2020, loss 0.468539, acc 0.765625\n",
      "2018-05-23T03:33:43.105103: step 2021, loss 0.435271, acc 0.796875\n",
      "2018-05-23T03:33:43.484085: step 2022, loss 0.436389, acc 0.765625\n",
      "2018-05-23T03:33:43.844122: step 2023, loss 0.427144, acc 0.765625\n",
      "2018-05-23T03:33:44.193190: step 2024, loss 0.642649, acc 0.75\n",
      "2018-05-23T03:33:44.551230: step 2025, loss 0.431093, acc 0.828125\n",
      "2018-05-23T03:33:44.964127: step 2026, loss 0.57844, acc 0.671875\n",
      "2018-05-23T03:33:45.345107: step 2027, loss 0.522499, acc 0.765625\n",
      "2018-05-23T03:33:45.703150: step 2028, loss 0.488027, acc 0.75\n",
      "2018-05-23T03:33:46.048228: step 2029, loss 0.473644, acc 0.734375\n",
      "2018-05-23T03:33:46.393302: step 2030, loss 0.554739, acc 0.734375\n",
      "2018-05-23T03:33:46.782264: step 2031, loss 0.360389, acc 0.78125\n",
      "2018-05-23T03:33:47.129334: step 2032, loss 0.463259, acc 0.75\n",
      "2018-05-23T03:33:47.494356: step 2033, loss 0.53745, acc 0.71875\n",
      "2018-05-23T03:33:47.855391: step 2034, loss 0.49961, acc 0.75\n",
      "2018-05-23T03:33:48.247342: step 2035, loss 0.453512, acc 0.828125\n",
      "2018-05-23T03:33:48.594413: step 2036, loss 0.527072, acc 0.765625\n",
      "2018-05-23T03:33:48.953457: step 2037, loss 0.501951, acc 0.78125\n",
      "2018-05-23T03:33:49.327453: step 2038, loss 0.490267, acc 0.703125\n",
      "2018-05-23T03:33:49.682515: step 2039, loss 0.397758, acc 0.859375\n",
      "2018-05-23T03:33:50.055505: step 2040, loss 0.487384, acc 0.765625\n",
      "2018-05-23T03:33:50.403577: step 2041, loss 0.545841, acc 0.703125\n",
      "2018-05-23T03:33:50.758625: step 2042, loss 0.487393, acc 0.6875\n",
      "2018-05-23T03:33:51.130630: step 2043, loss 0.461307, acc 0.765625\n",
      "2018-05-23T03:33:51.524576: step 2044, loss 0.643891, acc 0.734375\n",
      "2018-05-23T03:33:51.893587: step 2045, loss 0.511999, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:33:52.270579: step 2046, loss 0.440686, acc 0.796875\n",
      "2018-05-23T03:33:52.642584: step 2047, loss 0.569706, acc 0.6875\n",
      "2018-05-23T03:33:53.006710: step 2048, loss 0.533589, acc 0.765625\n",
      "2018-05-23T03:33:53.414617: step 2049, loss 0.62544, acc 0.71875\n",
      "2018-05-23T03:33:53.791609: step 2050, loss 0.50119, acc 0.78125\n",
      "2018-05-23T03:33:54.163616: step 2051, loss 0.355606, acc 0.859375\n",
      "2018-05-23T03:33:54.553570: step 2052, loss 0.484207, acc 0.8125\n",
      "2018-05-23T03:33:54.908621: step 2053, loss 0.505369, acc 0.734375\n",
      "2018-05-23T03:33:55.268659: step 2054, loss 0.502759, acc 0.734375\n",
      "2018-05-23T03:33:55.665596: step 2055, loss 0.499145, acc 0.78125\n",
      "2018-05-23T03:33:56.023638: step 2056, loss 0.473995, acc 0.78125\n",
      "2018-05-23T03:33:56.383676: step 2057, loss 0.453453, acc 0.765625\n",
      "2018-05-23T03:33:56.751692: step 2058, loss 0.590386, acc 0.6875\n",
      "2018-05-23T03:33:57.130677: step 2059, loss 0.473704, acc 0.78125\n",
      "2018-05-23T03:33:57.495700: step 2060, loss 0.490578, acc 0.734375\n",
      "2018-05-23T03:33:57.908597: step 2061, loss 0.383255, acc 0.796875\n",
      "2018-05-23T03:33:58.272623: step 2062, loss 0.428018, acc 0.84375\n",
      "2018-05-23T03:33:58.655597: step 2063, loss 0.748278, acc 0.609375\n",
      "2018-05-23T03:33:59.014639: step 2064, loss 0.366102, acc 0.84375\n",
      "2018-05-23T03:33:59.393622: step 2065, loss 0.439513, acc 0.828125\n",
      "2018-05-23T03:33:59.792557: step 2066, loss 0.404064, acc 0.875\n",
      "2018-05-23T03:34:00.175531: step 2067, loss 0.395779, acc 0.8125\n",
      "2018-05-23T03:34:00.605390: step 2068, loss 0.473633, acc 0.78125\n",
      "2018-05-23T03:34:01.043211: step 2069, loss 0.525941, acc 0.765625\n",
      "2018-05-23T03:34:01.428181: step 2070, loss 0.488862, acc 0.78125\n",
      "2018-05-23T03:34:01.931833: step 2071, loss 0.373439, acc 0.859375\n",
      "2018-05-23T03:34:02.286883: step 2072, loss 0.479179, acc 0.84375\n",
      "2018-05-23T03:34:02.646953: step 2073, loss 0.428265, acc 0.828125\n",
      "2018-05-23T03:34:03.076769: step 2074, loss 0.396794, acc 0.828125\n",
      "2018-05-23T03:34:03.449772: step 2075, loss 0.35756, acc 0.84375\n",
      "2018-05-23T03:34:03.803826: step 2076, loss 0.445205, acc 0.796875\n",
      "2018-05-23T03:34:04.148902: step 2077, loss 0.513173, acc 0.734375\n",
      "2018-05-23T03:34:04.494976: step 2078, loss 0.497588, acc 0.765625\n",
      "2018-05-23T03:34:04.848032: step 2079, loss 0.530146, acc 0.6875\n",
      "2018-05-23T03:34:05.185177: step 2080, loss 0.449859, acc 0.765625\n",
      "2018-05-23T03:34:05.549156: step 2081, loss 0.549748, acc 0.75\n",
      "2018-05-23T03:34:05.900216: step 2082, loss 0.494469, acc 0.796875\n",
      "2018-05-23T03:34:06.244297: step 2083, loss 0.434086, acc 0.796875\n",
      "2018-05-23T03:34:06.589373: step 2084, loss 0.379705, acc 0.8125\n",
      "2018-05-23T03:34:06.944422: step 2085, loss 0.490093, acc 0.71875\n",
      "2018-05-23T03:34:07.338368: step 2086, loss 0.417042, acc 0.796875\n",
      "2018-05-23T03:34:07.693418: step 2087, loss 0.462585, acc 0.75\n",
      "2018-05-23T03:34:08.035504: step 2088, loss 0.553516, acc 0.671875\n",
      "2018-05-23T03:34:08.378586: step 2089, loss 0.345702, acc 0.828125\n",
      "2018-05-23T03:34:08.730643: step 2090, loss 0.530716, acc 0.71875\n",
      "2018-05-23T03:34:09.094671: step 2091, loss 0.513144, acc 0.765625\n",
      "2018-05-23T03:34:09.478643: step 2092, loss 0.50404, acc 0.78125\n",
      "2018-05-23T03:34:09.827490: step 2093, loss 0.430935, acc 0.84375\n",
      "2018-05-23T03:34:10.213453: step 2094, loss 0.483352, acc 0.734375\n",
      "2018-05-23T03:34:10.568506: step 2095, loss 0.453368, acc 0.796875\n",
      "2018-05-23T03:34:10.929538: step 2096, loss 0.410138, acc 0.78125\n",
      "2018-05-23T03:34:11.313541: step 2097, loss 0.461359, acc 0.734375\n",
      "2018-05-23T03:34:11.667565: step 2098, loss 0.55946, acc 0.765625\n",
      "2018-05-23T03:34:12.017629: step 2099, loss 0.386955, acc 0.78125\n",
      "2018-05-23T03:34:12.365697: step 2100, loss 0.550236, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:34:17.111999: step 2100, loss 0.553143, acc 0.725961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2100\n",
      "\n",
      "2018-05-23T03:34:18.851347: step 2101, loss 0.44786, acc 0.78125\n",
      "2018-05-23T03:34:19.234321: step 2102, loss 0.462312, acc 0.78125\n",
      "2018-05-23T03:34:19.624285: step 2103, loss 0.497347, acc 0.734375\n",
      "2018-05-23T03:34:20.000272: step 2104, loss 0.548318, acc 0.71875\n",
      "2018-05-23T03:34:20.367292: step 2105, loss 0.474214, acc 0.828125\n",
      "2018-05-23T03:34:20.736303: step 2106, loss 0.500688, acc 0.703125\n",
      "2018-05-23T03:34:21.101326: step 2107, loss 0.533566, acc 0.71875\n",
      "2018-05-23T03:34:21.483305: step 2108, loss 0.426512, acc 0.796875\n",
      "2018-05-23T03:34:21.878248: step 2109, loss 0.468046, acc 0.828125\n",
      "2018-05-23T03:34:22.236290: step 2110, loss 0.473002, acc 0.71875\n",
      "2018-05-23T03:34:22.586355: step 2111, loss 0.575071, acc 0.71875\n",
      "2018-05-23T03:34:22.993265: step 2112, loss 0.556776, acc 0.6875\n",
      "2018-05-23T03:34:23.340338: step 2113, loss 0.472685, acc 0.78125\n",
      "2018-05-23T03:34:23.692396: step 2114, loss 0.49872, acc 0.75\n",
      "2018-05-23T03:34:24.037472: step 2115, loss 0.466855, acc 0.828125\n",
      "2018-05-23T03:34:24.387308: step 2116, loss 0.419714, acc 0.765625\n",
      "2018-05-23T03:34:24.742357: step 2117, loss 0.514596, acc 0.765625\n",
      "2018-05-23T03:34:25.119350: step 2118, loss 0.548177, acc 0.75\n",
      "2018-05-23T03:34:25.468415: step 2119, loss 0.384128, acc 0.84375\n",
      "2018-05-23T03:34:25.823467: step 2120, loss 0.367256, acc 0.84375\n",
      "2018-05-23T03:34:26.167545: step 2121, loss 0.378944, acc 0.890625\n",
      "2018-05-23T03:34:26.520600: step 2122, loss 0.551308, acc 0.78125\n",
      "2018-05-23T03:34:26.881635: step 2123, loss 0.443272, acc 0.71875\n",
      "2018-05-23T03:34:27.246658: step 2124, loss 0.447212, acc 0.765625\n",
      "2018-05-23T03:34:27.603704: step 2125, loss 0.531136, acc 0.765625\n",
      "2018-05-23T03:34:27.962748: step 2126, loss 0.479293, acc 0.78125\n",
      "2018-05-23T03:34:28.375638: step 2127, loss 0.426192, acc 0.8125\n",
      "2018-05-23T03:34:28.735677: step 2128, loss 0.523988, acc 0.703125\n",
      "2018-05-23T03:34:29.094714: step 2129, loss 0.467498, acc 0.828125\n",
      "2018-05-23T03:34:29.438793: step 2130, loss 0.409617, acc 0.796875\n",
      "2018-05-23T03:34:29.799828: step 2131, loss 0.445047, acc 0.796875\n",
      "2018-05-23T03:34:30.171833: step 2132, loss 0.483728, acc 0.78125\n",
      "2018-05-23T03:34:30.535858: step 2133, loss 0.564573, acc 0.671875\n",
      "2018-05-23T03:34:30.895895: step 2134, loss 0.554516, acc 0.765625\n",
      "2018-05-23T03:34:31.278871: step 2135, loss 0.355843, acc 0.84375\n",
      "2018-05-23T03:34:31.641045: step 2136, loss 0.478897, acc 0.75\n",
      "2018-05-23T03:34:31.998092: step 2137, loss 0.406365, acc 0.84375\n",
      "2018-05-23T03:34:32.349153: step 2138, loss 0.578074, acc 0.71875\n",
      "2018-05-23T03:34:32.694230: step 2139, loss 0.507726, acc 0.71875\n",
      "2018-05-23T03:34:33.051273: step 2140, loss 0.686913, acc 0.734375\n",
      "2018-05-23T03:34:33.399341: step 2141, loss 0.431579, acc 0.859375\n",
      "2018-05-23T03:34:33.747411: step 2142, loss 0.499909, acc 0.734375\n",
      "2018-05-23T03:34:34.103458: step 2143, loss 0.438733, acc 0.8125\n",
      "2018-05-23T03:34:34.449532: step 2144, loss 0.387093, acc 0.796875\n",
      "2018-05-23T03:34:34.793615: step 2145, loss 0.436063, acc 0.796875\n",
      "2018-05-23T03:34:35.144674: step 2146, loss 0.597326, acc 0.71875\n",
      "2018-05-23T03:34:35.581506: step 2147, loss 0.541907, acc 0.75\n",
      "2018-05-23T03:34:35.929574: step 2148, loss 0.377643, acc 0.84375\n",
      "2018-05-23T03:34:36.289610: step 2149, loss 0.608818, acc 0.734375\n",
      "2018-05-23T03:34:36.640672: step 2150, loss 0.566534, acc 0.671875\n",
      "2018-05-23T03:34:36.990734: step 2151, loss 0.566194, acc 0.734375\n",
      "2018-05-23T03:34:37.341795: step 2152, loss 0.52161, acc 0.78125\n",
      "2018-05-23T03:34:37.682884: step 2153, loss 0.504613, acc 0.734375\n",
      "2018-05-23T03:34:38.037935: step 2154, loss 0.467676, acc 0.78125\n",
      "2018-05-23T03:34:38.386001: step 2155, loss 0.44342, acc 0.78125\n",
      "2018-05-23T03:34:38.740055: step 2156, loss 0.493803, acc 0.8125\n",
      "2018-05-23T03:34:39.092115: step 2157, loss 0.485999, acc 0.78125\n",
      "2018-05-23T03:34:39.442176: step 2158, loss 0.498918, acc 0.75\n",
      "2018-05-23T03:34:39.794236: step 2159, loss 0.580481, acc 0.734375\n",
      "2018-05-23T03:34:40.145295: step 2160, loss 0.449178, acc 0.796875\n",
      "2018-05-23T03:34:40.504336: step 2161, loss 0.435569, acc 0.796875\n",
      "2018-05-23T03:34:40.897285: step 2162, loss 0.397152, acc 0.8125\n",
      "2018-05-23T03:34:41.250341: step 2163, loss 0.399589, acc 0.828125\n",
      "2018-05-23T03:34:41.603394: step 2164, loss 0.470037, acc 0.75\n",
      "2018-05-23T03:34:41.959442: step 2165, loss 0.497028, acc 0.78125\n",
      "2018-05-23T03:34:42.318481: step 2166, loss 0.595841, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:34:42.705446: step 2167, loss 0.520518, acc 0.71875\n",
      "2018-05-23T03:34:43.052517: step 2168, loss 0.532391, acc 0.71875\n",
      "2018-05-23T03:34:43.411557: step 2169, loss 0.413238, acc 0.84375\n",
      "2018-05-23T03:34:43.764612: step 2170, loss 0.364, acc 0.8125\n",
      "2018-05-23T03:34:44.136619: step 2171, loss 0.736715, acc 0.59375\n",
      "2018-05-23T03:34:44.487680: step 2172, loss 0.412912, acc 0.828125\n",
      "2018-05-23T03:34:44.843725: step 2173, loss 0.451404, acc 0.796875\n",
      "2018-05-23T03:34:45.187807: step 2174, loss 0.623225, acc 0.640625\n",
      "2018-05-23T03:34:45.542855: step 2175, loss 0.496978, acc 0.765625\n",
      "2018-05-23T03:34:45.890927: step 2176, loss 0.491937, acc 0.78125\n",
      "2018-05-23T03:34:46.237995: step 2177, loss 0.662246, acc 0.65625\n",
      "2018-05-23T03:34:46.597038: step 2178, loss 0.460349, acc 0.828125\n",
      "2018-05-23T03:34:46.957074: step 2179, loss 0.555215, acc 0.75\n",
      "2018-05-23T03:34:47.307135: step 2180, loss 0.331967, acc 0.859375\n",
      "2018-05-23T03:34:47.666176: step 2181, loss 0.688093, acc 0.625\n",
      "2018-05-23T03:34:48.061120: step 2182, loss 0.446281, acc 0.765625\n",
      "2018-05-23T03:34:48.416168: step 2183, loss 0.549631, acc 0.78125\n",
      "2018-05-23T03:34:48.770223: step 2184, loss 0.497983, acc 0.796875\n",
      "2018-05-23T03:34:49.122280: step 2185, loss 0.41215, acc 0.8125\n",
      "2018-05-23T03:34:49.467357: step 2186, loss 0.383989, acc 0.84375\n",
      "2018-05-23T03:34:49.836371: step 2187, loss 0.616183, acc 0.625\n",
      "2018-05-23T03:34:50.216577: step 2188, loss 0.376671, acc 0.859375\n",
      "2018-05-23T03:34:50.568635: step 2189, loss 0.444684, acc 0.8125\n",
      "2018-05-23T03:34:50.927501: step 2190, loss 0.444454, acc 0.765625\n",
      "2018-05-23T03:34:51.273576: step 2191, loss 0.524986, acc 0.734375\n",
      "2018-05-23T03:34:51.628625: step 2192, loss 0.461341, acc 0.796875\n",
      "2018-05-23T03:34:51.977692: step 2193, loss 0.521997, acc 0.71875\n",
      "2018-05-23T03:34:52.328753: step 2194, loss 0.545827, acc 0.734375\n",
      "2018-05-23T03:34:52.717713: step 2195, loss 0.486034, acc 0.734375\n",
      "2018-05-23T03:34:53.063788: step 2196, loss 0.336609, acc 0.875\n",
      "2018-05-23T03:34:53.415844: step 2197, loss 0.593803, acc 0.6875\n",
      "2018-05-23T03:34:53.772889: step 2198, loss 0.512218, acc 0.765625\n",
      "2018-05-23T03:34:54.124947: step 2199, loss 0.503061, acc 0.78125\n",
      "2018-05-23T03:34:54.474016: step 2200, loss 0.439041, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:34:59.216326: step 2200, loss 0.535415, acc 0.734391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2200\n",
      "\n",
      "2018-05-23T03:35:00.845967: step 2201, loss 0.538677, acc 0.703125\n",
      "2018-05-23T03:35:01.331668: step 2202, loss 0.587517, acc 0.6875\n",
      "2018-05-23T03:35:01.790440: step 2203, loss 0.467774, acc 0.765625\n",
      "2018-05-23T03:35:02.214306: step 2204, loss 0.462664, acc 0.765625\n",
      "2018-05-23T03:35:02.649144: step 2205, loss 0.531127, acc 0.71875\n",
      "2018-05-23T03:35:03.005191: step 2206, loss 0.512846, acc 0.734375\n",
      "2018-05-23T03:35:03.378227: step 2207, loss 0.576171, acc 0.703125\n",
      "2018-05-23T03:35:03.758177: step 2208, loss 0.526817, acc 0.703125\n",
      "2018-05-23T03:35:04.102257: step 2209, loss 0.417617, acc 0.828125\n",
      "2018-05-23T03:35:04.458306: step 2210, loss 0.526857, acc 0.671875\n",
      "2018-05-23T03:35:04.803381: step 2211, loss 0.432733, acc 0.84375\n",
      "2018-05-23T03:35:05.152447: step 2212, loss 0.428345, acc 0.8125\n",
      "2018-05-23T03:35:05.501513: step 2213, loss 0.520841, acc 0.71875\n",
      "2018-05-23T03:35:05.844595: step 2214, loss 0.556234, acc 0.734375\n",
      "2018-05-23T03:35:06.194661: step 2215, loss 0.497417, acc 0.796875\n",
      "2018-05-23T03:35:06.550707: step 2216, loss 0.414277, acc 0.796875\n",
      "2018-05-23T03:35:06.892792: step 2217, loss 0.385608, acc 0.828125\n",
      "2018-05-23T03:35:07.244849: step 2218, loss 0.471334, acc 0.75\n",
      "2018-05-23T03:35:07.595911: step 2219, loss 0.398604, acc 0.8125\n",
      "2018-05-23T03:35:08.002821: step 2220, loss 0.421674, acc 0.859375\n",
      "2018-05-23T03:35:08.344905: step 2221, loss 0.3249, acc 0.890625\n",
      "2018-05-23T03:35:08.692975: step 2222, loss 0.541424, acc 0.734375\n",
      "2018-05-23T03:35:09.086923: step 2223, loss 0.408686, acc 0.828125\n",
      "2018-05-23T03:35:09.449950: step 2224, loss 0.374493, acc 0.859375\n",
      "2018-05-23T03:35:09.810985: step 2225, loss 0.380585, acc 0.8125\n",
      "2018-05-23T03:35:10.169026: step 2226, loss 0.544374, acc 0.71875\n",
      "2018-05-23T03:35:10.513106: step 2227, loss 0.429472, acc 0.8125\n",
      "2018-05-23T03:35:10.863170: step 2228, loss 0.531735, acc 0.796875\n",
      "2018-05-23T03:35:11.208247: step 2229, loss 0.437517, acc 0.796875\n",
      "2018-05-23T03:35:11.568285: step 2230, loss 0.491604, acc 0.765625\n",
      "2018-05-23T03:35:11.928320: step 2231, loss 0.629841, acc 0.65625\n",
      "2018-05-23T03:35:12.269409: step 2232, loss 0.405325, acc 0.78125\n",
      "2018-05-23T03:35:12.627449: step 2233, loss 0.465166, acc 0.6875\n",
      "2018-05-23T03:35:12.978510: step 2234, loss 0.385594, acc 0.84375\n",
      "2018-05-23T03:35:13.326579: step 2235, loss 0.417536, acc 0.8125\n",
      "2018-05-23T03:35:13.678640: step 2236, loss 0.512174, acc 0.75\n",
      "2018-05-23T03:35:14.028701: step 2237, loss 0.422134, acc 0.796875\n",
      "2018-05-23T03:35:14.376771: step 2238, loss 0.405994, acc 0.8125\n",
      "2018-05-23T03:35:14.729826: step 2239, loss 0.467532, acc 0.78125\n",
      "2018-05-23T03:35:15.077894: step 2240, loss 0.538806, acc 0.71875\n",
      "2018-05-23T03:35:15.448902: step 2241, loss 0.417428, acc 0.8125\n",
      "2018-05-23T03:35:15.810934: step 2242, loss 0.458004, acc 0.796875\n",
      "2018-05-23T03:35:16.188924: step 2243, loss 0.393499, acc 0.84375\n",
      "2018-05-23T03:35:16.598826: step 2244, loss 0.466559, acc 0.78125\n",
      "2018-05-23T03:35:16.987787: step 2245, loss 0.406204, acc 0.796875\n",
      "2018-05-23T03:35:17.360789: step 2246, loss 0.575881, acc 0.71875\n",
      "2018-05-23T03:35:17.720823: step 2247, loss 0.495991, acc 0.765625\n",
      "2018-05-23T03:35:18.160649: step 2248, loss 0.459522, acc 0.75\n",
      "2018-05-23T03:35:18.520686: step 2249, loss 0.5564, acc 0.6875\n",
      "2018-05-23T03:35:18.893688: step 2250, loss 0.319346, acc 0.90625\n",
      "2018-05-23T03:35:19.284642: step 2251, loss 0.593715, acc 0.671875\n",
      "2018-05-23T03:35:19.675594: step 2252, loss 0.492719, acc 0.765625\n",
      "2018-05-23T03:35:20.061563: step 2253, loss 0.678271, acc 0.640625\n",
      "2018-05-23T03:35:20.450522: step 2254, loss 0.477828, acc 0.78125\n",
      "2018-05-23T03:35:20.809562: step 2255, loss 0.481073, acc 0.8125\n",
      "2018-05-23T03:35:21.176581: step 2256, loss 0.498473, acc 0.796875\n",
      "2018-05-23T03:35:21.563544: step 2257, loss 0.413475, acc 0.765625\n",
      "2018-05-23T03:35:21.918427: step 2258, loss 0.602024, acc 0.6875\n",
      "2018-05-23T03:35:22.268488: step 2259, loss 0.459369, acc 0.734375\n",
      "2018-05-23T03:35:22.646476: step 2260, loss 0.45288, acc 0.796875\n",
      "2018-05-23T03:35:22.999532: step 2261, loss 0.577926, acc 0.71875\n",
      "2018-05-23T03:35:23.355580: step 2262, loss 0.536933, acc 0.71875\n",
      "2018-05-23T03:35:23.704647: step 2263, loss 0.424098, acc 0.8125\n",
      "2018-05-23T03:35:24.058701: step 2264, loss 0.430242, acc 0.828125\n",
      "2018-05-23T03:35:24.401782: step 2265, loss 0.506784, acc 0.703125\n",
      "2018-05-23T03:35:24.751844: step 2266, loss 0.484747, acc 0.796875\n",
      "2018-05-23T03:35:25.110884: step 2267, loss 0.468368, acc 0.8125\n",
      "2018-05-23T03:35:25.457956: step 2268, loss 0.530671, acc 0.71875\n",
      "2018-05-23T03:35:25.807022: step 2269, loss 0.569761, acc 0.703125\n",
      "2018-05-23T03:35:26.160113: step 2270, loss 0.622575, acc 0.71875\n",
      "2018-05-23T03:35:26.511140: step 2271, loss 0.542593, acc 0.65625\n",
      "2018-05-23T03:35:26.888133: step 2272, loss 0.48175, acc 0.75\n",
      "2018-05-23T03:35:27.244179: step 2273, loss 0.46526, acc 0.734375\n",
      "2018-05-23T03:35:27.594242: step 2274, loss 0.358288, acc 0.875\n",
      "2018-05-23T03:35:28.001153: step 2275, loss 0.654564, acc 0.75\n",
      "2018-05-23T03:35:28.378143: step 2276, loss 0.361117, acc 0.828125\n",
      "2018-05-23T03:35:28.735189: step 2277, loss 0.577737, acc 0.734375\n",
      "2018-05-23T03:35:29.135151: step 2278, loss 0.541122, acc 0.75\n",
      "2018-05-23T03:35:29.501139: step 2279, loss 0.443609, acc 0.828125\n",
      "2018-05-23T03:35:29.855193: step 2280, loss 0.46723, acc 0.796875\n",
      "2018-05-23T03:35:30.202266: step 2281, loss 0.461825, acc 0.765625\n",
      "2018-05-23T03:35:30.583245: step 2282, loss 0.519364, acc 0.703125\n",
      "2018-05-23T03:35:30.934305: step 2283, loss 0.494626, acc 0.828125\n",
      "2018-05-23T03:35:31.275395: step 2284, loss 0.610941, acc 0.65625\n",
      "2018-05-23T03:35:31.632438: step 2285, loss 0.479535, acc 0.796875\n",
      "2018-05-23T03:35:32.031371: step 2286, loss 0.62227, acc 0.734375\n",
      "2018-05-23T03:35:32.391407: step 2287, loss 0.512864, acc 0.75\n",
      "2018-05-23T03:35:32.741471: step 2288, loss 0.555864, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:35:33.208224: step 2289, loss 0.430904, acc 0.828125\n",
      "2018-05-23T03:35:33.608152: step 2290, loss 0.477072, acc 0.71875\n",
      "2018-05-23T03:35:33.994122: step 2291, loss 0.439896, acc 0.765625\n",
      "2018-05-23T03:35:34.360142: step 2292, loss 0.486024, acc 0.765625\n",
      "2018-05-23T03:35:34.720179: step 2293, loss 0.519932, acc 0.78125\n",
      "2018-05-23T03:35:35.140054: step 2294, loss 0.381323, acc 0.828125\n",
      "2018-05-23T03:35:35.495105: step 2295, loss 0.53148, acc 0.75\n",
      "2018-05-23T03:35:35.853146: step 2296, loss 0.333891, acc 0.828125\n",
      "2018-05-23T03:35:36.244294: step 2297, loss 0.430497, acc 0.859375\n",
      "2018-05-23T03:35:36.586379: step 2298, loss 0.45729, acc 0.78125\n",
      "2018-05-23T03:35:36.942429: step 2299, loss 0.473694, acc 0.734375\n",
      "2018-05-23T03:35:37.316425: step 2300, loss 0.446653, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:35:42.115587: step 2300, loss 0.533812, acc 0.727818\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2300\n",
      "\n",
      "2018-05-23T03:35:43.665441: step 2301, loss 0.558131, acc 0.703125\n",
      "2018-05-23T03:35:44.090304: step 2302, loss 0.546569, acc 0.75\n",
      "2018-05-23T03:35:44.489246: step 2303, loss 0.581805, acc 0.75\n",
      "2018-05-23T03:35:44.878199: step 2304, loss 0.535329, acc 0.78125\n",
      "2018-05-23T03:35:45.229257: step 2305, loss 0.556319, acc 0.734375\n",
      "2018-05-23T03:35:45.628190: step 2306, loss 0.541986, acc 0.765625\n",
      "2018-05-23T03:35:45.988230: step 2307, loss 0.639156, acc 0.671875\n",
      "2018-05-23T03:35:46.334301: step 2308, loss 0.627265, acc 0.671875\n",
      "2018-05-23T03:35:46.712290: step 2309, loss 0.466547, acc 0.78125\n",
      "2018-05-23T03:35:47.065327: step 2310, loss 0.469051, acc 0.71875\n",
      "2018-05-23T03:35:47.412399: step 2311, loss 0.412708, acc 0.796875\n",
      "2018-05-23T03:35:47.763460: step 2312, loss 0.462657, acc 0.796875\n",
      "2018-05-23T03:35:48.113523: step 2313, loss 0.677959, acc 0.6875\n",
      "2018-05-23T03:35:48.458600: step 2314, loss 0.481241, acc 0.796875\n",
      "2018-05-23T03:35:48.804681: step 2315, loss 0.378875, acc 0.859375\n",
      "2018-05-23T03:35:49.152743: step 2316, loss 0.692649, acc 0.71875\n",
      "2018-05-23T03:35:49.499815: step 2317, loss 0.488705, acc 0.828125\n",
      "2018-05-23T03:35:49.847924: step 2318, loss 0.636198, acc 0.640625\n",
      "2018-05-23T03:35:50.202973: step 2319, loss 0.449961, acc 0.796875\n",
      "2018-05-23T03:35:50.549047: step 2320, loss 0.558072, acc 0.734375\n",
      "2018-05-23T03:35:50.900107: step 2321, loss 0.542344, acc 0.6875\n",
      "2018-05-23T03:35:51.253166: step 2322, loss 0.626248, acc 0.6875\n",
      "2018-05-23T03:35:51.603229: step 2323, loss 0.536447, acc 0.765625\n",
      "2018-05-23T03:35:51.958280: step 2324, loss 0.575199, acc 0.75\n",
      "2018-05-23T03:35:52.349233: step 2325, loss 0.425344, acc 0.78125\n",
      "2018-05-23T03:35:52.695305: step 2326, loss 0.457289, acc 0.8125\n",
      "2018-05-23T03:35:53.088256: step 2327, loss 0.426886, acc 0.828125\n",
      "2018-05-23T03:35:53.458266: step 2328, loss 0.522135, acc 0.71875\n",
      "2018-05-23T03:35:53.804342: step 2329, loss 0.464891, acc 0.78125\n",
      "2018-05-23T03:35:54.152409: step 2330, loss 0.431382, acc 0.84375\n",
      "2018-05-23T03:35:54.509452: step 2331, loss 0.421346, acc 0.78125\n",
      "2018-05-23T03:35:54.859516: step 2332, loss 0.392758, acc 0.828125\n",
      "2018-05-23T03:35:55.245483: step 2333, loss 0.555709, acc 0.734375\n",
      "2018-05-23T03:35:55.586573: step 2334, loss 0.502266, acc 0.734375\n",
      "2018-05-23T03:35:55.940624: step 2335, loss 0.59986, acc 0.734375\n",
      "2018-05-23T03:35:56.288726: step 2336, loss 0.526196, acc 0.75\n",
      "2018-05-23T03:35:56.642745: step 2337, loss 0.382531, acc 0.84375\n",
      "2018-05-23T03:35:56.986828: step 2338, loss 0.501951, acc 0.703125\n",
      "2018-05-23T03:35:57.333897: step 2339, loss 0.486106, acc 0.734375\n",
      "2018-05-23T03:35:57.692938: step 2340, loss 0.554498, acc 0.71875\n",
      "2018-05-23T03:35:58.040009: step 2341, loss 0.449405, acc 0.8125\n",
      "2018-05-23T03:35:58.403041: step 2342, loss 0.527403, acc 0.78125\n",
      "2018-05-23T03:35:58.755095: step 2343, loss 0.488321, acc 0.8125\n",
      "2018-05-23T03:35:59.108730: step 2344, loss 0.393539, acc 0.84375\n",
      "2018-05-23T03:35:59.451812: step 2345, loss 0.460249, acc 0.75\n",
      "2018-05-23T03:35:59.808857: step 2346, loss 0.457929, acc 0.765625\n",
      "2018-05-23T03:36:00.170888: step 2347, loss 0.449459, acc 0.765625\n",
      "2018-05-23T03:36:00.579793: step 2348, loss 0.515562, acc 0.734375\n",
      "2018-05-23T03:36:00.992688: step 2349, loss 0.593852, acc 0.703125\n",
      "2018-05-23T03:36:01.356715: step 2350, loss 0.450531, acc 0.75\n",
      "2018-05-23T03:36:01.775595: step 2351, loss 0.511335, acc 0.765625\n",
      "2018-05-23T03:36:02.174528: step 2352, loss 0.494972, acc 0.78125\n",
      "2018-05-23T03:36:02.535562: step 2353, loss 0.350671, acc 0.8125\n",
      "2018-05-23T03:36:02.963418: step 2354, loss 0.49098, acc 0.8125\n",
      "2018-05-23T03:36:03.344396: step 2355, loss 0.491409, acc 0.71875\n",
      "2018-05-23T03:36:03.757293: step 2356, loss 0.580856, acc 0.75\n",
      "2018-05-23T03:36:04.136279: step 2357, loss 0.531325, acc 0.75\n",
      "2018-05-23T03:36:04.515266: step 2358, loss 0.378889, acc 0.859375\n",
      "2018-05-23T03:36:04.880325: step 2359, loss 0.435778, acc 0.75\n",
      "2018-05-23T03:36:05.237334: step 2360, loss 0.497629, acc 0.78125\n",
      "2018-05-23T03:36:05.613328: step 2361, loss 0.413358, acc 0.84375\n",
      "2018-05-23T03:36:05.964388: step 2362, loss 0.441937, acc 0.84375\n",
      "2018-05-23T03:36:06.305475: step 2363, loss 0.558557, acc 0.75\n",
      "2018-05-23T03:36:06.654544: step 2364, loss 0.47009, acc 0.78125\n",
      "2018-05-23T03:36:07.005638: step 2365, loss 0.537663, acc 0.6875\n",
      "2018-05-23T03:36:07.352674: step 2366, loss 0.43114, acc 0.78125\n",
      "2018-05-23T03:36:07.757591: step 2367, loss 0.506561, acc 0.734375\n",
      "2018-05-23T03:36:08.105662: step 2368, loss 0.483902, acc 0.71875\n",
      "2018-05-23T03:36:08.460710: step 2369, loss 0.437183, acc 0.84375\n",
      "2018-05-23T03:36:08.812768: step 2370, loss 0.473877, acc 0.71875\n",
      "2018-05-23T03:36:09.211700: step 2371, loss 0.408587, acc 0.859375\n",
      "2018-05-23T03:36:09.578718: step 2372, loss 0.412838, acc 0.8125\n",
      "2018-05-23T03:36:09.958704: step 2373, loss 0.391173, acc 0.828125\n",
      "2018-05-23T03:36:10.304776: step 2374, loss 0.472788, acc 0.8125\n",
      "2018-05-23T03:36:10.691741: step 2375, loss 0.388982, acc 0.796875\n",
      "2018-05-23T03:36:11.073719: step 2376, loss 0.487375, acc 0.796875\n",
      "2018-05-23T03:36:11.480631: step 2377, loss 0.467041, acc 0.78125\n",
      "2018-05-23T03:36:11.866598: step 2378, loss 0.549842, acc 0.703125\n",
      "2018-05-23T03:36:12.246583: step 2379, loss 0.384982, acc 0.796875\n",
      "2018-05-23T03:36:12.632548: step 2380, loss 0.443635, acc 0.796875\n",
      "2018-05-23T03:36:13.075365: step 2381, loss 0.441063, acc 0.828125\n",
      "2018-05-23T03:36:13.463328: step 2382, loss 0.550038, acc 0.75\n",
      "2018-05-23T03:36:13.848298: step 2383, loss 0.431182, acc 0.765625\n",
      "2018-05-23T03:36:14.202349: step 2384, loss 0.435248, acc 0.703125\n",
      "2018-05-23T03:36:14.557402: step 2385, loss 0.374589, acc 0.8125\n",
      "2018-05-23T03:36:14.937384: step 2386, loss 0.428909, acc 0.734375\n",
      "2018-05-23T03:36:15.282461: step 2387, loss 0.357915, acc 0.859375\n",
      "2018-05-23T03:36:15.729264: step 2388, loss 0.447242, acc 0.84375\n",
      "2018-05-23T03:36:16.078333: step 2389, loss 0.482817, acc 0.8125\n",
      "2018-05-23T03:36:16.423409: step 2390, loss 0.545164, acc 0.734375\n",
      "2018-05-23T03:36:16.777462: step 2391, loss 0.441135, acc 0.78125\n",
      "2018-05-23T03:36:17.128523: step 2392, loss 0.453813, acc 0.8125\n",
      "2018-05-23T03:36:17.483572: step 2393, loss 0.577275, acc 0.765625\n",
      "2018-05-23T03:36:17.863556: step 2394, loss 0.552333, acc 0.765625\n",
      "2018-05-23T03:36:18.262490: step 2395, loss 0.475855, acc 0.8125\n",
      "2018-05-23T03:36:18.609561: step 2396, loss 0.447881, acc 0.765625\n",
      "2018-05-23T03:36:18.957630: step 2397, loss 0.461445, acc 0.75\n",
      "2018-05-23T03:36:19.300711: step 2398, loss 0.488826, acc 0.75\n",
      "2018-05-23T03:36:19.652771: step 2399, loss 0.669683, acc 0.640625\n",
      "2018-05-23T03:36:20.025770: step 2400, loss 0.422676, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:36:24.661370: step 2400, loss 0.547658, acc 0.726675\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2400\n",
      "\n",
      "2018-05-23T03:36:26.044671: step 2401, loss 0.575715, acc 0.6875\n",
      "2018-05-23T03:36:26.492471: step 2402, loss 0.59139, acc 0.6875\n",
      "2018-05-23T03:36:26.848521: step 2403, loss 0.516669, acc 0.703125\n",
      "2018-05-23T03:36:27.201722: step 2404, loss 0.528861, acc 0.75\n",
      "2018-05-23T03:36:27.570735: step 2405, loss 0.478127, acc 0.75\n",
      "2018-05-23T03:36:27.924787: step 2406, loss 0.623691, acc 0.75\n",
      "2018-05-23T03:36:28.276848: step 2407, loss 0.440241, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:36:28.673784: step 2408, loss 0.46863, acc 0.765625\n",
      "2018-05-23T03:36:29.022850: step 2409, loss 0.447964, acc 0.78125\n",
      "2018-05-23T03:36:29.374908: step 2410, loss 0.610765, acc 0.671875\n",
      "2018-05-23T03:36:29.721981: step 2411, loss 0.489937, acc 0.75\n",
      "2018-05-23T03:36:30.065932: step 2412, loss 0.491122, acc 0.71875\n",
      "2018-05-23T03:36:30.439931: step 2413, loss 0.65894, acc 0.71875\n",
      "2018-05-23T03:36:30.784013: step 2414, loss 0.576787, acc 0.71875\n",
      "2018-05-23T03:36:31.131083: step 2415, loss 0.455975, acc 0.765625\n",
      "2018-05-23T03:36:31.480148: step 2416, loss 0.539073, acc 0.734375\n",
      "2018-05-23T03:36:31.830214: step 2417, loss 0.484176, acc 0.828125\n",
      "2018-05-23T03:36:32.176286: step 2418, loss 0.519287, acc 0.734375\n",
      "2018-05-23T03:36:32.527851: step 2419, loss 0.44601, acc 0.765625\n",
      "2018-05-23T03:36:32.879910: step 2420, loss 0.369848, acc 0.875\n",
      "2018-05-23T03:36:33.223988: step 2421, loss 0.443312, acc 0.78125\n",
      "2018-05-23T03:36:33.576047: step 2422, loss 0.444783, acc 0.8125\n",
      "2018-05-23T03:36:33.928106: step 2423, loss 0.477149, acc 0.75\n",
      "2018-05-23T03:36:34.270189: step 2424, loss 0.455326, acc 0.78125\n",
      "2018-05-23T03:36:34.615270: step 2425, loss 0.585483, acc 0.65625\n",
      "2018-05-23T03:36:35.031154: step 2426, loss 0.476612, acc 0.796875\n",
      "2018-05-23T03:36:35.367255: step 2427, loss 0.385923, acc 0.859375\n",
      "2018-05-23T03:36:35.711746: step 2428, loss 0.501065, acc 0.78125\n",
      "2018-05-23T03:36:36.058818: step 2429, loss 0.492287, acc 0.71875\n",
      "2018-05-23T03:36:36.403893: step 2430, loss 0.521981, acc 0.71875\n",
      "2018-05-23T03:36:36.761938: step 2431, loss 0.372158, acc 0.8125\n",
      "2018-05-23T03:36:37.109008: step 2432, loss 0.515321, acc 0.765625\n",
      "2018-05-23T03:36:37.454086: step 2433, loss 0.583398, acc 0.640625\n",
      "2018-05-23T03:36:37.813125: step 2434, loss 0.514889, acc 0.765625\n",
      "2018-05-23T03:36:38.151220: step 2435, loss 0.354342, acc 0.859375\n",
      "2018-05-23T03:36:38.489314: step 2436, loss 0.28024, acc 0.890625\n",
      "2018-05-23T03:36:38.841375: step 2437, loss 0.428405, acc 0.78125\n",
      "2018-05-23T03:36:39.194429: step 2438, loss 0.58374, acc 0.703125\n",
      "2018-05-23T03:36:39.537513: step 2439, loss 0.470785, acc 0.796875\n",
      "2018-05-23T03:36:39.887577: step 2440, loss 0.474951, acc 0.734375\n",
      "2018-05-23T03:36:40.232651: step 2441, loss 0.399212, acc 0.8125\n",
      "2018-05-23T03:36:40.575735: step 2442, loss 0.520362, acc 0.8125\n",
      "2018-05-23T03:36:40.932778: step 2443, loss 0.492062, acc 0.796875\n",
      "2018-05-23T03:36:41.272868: step 2444, loss 0.460985, acc 0.78125\n",
      "2018-05-23T03:36:41.619940: step 2445, loss 0.484626, acc 0.703125\n",
      "2018-05-23T03:36:41.972996: step 2446, loss 0.547587, acc 0.78125\n",
      "2018-05-23T03:36:42.318074: step 2447, loss 0.329068, acc 0.84375\n",
      "2018-05-23T03:36:42.661155: step 2448, loss 0.715222, acc 0.65625\n",
      "2018-05-23T03:36:43.010222: step 2449, loss 0.490649, acc 0.75\n",
      "2018-05-23T03:36:43.403170: step 2450, loss 0.541321, acc 0.75\n",
      "2018-05-23T03:36:43.755228: step 2451, loss 0.476173, acc 0.796875\n",
      "2018-05-23T03:36:44.113271: step 2452, loss 0.475343, acc 0.765625\n",
      "2018-05-23T03:36:44.454358: step 2453, loss 0.460754, acc 0.75\n",
      "2018-05-23T03:36:44.795444: step 2454, loss 0.51151, acc 0.796875\n",
      "2018-05-23T03:36:45.144512: step 2455, loss 0.543975, acc 0.78125\n",
      "2018-05-23T03:36:45.528486: step 2456, loss 0.481306, acc 0.75\n",
      "2018-05-23T03:36:45.885530: step 2457, loss 0.441215, acc 0.78125\n",
      "2018-05-23T03:36:46.242573: step 2458, loss 0.555997, acc 0.796875\n",
      "2018-05-23T03:36:46.585661: step 2459, loss 0.625914, acc 0.671875\n",
      "2018-05-23T03:36:46.940705: step 2460, loss 0.52086, acc 0.75\n",
      "2018-05-23T03:36:47.296754: step 2461, loss 0.425579, acc 0.796875\n",
      "2018-05-23T03:36:47.645821: step 2462, loss 0.449591, acc 0.78125\n",
      "2018-05-23T03:36:47.988903: step 2463, loss 0.347262, acc 0.828125\n",
      "2018-05-23T03:36:48.333981: step 2464, loss 0.551276, acc 0.703125\n",
      "2018-05-23T03:36:48.678064: step 2465, loss 0.45839, acc 0.78125\n",
      "2018-05-23T03:36:49.028122: step 2466, loss 0.530056, acc 0.765625\n",
      "2018-05-23T03:36:49.380184: step 2467, loss 0.439934, acc 0.765625\n",
      "2018-05-23T03:36:49.729246: step 2468, loss 0.546233, acc 0.75\n",
      "2018-05-23T03:36:50.072329: step 2469, loss 0.48921, acc 0.75\n",
      "2018-05-23T03:36:50.429374: step 2470, loss 0.395084, acc 0.796875\n",
      "2018-05-23T03:36:50.830304: step 2471, loss 0.558545, acc 0.75\n",
      "2018-05-23T03:36:51.187347: step 2472, loss 0.425023, acc 0.796875\n",
      "2018-05-23T03:36:51.545388: step 2473, loss 0.52743, acc 0.75\n",
      "2018-05-23T03:36:51.914400: step 2474, loss 0.419524, acc 0.8125\n",
      "2018-05-23T03:36:52.276433: step 2475, loss 0.373163, acc 0.796875\n",
      "2018-05-23T03:36:52.666388: step 2476, loss 0.519412, acc 0.75\n",
      "2018-05-23T03:36:53.017451: step 2477, loss 0.459598, acc 0.765625\n",
      "2018-05-23T03:36:53.366516: step 2478, loss 0.509995, acc 0.671875\n",
      "2018-05-23T03:36:53.736526: step 2479, loss 0.506196, acc 0.78125\n",
      "2018-05-23T03:36:54.079608: step 2480, loss 0.602281, acc 0.703125\n",
      "2018-05-23T03:36:54.435656: step 2481, loss 0.426745, acc 0.765625\n",
      "2018-05-23T03:36:54.809657: step 2482, loss 0.456149, acc 0.828125\n",
      "2018-05-23T03:36:55.164705: step 2483, loss 0.450263, acc 0.765625\n",
      "2018-05-23T03:36:55.526737: step 2484, loss 0.655799, acc 0.765625\n",
      "2018-05-23T03:36:55.877799: step 2485, loss 0.617424, acc 0.6875\n",
      "2018-05-23T03:36:56.221876: step 2486, loss 0.481504, acc 0.734375\n",
      "2018-05-23T03:36:56.583909: step 2487, loss 0.447139, acc 0.796875\n",
      "2018-05-23T03:36:56.980848: step 2488, loss 0.56413, acc 0.703125\n",
      "2018-05-23T03:36:57.321935: step 2489, loss 0.554248, acc 0.734375\n",
      "2018-05-23T03:36:57.681970: step 2490, loss 0.387294, acc 0.828125\n",
      "2018-05-23T03:36:58.049987: step 2491, loss 0.449916, acc 0.765625\n",
      "2018-05-23T03:36:58.408028: step 2492, loss 0.540522, acc 0.796875\n",
      "2018-05-23T03:36:58.778039: step 2493, loss 0.532823, acc 0.703125\n",
      "2018-05-23T03:36:59.144061: step 2494, loss 0.552973, acc 0.765625\n",
      "2018-05-23T03:36:59.500108: step 2495, loss 0.47466, acc 0.796875\n",
      "2018-05-23T03:36:59.866130: step 2496, loss 0.527819, acc 0.734375\n",
      "2018-05-23T03:37:00.235659: step 2497, loss 0.52034, acc 0.71875\n",
      "2018-05-23T03:37:00.680468: step 2498, loss 0.379588, acc 0.84375\n",
      "2018-05-23T03:37:01.069429: step 2499, loss 0.454861, acc 0.859375\n",
      "2018-05-23T03:37:01.421485: step 2500, loss 0.468706, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:37:06.099970: step 2500, loss 0.531951, acc 0.739106\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2500\n",
      "\n",
      "2018-05-23T03:37:07.435416: step 2501, loss 0.427447, acc 0.828125\n",
      "2018-05-23T03:37:07.810411: step 2502, loss 0.448186, acc 0.75\n",
      "2018-05-23T03:37:08.175431: step 2503, loss 0.376347, acc 0.875\n",
      "2018-05-23T03:37:08.570375: step 2504, loss 0.411226, acc 0.828125\n",
      "2018-05-23T03:37:08.942381: step 2505, loss 0.641956, acc 0.6875\n",
      "2018-05-23T03:37:09.309399: step 2506, loss 0.666678, acc 0.609375\n",
      "2018-05-23T03:37:09.654477: step 2507, loss 0.619794, acc 0.703125\n",
      "2018-05-23T03:37:09.998555: step 2508, loss 0.345608, acc 0.8125\n",
      "2018-05-23T03:37:10.380533: step 2509, loss 0.472073, acc 0.75\n",
      "2018-05-23T03:37:10.746555: step 2510, loss 0.542562, acc 0.703125\n",
      "2018-05-23T03:37:11.095622: step 2511, loss 0.537973, acc 0.71875\n",
      "2018-05-23T03:37:11.481589: step 2512, loss 0.503509, acc 0.734375\n",
      "2018-05-23T03:37:11.853593: step 2513, loss 0.314614, acc 0.875\n",
      "2018-05-23T03:37:12.207647: step 2514, loss 0.588378, acc 0.6875\n",
      "2018-05-23T03:37:12.575662: step 2515, loss 0.586869, acc 0.703125\n",
      "2018-05-23T03:37:12.924728: step 2516, loss 0.585385, acc 0.828125\n",
      "2018-05-23T03:37:13.277782: step 2517, loss 0.455798, acc 0.796875\n",
      "2018-05-23T03:37:13.665744: step 2518, loss 0.467387, acc 0.75\n",
      "2018-05-23T03:37:14.010822: step 2519, loss 0.510568, acc 0.75\n",
      "2018-05-23T03:37:14.365871: step 2520, loss 0.505728, acc 0.734375\n",
      "2018-05-23T03:37:14.722916: step 2521, loss 0.624675, acc 0.765625\n",
      "2018-05-23T03:37:15.072981: step 2522, loss 0.463089, acc 0.796875\n",
      "2018-05-23T03:37:15.436008: step 2523, loss 0.489323, acc 0.75\n",
      "2018-05-23T03:37:15.786072: step 2524, loss 0.442602, acc 0.796875\n",
      "2018-05-23T03:37:16.129154: step 2525, loss 0.571882, acc 0.71875\n",
      "2018-05-23T03:37:16.488195: step 2526, loss 0.488181, acc 0.75\n",
      "2018-05-23T03:37:16.937990: step 2527, loss 0.520024, acc 0.71875\n",
      "2018-05-23T03:37:17.292045: step 2528, loss 0.468119, acc 0.765625\n",
      "2018-05-23T03:37:17.672027: step 2529, loss 0.496309, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:37:18.029074: step 2530, loss 0.583797, acc 0.6875\n",
      "2018-05-23T03:37:18.384124: step 2531, loss 0.509216, acc 0.828125\n",
      "2018-05-23T03:37:18.803001: step 2532, loss 0.4408, acc 0.796875\n",
      "2018-05-23T03:37:19.168027: step 2533, loss 0.624441, acc 0.671875\n",
      "2018-05-23T03:37:19.538035: step 2534, loss 0.556861, acc 0.703125\n",
      "2018-05-23T03:37:19.937966: step 2535, loss 0.460737, acc 0.796875\n",
      "2018-05-23T03:37:20.301994: step 2536, loss 0.455415, acc 0.75\n",
      "2018-05-23T03:37:20.664022: step 2537, loss 0.389625, acc 0.8125\n",
      "2018-05-23T03:37:21.036029: step 2538, loss 0.485038, acc 0.765625\n",
      "2018-05-23T03:37:21.383099: step 2539, loss 0.368119, acc 0.8125\n",
      "2018-05-23T03:37:21.735157: step 2540, loss 0.513399, acc 0.703125\n",
      "2018-05-23T03:37:22.115142: step 2541, loss 0.468595, acc 0.765625\n",
      "2018-05-23T03:37:22.463212: step 2542, loss 0.493181, acc 0.75\n",
      "2018-05-23T03:37:22.835215: step 2543, loss 0.484149, acc 0.796875\n",
      "2018-05-23T03:37:23.181291: step 2544, loss 0.353981, acc 0.828125\n",
      "2018-05-23T03:37:23.524372: step 2545, loss 0.61175, acc 0.6875\n",
      "2018-05-23T03:37:23.895379: step 2546, loss 0.466397, acc 0.8125\n",
      "2018-05-23T03:37:24.271373: step 2547, loss 0.523196, acc 0.734375\n",
      "2018-05-23T03:37:24.622120: step 2548, loss 0.587557, acc 0.609375\n",
      "2018-05-23T03:37:24.970188: step 2549, loss 0.596051, acc 0.65625\n",
      "2018-05-23T03:37:25.341198: step 2550, loss 0.591482, acc 0.734375\n",
      "2018-05-23T03:37:25.702233: step 2551, loss 0.369778, acc 0.8125\n",
      "2018-05-23T03:37:26.046311: step 2552, loss 0.49433, acc 0.6875\n",
      "2018-05-23T03:37:26.471175: step 2553, loss 0.630995, acc 0.703125\n",
      "2018-05-23T03:37:26.830215: step 2554, loss 0.411772, acc 0.78125\n",
      "2018-05-23T03:37:27.182272: step 2555, loss 0.379366, acc 0.828125\n",
      "2018-05-23T03:37:27.528347: step 2556, loss 0.4379, acc 0.78125\n",
      "2018-05-23T03:37:27.882401: step 2557, loss 0.470515, acc 0.8125\n",
      "2018-05-23T03:37:28.231465: step 2558, loss 0.509379, acc 0.796875\n",
      "2018-05-23T03:37:28.644361: step 2559, loss 0.418428, acc 0.8125\n",
      "2018-05-23T03:37:28.995425: step 2560, loss 0.554127, acc 0.6875\n",
      "2018-05-23T03:37:29.333518: step 2561, loss 0.456848, acc 0.796875\n",
      "2018-05-23T03:37:29.738433: step 2562, loss 0.512002, acc 0.78125\n",
      "2018-05-23T03:37:30.088498: step 2563, loss 0.39921, acc 0.828125\n",
      "2018-05-23T03:37:30.486432: step 2564, loss 0.455587, acc 0.78125\n",
      "2018-05-23T03:37:30.882373: step 2565, loss 0.389955, acc 0.859375\n",
      "2018-05-23T03:37:31.230443: step 2566, loss 0.499603, acc 0.734375\n",
      "2018-05-23T03:37:31.585492: step 2567, loss 0.511834, acc 0.75\n",
      "2018-05-23T03:37:31.937549: step 2568, loss 0.650138, acc 0.65625\n",
      "2018-05-23T03:37:32.283626: step 2569, loss 0.497152, acc 0.78125\n",
      "2018-05-23T03:37:32.628701: step 2570, loss 0.437381, acc 0.8125\n",
      "2018-05-23T03:37:32.977769: step 2571, loss 0.449023, acc 0.75\n",
      "2018-05-23T03:37:33.323843: step 2572, loss 0.436258, acc 0.8125\n",
      "2018-05-23T03:37:33.732750: step 2573, loss 0.478254, acc 0.796875\n",
      "2018-05-23T03:37:34.083810: step 2574, loss 0.470784, acc 0.75\n",
      "2018-05-23T03:37:34.425895: step 2575, loss 0.510418, acc 0.8125\n",
      "2018-05-23T03:37:34.812859: step 2576, loss 0.452403, acc 0.78125\n",
      "2018-05-23T03:37:35.181873: step 2577, loss 0.574627, acc 0.671875\n",
      "2018-05-23T03:37:35.522959: step 2578, loss 0.454681, acc 0.8125\n",
      "2018-05-23T03:37:35.898952: step 2579, loss 0.538723, acc 0.6875\n",
      "2018-05-23T03:37:36.260984: step 2580, loss 0.459127, acc 0.765625\n",
      "2018-05-23T03:37:36.623017: step 2581, loss 0.565482, acc 0.671875\n",
      "2018-05-23T03:37:36.967097: step 2582, loss 0.449662, acc 0.75\n",
      "2018-05-23T03:37:37.433848: step 2583, loss 0.556672, acc 0.765625\n",
      "2018-05-23T03:37:37.935505: step 2584, loss 0.586122, acc 0.75\n",
      "2018-05-23T03:37:38.414223: step 2585, loss 0.45813, acc 0.734375\n",
      "2018-05-23T03:37:39.026586: step 2586, loss 0.702458, acc 0.65625\n",
      "2018-05-23T03:37:39.517273: step 2587, loss 0.443866, acc 0.8125\n",
      "2018-05-23T03:37:40.065806: step 2588, loss 0.418859, acc 0.828125\n",
      "2018-05-23T03:37:40.618327: step 2589, loss 0.454992, acc 0.8125\n",
      "2018-05-23T03:37:41.040198: step 2590, loss 0.624858, acc 0.6875\n",
      "2018-05-23T03:37:41.600699: step 2591, loss 0.858265, acc 0.59375\n",
      "2018-05-23T03:37:42.061466: step 2592, loss 0.574226, acc 0.734375\n",
      "2018-05-23T03:37:42.515252: step 2593, loss 0.437117, acc 0.78125\n",
      "2018-05-23T03:37:42.956079: step 2594, loss 0.625171, acc 0.71875\n",
      "2018-05-23T03:37:43.342043: step 2595, loss 0.486783, acc 0.75\n",
      "2018-05-23T03:37:43.912517: step 2596, loss 0.678465, acc 0.6875\n",
      "2018-05-23T03:37:44.486248: step 2597, loss 0.366291, acc 0.828125\n",
      "2018-05-23T03:37:45.136223: step 2598, loss 0.402408, acc 0.859375\n",
      "2018-05-23T03:37:45.644860: step 2599, loss 0.371595, acc 0.828125\n",
      "2018-05-23T03:37:46.129564: step 2600, loss 0.551881, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:37:52.452647: step 2600, loss 0.53467, acc 0.734391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2600\n",
      "\n",
      "2018-05-23T03:37:53.863872: step 2601, loss 0.515421, acc 0.71875\n",
      "2018-05-23T03:37:54.261809: step 2602, loss 0.536553, acc 0.671875\n",
      "2018-05-23T03:37:54.667722: step 2603, loss 0.480906, acc 0.78125\n",
      "2018-05-23T03:37:55.016790: step 2604, loss 0.529551, acc 0.78125\n",
      "2018-05-23T03:37:55.367849: step 2605, loss 0.508436, acc 0.734375\n",
      "2018-05-23T03:37:55.743844: step 2606, loss 0.564512, acc 0.734375\n",
      "2018-05-23T03:37:56.096898: step 2607, loss 0.459601, acc 0.8125\n",
      "2018-05-23T03:37:56.456938: step 2608, loss 0.395721, acc 0.796875\n",
      "2018-05-23T03:37:56.848889: step 2609, loss 0.661635, acc 0.71875\n",
      "2018-05-23T03:37:57.205933: step 2610, loss 0.37052, acc 0.84375\n",
      "2018-05-23T03:37:57.546024: step 2611, loss 0.648281, acc 0.671875\n",
      "2018-05-23T03:37:57.954928: step 2612, loss 0.541049, acc 0.703125\n",
      "2018-05-23T03:37:58.307983: step 2613, loss 0.472934, acc 0.765625\n",
      "2018-05-23T03:37:58.667024: step 2614, loss 0.444079, acc 0.828125\n",
      "2018-05-23T03:37:59.017088: step 2615, loss 0.535, acc 0.703125\n",
      "2018-05-23T03:37:59.365155: step 2616, loss 0.497052, acc 0.796875\n",
      "2018-05-23T03:37:59.722201: step 2617, loss 0.505225, acc 0.734375\n",
      "2018-05-23T03:38:00.135096: step 2618, loss 0.472194, acc 0.75\n",
      "2018-05-23T03:38:00.535026: step 2619, loss 0.461237, acc 0.78125\n",
      "2018-05-23T03:38:00.972855: step 2620, loss 0.545868, acc 0.71875\n",
      "2018-05-23T03:38:01.357827: step 2621, loss 0.469896, acc 0.765625\n",
      "2018-05-23T03:38:01.791665: step 2622, loss 0.475181, acc 0.796875\n",
      "2018-05-23T03:38:02.170651: step 2623, loss 0.484734, acc 0.75\n",
      "2018-05-23T03:38:02.542657: step 2624, loss 0.664871, acc 0.578125\n",
      "2018-05-23T03:38:02.958423: step 2625, loss 0.384959, acc 0.84375\n",
      "2018-05-23T03:38:03.320452: step 2626, loss 0.477111, acc 0.75\n",
      "2018-05-23T03:38:03.686473: step 2627, loss 0.590388, acc 0.640625\n",
      "2018-05-23T03:38:04.050500: step 2628, loss 0.405114, acc 0.8125\n",
      "2018-05-23T03:38:04.406580: step 2629, loss 0.497204, acc 0.71875\n",
      "2018-05-23T03:38:04.766582: step 2630, loss 0.472816, acc 0.734375\n",
      "2018-05-23T03:38:05.109665: step 2631, loss 0.579577, acc 0.703125\n",
      "2018-05-23T03:38:05.443774: step 2632, loss 0.483194, acc 0.71875\n",
      "2018-05-23T03:38:05.781866: step 2633, loss 0.401984, acc 0.84375\n",
      "2018-05-23T03:38:06.120959: step 2634, loss 0.493167, acc 0.765625\n",
      "2018-05-23T03:38:06.466036: step 2635, loss 0.517899, acc 0.703125\n",
      "2018-05-23T03:38:06.804131: step 2636, loss 0.518987, acc 0.703125\n",
      "2018-05-23T03:38:07.146449: step 2637, loss 0.552967, acc 0.71875\n",
      "2018-05-23T03:38:07.495515: step 2638, loss 0.514746, acc 0.765625\n",
      "2018-05-23T03:38:07.841589: step 2639, loss 0.605661, acc 0.71875\n",
      "2018-05-23T03:38:08.175697: step 2640, loss 0.465635, acc 0.75\n",
      "2018-05-23T03:38:08.527754: step 2641, loss 0.543673, acc 0.71875\n",
      "2018-05-23T03:38:08.875822: step 2642, loss 0.400592, acc 0.8125\n",
      "2018-05-23T03:38:09.209928: step 2643, loss 0.337463, acc 0.875\n",
      "2018-05-23T03:38:09.550019: step 2644, loss 0.486571, acc 0.765625\n",
      "2018-05-23T03:38:09.890110: step 2645, loss 0.40416, acc 0.8125\n",
      "2018-05-23T03:38:10.234190: step 2646, loss 0.499969, acc 0.8125\n",
      "2018-05-23T03:38:10.577270: step 2647, loss 0.507077, acc 0.78125\n",
      "2018-05-23T03:38:10.914368: step 2648, loss 0.475545, acc 0.75\n",
      "2018-05-23T03:38:11.252464: step 2649, loss 0.514585, acc 0.71875\n",
      "2018-05-23T03:38:11.583578: step 2650, loss 0.509732, acc 0.71875\n",
      "2018-05-23T03:38:11.937632: step 2651, loss 0.520699, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:38:12.286698: step 2652, loss 0.462732, acc 0.796875\n",
      "2018-05-23T03:38:12.629780: step 2653, loss 0.399029, acc 0.796875\n",
      "2018-05-23T03:38:12.958899: step 2654, loss 0.50798, acc 0.75\n",
      "2018-05-23T03:38:13.287021: step 2655, loss 0.485782, acc 0.703125\n",
      "2018-05-23T03:38:13.649054: step 2656, loss 0.382911, acc 0.859375\n",
      "2018-05-23T03:38:13.984157: step 2657, loss 0.50075, acc 0.796875\n",
      "2018-05-23T03:38:14.311217: step 2658, loss 0.45294, acc 0.78125\n",
      "2018-05-23T03:38:14.639339: step 2659, loss 0.46772, acc 0.765625\n",
      "2018-05-23T03:38:14.969458: step 2660, loss 0.462238, acc 0.703125\n",
      "2018-05-23T03:38:15.299575: step 2661, loss 0.550769, acc 0.75\n",
      "2018-05-23T03:38:15.677564: step 2662, loss 0.638928, acc 0.6875\n",
      "2018-05-23T03:38:16.005685: step 2663, loss 0.464125, acc 0.796875\n",
      "2018-05-23T03:38:16.328821: step 2664, loss 0.615192, acc 0.75\n",
      "2018-05-23T03:38:16.664920: step 2665, loss 0.683721, acc 0.703125\n",
      "2018-05-23T03:38:16.989053: step 2666, loss 0.446531, acc 0.8125\n",
      "2018-05-23T03:38:17.318176: step 2667, loss 0.51274, acc 0.734375\n",
      "2018-05-23T03:38:17.650285: step 2668, loss 0.556915, acc 0.703125\n",
      "2018-05-23T03:38:17.974417: step 2669, loss 0.525917, acc 0.71875\n",
      "2018-05-23T03:38:18.298915: step 2670, loss 0.475159, acc 0.75\n",
      "2018-05-23T03:38:18.626039: step 2671, loss 0.492392, acc 0.765625\n",
      "2018-05-23T03:38:18.956155: step 2672, loss 0.421323, acc 0.796875\n",
      "2018-05-23T03:38:19.283280: step 2673, loss 0.415374, acc 0.828125\n",
      "2018-05-23T03:38:19.609407: step 2674, loss 0.528369, acc 0.796875\n",
      "2018-05-23T03:38:19.940524: step 2675, loss 0.506738, acc 0.78125\n",
      "2018-05-23T03:38:20.266649: step 2676, loss 0.508663, acc 0.734375\n",
      "2018-05-23T03:38:20.590818: step 2677, loss 0.40502, acc 0.828125\n",
      "2018-05-23T03:38:20.921897: step 2678, loss 0.502163, acc 0.734375\n",
      "2018-05-23T03:38:21.247029: step 2679, loss 0.393874, acc 0.796875\n",
      "2018-05-23T03:38:21.571162: step 2680, loss 0.458678, acc 0.734375\n",
      "2018-05-23T03:38:21.924215: step 2681, loss 0.436393, acc 0.828125\n",
      "2018-05-23T03:38:22.252338: step 2682, loss 0.563572, acc 0.671875\n",
      "2018-05-23T03:38:22.576472: step 2683, loss 0.415403, acc 0.796875\n",
      "2018-05-23T03:38:22.906587: step 2684, loss 0.452455, acc 0.84375\n",
      "2018-05-23T03:38:23.228725: step 2685, loss 0.477026, acc 0.8125\n",
      "2018-05-23T03:38:23.552858: step 2686, loss 0.585239, acc 0.75\n",
      "2018-05-23T03:38:23.882976: step 2687, loss 0.417977, acc 0.8125\n",
      "2018-05-23T03:38:24.212096: step 2688, loss 0.548511, acc 0.71875\n",
      "2018-05-23T03:38:24.539220: step 2689, loss 0.367691, acc 0.84375\n",
      "2018-05-23T03:38:24.865347: step 2690, loss 0.522213, acc 0.765625\n",
      "2018-05-23T03:38:25.194466: step 2691, loss 0.428338, acc 0.828125\n",
      "2018-05-23T03:38:25.520595: step 2692, loss 0.521772, acc 0.703125\n",
      "2018-05-23T03:38:25.852708: step 2693, loss 0.462429, acc 0.734375\n",
      "2018-05-23T03:38:26.187809: step 2694, loss 0.483352, acc 0.75\n",
      "2018-05-23T03:38:26.512940: step 2695, loss 0.391529, acc 0.828125\n",
      "2018-05-23T03:38:26.842061: step 2696, loss 0.448447, acc 0.734375\n",
      "2018-05-23T03:38:27.185141: step 2697, loss 0.514623, acc 0.734375\n",
      "2018-05-23T03:38:27.509275: step 2698, loss 0.414868, acc 0.75\n",
      "2018-05-23T03:38:27.838393: step 2699, loss 0.449702, acc 0.734375\n",
      "2018-05-23T03:38:28.179481: step 2700, loss 0.401802, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:38:32.610628: step 2700, loss 0.519094, acc 0.739534\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2700\n",
      "\n",
      "2018-05-23T03:38:33.710934: step 2701, loss 0.47306, acc 0.75\n",
      "2018-05-23T03:38:34.179679: step 2702, loss 0.466621, acc 0.75\n",
      "2018-05-23T03:38:34.586590: step 2703, loss 0.459297, acc 0.765625\n",
      "2018-05-23T03:38:34.933663: step 2704, loss 0.543636, acc 0.71875\n",
      "2018-05-23T03:38:35.266773: step 2705, loss 0.579048, acc 0.65625\n",
      "2018-05-23T03:38:35.632791: step 2706, loss 0.338391, acc 0.84375\n",
      "2018-05-23T03:38:35.967896: step 2707, loss 0.436686, acc 0.796875\n",
      "2018-05-23T03:38:36.297017: step 2708, loss 0.6131, acc 0.734375\n",
      "2018-05-23T03:38:36.662039: step 2709, loss 0.42922, acc 0.796875\n",
      "2018-05-23T03:38:36.990162: step 2710, loss 0.450878, acc 0.78125\n",
      "2018-05-23T03:38:37.325263: step 2711, loss 0.655199, acc 0.6875\n",
      "2018-05-23T03:38:37.669343: step 2712, loss 0.586763, acc 0.6875\n",
      "2018-05-23T03:38:37.993477: step 2713, loss 0.632559, acc 0.6875\n",
      "2018-05-23T03:38:38.327582: step 2714, loss 0.334347, acc 0.890625\n",
      "2018-05-23T03:38:38.653710: step 2715, loss 0.51054, acc 0.765625\n",
      "2018-05-23T03:38:38.984826: step 2716, loss 0.548782, acc 0.703125\n",
      "2018-05-23T03:38:39.314941: step 2717, loss 0.476449, acc 0.828125\n",
      "2018-05-23T03:38:39.655031: step 2718, loss 0.488172, acc 0.734375\n",
      "2018-05-23T03:38:39.991132: step 2719, loss 0.500198, acc 0.765625\n",
      "2018-05-23T03:38:40.337206: step 2720, loss 0.341889, acc 0.890625\n",
      "2018-05-23T03:38:40.689265: step 2721, loss 0.441105, acc 0.796875\n",
      "2018-05-23T03:38:41.037334: step 2722, loss 0.464436, acc 0.75\n",
      "2018-05-23T03:38:41.384406: step 2723, loss 0.445837, acc 0.828125\n",
      "2018-05-23T03:38:41.734468: step 2724, loss 0.413039, acc 0.8125\n",
      "2018-05-23T03:38:42.087524: step 2725, loss 0.418006, acc 0.828125\n",
      "2018-05-23T03:38:42.435594: step 2726, loss 0.528989, acc 0.71875\n",
      "2018-05-23T03:38:42.793636: step 2727, loss 0.613816, acc 0.671875\n",
      "2018-05-23T03:38:43.136717: step 2728, loss 0.473919, acc 0.734375\n",
      "2018-05-23T03:38:43.480797: step 2729, loss 0.514546, acc 0.765625\n",
      "2018-05-23T03:38:43.840832: step 2730, loss 0.444383, acc 0.71875\n",
      "2018-05-23T03:38:44.184914: step 2731, loss 0.456754, acc 0.734375\n",
      "2018-05-23T03:38:44.515031: step 2732, loss 0.412065, acc 0.828125\n",
      "2018-05-23T03:38:44.862103: step 2733, loss 0.430849, acc 0.796875\n",
      "2018-05-23T03:38:45.192218: step 2734, loss 0.429281, acc 0.765625\n",
      "2018-05-23T03:38:45.542281: step 2735, loss 0.609657, acc 0.71875\n",
      "2018-05-23T03:38:45.893342: step 2736, loss 0.480111, acc 0.8125\n",
      "2018-05-23T03:38:46.245400: step 2737, loss 0.554756, acc 0.796875\n",
      "2018-05-23T03:38:46.586488: step 2738, loss 0.435915, acc 0.796875\n",
      "2018-05-23T03:38:46.920595: step 2739, loss 0.531582, acc 0.703125\n",
      "2018-05-23T03:38:47.249714: step 2740, loss 0.362204, acc 0.875\n",
      "2018-05-23T03:38:47.575841: step 2741, loss 0.57891, acc 0.703125\n",
      "2018-05-23T03:38:47.914934: step 2742, loss 0.413818, acc 0.796875\n",
      "2018-05-23T03:38:48.238071: step 2743, loss 0.555349, acc 0.734375\n",
      "2018-05-23T03:38:48.567191: step 2744, loss 0.460684, acc 0.828125\n",
      "2018-05-23T03:38:48.901296: step 2745, loss 0.593707, acc 0.65625\n",
      "2018-05-23T03:38:49.234405: step 2746, loss 0.546326, acc 0.75\n",
      "2018-05-23T03:38:49.563525: step 2747, loss 0.40437, acc 0.796875\n",
      "2018-05-23T03:38:49.898628: step 2748, loss 0.559254, acc 0.75\n",
      "2018-05-23T03:38:50.232733: step 2749, loss 0.404746, acc 0.84375\n",
      "2018-05-23T03:38:50.556866: step 2750, loss 0.417265, acc 0.78125\n",
      "2018-05-23T03:38:50.884988: step 2751, loss 0.433492, acc 0.78125\n",
      "2018-05-23T03:38:51.222086: step 2752, loss 0.402516, acc 0.828125\n",
      "2018-05-23T03:38:51.552204: step 2753, loss 0.40745, acc 0.796875\n",
      "2018-05-23T03:38:51.885314: step 2754, loss 0.584421, acc 0.6875\n",
      "2018-05-23T03:38:52.218423: step 2755, loss 0.522498, acc 0.703125\n",
      "2018-05-23T03:38:52.546543: step 2756, loss 0.562359, acc 0.703125\n",
      "2018-05-23T03:38:52.880652: step 2757, loss 0.422912, acc 0.78125\n",
      "2018-05-23T03:38:53.217750: step 2758, loss 0.488275, acc 0.78125\n",
      "2018-05-23T03:38:53.546868: step 2759, loss 0.447578, acc 0.796875\n",
      "2018-05-23T03:38:53.880975: step 2760, loss 0.720061, acc 0.625\n",
      "2018-05-23T03:38:54.220069: step 2761, loss 0.745469, acc 0.65625\n",
      "2018-05-23T03:38:54.553177: step 2762, loss 0.378675, acc 0.828125\n",
      "2018-05-23T03:38:54.882297: step 2763, loss 0.445482, acc 0.78125\n",
      "2018-05-23T03:38:55.212415: step 2764, loss 0.477206, acc 0.796875\n",
      "2018-05-23T03:38:55.541532: step 2765, loss 0.575448, acc 0.6875\n",
      "2018-05-23T03:38:55.868657: step 2766, loss 0.428233, acc 0.78125\n",
      "2018-05-23T03:38:56.198774: step 2767, loss 0.461884, acc 0.796875\n",
      "2018-05-23T03:38:56.530885: step 2768, loss 0.598782, acc 0.65625\n",
      "2018-05-23T03:38:56.867983: step 2769, loss 0.403156, acc 0.78125\n",
      "2018-05-23T03:38:57.194112: step 2770, loss 0.498616, acc 0.796875\n",
      "2018-05-23T03:38:57.517248: step 2771, loss 0.436671, acc 0.8125\n",
      "2018-05-23T03:38:57.858334: step 2772, loss 0.420674, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:38:58.197429: step 2773, loss 0.427517, acc 0.8125\n",
      "2018-05-23T03:38:58.534527: step 2774, loss 0.549266, acc 0.71875\n",
      "2018-05-23T03:38:58.865641: step 2775, loss 0.460155, acc 0.796875\n",
      "2018-05-23T03:38:59.199747: step 2776, loss 0.525241, acc 0.75\n",
      "2018-05-23T03:38:59.520887: step 2777, loss 0.503571, acc 0.8125\n",
      "2018-05-23T03:38:59.848012: step 2778, loss 0.594265, acc 0.703125\n",
      "2018-05-23T03:39:00.196079: step 2779, loss 0.518859, acc 0.734375\n",
      "2018-05-23T03:39:00.546143: step 2780, loss 0.499484, acc 0.796875\n",
      "2018-05-23T03:39:00.886235: step 2781, loss 0.34504, acc 0.8125\n",
      "2018-05-23T03:39:01.221339: step 2782, loss 0.354428, acc 0.84375\n",
      "2018-05-23T03:39:01.562425: step 2783, loss 0.491717, acc 0.734375\n",
      "2018-05-23T03:39:01.910494: step 2784, loss 0.436884, acc 0.75\n",
      "2018-05-23T03:39:02.256568: step 2785, loss 0.564435, acc 0.734375\n",
      "2018-05-23T03:39:02.606633: step 2786, loss 0.527681, acc 0.71875\n",
      "2018-05-23T03:39:02.952706: step 2787, loss 0.563719, acc 0.703125\n",
      "2018-05-23T03:39:03.307756: step 2788, loss 0.493177, acc 0.765625\n",
      "2018-05-23T03:39:03.655825: step 2789, loss 0.411346, acc 0.8125\n",
      "2018-05-23T03:39:04.011873: step 2790, loss 0.467354, acc 0.765625\n",
      "2018-05-23T03:39:04.366923: step 2791, loss 0.490097, acc 0.71875\n",
      "2018-05-23T03:39:04.711003: step 2792, loss 0.509744, acc 0.765625\n",
      "2018-05-23T03:39:05.057075: step 2793, loss 0.573808, acc 0.75\n",
      "2018-05-23T03:39:05.413125: step 2794, loss 0.621426, acc 0.59375\n",
      "2018-05-23T03:39:05.760195: step 2795, loss 0.571671, acc 0.75\n",
      "2018-05-23T03:39:06.161126: step 2796, loss 0.429954, acc 0.8125\n",
      "2018-05-23T03:39:06.502212: step 2797, loss 0.493845, acc 0.75\n",
      "2018-05-23T03:39:06.897153: step 2798, loss 0.381803, acc 0.859375\n",
      "2018-05-23T03:39:07.219293: step 2799, loss 0.509997, acc 0.765625\n",
      "2018-05-23T03:39:07.546417: step 2800, loss 0.56289, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:39:11.957617: step 2800, loss 0.532239, acc 0.73982\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2800\n",
      "\n",
      "2018-05-23T03:39:13.136768: step 2801, loss 0.329719, acc 0.84375\n",
      "2018-05-23T03:39:13.570608: step 2802, loss 0.362082, acc 0.8125\n",
      "2018-05-23T03:39:13.918679: step 2803, loss 0.414672, acc 0.78125\n",
      "2018-05-23T03:39:14.295667: step 2804, loss 0.414258, acc 0.765625\n",
      "2018-05-23T03:39:14.623789: step 2805, loss 0.575546, acc 0.75\n",
      "2018-05-23T03:39:14.950917: step 2806, loss 0.594289, acc 0.734375\n",
      "2018-05-23T03:39:15.289012: step 2807, loss 0.512994, acc 0.703125\n",
      "2018-05-23T03:39:15.661015: step 2808, loss 0.432408, acc 0.828125\n",
      "2018-05-23T03:39:15.986146: step 2809, loss 0.445822, acc 0.75\n",
      "2018-05-23T03:39:16.308284: step 2810, loss 0.392281, acc 0.8125\n",
      "2018-05-23T03:39:16.658347: step 2811, loss 0.599908, acc 0.75\n",
      "2018-05-23T03:39:16.979488: step 2812, loss 0.444487, acc 0.828125\n",
      "2018-05-23T03:39:17.300631: step 2813, loss 0.446431, acc 0.84375\n",
      "2018-05-23T03:39:17.628751: step 2814, loss 0.485371, acc 0.78125\n",
      "2018-05-23T03:39:17.949892: step 2815, loss 0.439898, acc 0.796875\n",
      "2018-05-23T03:39:18.277053: step 2816, loss 0.470297, acc 0.75\n",
      "2018-05-23T03:39:18.590181: step 2817, loss 0.408543, acc 0.859375\n",
      "2018-05-23T03:39:18.912319: step 2818, loss 0.649336, acc 0.6875\n",
      "2018-05-23T03:39:19.232464: step 2819, loss 0.523668, acc 0.703125\n",
      "2018-05-23T03:39:19.551607: step 2820, loss 0.306136, acc 0.875\n",
      "2018-05-23T03:39:19.881724: step 2821, loss 0.450108, acc 0.796875\n",
      "2018-05-23T03:39:20.200870: step 2822, loss 0.516743, acc 0.75\n",
      "2018-05-23T03:39:20.527000: step 2823, loss 0.445911, acc 0.734375\n",
      "2018-05-23T03:39:20.861104: step 2824, loss 0.384898, acc 0.828125\n",
      "2018-05-23T03:39:21.183243: step 2825, loss 0.472393, acc 0.796875\n",
      "2018-05-23T03:39:21.506379: step 2826, loss 0.55824, acc 0.734375\n",
      "2018-05-23T03:39:21.835499: step 2827, loss 0.40994, acc 0.8125\n",
      "2018-05-23T03:39:22.163622: step 2828, loss 0.417665, acc 0.84375\n",
      "2018-05-23T03:39:22.484761: step 2829, loss 0.562214, acc 0.6875\n",
      "2018-05-23T03:39:22.819866: step 2830, loss 0.449168, acc 0.8125\n",
      "2018-05-23T03:39:23.145992: step 2831, loss 0.618194, acc 0.703125\n",
      "2018-05-23T03:39:23.471124: step 2832, loss 0.575368, acc 0.75\n",
      "2018-05-23T03:39:23.795255: step 2833, loss 0.491847, acc 0.71875\n",
      "2018-05-23T03:39:24.127367: step 2834, loss 0.297911, acc 0.90625\n",
      "2018-05-23T03:39:24.459480: step 2835, loss 0.610538, acc 0.671875\n",
      "2018-05-23T03:39:24.797573: step 2836, loss 0.537238, acc 0.734375\n",
      "2018-05-23T03:39:25.137664: step 2837, loss 0.577839, acc 0.75\n",
      "2018-05-23T03:39:25.544576: step 2838, loss 0.455914, acc 0.8125\n",
      "2018-05-23T03:39:25.923561: step 2839, loss 0.441454, acc 0.78125\n",
      "2018-05-23T03:39:26.299557: step 2840, loss 0.461726, acc 0.796875\n",
      "2018-05-23T03:39:26.668567: step 2841, loss 0.557471, acc 0.6875\n",
      "2018-05-23T03:39:27.020627: step 2842, loss 0.461044, acc 0.8125\n",
      "2018-05-23T03:39:27.396621: step 2843, loss 0.374471, acc 0.84375\n",
      "2018-05-23T03:39:27.786577: step 2844, loss 0.560274, acc 0.65625\n",
      "2018-05-23T03:39:28.177531: step 2845, loss 0.630964, acc 0.703125\n",
      "2018-05-23T03:39:28.567487: step 2846, loss 0.459589, acc 0.71875\n",
      "2018-05-23T03:39:28.928523: step 2847, loss 0.594987, acc 0.703125\n",
      "2018-05-23T03:39:29.272601: step 2848, loss 0.547255, acc 0.765625\n",
      "2018-05-23T03:39:29.610697: step 2849, loss 0.450512, acc 0.8125\n",
      "2018-05-23T03:39:29.962756: step 2850, loss 0.439974, acc 0.75\n",
      "2018-05-23T03:39:30.302848: step 2851, loss 0.412991, acc 0.828125\n",
      "2018-05-23T03:39:30.664879: step 2852, loss 0.544207, acc 0.71875\n",
      "2018-05-23T03:39:31.012946: step 2853, loss 0.413165, acc 0.8125\n",
      "2018-05-23T03:39:31.354033: step 2854, loss 0.490543, acc 0.734375\n",
      "2018-05-23T03:39:31.695120: step 2855, loss 0.608075, acc 0.703125\n",
      "2018-05-23T03:39:32.044187: step 2856, loss 0.403383, acc 0.828125\n",
      "2018-05-23T03:39:32.385275: step 2857, loss 0.536685, acc 0.78125\n",
      "2018-05-23T03:39:32.735338: step 2858, loss 0.521825, acc 0.75\n",
      "2018-05-23T03:39:33.078421: step 2859, loss 0.495549, acc 0.765625\n",
      "2018-05-23T03:39:33.414522: step 2860, loss 0.497485, acc 0.796875\n",
      "2018-05-23T03:39:33.745637: step 2861, loss 0.564092, acc 0.734375\n",
      "2018-05-23T03:39:34.074757: step 2862, loss 0.60728, acc 0.71875\n",
      "2018-05-23T03:39:34.400882: step 2863, loss 0.391309, acc 0.859375\n",
      "2018-05-23T03:39:34.729004: step 2864, loss 0.451531, acc 0.78125\n",
      "2018-05-23T03:39:35.054136: step 2865, loss 0.444439, acc 0.78125\n",
      "2018-05-23T03:39:35.383255: step 2866, loss 0.540201, acc 0.71875\n",
      "2018-05-23T03:39:35.712374: step 2867, loss 0.447267, acc 0.75\n",
      "2018-05-23T03:39:36.042491: step 2868, loss 0.49392, acc 0.703125\n",
      "2018-05-23T03:39:36.371611: step 2869, loss 0.468033, acc 0.71875\n",
      "2018-05-23T03:39:36.702725: step 2870, loss 0.515203, acc 0.75\n",
      "2018-05-23T03:39:37.144543: step 2871, loss 0.491513, acc 0.796875\n",
      "2018-05-23T03:39:37.535496: step 2872, loss 0.535322, acc 0.75\n",
      "2018-05-23T03:39:37.947395: step 2873, loss 0.520883, acc 0.78125\n",
      "2018-05-23T03:39:38.360291: step 2874, loss 0.447502, acc 0.765625\n",
      "2018-05-23T03:39:38.759224: step 2875, loss 0.435197, acc 0.84375\n",
      "2018-05-23T03:39:39.089341: step 2876, loss 0.440579, acc 0.828125\n",
      "2018-05-23T03:39:39.499244: step 2877, loss 0.612493, acc 0.703125\n",
      "2018-05-23T03:39:39.826368: step 2878, loss 0.587668, acc 0.75\n",
      "2018-05-23T03:39:40.159478: step 2879, loss 0.38604, acc 0.796875\n",
      "2018-05-23T03:39:40.492586: step 2880, loss 0.448334, acc 0.8125\n",
      "2018-05-23T03:39:40.823700: step 2881, loss 0.548396, acc 0.765625\n",
      "2018-05-23T03:39:41.153818: step 2882, loss 0.500824, acc 0.734375\n",
      "2018-05-23T03:39:41.476952: step 2883, loss 0.415723, acc 0.828125\n",
      "2018-05-23T03:39:41.802114: step 2884, loss 0.466934, acc 0.765625\n",
      "2018-05-23T03:39:42.125219: step 2885, loss 0.537831, acc 0.734375\n",
      "2018-05-23T03:39:42.461319: step 2886, loss 0.50789, acc 0.75\n",
      "2018-05-23T03:39:42.787449: step 2887, loss 0.357577, acc 0.859375\n",
      "2018-05-23T03:39:43.110584: step 2888, loss 0.531444, acc 0.75\n",
      "2018-05-23T03:39:43.441698: step 2889, loss 0.558255, acc 0.703125\n",
      "2018-05-23T03:39:43.785633: step 2890, loss 0.486965, acc 0.765625\n",
      "2018-05-23T03:39:44.120769: step 2891, loss 0.515795, acc 0.796875\n",
      "2018-05-23T03:39:44.465812: step 2892, loss 0.473146, acc 0.796875\n",
      "2018-05-23T03:39:44.793935: step 2893, loss 0.560453, acc 0.6875\n",
      "2018-05-23T03:39:45.125049: step 2894, loss 0.489081, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:39:45.474116: step 2895, loss 0.465274, acc 0.765625\n",
      "2018-05-23T03:39:45.800242: step 2896, loss 0.511534, acc 0.765625\n",
      "2018-05-23T03:39:46.127368: step 2897, loss 0.565143, acc 0.703125\n",
      "2018-05-23T03:39:46.460477: step 2898, loss 0.502832, acc 0.75\n",
      "2018-05-23T03:39:46.791590: step 2899, loss 0.492566, acc 0.765625\n",
      "2018-05-23T03:39:47.121709: step 2900, loss 0.636674, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:39:51.547867: step 2900, loss 0.522572, acc 0.73882\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-2900\n",
      "\n",
      "2018-05-23T03:39:52.965075: step 2901, loss 0.515535, acc 0.78125\n",
      "2018-05-23T03:39:53.313144: step 2902, loss 0.481461, acc 0.765625\n",
      "2018-05-23T03:39:53.669193: step 2903, loss 0.492252, acc 0.75\n",
      "2018-05-23T03:39:54.004295: step 2904, loss 0.454293, acc 0.796875\n",
      "2018-05-23T03:39:54.331420: step 2905, loss 0.466526, acc 0.8125\n",
      "2018-05-23T03:39:54.681485: step 2906, loss 0.57842, acc 0.6875\n",
      "2018-05-23T03:39:55.008608: step 2907, loss 0.426462, acc 0.78125\n",
      "2018-05-23T03:39:55.335733: step 2908, loss 0.384706, acc 0.8125\n",
      "2018-05-23T03:39:55.667882: step 2909, loss 0.539776, acc 0.703125\n",
      "2018-05-23T03:39:56.029877: step 2910, loss 0.502113, acc 0.75\n",
      "2018-05-23T03:39:56.392907: step 2911, loss 0.416454, acc 0.8125\n",
      "2018-05-23T03:39:56.733995: step 2912, loss 0.480505, acc 0.71875\n",
      "2018-05-23T03:39:57.066104: step 2913, loss 0.435219, acc 0.796875\n",
      "2018-05-23T03:39:57.397218: step 2914, loss 0.497784, acc 0.78125\n",
      "2018-05-23T03:39:57.729330: step 2915, loss 0.485668, acc 0.84375\n",
      "2018-05-23T03:39:58.067459: step 2916, loss 0.423119, acc 0.78125\n",
      "2018-05-23T03:39:58.391560: step 2917, loss 0.541621, acc 0.71875\n",
      "2018-05-23T03:39:58.736635: step 2918, loss 0.464004, acc 0.78125\n",
      "2018-05-23T03:39:59.061765: step 2919, loss 0.505159, acc 0.703125\n",
      "2018-05-23T03:39:59.392879: step 2920, loss 0.349902, acc 0.875\n",
      "2018-05-23T03:39:59.735963: step 2921, loss 0.463313, acc 0.75\n",
      "2018-05-23T03:40:00.066080: step 2922, loss 0.441169, acc 0.765625\n",
      "2018-05-23T03:40:00.400187: step 2923, loss 0.498899, acc 0.796875\n",
      "2018-05-23T03:40:00.827045: step 2924, loss 0.569735, acc 0.75\n",
      "2018-05-23T03:40:01.159157: step 2925, loss 0.411119, acc 0.828125\n",
      "2018-05-23T03:40:01.535154: step 2926, loss 0.418431, acc 0.796875\n",
      "2018-05-23T03:40:01.899175: step 2927, loss 0.640359, acc 0.6875\n",
      "2018-05-23T03:40:02.227300: step 2928, loss 0.40527, acc 0.84375\n",
      "2018-05-23T03:40:02.605287: step 2929, loss 0.51219, acc 0.71875\n",
      "2018-05-23T03:40:02.991253: step 2930, loss 0.369185, acc 0.84375\n",
      "2018-05-23T03:40:03.327354: step 2931, loss 0.470881, acc 0.828125\n",
      "2018-05-23T03:40:03.672431: step 2932, loss 0.447666, acc 0.71875\n",
      "2018-05-23T03:40:04.008533: step 2933, loss 0.494317, acc 0.8125\n",
      "2018-05-23T03:40:04.352612: step 2934, loss 0.508136, acc 0.6875\n",
      "2018-05-23T03:40:04.725613: step 2935, loss 0.483209, acc 0.703125\n",
      "2018-05-23T03:40:05.061714: step 2936, loss 0.528552, acc 0.6875\n",
      "2018-05-23T03:40:05.393827: step 2937, loss 0.361418, acc 0.828125\n",
      "2018-05-23T03:40:05.724940: step 2938, loss 0.500312, acc 0.75\n",
      "2018-05-23T03:40:06.057052: step 2939, loss 0.402762, acc 0.875\n",
      "2018-05-23T03:40:06.380190: step 2940, loss 0.612041, acc 0.75\n",
      "2018-05-23T03:40:06.714296: step 2941, loss 0.58978, acc 0.71875\n",
      "2018-05-23T03:40:07.046405: step 2942, loss 0.538634, acc 0.796875\n",
      "2018-05-23T03:40:07.378516: step 2943, loss 0.636176, acc 0.640625\n",
      "2018-05-23T03:40:07.702652: step 2944, loss 0.333111, acc 0.875\n",
      "2018-05-23T03:40:08.032766: step 2945, loss 0.475034, acc 0.796875\n",
      "2018-05-23T03:40:08.362884: step 2946, loss 0.483098, acc 0.78125\n",
      "2018-05-23T03:40:08.688016: step 2947, loss 0.507536, acc 0.8125\n",
      "2018-05-23T03:40:09.016138: step 2948, loss 0.458683, acc 0.796875\n",
      "2018-05-23T03:40:09.344259: step 2949, loss 0.618359, acc 0.6875\n",
      "2018-05-23T03:40:09.668392: step 2950, loss 0.559415, acc 0.671875\n",
      "2018-05-23T03:40:09.995516: step 2951, loss 0.404474, acc 0.796875\n",
      "2018-05-23T03:40:10.328624: step 2952, loss 0.379145, acc 0.828125\n",
      "2018-05-23T03:40:10.659740: step 2953, loss 0.562001, acc 0.71875\n",
      "2018-05-23T03:40:10.986863: step 2954, loss 0.364319, acc 0.828125\n",
      "2018-05-23T03:40:11.222235: step 2955, loss 0.866656, acc 0.647059\n",
      "2018-05-23T03:40:11.560332: step 2956, loss 0.55923, acc 0.75\n",
      "2018-05-23T03:40:11.891444: step 2957, loss 0.358498, acc 0.859375\n",
      "2018-05-23T03:40:12.220562: step 2958, loss 0.356546, acc 0.828125\n",
      "2018-05-23T03:40:12.549682: step 2959, loss 0.4289, acc 0.78125\n",
      "2018-05-23T03:40:12.889772: step 2960, loss 0.349682, acc 0.890625\n",
      "2018-05-23T03:40:13.221884: step 2961, loss 0.51925, acc 0.734375\n",
      "2018-05-23T03:40:13.572945: step 2962, loss 0.582615, acc 0.765625\n",
      "2018-05-23T03:40:13.902065: step 2963, loss 0.357752, acc 0.8125\n",
      "2018-05-23T03:40:14.225200: step 2964, loss 0.423496, acc 0.8125\n",
      "2018-05-23T03:40:14.558311: step 2965, loss 0.479222, acc 0.765625\n",
      "2018-05-23T03:40:14.888428: step 2966, loss 0.487045, acc 0.75\n",
      "2018-05-23T03:40:15.219543: step 2967, loss 0.313295, acc 0.875\n",
      "2018-05-23T03:40:15.564620: step 2968, loss 0.468495, acc 0.78125\n",
      "2018-05-23T03:40:15.894734: step 2969, loss 0.47569, acc 0.796875\n",
      "2018-05-23T03:40:16.218867: step 2970, loss 0.521283, acc 0.734375\n",
      "2018-05-23T03:40:16.545991: step 2971, loss 0.376437, acc 0.8125\n",
      "2018-05-23T03:40:16.874115: step 2972, loss 0.356631, acc 0.8125\n",
      "2018-05-23T03:40:17.202236: step 2973, loss 0.418084, acc 0.796875\n",
      "2018-05-23T03:40:17.542327: step 2974, loss 0.289341, acc 0.890625\n",
      "2018-05-23T03:40:17.873443: step 2975, loss 0.341691, acc 0.84375\n",
      "2018-05-23T03:40:18.197573: step 2976, loss 0.415122, acc 0.84375\n",
      "2018-05-23T03:40:18.532678: step 2977, loss 0.332814, acc 0.84375\n",
      "2018-05-23T03:40:18.863791: step 2978, loss 0.464134, acc 0.734375\n",
      "2018-05-23T03:40:19.183934: step 2979, loss 0.423013, acc 0.78125\n",
      "2018-05-23T03:40:19.511059: step 2980, loss 0.518342, acc 0.734375\n",
      "2018-05-23T03:40:19.846163: step 2981, loss 0.461648, acc 0.765625\n",
      "2018-05-23T03:40:20.175282: step 2982, loss 0.417361, acc 0.78125\n",
      "2018-05-23T03:40:20.509389: step 2983, loss 0.489612, acc 0.828125\n",
      "2018-05-23T03:40:20.839506: step 2984, loss 0.397235, acc 0.796875\n",
      "2018-05-23T03:40:21.168626: step 2985, loss 0.440002, acc 0.765625\n",
      "2018-05-23T03:40:21.491764: step 2986, loss 0.365279, acc 0.828125\n",
      "2018-05-23T03:40:21.819883: step 2987, loss 0.408142, acc 0.796875\n",
      "2018-05-23T03:40:22.152992: step 2988, loss 0.530953, acc 0.703125\n",
      "2018-05-23T03:40:22.480117: step 2989, loss 0.344849, acc 0.8125\n",
      "2018-05-23T03:40:22.823199: step 2990, loss 0.432612, acc 0.8125\n",
      "2018-05-23T03:40:23.156307: step 2991, loss 0.47148, acc 0.78125\n",
      "2018-05-23T03:40:23.479444: step 2992, loss 0.45262, acc 0.8125\n",
      "2018-05-23T03:40:23.811559: step 2993, loss 0.467124, acc 0.796875\n",
      "2018-05-23T03:40:24.137684: step 2994, loss 0.512631, acc 0.75\n",
      "2018-05-23T03:40:24.462813: step 2995, loss 0.394053, acc 0.890625\n",
      "2018-05-23T03:40:24.792929: step 2996, loss 0.430362, acc 0.734375\n",
      "2018-05-23T03:40:25.119057: step 2997, loss 0.382131, acc 0.84375\n",
      "2018-05-23T03:40:25.448179: step 2998, loss 0.431101, acc 0.828125\n",
      "2018-05-23T03:40:25.783280: step 2999, loss 0.459272, acc 0.75\n",
      "2018-05-23T03:40:26.116391: step 3000, loss 0.433684, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:40:30.546538: step 3000, loss 0.530067, acc 0.739248\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3000\n",
      "\n",
      "2018-05-23T03:40:31.576783: step 3001, loss 0.467259, acc 0.78125\n",
      "2018-05-23T03:40:32.116339: step 3002, loss 0.42093, acc 0.796875\n",
      "2018-05-23T03:40:32.499318: step 3003, loss 0.510106, acc 0.734375\n",
      "2018-05-23T03:40:32.851404: step 3004, loss 0.321139, acc 0.890625\n",
      "2018-05-23T03:40:33.185480: step 3005, loss 0.454426, acc 0.78125\n",
      "2018-05-23T03:40:33.516593: step 3006, loss 0.405202, acc 0.828125\n",
      "2018-05-23T03:40:33.851695: step 3007, loss 0.479374, acc 0.765625\n",
      "2018-05-23T03:40:34.187797: step 3008, loss 0.514458, acc 0.71875\n",
      "2018-05-23T03:40:34.513926: step 3009, loss 0.443829, acc 0.8125\n",
      "2018-05-23T03:40:34.847035: step 3010, loss 0.36782, acc 0.875\n",
      "2018-05-23T03:40:35.184131: step 3011, loss 0.419434, acc 0.828125\n",
      "2018-05-23T03:40:35.509262: step 3012, loss 0.409156, acc 0.8125\n",
      "2018-05-23T03:40:35.838381: step 3013, loss 0.38763, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:40:36.177473: step 3014, loss 0.382145, acc 0.78125\n",
      "2018-05-23T03:40:36.505596: step 3015, loss 0.303377, acc 0.90625\n",
      "2018-05-23T03:40:36.834715: step 3016, loss 0.439732, acc 0.828125\n",
      "2018-05-23T03:40:37.168823: step 3017, loss 0.431421, acc 0.8125\n",
      "2018-05-23T03:40:37.498938: step 3018, loss 0.424166, acc 0.78125\n",
      "2018-05-23T03:40:37.837034: step 3019, loss 0.360449, acc 0.828125\n",
      "2018-05-23T03:40:38.165157: step 3020, loss 0.344577, acc 0.84375\n",
      "2018-05-23T03:40:38.489289: step 3021, loss 0.404526, acc 0.828125\n",
      "2018-05-23T03:40:38.820403: step 3022, loss 0.30608, acc 0.828125\n",
      "2018-05-23T03:40:39.148528: step 3023, loss 0.482825, acc 0.78125\n",
      "2018-05-23T03:40:39.466709: step 3024, loss 0.544867, acc 0.75\n",
      "2018-05-23T03:40:39.789810: step 3025, loss 0.409919, acc 0.828125\n",
      "2018-05-23T03:40:40.113942: step 3026, loss 0.493663, acc 0.703125\n",
      "2018-05-23T03:40:40.449046: step 3027, loss 0.415906, acc 0.8125\n",
      "2018-05-23T03:40:40.778165: step 3028, loss 0.321024, acc 0.90625\n",
      "2018-05-23T03:40:41.107285: step 3029, loss 0.477484, acc 0.703125\n",
      "2018-05-23T03:40:41.437403: step 3030, loss 0.38094, acc 0.828125\n",
      "2018-05-23T03:40:41.766522: step 3031, loss 0.443145, acc 0.78125\n",
      "2018-05-23T03:40:42.093646: step 3032, loss 0.384655, acc 0.8125\n",
      "2018-05-23T03:40:42.426755: step 3033, loss 0.491571, acc 0.734375\n",
      "2018-05-23T03:40:42.752883: step 3034, loss 0.399495, acc 0.875\n",
      "2018-05-23T03:40:43.083997: step 3035, loss 0.29604, acc 0.875\n",
      "2018-05-23T03:40:43.415113: step 3036, loss 0.4389, acc 0.8125\n",
      "2018-05-23T03:40:43.792103: step 3037, loss 0.367407, acc 0.796875\n",
      "2018-05-23T03:40:44.124216: step 3038, loss 0.349821, acc 0.859375\n",
      "2018-05-23T03:40:44.460317: step 3039, loss 0.577677, acc 0.765625\n",
      "2018-05-23T03:40:44.792430: step 3040, loss 0.362359, acc 0.84375\n",
      "2018-05-23T03:40:45.123541: step 3041, loss 0.457275, acc 0.8125\n",
      "2018-05-23T03:40:45.453658: step 3042, loss 0.371924, acc 0.828125\n",
      "2018-05-23T03:40:45.798735: step 3043, loss 0.265053, acc 0.890625\n",
      "2018-05-23T03:40:46.143814: step 3044, loss 0.408934, acc 0.84375\n",
      "2018-05-23T03:40:46.484899: step 3045, loss 0.46144, acc 0.765625\n",
      "2018-05-23T03:40:46.837987: step 3046, loss 0.388601, acc 0.828125\n",
      "2018-05-23T03:40:47.180041: step 3047, loss 0.377385, acc 0.796875\n",
      "2018-05-23T03:40:47.524121: step 3048, loss 0.456611, acc 0.734375\n",
      "2018-05-23T03:40:47.876178: step 3049, loss 0.388065, acc 0.828125\n",
      "2018-05-23T03:40:48.223249: step 3050, loss 0.396044, acc 0.75\n",
      "2018-05-23T03:40:48.573315: step 3051, loss 0.48229, acc 0.828125\n",
      "2018-05-23T03:40:48.915398: step 3052, loss 0.454968, acc 0.765625\n",
      "2018-05-23T03:40:49.238533: step 3053, loss 0.428755, acc 0.796875\n",
      "2018-05-23T03:40:49.581651: step 3054, loss 0.503794, acc 0.765625\n",
      "2018-05-23T03:40:49.912729: step 3055, loss 0.337449, acc 0.859375\n",
      "2018-05-23T03:40:50.233870: step 3056, loss 0.507309, acc 0.765625\n",
      "2018-05-23T03:40:50.562991: step 3057, loss 0.535082, acc 0.78125\n",
      "2018-05-23T03:40:50.889117: step 3058, loss 0.416749, acc 0.84375\n",
      "2018-05-23T03:40:51.230206: step 3059, loss 0.391393, acc 0.859375\n",
      "2018-05-23T03:40:51.560323: step 3060, loss 0.410285, acc 0.765625\n",
      "2018-05-23T03:40:51.887446: step 3061, loss 0.636356, acc 0.671875\n",
      "2018-05-23T03:40:52.212579: step 3062, loss 0.448557, acc 0.796875\n",
      "2018-05-23T03:40:52.543692: step 3063, loss 0.444588, acc 0.8125\n",
      "2018-05-23T03:40:52.879792: step 3064, loss 0.465903, acc 0.78125\n",
      "2018-05-23T03:40:53.203926: step 3065, loss 0.387056, acc 0.796875\n",
      "2018-05-23T03:40:53.528058: step 3066, loss 0.503068, acc 0.8125\n",
      "2018-05-23T03:40:53.867150: step 3067, loss 0.360463, acc 0.84375\n",
      "2018-05-23T03:40:54.201258: step 3068, loss 0.384964, acc 0.828125\n",
      "2018-05-23T03:40:54.530376: step 3069, loss 0.459872, acc 0.765625\n",
      "2018-05-23T03:40:54.864485: step 3070, loss 0.40092, acc 0.8125\n",
      "2018-05-23T03:40:55.192607: step 3071, loss 0.447242, acc 0.75\n",
      "2018-05-23T03:40:55.519731: step 3072, loss 0.376281, acc 0.8125\n",
      "2018-05-23T03:40:55.852838: step 3073, loss 0.491611, acc 0.796875\n",
      "2018-05-23T03:40:56.182955: step 3074, loss 0.410691, acc 0.828125\n",
      "2018-05-23T03:40:56.505093: step 3075, loss 0.395732, acc 0.8125\n",
      "2018-05-23T03:40:56.839202: step 3076, loss 0.363624, acc 0.828125\n",
      "2018-05-23T03:40:57.161994: step 3077, loss 0.391527, acc 0.828125\n",
      "2018-05-23T03:40:57.494106: step 3078, loss 0.359814, acc 0.890625\n",
      "2018-05-23T03:40:57.826217: step 3079, loss 0.33441, acc 0.859375\n",
      "2018-05-23T03:40:58.156333: step 3080, loss 0.378426, acc 0.8125\n",
      "2018-05-23T03:40:58.491437: step 3081, loss 0.392047, acc 0.84375\n",
      "2018-05-23T03:40:58.827539: step 3082, loss 0.331444, acc 0.8125\n",
      "2018-05-23T03:40:59.156656: step 3083, loss 0.385688, acc 0.859375\n",
      "2018-05-23T03:40:59.484779: step 3084, loss 0.542057, acc 0.765625\n",
      "2018-05-23T03:40:59.822876: step 3085, loss 0.459338, acc 0.765625\n",
      "2018-05-23T03:41:00.159973: step 3086, loss 0.422471, acc 0.75\n",
      "2018-05-23T03:41:00.484109: step 3087, loss 0.457744, acc 0.78125\n",
      "2018-05-23T03:41:00.866084: step 3088, loss 0.421818, acc 0.8125\n",
      "2018-05-23T03:41:01.190217: step 3089, loss 0.371439, acc 0.859375\n",
      "2018-05-23T03:41:01.516346: step 3090, loss 0.442268, acc 0.78125\n",
      "2018-05-23T03:41:01.841477: step 3091, loss 0.441903, acc 0.8125\n",
      "2018-05-23T03:41:02.176580: step 3092, loss 0.417279, acc 0.84375\n",
      "2018-05-23T03:41:02.507692: step 3093, loss 0.405349, acc 0.90625\n",
      "2018-05-23T03:41:02.849778: step 3094, loss 0.287395, acc 0.859375\n",
      "2018-05-23T03:41:03.177901: step 3095, loss 0.466474, acc 0.796875\n",
      "2018-05-23T03:41:03.505026: step 3096, loss 0.491726, acc 0.765625\n",
      "2018-05-23T03:41:03.837139: step 3097, loss 0.479206, acc 0.75\n",
      "2018-05-23T03:41:04.173237: step 3098, loss 0.508588, acc 0.765625\n",
      "2018-05-23T03:41:04.498366: step 3099, loss 0.372646, acc 0.828125\n",
      "2018-05-23T03:41:04.832475: step 3100, loss 0.326269, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:41:09.246666: step 3100, loss 0.538819, acc 0.74282\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3100\n",
      "\n",
      "2018-05-23T03:41:10.538211: step 3101, loss 0.498184, acc 0.765625\n",
      "2018-05-23T03:41:10.986013: step 3102, loss 0.317266, acc 0.875\n",
      "2018-05-23T03:41:11.348043: step 3103, loss 0.285847, acc 0.90625\n",
      "2018-05-23T03:41:11.726034: step 3104, loss 0.433677, acc 0.828125\n",
      "2018-05-23T03:41:12.058143: step 3105, loss 0.455059, acc 0.734375\n",
      "2018-05-23T03:41:12.393247: step 3106, loss 0.330501, acc 0.828125\n",
      "2018-05-23T03:41:12.754282: step 3107, loss 0.4336, acc 0.78125\n",
      "2018-05-23T03:41:13.078415: step 3108, loss 0.444582, acc 0.765625\n",
      "2018-05-23T03:41:13.418505: step 3109, loss 0.35512, acc 0.828125\n",
      "2018-05-23T03:41:13.778542: step 3110, loss 0.504657, acc 0.671875\n",
      "2018-05-23T03:41:14.110654: step 3111, loss 0.462339, acc 0.78125\n",
      "2018-05-23T03:41:14.443763: step 3112, loss 0.39704, acc 0.8125\n",
      "2018-05-23T03:41:14.773879: step 3113, loss 0.382469, acc 0.84375\n",
      "2018-05-23T03:41:15.100005: step 3114, loss 0.44322, acc 0.78125\n",
      "2018-05-23T03:41:15.430124: step 3115, loss 0.434719, acc 0.75\n",
      "2018-05-23T03:41:15.828058: step 3116, loss 0.453343, acc 0.765625\n",
      "2018-05-23T03:41:16.158174: step 3117, loss 0.33416, acc 0.828125\n",
      "2018-05-23T03:41:16.495274: step 3118, loss 0.492537, acc 0.75\n",
      "2018-05-23T03:41:16.870271: step 3119, loss 0.428607, acc 0.828125\n",
      "2018-05-23T03:41:17.197395: step 3120, loss 0.206912, acc 0.953125\n",
      "2018-05-23T03:41:17.534493: step 3121, loss 0.465407, acc 0.78125\n",
      "2018-05-23T03:41:17.869636: step 3122, loss 0.573673, acc 0.6875\n",
      "2018-05-23T03:41:18.206777: step 3123, loss 0.423823, acc 0.78125\n",
      "2018-05-23T03:41:18.530867: step 3124, loss 0.714635, acc 0.640625\n",
      "2018-05-23T03:41:18.864975: step 3125, loss 0.258011, acc 0.875\n",
      "2018-05-23T03:41:19.191102: step 3126, loss 0.345843, acc 0.875\n",
      "2018-05-23T03:41:19.527202: step 3127, loss 0.368005, acc 0.8125\n",
      "2018-05-23T03:41:19.858318: step 3128, loss 0.625838, acc 0.78125\n",
      "2018-05-23T03:41:20.188434: step 3129, loss 0.326906, acc 0.875\n",
      "2018-05-23T03:41:20.524536: step 3130, loss 0.360291, acc 0.8125\n",
      "2018-05-23T03:41:20.856645: step 3131, loss 0.524984, acc 0.78125\n",
      "2018-05-23T03:41:21.183772: step 3132, loss 0.33553, acc 0.828125\n",
      "2018-05-23T03:41:21.511893: step 3133, loss 0.402689, acc 0.859375\n",
      "2018-05-23T03:41:21.848992: step 3134, loss 0.397787, acc 0.859375\n",
      "2018-05-23T03:41:22.176118: step 3135, loss 0.46763, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:41:22.504238: step 3136, loss 0.437768, acc 0.765625\n",
      "2018-05-23T03:41:22.842333: step 3137, loss 0.490432, acc 0.8125\n",
      "2018-05-23T03:41:23.171454: step 3138, loss 0.468322, acc 0.765625\n",
      "2018-05-23T03:41:23.495588: step 3139, loss 0.397726, acc 0.84375\n",
      "2018-05-23T03:41:23.824707: step 3140, loss 0.494534, acc 0.796875\n",
      "2018-05-23T03:41:24.154823: step 3141, loss 0.397208, acc 0.78125\n",
      "2018-05-23T03:41:24.477958: step 3142, loss 0.464542, acc 0.796875\n",
      "2018-05-23T03:41:24.809075: step 3143, loss 0.449108, acc 0.8125\n",
      "2018-05-23T03:41:25.137197: step 3144, loss 0.407799, acc 0.8125\n",
      "2018-05-23T03:41:25.473297: step 3145, loss 0.464545, acc 0.765625\n",
      "2018-05-23T03:41:25.813386: step 3146, loss 0.434539, acc 0.8125\n",
      "2018-05-23T03:41:26.142505: step 3147, loss 0.268373, acc 0.921875\n",
      "2018-05-23T03:41:26.468634: step 3148, loss 0.319153, acc 0.890625\n",
      "2018-05-23T03:41:26.800746: step 3149, loss 0.441225, acc 0.828125\n",
      "2018-05-23T03:41:27.123880: step 3150, loss 0.330764, acc 0.875\n",
      "2018-05-23T03:41:27.449009: step 3151, loss 0.499098, acc 0.8125\n",
      "2018-05-23T03:41:27.775138: step 3152, loss 0.488928, acc 0.75\n",
      "2018-05-23T03:41:28.105256: step 3153, loss 0.428569, acc 0.765625\n",
      "2018-05-23T03:41:28.429389: step 3154, loss 0.425514, acc 0.890625\n",
      "2018-05-23T03:41:28.804386: step 3155, loss 0.285068, acc 0.90625\n",
      "2018-05-23T03:41:29.130511: step 3156, loss 0.529799, acc 0.71875\n",
      "2018-05-23T03:41:29.462624: step 3157, loss 0.534625, acc 0.75\n",
      "2018-05-23T03:41:29.800720: step 3158, loss 0.545804, acc 0.796875\n",
      "2018-05-23T03:41:30.125883: step 3159, loss 0.413294, acc 0.78125\n",
      "2018-05-23T03:41:30.457961: step 3160, loss 0.420699, acc 0.765625\n",
      "2018-05-23T03:41:30.791070: step 3161, loss 0.446723, acc 0.734375\n",
      "2018-05-23T03:41:31.121186: step 3162, loss 0.364728, acc 0.84375\n",
      "2018-05-23T03:41:31.454297: step 3163, loss 0.470296, acc 0.8125\n",
      "2018-05-23T03:41:31.779427: step 3164, loss 0.407044, acc 0.8125\n",
      "2018-05-23T03:41:32.115527: step 3165, loss 0.448884, acc 0.796875\n",
      "2018-05-23T03:41:32.447639: step 3166, loss 0.511987, acc 0.78125\n",
      "2018-05-23T03:41:32.784736: step 3167, loss 0.459209, acc 0.8125\n",
      "2018-05-23T03:41:33.117846: step 3168, loss 0.374853, acc 0.796875\n",
      "2018-05-23T03:41:33.447961: step 3169, loss 0.376346, acc 0.796875\n",
      "2018-05-23T03:41:33.781072: step 3170, loss 0.332928, acc 0.859375\n",
      "2018-05-23T03:41:34.112185: step 3171, loss 0.417108, acc 0.84375\n",
      "2018-05-23T03:41:34.438314: step 3172, loss 0.340807, acc 0.859375\n",
      "2018-05-23T03:41:34.763442: step 3173, loss 0.448782, acc 0.796875\n",
      "2018-05-23T03:41:35.091565: step 3174, loss 0.51902, acc 0.765625\n",
      "2018-05-23T03:41:35.421683: step 3175, loss 0.471394, acc 0.75\n",
      "2018-05-23T03:41:35.750803: step 3176, loss 0.482772, acc 0.828125\n",
      "2018-05-23T03:41:36.081915: step 3177, loss 0.351451, acc 0.828125\n",
      "2018-05-23T03:41:36.415026: step 3178, loss 0.419749, acc 0.765625\n",
      "2018-05-23T03:41:36.746140: step 3179, loss 0.585228, acc 0.703125\n",
      "2018-05-23T03:41:37.069273: step 3180, loss 0.420625, acc 0.765625\n",
      "2018-05-23T03:41:37.402383: step 3181, loss 0.397423, acc 0.8125\n",
      "2018-05-23T03:41:37.729507: step 3182, loss 0.405832, acc 0.828125\n",
      "2018-05-23T03:41:38.062619: step 3183, loss 0.416174, acc 0.828125\n",
      "2018-05-23T03:41:38.387748: step 3184, loss 0.466631, acc 0.78125\n",
      "2018-05-23T03:41:38.717865: step 3185, loss 0.323532, acc 0.828125\n",
      "2018-05-23T03:41:39.047981: step 3186, loss 0.475675, acc 0.78125\n",
      "2018-05-23T03:41:39.382088: step 3187, loss 0.47112, acc 0.765625\n",
      "2018-05-23T03:41:39.712205: step 3188, loss 0.351971, acc 0.859375\n",
      "2018-05-23T03:41:40.032348: step 3189, loss 0.37104, acc 0.828125\n",
      "2018-05-23T03:41:40.362463: step 3190, loss 0.370892, acc 0.75\n",
      "2018-05-23T03:41:40.691584: step 3191, loss 0.449815, acc 0.765625\n",
      "2018-05-23T03:41:41.038655: step 3192, loss 0.481185, acc 0.75\n",
      "2018-05-23T03:41:41.366778: step 3193, loss 0.513184, acc 0.75\n",
      "2018-05-23T03:41:41.698889: step 3194, loss 0.490378, acc 0.75\n",
      "2018-05-23T03:41:42.026013: step 3195, loss 0.342474, acc 0.875\n",
      "2018-05-23T03:41:42.352143: step 3196, loss 0.377875, acc 0.828125\n",
      "2018-05-23T03:41:42.687247: step 3197, loss 0.382657, acc 0.78125\n",
      "2018-05-23T03:41:43.012376: step 3198, loss 0.472495, acc 0.75\n",
      "2018-05-23T03:41:43.339502: step 3199, loss 0.400985, acc 0.828125\n",
      "2018-05-23T03:41:43.692556: step 3200, loss 0.437366, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:41:48.088794: step 3200, loss 0.553226, acc 0.734391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3200\n",
      "\n",
      "2018-05-23T03:41:49.332469: step 3201, loss 0.304647, acc 0.890625\n",
      "2018-05-23T03:41:49.802211: step 3202, loss 0.504094, acc 0.78125\n",
      "2018-05-23T03:41:50.168232: step 3203, loss 0.347041, acc 0.875\n",
      "2018-05-23T03:41:50.510317: step 3204, loss 0.38615, acc 0.859375\n",
      "2018-05-23T03:41:50.844424: step 3205, loss 0.539156, acc 0.734375\n",
      "2018-05-23T03:41:51.241360: step 3206, loss 0.448646, acc 0.765625\n",
      "2018-05-23T03:41:51.607383: step 3207, loss 0.331985, acc 0.828125\n",
      "2018-05-23T03:41:51.940491: step 3208, loss 0.449152, acc 0.734375\n",
      "2018-05-23T03:41:52.277588: step 3209, loss 0.291071, acc 0.859375\n",
      "2018-05-23T03:41:52.604713: step 3210, loss 0.478812, acc 0.796875\n",
      "2018-05-23T03:41:52.938819: step 3211, loss 0.516021, acc 0.671875\n",
      "2018-05-23T03:41:53.263952: step 3212, loss 0.448828, acc 0.796875\n",
      "2018-05-23T03:41:53.596062: step 3213, loss 0.320863, acc 0.875\n",
      "2018-05-23T03:41:53.930170: step 3214, loss 0.318265, acc 0.875\n",
      "2018-05-23T03:41:54.256296: step 3215, loss 0.42109, acc 0.828125\n",
      "2018-05-23T03:41:54.582422: step 3216, loss 0.502068, acc 0.734375\n",
      "2018-05-23T03:41:54.928499: step 3217, loss 0.32927, acc 0.90625\n",
      "2018-05-23T03:41:55.249639: step 3218, loss 0.441841, acc 0.828125\n",
      "2018-05-23T03:41:55.579755: step 3219, loss 0.542942, acc 0.796875\n",
      "2018-05-23T03:41:55.908874: step 3220, loss 0.468741, acc 0.828125\n",
      "2018-05-23T03:41:56.235002: step 3221, loss 0.47613, acc 0.796875\n",
      "2018-05-23T03:41:56.559136: step 3222, loss 0.524616, acc 0.828125\n",
      "2018-05-23T03:41:56.891247: step 3223, loss 0.450511, acc 0.8125\n",
      "2018-05-23T03:41:57.220368: step 3224, loss 0.4182, acc 0.8125\n",
      "2018-05-23T03:41:57.550483: step 3225, loss 0.451646, acc 0.8125\n",
      "2018-05-23T03:41:57.881596: step 3226, loss 0.569294, acc 0.765625\n",
      "2018-05-23T03:41:58.212712: step 3227, loss 0.54593, acc 0.734375\n",
      "2018-05-23T03:41:58.542829: step 3228, loss 0.347564, acc 0.765625\n",
      "2018-05-23T03:41:58.870950: step 3229, loss 0.593061, acc 0.6875\n",
      "2018-05-23T03:41:59.197080: step 3230, loss 0.54324, acc 0.75\n",
      "2018-05-23T03:41:59.528193: step 3231, loss 0.449049, acc 0.796875\n",
      "2018-05-23T03:41:59.871275: step 3232, loss 0.482624, acc 0.8125\n",
      "2018-05-23T03:42:00.213358: step 3233, loss 0.376014, acc 0.828125\n",
      "2018-05-23T03:42:00.560430: step 3234, loss 0.446361, acc 0.75\n",
      "2018-05-23T03:42:00.991278: step 3235, loss 0.227604, acc 0.9375\n",
      "2018-05-23T03:42:01.331369: step 3236, loss 0.479042, acc 0.734375\n",
      "2018-05-23T03:42:01.729306: step 3237, loss 0.372566, acc 0.828125\n",
      "2018-05-23T03:42:02.106296: step 3238, loss 0.381097, acc 0.859375\n",
      "2018-05-23T03:42:02.445390: step 3239, loss 0.495304, acc 0.765625\n",
      "2018-05-23T03:42:02.814401: step 3240, loss 0.432652, acc 0.78125\n",
      "2018-05-23T03:42:03.151501: step 3241, loss 0.392275, acc 0.859375\n",
      "2018-05-23T03:42:03.493584: step 3242, loss 0.299741, acc 0.828125\n",
      "2018-05-23T03:42:03.827692: step 3243, loss 0.452382, acc 0.8125\n",
      "2018-05-23T03:42:04.156811: step 3244, loss 0.501196, acc 0.765625\n",
      "2018-05-23T03:42:04.484933: step 3245, loss 0.315483, acc 0.859375\n",
      "2018-05-23T03:42:04.818043: step 3246, loss 0.299153, acc 0.90625\n",
      "2018-05-23T03:42:05.144170: step 3247, loss 0.522801, acc 0.796875\n",
      "2018-05-23T03:42:05.475284: step 3248, loss 0.402494, acc 0.8125\n",
      "2018-05-23T03:42:05.806399: step 3249, loss 0.705028, acc 0.65625\n",
      "2018-05-23T03:42:06.150477: step 3250, loss 0.416696, acc 0.859375\n",
      "2018-05-23T03:42:06.478599: step 3251, loss 0.547086, acc 0.734375\n",
      "2018-05-23T03:42:06.812706: step 3252, loss 0.440062, acc 0.84375\n",
      "2018-05-23T03:42:07.144815: step 3253, loss 0.345364, acc 0.875\n",
      "2018-05-23T03:42:07.466956: step 3254, loss 0.453225, acc 0.75\n",
      "2018-05-23T03:42:07.798070: step 3255, loss 0.44308, acc 0.8125\n",
      "2018-05-23T03:42:08.129183: step 3256, loss 0.504838, acc 0.75\n",
      "2018-05-23T03:42:08.455312: step 3257, loss 0.412901, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:42:08.788419: step 3258, loss 0.41375, acc 0.8125\n",
      "2018-05-23T03:42:09.118536: step 3259, loss 0.310634, acc 0.875\n",
      "2018-05-23T03:42:09.444664: step 3260, loss 0.476695, acc 0.78125\n",
      "2018-05-23T03:42:09.774782: step 3261, loss 0.412489, acc 0.8125\n",
      "2018-05-23T03:42:10.105896: step 3262, loss 0.376366, acc 0.84375\n",
      "2018-05-23T03:42:10.430030: step 3263, loss 0.36416, acc 0.84375\n",
      "2018-05-23T03:42:10.758184: step 3264, loss 0.417173, acc 0.828125\n",
      "2018-05-23T03:42:11.088266: step 3265, loss 0.552848, acc 0.765625\n",
      "2018-05-23T03:42:11.420380: step 3266, loss 0.417651, acc 0.796875\n",
      "2018-05-23T03:42:11.746508: step 3267, loss 0.503928, acc 0.796875\n",
      "2018-05-23T03:42:12.076622: step 3268, loss 0.373478, acc 0.84375\n",
      "2018-05-23T03:42:12.401753: step 3269, loss 0.495854, acc 0.765625\n",
      "2018-05-23T03:42:12.732868: step 3270, loss 0.403422, acc 0.796875\n",
      "2018-05-23T03:42:13.061986: step 3271, loss 0.563583, acc 0.734375\n",
      "2018-05-23T03:42:13.388149: step 3272, loss 0.436922, acc 0.765625\n",
      "2018-05-23T03:42:13.726211: step 3273, loss 0.429877, acc 0.84375\n",
      "2018-05-23T03:42:14.051340: step 3274, loss 0.383561, acc 0.828125\n",
      "2018-05-23T03:42:14.380461: step 3275, loss 0.304724, acc 0.890625\n",
      "2018-05-23T03:42:14.708581: step 3276, loss 0.426874, acc 0.796875\n",
      "2018-05-23T03:42:15.034116: step 3277, loss 0.419087, acc 0.765625\n",
      "2018-05-23T03:42:15.360243: step 3278, loss 0.447999, acc 0.78125\n",
      "2018-05-23T03:42:15.692354: step 3279, loss 0.533875, acc 0.78125\n",
      "2018-05-23T03:42:16.018481: step 3280, loss 0.432896, acc 0.8125\n",
      "2018-05-23T03:42:16.346604: step 3281, loss 0.354087, acc 0.828125\n",
      "2018-05-23T03:42:16.674724: step 3282, loss 0.441211, acc 0.796875\n",
      "2018-05-23T03:42:16.999854: step 3283, loss 0.414089, acc 0.828125\n",
      "2018-05-23T03:42:17.331968: step 3284, loss 0.523494, acc 0.765625\n",
      "2018-05-23T03:42:17.662085: step 3285, loss 0.590882, acc 0.734375\n",
      "2018-05-23T03:42:18.000181: step 3286, loss 0.386606, acc 0.84375\n",
      "2018-05-23T03:42:18.338275: step 3287, loss 0.494904, acc 0.75\n",
      "2018-05-23T03:42:18.672381: step 3288, loss 0.472486, acc 0.796875\n",
      "2018-05-23T03:42:18.995518: step 3289, loss 0.467646, acc 0.78125\n",
      "2018-05-23T03:42:19.331617: step 3290, loss 0.324943, acc 0.859375\n",
      "2018-05-23T03:42:19.663729: step 3291, loss 0.369439, acc 0.765625\n",
      "2018-05-23T03:42:19.993847: step 3292, loss 0.521695, acc 0.78125\n",
      "2018-05-23T03:42:20.319973: step 3293, loss 0.389684, acc 0.734375\n",
      "2018-05-23T03:42:20.644106: step 3294, loss 0.526562, acc 0.765625\n",
      "2018-05-23T03:42:20.972229: step 3295, loss 0.438576, acc 0.796875\n",
      "2018-05-23T03:42:21.299352: step 3296, loss 0.297937, acc 0.859375\n",
      "2018-05-23T03:42:21.624482: step 3297, loss 0.451961, acc 0.765625\n",
      "2018-05-23T03:42:21.955172: step 3298, loss 0.329823, acc 0.84375\n",
      "2018-05-23T03:42:22.282296: step 3299, loss 0.501679, acc 0.734375\n",
      "2018-05-23T03:42:22.608154: step 3300, loss 0.590235, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:42:27.014365: step 3300, loss 0.543729, acc 0.729247\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3300\n",
      "\n",
      "2018-05-23T03:42:28.282221: step 3301, loss 0.418199, acc 0.8125\n",
      "2018-05-23T03:42:28.692124: step 3302, loss 0.422883, acc 0.8125\n",
      "2018-05-23T03:42:29.076096: step 3303, loss 0.501979, acc 0.75\n",
      "2018-05-23T03:42:29.406212: step 3304, loss 0.489108, acc 0.796875\n",
      "2018-05-23T03:42:29.740318: step 3305, loss 0.446414, acc 0.78125\n",
      "2018-05-23T03:42:30.075422: step 3306, loss 0.506823, acc 0.703125\n",
      "2018-05-23T03:42:30.404541: step 3307, loss 0.405999, acc 0.8125\n",
      "2018-05-23T03:42:30.747624: step 3308, loss 0.377922, acc 0.8125\n",
      "2018-05-23T03:42:31.075746: step 3309, loss 0.465803, acc 0.84375\n",
      "2018-05-23T03:42:31.407857: step 3310, loss 0.349771, acc 0.828125\n",
      "2018-05-23T03:42:31.739969: step 3311, loss 0.367938, acc 0.828125\n",
      "2018-05-23T03:42:32.068093: step 3312, loss 0.370907, acc 0.875\n",
      "2018-05-23T03:42:32.396213: step 3313, loss 0.565697, acc 0.75\n",
      "2018-05-23T03:42:32.726330: step 3314, loss 0.512444, acc 0.734375\n",
      "2018-05-23T03:42:33.064426: step 3315, loss 0.477054, acc 0.78125\n",
      "2018-05-23T03:42:33.392550: step 3316, loss 0.370994, acc 0.765625\n",
      "2018-05-23T03:42:33.720670: step 3317, loss 0.397869, acc 0.8125\n",
      "2018-05-23T03:42:34.044805: step 3318, loss 0.412938, acc 0.828125\n",
      "2018-05-23T03:42:34.370933: step 3319, loss 0.360714, acc 0.765625\n",
      "2018-05-23T03:42:34.702044: step 3320, loss 0.609427, acc 0.75\n",
      "2018-05-23T03:42:35.031164: step 3321, loss 0.440516, acc 0.78125\n",
      "2018-05-23T03:42:35.359289: step 3322, loss 0.357468, acc 0.859375\n",
      "2018-05-23T03:42:35.689405: step 3323, loss 0.400817, acc 0.78125\n",
      "2018-05-23T03:42:36.019520: step 3324, loss 0.389485, acc 0.828125\n",
      "2018-05-23T03:42:36.346646: step 3325, loss 0.448132, acc 0.78125\n",
      "2018-05-23T03:42:36.675765: step 3326, loss 0.385666, acc 0.859375\n",
      "2018-05-23T03:42:37.005881: step 3327, loss 0.539091, acc 0.75\n",
      "2018-05-23T03:42:37.331014: step 3328, loss 0.475677, acc 0.734375\n",
      "2018-05-23T03:42:37.659134: step 3329, loss 0.430753, acc 0.796875\n",
      "2018-05-23T03:42:37.992243: step 3330, loss 0.345935, acc 0.859375\n",
      "2018-05-23T03:42:38.319370: step 3331, loss 0.464631, acc 0.796875\n",
      "2018-05-23T03:42:38.643501: step 3332, loss 0.397663, acc 0.765625\n",
      "2018-05-23T03:42:38.970626: step 3333, loss 0.459129, acc 0.78125\n",
      "2018-05-23T03:42:39.295758: step 3334, loss 0.449697, acc 0.828125\n",
      "2018-05-23T03:42:39.620510: step 3335, loss 0.465348, acc 0.8125\n",
      "2018-05-23T03:42:39.954615: step 3336, loss 0.392273, acc 0.828125\n",
      "2018-05-23T03:42:40.276752: step 3337, loss 0.644962, acc 0.734375\n",
      "2018-05-23T03:42:40.605872: step 3338, loss 0.412039, acc 0.8125\n",
      "2018-05-23T03:42:40.938982: step 3339, loss 0.408957, acc 0.8125\n",
      "2018-05-23T03:42:41.271093: step 3340, loss 0.469038, acc 0.796875\n",
      "2018-05-23T03:42:41.596223: step 3341, loss 0.434806, acc 0.796875\n",
      "2018-05-23T03:42:41.929332: step 3342, loss 0.302961, acc 0.859375\n",
      "2018-05-23T03:42:42.259450: step 3343, loss 0.383116, acc 0.796875\n",
      "2018-05-23T03:42:42.587572: step 3344, loss 0.340155, acc 0.890625\n",
      "2018-05-23T03:42:42.919683: step 3345, loss 0.417805, acc 0.8125\n",
      "2018-05-23T03:42:43.251794: step 3346, loss 0.302907, acc 0.84375\n",
      "2018-05-23T03:42:43.586898: step 3347, loss 0.536179, acc 0.734375\n",
      "2018-05-23T03:42:43.927987: step 3348, loss 0.449224, acc 0.765625\n",
      "2018-05-23T03:42:44.262091: step 3349, loss 0.379135, acc 0.84375\n",
      "2018-05-23T03:42:44.586224: step 3350, loss 0.332871, acc 0.859375\n",
      "2018-05-23T03:42:44.921328: step 3351, loss 0.408166, acc 0.828125\n",
      "2018-05-23T03:42:45.246457: step 3352, loss 0.384782, acc 0.828125\n",
      "2018-05-23T03:42:45.576574: step 3353, loss 0.439333, acc 0.796875\n",
      "2018-05-23T03:42:45.972515: step 3354, loss 0.262879, acc 0.890625\n",
      "2018-05-23T03:42:46.295651: step 3355, loss 0.389922, acc 0.84375\n",
      "2018-05-23T03:42:46.623774: step 3356, loss 0.528896, acc 0.75\n",
      "2018-05-23T03:42:46.963863: step 3357, loss 0.601242, acc 0.71875\n",
      "2018-05-23T03:42:47.291985: step 3358, loss 0.531284, acc 0.71875\n",
      "2018-05-23T03:42:47.629085: step 3359, loss 0.417754, acc 0.796875\n",
      "2018-05-23T03:42:47.959200: step 3360, loss 0.341847, acc 0.828125\n",
      "2018-05-23T03:42:48.285327: step 3361, loss 0.552486, acc 0.703125\n",
      "2018-05-23T03:42:48.620431: step 3362, loss 0.299467, acc 0.921875\n",
      "2018-05-23T03:42:48.961521: step 3363, loss 0.419325, acc 0.828125\n",
      "2018-05-23T03:42:49.288644: step 3364, loss 0.516915, acc 0.765625\n",
      "2018-05-23T03:42:49.621755: step 3365, loss 0.610853, acc 0.796875\n",
      "2018-05-23T03:42:49.951870: step 3366, loss 0.40507, acc 0.859375\n",
      "2018-05-23T03:42:50.279992: step 3367, loss 0.547702, acc 0.765625\n",
      "2018-05-23T03:42:50.602129: step 3368, loss 0.516847, acc 0.75\n",
      "2018-05-23T03:42:50.984108: step 3369, loss 0.416103, acc 0.78125\n",
      "2018-05-23T03:42:51.370078: step 3370, loss 0.659368, acc 0.703125\n",
      "2018-05-23T03:42:51.828849: step 3371, loss 0.38732, acc 0.859375\n",
      "2018-05-23T03:42:52.320533: step 3372, loss 0.318357, acc 0.859375\n",
      "2018-05-23T03:42:52.811220: step 3373, loss 0.350224, acc 0.828125\n",
      "2018-05-23T03:42:53.284961: step 3374, loss 0.468571, acc 0.796875\n",
      "2018-05-23T03:42:53.764671: step 3375, loss 0.356072, acc 0.828125\n",
      "2018-05-23T03:42:54.242392: step 3376, loss 0.301615, acc 0.875\n",
      "2018-05-23T03:42:54.687201: step 3377, loss 0.413913, acc 0.8125\n",
      "2018-05-23T03:42:55.037265: step 3378, loss 0.322682, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:42:55.397302: step 3379, loss 0.359863, acc 0.859375\n",
      "2018-05-23T03:42:55.778283: step 3380, loss 0.434821, acc 0.796875\n",
      "2018-05-23T03:42:56.160261: step 3381, loss 0.577561, acc 0.75\n",
      "2018-05-23T03:42:56.521297: step 3382, loss 0.471059, acc 0.796875\n",
      "2018-05-23T03:42:56.894298: step 3383, loss 0.490223, acc 0.84375\n",
      "2018-05-23T03:42:57.258325: step 3384, loss 0.549227, acc 0.71875\n",
      "2018-05-23T03:42:57.722083: step 3385, loss 0.470616, acc 0.765625\n",
      "2018-05-23T03:42:58.127000: step 3386, loss 0.391623, acc 0.796875\n",
      "2018-05-23T03:42:58.512970: step 3387, loss 0.349834, acc 0.859375\n",
      "2018-05-23T03:42:59.014625: step 3388, loss 0.342059, acc 0.859375\n",
      "2018-05-23T03:42:59.365686: step 3389, loss 0.474374, acc 0.734375\n",
      "2018-05-23T03:42:59.715749: step 3390, loss 0.4353, acc 0.84375\n",
      "2018-05-23T03:43:00.068805: step 3391, loss 0.367685, acc 0.84375\n",
      "2018-05-23T03:43:00.433831: step 3392, loss 0.341913, acc 0.84375\n",
      "2018-05-23T03:43:00.850715: step 3393, loss 0.418169, acc 0.78125\n",
      "2018-05-23T03:43:01.208755: step 3394, loss 0.437133, acc 0.796875\n",
      "2018-05-23T03:43:01.567795: step 3395, loss 0.588613, acc 0.75\n",
      "2018-05-23T03:43:01.914866: step 3396, loss 0.423897, acc 0.8125\n",
      "2018-05-23T03:43:02.296844: step 3397, loss 0.386965, acc 0.796875\n",
      "2018-05-23T03:43:02.660870: step 3398, loss 0.343736, acc 0.90625\n",
      "2018-05-23T03:43:02.998968: step 3399, loss 0.535037, acc 0.6875\n",
      "2018-05-23T03:43:03.350026: step 3400, loss 0.512109, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:43:07.846996: step 3400, loss 0.54452, acc 0.736534\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3400\n",
      "\n",
      "2018-05-23T03:43:09.282157: step 3401, loss 0.507815, acc 0.703125\n",
      "2018-05-23T03:43:09.645186: step 3402, loss 0.30889, acc 0.875\n",
      "2018-05-23T03:43:09.992257: step 3403, loss 0.424208, acc 0.75\n",
      "2018-05-23T03:43:10.324370: step 3404, loss 0.499762, acc 0.734375\n",
      "2018-05-23T03:43:10.654486: step 3405, loss 0.470459, acc 0.78125\n",
      "2018-05-23T03:43:10.994577: step 3406, loss 0.37443, acc 0.796875\n",
      "2018-05-23T03:43:11.331674: step 3407, loss 0.369702, acc 0.875\n",
      "2018-05-23T03:43:11.672764: step 3408, loss 0.377302, acc 0.8125\n",
      "2018-05-23T03:43:12.015845: step 3409, loss 0.442557, acc 0.734375\n",
      "2018-05-23T03:43:12.352943: step 3410, loss 0.487964, acc 0.78125\n",
      "2018-05-23T03:43:12.687049: step 3411, loss 0.487064, acc 0.734375\n",
      "2018-05-23T03:43:13.015171: step 3412, loss 0.40693, acc 0.8125\n",
      "2018-05-23T03:43:13.350275: step 3413, loss 0.480222, acc 0.75\n",
      "2018-05-23T03:43:13.697346: step 3414, loss 0.468946, acc 0.84375\n",
      "2018-05-23T03:43:14.027462: step 3415, loss 0.567593, acc 0.703125\n",
      "2018-05-23T03:43:14.364562: step 3416, loss 0.32219, acc 0.84375\n",
      "2018-05-23T03:43:14.703654: step 3417, loss 0.328668, acc 0.90625\n",
      "2018-05-23T03:43:15.046738: step 3418, loss 0.344238, acc 0.796875\n",
      "2018-05-23T03:43:15.382837: step 3419, loss 0.320319, acc 0.890625\n",
      "2018-05-23T03:43:15.732900: step 3420, loss 0.338416, acc 0.8125\n",
      "2018-05-23T03:43:16.064015: step 3421, loss 0.40698, acc 0.828125\n",
      "2018-05-23T03:43:16.411088: step 3422, loss 0.431898, acc 0.765625\n",
      "2018-05-23T03:43:16.787080: step 3423, loss 0.335687, acc 0.84375\n",
      "2018-05-23T03:43:17.123182: step 3424, loss 0.311662, acc 0.859375\n",
      "2018-05-23T03:43:17.489203: step 3425, loss 0.415562, acc 0.8125\n",
      "2018-05-23T03:43:17.845251: step 3426, loss 0.310206, acc 0.859375\n",
      "2018-05-23T03:43:18.185339: step 3427, loss 0.336907, acc 0.859375\n",
      "2018-05-23T03:43:18.520445: step 3428, loss 0.392732, acc 0.828125\n",
      "2018-05-23T03:43:18.860535: step 3429, loss 0.323029, acc 0.828125\n",
      "2018-05-23T03:43:19.200623: step 3430, loss 0.400272, acc 0.765625\n",
      "2018-05-23T03:43:19.537722: step 3431, loss 0.581396, acc 0.78125\n",
      "2018-05-23T03:43:19.882801: step 3432, loss 0.508926, acc 0.78125\n",
      "2018-05-23T03:43:20.220896: step 3433, loss 0.556851, acc 0.78125\n",
      "2018-05-23T03:43:20.555001: step 3434, loss 0.454761, acc 0.8125\n",
      "2018-05-23T03:43:20.893097: step 3435, loss 0.531049, acc 0.78125\n",
      "2018-05-23T03:43:21.231192: step 3436, loss 0.3429, acc 0.859375\n",
      "2018-05-23T03:43:21.568291: step 3437, loss 0.581013, acc 0.734375\n",
      "2018-05-23T03:43:21.909378: step 3438, loss 0.520924, acc 0.75\n",
      "2018-05-23T03:43:22.239496: step 3439, loss 0.323239, acc 0.84375\n",
      "2018-05-23T03:43:22.589557: step 3440, loss 0.454881, acc 0.796875\n",
      "2018-05-23T03:43:22.944607: step 3441, loss 0.391628, acc 0.84375\n",
      "2018-05-23T03:43:23.280710: step 3442, loss 0.332585, acc 0.84375\n",
      "2018-05-23T03:43:23.608831: step 3443, loss 0.581766, acc 0.703125\n",
      "2018-05-23T03:43:23.946929: step 3444, loss 0.320289, acc 0.875\n",
      "2018-05-23T03:43:24.285024: step 3445, loss 0.342717, acc 0.875\n",
      "2018-05-23T03:43:24.617136: step 3446, loss 0.441514, acc 0.8125\n",
      "2018-05-23T03:43:24.958220: step 3447, loss 0.400132, acc 0.859375\n",
      "2018-05-23T03:43:25.295319: step 3448, loss 0.339241, acc 0.828125\n",
      "2018-05-23T03:43:25.624438: step 3449, loss 0.400978, acc 0.78125\n",
      "2018-05-23T03:43:25.959544: step 3450, loss 0.484393, acc 0.78125\n",
      "2018-05-23T03:43:26.290656: step 3451, loss 0.536717, acc 0.765625\n",
      "2018-05-23T03:43:26.624765: step 3452, loss 0.488018, acc 0.75\n",
      "2018-05-23T03:43:26.974828: step 3453, loss 0.502894, acc 0.734375\n",
      "2018-05-23T03:43:27.310928: step 3454, loss 0.310181, acc 0.859375\n",
      "2018-05-23T03:43:27.649022: step 3455, loss 0.262042, acc 0.921875\n",
      "2018-05-23T03:43:27.981133: step 3456, loss 0.56256, acc 0.75\n",
      "2018-05-23T03:43:28.315241: step 3457, loss 0.555254, acc 0.71875\n",
      "2018-05-23T03:43:28.671290: step 3458, loss 0.373819, acc 0.84375\n",
      "2018-05-23T03:43:29.009386: step 3459, loss 0.285928, acc 0.921875\n",
      "2018-05-23T03:43:29.341497: step 3460, loss 0.47597, acc 0.765625\n",
      "2018-05-23T03:43:29.679590: step 3461, loss 0.375698, acc 0.8125\n",
      "2018-05-23T03:43:30.007714: step 3462, loss 0.622884, acc 0.734375\n",
      "2018-05-23T03:43:30.349798: step 3463, loss 0.547271, acc 0.734375\n",
      "2018-05-23T03:43:30.707840: step 3464, loss 0.334998, acc 0.8125\n",
      "2018-05-23T03:43:31.038954: step 3465, loss 0.516666, acc 0.78125\n",
      "2018-05-23T03:43:31.372097: step 3466, loss 0.409702, acc 0.796875\n",
      "2018-05-23T03:43:31.721130: step 3467, loss 0.511118, acc 0.78125\n",
      "2018-05-23T03:43:32.059265: step 3468, loss 0.399447, acc 0.875\n",
      "2018-05-23T03:43:32.399354: step 3469, loss 0.357357, acc 0.828125\n",
      "2018-05-23T03:43:32.752409: step 3470, loss 0.307881, acc 0.859375\n",
      "2018-05-23T03:43:33.093499: step 3471, loss 0.356799, acc 0.796875\n",
      "2018-05-23T03:43:33.433587: step 3472, loss 0.450899, acc 0.75\n",
      "2018-05-23T03:43:33.777670: step 3473, loss 0.573489, acc 0.703125\n",
      "2018-05-23T03:43:34.114765: step 3474, loss 0.408525, acc 0.828125\n",
      "2018-05-23T03:43:34.447873: step 3475, loss 0.405433, acc 0.8125\n",
      "2018-05-23T03:43:34.783975: step 3476, loss 0.255819, acc 0.90625\n",
      "2018-05-23T03:43:35.120076: step 3477, loss 0.44553, acc 0.78125\n",
      "2018-05-23T03:43:35.458171: step 3478, loss 0.462117, acc 0.828125\n",
      "2018-05-23T03:43:35.799258: step 3479, loss 0.434894, acc 0.859375\n",
      "2018-05-23T03:43:36.140346: step 3480, loss 0.394494, acc 0.828125\n",
      "2018-05-23T03:43:36.495117: step 3481, loss 0.392416, acc 0.765625\n",
      "2018-05-23T03:43:36.832217: step 3482, loss 0.392522, acc 0.796875\n",
      "2018-05-23T03:43:37.161338: step 3483, loss 0.472527, acc 0.78125\n",
      "2018-05-23T03:43:37.496439: step 3484, loss 0.51618, acc 0.765625\n",
      "2018-05-23T03:43:37.836530: step 3485, loss 0.387324, acc 0.828125\n",
      "2018-05-23T03:43:38.171633: step 3486, loss 0.440597, acc 0.765625\n",
      "2018-05-23T03:43:38.515713: step 3487, loss 0.309931, acc 0.875\n",
      "2018-05-23T03:43:38.874752: step 3488, loss 0.466369, acc 0.78125\n",
      "2018-05-23T03:43:39.203871: step 3489, loss 0.367269, acc 0.8125\n",
      "2018-05-23T03:43:39.538975: step 3490, loss 0.410016, acc 0.796875\n",
      "2018-05-23T03:43:39.880064: step 3491, loss 0.420432, acc 0.796875\n",
      "2018-05-23T03:43:40.211177: step 3492, loss 0.419816, acc 0.78125\n",
      "2018-05-23T03:43:40.545316: step 3493, loss 0.501431, acc 0.78125\n",
      "2018-05-23T03:43:40.879389: step 3494, loss 0.444841, acc 0.796875\n",
      "2018-05-23T03:43:41.210503: step 3495, loss 0.262826, acc 0.90625\n",
      "2018-05-23T03:43:41.547604: step 3496, loss 0.461138, acc 0.75\n",
      "2018-05-23T03:43:41.886695: step 3497, loss 0.365335, acc 0.859375\n",
      "2018-05-23T03:43:42.220803: step 3498, loss 0.530171, acc 0.75\n",
      "2018-05-23T03:43:42.562885: step 3499, loss 0.348856, acc 0.828125\n",
      "2018-05-23T03:43:42.893002: step 3500, loss 0.472158, acc 0.765625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:43:47.452804: step 3500, loss 0.54424, acc 0.738963\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3500\n",
      "\n",
      "2018-05-23T03:43:48.537901: step 3501, loss 0.497763, acc 0.75\n",
      "2018-05-23T03:43:49.018617: step 3502, loss 0.46045, acc 0.8125\n",
      "2018-05-23T03:43:49.382641: step 3503, loss 0.333109, acc 0.84375\n",
      "2018-05-23T03:43:49.790553: step 3504, loss 0.444569, acc 0.75\n",
      "2018-05-23T03:43:50.120667: step 3505, loss 0.516147, acc 0.734375\n",
      "2018-05-23T03:43:50.455770: step 3506, loss 0.355008, acc 0.828125\n",
      "2018-05-23T03:43:50.792869: step 3507, loss 0.376069, acc 0.796875\n",
      "2018-05-23T03:43:51.121988: step 3508, loss 0.468497, acc 0.75\n",
      "2018-05-23T03:43:51.457092: step 3509, loss 0.489131, acc 0.71875\n",
      "2018-05-23T03:43:51.789203: step 3510, loss 0.424326, acc 0.78125\n",
      "2018-05-23T03:43:52.125306: step 3511, loss 0.471855, acc 0.734375\n",
      "2018-05-23T03:43:52.452431: step 3512, loss 0.427809, acc 0.78125\n",
      "2018-05-23T03:43:52.782546: step 3513, loss 0.493734, acc 0.84375\n",
      "2018-05-23T03:43:53.120641: step 3514, loss 0.450294, acc 0.828125\n",
      "2018-05-23T03:43:53.451755: step 3515, loss 0.595552, acc 0.734375\n",
      "2018-05-23T03:43:53.780875: step 3516, loss 0.568365, acc 0.75\n",
      "2018-05-23T03:43:54.104011: step 3517, loss 0.609371, acc 0.71875\n",
      "2018-05-23T03:43:54.438119: step 3518, loss 0.368515, acc 0.796875\n",
      "2018-05-23T03:43:54.761253: step 3519, loss 0.452621, acc 0.8125\n",
      "2018-05-23T03:43:55.089377: step 3520, loss 0.422286, acc 0.765625\n",
      "2018-05-23T03:43:55.413509: step 3521, loss 0.489802, acc 0.859375\n",
      "2018-05-23T03:43:55.743627: step 3522, loss 0.379796, acc 0.84375\n",
      "2018-05-23T03:43:56.069753: step 3523, loss 0.286517, acc 0.890625\n",
      "2018-05-23T03:43:56.398874: step 3524, loss 0.367164, acc 0.84375\n",
      "2018-05-23T03:43:56.727991: step 3525, loss 0.286145, acc 0.9375\n",
      "2018-05-23T03:43:57.066086: step 3526, loss 0.444095, acc 0.859375\n",
      "2018-05-23T03:43:57.388227: step 3527, loss 0.534646, acc 0.71875\n",
      "2018-05-23T03:43:57.718342: step 3528, loss 0.304559, acc 0.890625\n",
      "2018-05-23T03:43:58.044469: step 3529, loss 0.578638, acc 0.75\n",
      "2018-05-23T03:43:58.370597: step 3530, loss 0.357573, acc 0.8125\n",
      "2018-05-23T03:43:58.714676: step 3531, loss 0.490436, acc 0.78125\n",
      "2018-05-23T03:43:59.041801: step 3532, loss 0.314823, acc 0.8125\n",
      "2018-05-23T03:43:59.373913: step 3533, loss 0.491679, acc 0.796875\n",
      "2018-05-23T03:43:59.704030: step 3534, loss 0.65266, acc 0.65625\n",
      "2018-05-23T03:44:00.039133: step 3535, loss 0.437426, acc 0.796875\n",
      "2018-05-23T03:44:00.373239: step 3536, loss 0.513532, acc 0.78125\n",
      "2018-05-23T03:44:00.756217: step 3537, loss 0.571839, acc 0.71875\n",
      "2018-05-23T03:44:01.109271: step 3538, loss 0.58527, acc 0.75\n",
      "2018-05-23T03:44:01.464323: step 3539, loss 0.381925, acc 0.828125\n",
      "2018-05-23T03:44:01.875222: step 3540, loss 0.55253, acc 0.71875\n",
      "2018-05-23T03:44:02.303077: step 3541, loss 0.496689, acc 0.78125\n",
      "2018-05-23T03:44:02.651146: step 3542, loss 0.492145, acc 0.8125\n",
      "2018-05-23T03:44:03.040105: step 3543, loss 0.590142, acc 0.734375\n",
      "2018-05-23T03:44:03.413107: step 3544, loss 0.403694, acc 0.78125\n",
      "2018-05-23T03:44:03.755195: step 3545, loss 0.529118, acc 0.765625\n",
      "2018-05-23T03:44:04.081319: step 3546, loss 0.509865, acc 0.75\n",
      "2018-05-23T03:44:04.448339: step 3547, loss 0.501183, acc 0.78125\n",
      "2018-05-23T03:44:04.776460: step 3548, loss 0.576071, acc 0.796875\n",
      "2018-05-23T03:44:05.105582: step 3549, loss 0.513124, acc 0.765625\n",
      "2018-05-23T03:44:05.453648: step 3550, loss 0.444004, acc 0.796875\n",
      "2018-05-23T03:44:05.795735: step 3551, loss 0.371288, acc 0.84375\n",
      "2018-05-23T03:44:06.135823: step 3552, loss 0.58826, acc 0.734375\n",
      "2018-05-23T03:44:06.489878: step 3553, loss 0.468444, acc 0.796875\n",
      "2018-05-23T03:44:06.827972: step 3554, loss 0.49313, acc 0.765625\n",
      "2018-05-23T03:44:07.152105: step 3555, loss 0.366544, acc 0.875\n",
      "2018-05-23T03:44:07.480228: step 3556, loss 0.367302, acc 0.828125\n",
      "2018-05-23T03:44:07.817900: step 3557, loss 0.492554, acc 0.75\n",
      "2018-05-23T03:44:08.155993: step 3558, loss 0.489376, acc 0.78125\n",
      "2018-05-23T03:44:08.495087: step 3559, loss 0.413154, acc 0.78125\n",
      "2018-05-23T03:44:08.830191: step 3560, loss 0.46346, acc 0.765625\n",
      "2018-05-23T03:44:09.155322: step 3561, loss 0.408885, acc 0.828125\n",
      "2018-05-23T03:44:09.488429: step 3562, loss 0.487185, acc 0.765625\n",
      "2018-05-23T03:44:09.819543: step 3563, loss 0.503395, acc 0.6875\n",
      "2018-05-23T03:44:10.147665: step 3564, loss 0.511588, acc 0.78125\n",
      "2018-05-23T03:44:10.491685: step 3565, loss 0.494409, acc 0.71875\n",
      "2018-05-23T03:44:10.826789: step 3566, loss 0.540119, acc 0.75\n",
      "2018-05-23T03:44:11.154913: step 3567, loss 0.383053, acc 0.8125\n",
      "2018-05-23T03:44:11.485027: step 3568, loss 0.551121, acc 0.75\n",
      "2018-05-23T03:44:11.809160: step 3569, loss 0.361332, acc 0.828125\n",
      "2018-05-23T03:44:12.148255: step 3570, loss 0.306557, acc 0.875\n",
      "2018-05-23T03:44:12.479367: step 3571, loss 0.521176, acc 0.75\n",
      "2018-05-23T03:44:12.812476: step 3572, loss 0.474663, acc 0.765625\n",
      "2018-05-23T03:44:13.149575: step 3573, loss 0.466292, acc 0.796875\n",
      "2018-05-23T03:44:13.485678: step 3574, loss 0.512156, acc 0.765625\n",
      "2018-05-23T03:44:13.923504: step 3575, loss 0.418797, acc 0.796875\n",
      "2018-05-23T03:44:14.257610: step 3576, loss 0.471844, acc 0.75\n",
      "2018-05-23T03:44:14.580748: step 3577, loss 0.468071, acc 0.78125\n",
      "2018-05-23T03:44:14.917846: step 3578, loss 0.434113, acc 0.78125\n",
      "2018-05-23T03:44:15.255942: step 3579, loss 0.435725, acc 0.796875\n",
      "2018-05-23T03:44:15.584062: step 3580, loss 0.411437, acc 0.8125\n",
      "2018-05-23T03:44:15.928144: step 3581, loss 0.381855, acc 0.8125\n",
      "2018-05-23T03:44:16.259256: step 3582, loss 0.555846, acc 0.71875\n",
      "2018-05-23T03:44:16.579399: step 3583, loss 0.402657, acc 0.8125\n",
      "2018-05-23T03:44:16.915500: step 3584, loss 0.344849, acc 0.890625\n",
      "2018-05-23T03:44:17.339366: step 3585, loss 0.397338, acc 0.8125\n",
      "2018-05-23T03:44:17.742290: step 3586, loss 0.363936, acc 0.859375\n",
      "2018-05-23T03:44:18.095344: step 3587, loss 0.550463, acc 0.703125\n",
      "2018-05-23T03:44:18.433439: step 3588, loss 0.550653, acc 0.734375\n",
      "2018-05-23T03:44:18.759567: step 3589, loss 0.446484, acc 0.78125\n",
      "2018-05-23T03:44:19.093673: step 3590, loss 0.47216, acc 0.78125\n",
      "2018-05-23T03:44:19.429774: step 3591, loss 0.387111, acc 0.8125\n",
      "2018-05-23T03:44:19.761886: step 3592, loss 0.506787, acc 0.765625\n",
      "2018-05-23T03:44:20.096989: step 3593, loss 0.410957, acc 0.8125\n",
      "2018-05-23T03:44:20.441070: step 3594, loss 0.400938, acc 0.8125\n",
      "2018-05-23T03:44:20.774177: step 3595, loss 0.407819, acc 0.75\n",
      "2018-05-23T03:44:21.099309: step 3596, loss 0.376612, acc 0.828125\n",
      "2018-05-23T03:44:21.434411: step 3597, loss 0.423085, acc 0.84375\n",
      "2018-05-23T03:44:21.765526: step 3598, loss 0.439108, acc 0.75\n",
      "2018-05-23T03:44:22.095642: step 3599, loss 0.406723, acc 0.796875\n",
      "2018-05-23T03:44:22.424763: step 3600, loss 0.506683, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:44:26.813024: step 3600, loss 0.54248, acc 0.737248\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3600\n",
      "\n",
      "2018-05-23T03:44:28.178371: step 3601, loss 0.391299, acc 0.859375\n",
      "2018-05-23T03:44:28.541399: step 3602, loss 0.39011, acc 0.84375\n",
      "2018-05-23T03:44:28.915398: step 3603, loss 0.344052, acc 0.828125\n",
      "2018-05-23T03:44:29.245515: step 3604, loss 0.485632, acc 0.765625\n",
      "2018-05-23T03:44:29.580619: step 3605, loss 0.432428, acc 0.765625\n",
      "2018-05-23T03:44:29.953621: step 3606, loss 0.426475, acc 0.8125\n",
      "2018-05-23T03:44:30.284735: step 3607, loss 0.443187, acc 0.796875\n",
      "2018-05-23T03:44:30.615849: step 3608, loss 0.501086, acc 0.75\n",
      "2018-05-23T03:44:30.973891: step 3609, loss 0.540693, acc 0.71875\n",
      "2018-05-23T03:44:31.306002: step 3610, loss 0.381364, acc 0.84375\n",
      "2018-05-23T03:44:31.632130: step 3611, loss 0.379111, acc 0.8125\n",
      "2018-05-23T03:44:32.014111: step 3612, loss 0.461476, acc 0.78125\n",
      "2018-05-23T03:44:32.346222: step 3613, loss 0.472413, acc 0.78125\n",
      "2018-05-23T03:44:32.681325: step 3614, loss 0.469122, acc 0.765625\n",
      "2018-05-23T03:44:33.035376: step 3615, loss 0.442654, acc 0.84375\n",
      "2018-05-23T03:44:33.372476: step 3616, loss 0.376403, acc 0.828125\n",
      "2018-05-23T03:44:33.710570: step 3617, loss 0.393299, acc 0.875\n",
      "2018-05-23T03:44:34.042681: step 3618, loss 0.585843, acc 0.71875\n",
      "2018-05-23T03:44:34.373796: step 3619, loss 0.332204, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:44:34.709897: step 3620, loss 0.309238, acc 0.859375\n",
      "2018-05-23T03:44:35.033033: step 3621, loss 0.329662, acc 0.890625\n",
      "2018-05-23T03:44:35.358162: step 3622, loss 0.455934, acc 0.78125\n",
      "2018-05-23T03:44:35.683292: step 3623, loss 0.393559, acc 0.859375\n",
      "2018-05-23T03:44:36.008423: step 3624, loss 0.284754, acc 0.859375\n",
      "2018-05-23T03:44:36.331558: step 3625, loss 0.431975, acc 0.84375\n",
      "2018-05-23T03:44:36.661675: step 3626, loss 0.417531, acc 0.828125\n",
      "2018-05-23T03:44:36.990795: step 3627, loss 0.463612, acc 0.75\n",
      "2018-05-23T03:44:37.321912: step 3628, loss 0.339962, acc 0.890625\n",
      "2018-05-23T03:44:37.646042: step 3629, loss 0.413514, acc 0.8125\n",
      "2018-05-23T03:44:37.986132: step 3630, loss 0.416604, acc 0.84375\n",
      "2018-05-23T03:44:38.314256: step 3631, loss 0.440599, acc 0.78125\n",
      "2018-05-23T03:44:38.640384: step 3632, loss 0.379461, acc 0.828125\n",
      "2018-05-23T03:44:38.972496: step 3633, loss 0.452901, acc 0.828125\n",
      "2018-05-23T03:44:39.301613: step 3634, loss 0.434852, acc 0.8125\n",
      "2018-05-23T03:44:39.629737: step 3635, loss 0.330747, acc 0.859375\n",
      "2018-05-23T03:44:39.966833: step 3636, loss 0.413011, acc 0.796875\n",
      "2018-05-23T03:44:40.288971: step 3637, loss 0.3168, acc 0.828125\n",
      "2018-05-23T03:44:40.614941: step 3638, loss 0.530191, acc 0.71875\n",
      "2018-05-23T03:44:40.950047: step 3639, loss 0.369667, acc 0.84375\n",
      "2018-05-23T03:44:41.272184: step 3640, loss 0.430996, acc 0.8125\n",
      "2018-05-23T03:44:41.596356: step 3641, loss 0.580144, acc 0.78125\n",
      "2018-05-23T03:44:41.933456: step 3642, loss 0.381986, acc 0.84375\n",
      "2018-05-23T03:44:42.256591: step 3643, loss 0.546593, acc 0.75\n",
      "2018-05-23T03:44:42.585711: step 3644, loss 0.509765, acc 0.75\n",
      "2018-05-23T03:44:42.914828: step 3645, loss 0.516511, acc 0.78125\n",
      "2018-05-23T03:44:43.249934: step 3646, loss 0.515254, acc 0.796875\n",
      "2018-05-23T03:44:43.578250: step 3647, loss 0.433668, acc 0.75\n",
      "2018-05-23T03:44:43.932304: step 3648, loss 0.460243, acc 0.796875\n",
      "2018-05-23T03:44:44.271398: step 3649, loss 0.432359, acc 0.796875\n",
      "2018-05-23T03:44:44.606499: step 3650, loss 0.584494, acc 0.703125\n",
      "2018-05-23T03:44:44.939609: step 3651, loss 0.310326, acc 0.90625\n",
      "2018-05-23T03:44:45.271720: step 3652, loss 0.350419, acc 0.859375\n",
      "2018-05-23T03:44:45.596850: step 3653, loss 0.338473, acc 0.859375\n",
      "2018-05-23T03:44:45.957886: step 3654, loss 0.551501, acc 0.78125\n",
      "2018-05-23T03:44:46.292988: step 3655, loss 0.487383, acc 0.8125\n",
      "2018-05-23T03:44:46.619115: step 3656, loss 0.370818, acc 0.8125\n",
      "2018-05-23T03:44:46.947237: step 3657, loss 0.426491, acc 0.796875\n",
      "2018-05-23T03:44:47.274362: step 3658, loss 0.416193, acc 0.84375\n",
      "2018-05-23T03:44:47.605479: step 3659, loss 0.437765, acc 0.84375\n",
      "2018-05-23T03:44:47.937588: step 3660, loss 0.491169, acc 0.8125\n",
      "2018-05-23T03:44:48.265712: step 3661, loss 0.581579, acc 0.75\n",
      "2018-05-23T03:44:48.591839: step 3662, loss 0.392896, acc 0.78125\n",
      "2018-05-23T03:44:48.924753: step 3663, loss 0.460475, acc 0.859375\n",
      "2018-05-23T03:44:49.246891: step 3664, loss 0.371098, acc 0.875\n",
      "2018-05-23T03:44:49.576012: step 3665, loss 0.42776, acc 0.8125\n",
      "2018-05-23T03:44:49.908122: step 3666, loss 0.522046, acc 0.71875\n",
      "2018-05-23T03:44:50.229263: step 3667, loss 0.410251, acc 0.796875\n",
      "2018-05-23T03:44:50.556388: step 3668, loss 0.432985, acc 0.828125\n",
      "2018-05-23T03:44:50.888500: step 3669, loss 0.418793, acc 0.8125\n",
      "2018-05-23T03:44:51.220613: step 3670, loss 0.345264, acc 0.84375\n",
      "2018-05-23T03:44:51.551725: step 3671, loss 0.639955, acc 0.671875\n",
      "2018-05-23T03:44:51.881842: step 3672, loss 0.439792, acc 0.8125\n",
      "2018-05-23T03:44:52.219938: step 3673, loss 0.483051, acc 0.78125\n",
      "2018-05-23T03:44:52.552051: step 3674, loss 0.566666, acc 0.78125\n",
      "2018-05-23T03:44:52.877181: step 3675, loss 0.524722, acc 0.71875\n",
      "2018-05-23T03:44:53.203307: step 3676, loss 0.438704, acc 0.78125\n",
      "2018-05-23T03:44:53.543401: step 3677, loss 0.585376, acc 0.703125\n",
      "2018-05-23T03:44:53.875509: step 3678, loss 0.484519, acc 0.828125\n",
      "2018-05-23T03:44:54.203634: step 3679, loss 0.339565, acc 0.8125\n",
      "2018-05-23T03:44:54.530756: step 3680, loss 0.415964, acc 0.796875\n",
      "2018-05-23T03:44:54.869849: step 3681, loss 0.533968, acc 0.75\n",
      "2018-05-23T03:44:55.200963: step 3682, loss 0.315738, acc 0.84375\n",
      "2018-05-23T03:44:55.538061: step 3683, loss 0.624446, acc 0.734375\n",
      "2018-05-23T03:44:55.877154: step 3684, loss 0.443994, acc 0.84375\n",
      "2018-05-23T03:44:56.205278: step 3685, loss 0.365516, acc 0.828125\n",
      "2018-05-23T03:44:56.538385: step 3686, loss 0.475322, acc 0.78125\n",
      "2018-05-23T03:44:56.871495: step 3687, loss 0.362763, acc 0.828125\n",
      "2018-05-23T03:44:57.194630: step 3688, loss 0.384046, acc 0.8125\n",
      "2018-05-23T03:44:57.526741: step 3689, loss 0.343458, acc 0.8125\n",
      "2018-05-23T03:44:57.856860: step 3690, loss 0.38162, acc 0.875\n",
      "2018-05-23T03:44:58.182986: step 3691, loss 0.340051, acc 0.859375\n",
      "2018-05-23T03:44:58.516095: step 3692, loss 0.394615, acc 0.875\n",
      "2018-05-23T03:44:58.864164: step 3693, loss 0.424671, acc 0.765625\n",
      "2018-05-23T03:44:59.194280: step 3694, loss 0.535232, acc 0.796875\n",
      "2018-05-23T03:44:59.523400: step 3695, loss 0.419895, acc 0.84375\n",
      "2018-05-23T03:44:59.854516: step 3696, loss 0.314908, acc 0.84375\n",
      "2018-05-23T03:45:00.195602: step 3697, loss 0.439701, acc 0.75\n",
      "2018-05-23T03:45:00.525719: step 3698, loss 0.405514, acc 0.78125\n",
      "2018-05-23T03:45:00.927644: step 3699, loss 0.438456, acc 0.71875\n",
      "2018-05-23T03:45:01.254768: step 3700, loss 0.411722, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:45:05.624080: step 3700, loss 0.55708, acc 0.728104\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3700\n",
      "\n",
      "2018-05-23T03:45:06.710759: step 3701, loss 0.47359, acc 0.8125\n",
      "2018-05-23T03:45:07.187485: step 3702, loss 0.432307, acc 0.78125\n",
      "2018-05-23T03:45:07.586419: step 3703, loss 0.367443, acc 0.8125\n",
      "2018-05-23T03:45:07.927504: step 3704, loss 0.456867, acc 0.78125\n",
      "2018-05-23T03:45:08.257622: step 3705, loss 0.368609, acc 0.859375\n",
      "2018-05-23T03:45:08.587738: step 3706, loss 0.364755, acc 0.828125\n",
      "2018-05-23T03:45:08.918853: step 3707, loss 0.47446, acc 0.78125\n",
      "2018-05-23T03:45:09.264928: step 3708, loss 0.43672, acc 0.828125\n",
      "2018-05-23T03:45:09.614990: step 3709, loss 0.494005, acc 0.734375\n",
      "2018-05-23T03:45:09.947103: step 3710, loss 0.390527, acc 0.875\n",
      "2018-05-23T03:45:10.276223: step 3711, loss 0.451275, acc 0.859375\n",
      "2018-05-23T03:45:10.608332: step 3712, loss 0.675245, acc 0.765625\n",
      "2018-05-23T03:45:10.947428: step 3713, loss 0.607327, acc 0.75\n",
      "2018-05-23T03:45:11.278541: step 3714, loss 0.424546, acc 0.8125\n",
      "2018-05-23T03:45:11.609655: step 3715, loss 0.561566, acc 0.71875\n",
      "2018-05-23T03:45:11.945756: step 3716, loss 0.462696, acc 0.78125\n",
      "2018-05-23T03:45:12.280858: step 3717, loss 0.381249, acc 0.8125\n",
      "2018-05-23T03:45:12.609978: step 3718, loss 0.480734, acc 0.71875\n",
      "2018-05-23T03:45:12.942089: step 3719, loss 0.500351, acc 0.8125\n",
      "2018-05-23T03:45:13.275198: step 3720, loss 0.465075, acc 0.796875\n",
      "2018-05-23T03:45:13.604319: step 3721, loss 0.296295, acc 0.859375\n",
      "2018-05-23T03:45:13.965354: step 3722, loss 0.392231, acc 0.8125\n",
      "2018-05-23T03:45:14.292477: step 3723, loss 0.423661, acc 0.859375\n",
      "2018-05-23T03:45:14.620601: step 3724, loss 0.41669, acc 0.78125\n",
      "2018-05-23T03:45:14.956701: step 3725, loss 0.588951, acc 0.75\n",
      "2018-05-23T03:45:15.282829: step 3726, loss 0.318618, acc 0.890625\n",
      "2018-05-23T03:45:15.610951: step 3727, loss 0.634014, acc 0.71875\n",
      "2018-05-23T03:45:15.954033: step 3728, loss 0.337038, acc 0.84375\n",
      "2018-05-23T03:45:16.278164: step 3729, loss 0.590209, acc 0.734375\n",
      "2018-05-23T03:45:16.610279: step 3730, loss 0.445076, acc 0.765625\n",
      "2018-05-23T03:45:16.971313: step 3731, loss 0.409078, acc 0.828125\n",
      "2018-05-23T03:45:17.302426: step 3732, loss 0.447594, acc 0.828125\n",
      "2018-05-23T03:45:17.635536: step 3733, loss 0.407265, acc 0.78125\n",
      "2018-05-23T03:45:17.966650: step 3734, loss 0.408816, acc 0.828125\n",
      "2018-05-23T03:45:18.292775: step 3735, loss 0.539584, acc 0.6875\n",
      "2018-05-23T03:45:18.625886: step 3736, loss 0.43423, acc 0.828125\n",
      "2018-05-23T03:45:18.965976: step 3737, loss 0.439112, acc 0.78125\n",
      "2018-05-23T03:45:19.294097: step 3738, loss 0.460087, acc 0.734375\n",
      "2018-05-23T03:45:19.619229: step 3739, loss 0.444015, acc 0.796875\n",
      "2018-05-23T03:45:19.953367: step 3740, loss 0.379396, acc 0.84375\n",
      "2018-05-23T03:45:20.278464: step 3741, loss 0.382769, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:45:20.604592: step 3742, loss 0.391307, acc 0.84375\n",
      "2018-05-23T03:45:20.937701: step 3743, loss 0.405398, acc 0.78125\n",
      "2018-05-23T03:45:21.265824: step 3744, loss 0.51185, acc 0.796875\n",
      "2018-05-23T03:45:21.594941: step 3745, loss 0.383101, acc 0.84375\n",
      "2018-05-23T03:45:21.923064: step 3746, loss 0.586294, acc 0.71875\n",
      "2018-05-23T03:45:22.244205: step 3747, loss 0.429716, acc 0.828125\n",
      "2018-05-23T03:45:22.573326: step 3748, loss 0.387152, acc 0.828125\n",
      "2018-05-23T03:45:22.902443: step 3749, loss 0.411328, acc 0.8125\n",
      "2018-05-23T03:45:23.228573: step 3750, loss 0.410652, acc 0.796875\n",
      "2018-05-23T03:45:23.557691: step 3751, loss 0.449182, acc 0.75\n",
      "2018-05-23T03:45:23.890802: step 3752, loss 0.370919, acc 0.859375\n",
      "2018-05-23T03:45:24.220918: step 3753, loss 0.376718, acc 0.859375\n",
      "2018-05-23T03:45:24.549039: step 3754, loss 0.567699, acc 0.703125\n",
      "2018-05-23T03:45:24.878161: step 3755, loss 0.588302, acc 0.78125\n",
      "2018-05-23T03:45:25.211270: step 3756, loss 0.513375, acc 0.765625\n",
      "2018-05-23T03:45:25.536399: step 3757, loss 0.428083, acc 0.8125\n",
      "2018-05-23T03:45:25.863524: step 3758, loss 0.470389, acc 0.796875\n",
      "2018-05-23T03:45:26.195635: step 3759, loss 0.430664, acc 0.8125\n",
      "2018-05-23T03:45:26.523758: step 3760, loss 0.377495, acc 0.859375\n",
      "2018-05-23T03:45:26.857862: step 3761, loss 0.55211, acc 0.75\n",
      "2018-05-23T03:45:27.180000: step 3762, loss 0.647535, acc 0.703125\n",
      "2018-05-23T03:45:27.506129: step 3763, loss 0.570088, acc 0.71875\n",
      "2018-05-23T03:45:27.837243: step 3764, loss 0.538956, acc 0.78125\n",
      "2018-05-23T03:45:28.172345: step 3765, loss 0.537665, acc 0.78125\n",
      "2018-05-23T03:45:28.502464: step 3766, loss 0.461294, acc 0.8125\n",
      "2018-05-23T03:45:28.904341: step 3767, loss 0.38482, acc 0.75\n",
      "2018-05-23T03:45:29.233460: step 3768, loss 0.501836, acc 0.828125\n",
      "2018-05-23T03:45:29.560588: step 3769, loss 0.622137, acc 0.78125\n",
      "2018-05-23T03:45:29.889707: step 3770, loss 0.369454, acc 0.84375\n",
      "2018-05-23T03:45:30.214835: step 3771, loss 0.575936, acc 0.75\n",
      "2018-05-23T03:45:30.538968: step 3772, loss 0.50039, acc 0.78125\n",
      "2018-05-23T03:45:30.917953: step 3773, loss 0.442559, acc 0.796875\n",
      "2018-05-23T03:45:31.245080: step 3774, loss 0.382126, acc 0.84375\n",
      "2018-05-23T03:45:31.571207: step 3775, loss 0.439463, acc 0.78125\n",
      "2018-05-23T03:45:31.902320: step 3776, loss 0.388712, acc 0.84375\n",
      "2018-05-23T03:45:32.230443: step 3777, loss 0.479993, acc 0.71875\n",
      "2018-05-23T03:45:32.562555: step 3778, loss 0.512559, acc 0.765625\n",
      "2018-05-23T03:45:32.902645: step 3779, loss 0.430047, acc 0.78125\n",
      "2018-05-23T03:45:33.248719: step 3780, loss 0.43322, acc 0.828125\n",
      "2018-05-23T03:45:33.594793: step 3781, loss 0.527221, acc 0.734375\n",
      "2018-05-23T03:45:33.945854: step 3782, loss 0.368514, acc 0.796875\n",
      "2018-05-23T03:45:34.305891: step 3783, loss 0.46339, acc 0.75\n",
      "2018-05-23T03:45:34.644020: step 3784, loss 0.503873, acc 0.734375\n",
      "2018-05-23T03:45:34.981085: step 3785, loss 0.452911, acc 0.828125\n",
      "2018-05-23T03:45:35.315191: step 3786, loss 0.453892, acc 0.8125\n",
      "2018-05-23T03:45:35.692183: step 3787, loss 0.552428, acc 0.75\n",
      "2018-05-23T03:45:36.015318: step 3788, loss 0.413046, acc 0.78125\n",
      "2018-05-23T03:45:36.351418: step 3789, loss 0.458596, acc 0.78125\n",
      "2018-05-23T03:45:36.708463: step 3790, loss 0.399084, acc 0.765625\n",
      "2018-05-23T03:45:37.033594: step 3791, loss 0.606576, acc 0.71875\n",
      "2018-05-23T03:45:37.369694: step 3792, loss 0.327503, acc 0.875\n",
      "2018-05-23T03:45:37.730730: step 3793, loss 0.414189, acc 0.796875\n",
      "2018-05-23T03:45:38.059848: step 3794, loss 0.41693, acc 0.796875\n",
      "2018-05-23T03:45:38.399939: step 3795, loss 0.482376, acc 0.796875\n",
      "2018-05-23T03:45:38.728096: step 3796, loss 0.602734, acc 0.71875\n",
      "2018-05-23T03:45:39.061172: step 3797, loss 0.440277, acc 0.796875\n",
      "2018-05-23T03:45:39.396273: step 3798, loss 0.51952, acc 0.71875\n",
      "2018-05-23T03:45:39.727387: step 3799, loss 0.606681, acc 0.703125\n",
      "2018-05-23T03:45:40.054511: step 3800, loss 0.448309, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:45:44.425818: step 3800, loss 0.529176, acc 0.743963\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3800\n",
      "\n",
      "2018-05-23T03:45:45.682882: step 3801, loss 0.421475, acc 0.765625\n",
      "2018-05-23T03:45:46.126695: step 3802, loss 0.387664, acc 0.90625\n",
      "2018-05-23T03:45:46.470775: step 3803, loss 0.40552, acc 0.8125\n",
      "2018-05-23T03:45:46.798896: step 3804, loss 0.470251, acc 0.8125\n",
      "2018-05-23T03:45:47.132005: step 3805, loss 0.48061, acc 0.75\n",
      "2018-05-23T03:45:47.468106: step 3806, loss 0.501289, acc 0.75\n",
      "2018-05-23T03:45:47.793236: step 3807, loss 0.506187, acc 0.78125\n",
      "2018-05-23T03:45:48.134325: step 3808, loss 0.511473, acc 0.765625\n",
      "2018-05-23T03:45:48.460450: step 3809, loss 0.393146, acc 0.828125\n",
      "2018-05-23T03:45:48.787576: step 3810, loss 0.469275, acc 0.8125\n",
      "2018-05-23T03:45:49.117693: step 3811, loss 0.276875, acc 0.859375\n",
      "2018-05-23T03:45:49.448808: step 3812, loss 0.389516, acc 0.8125\n",
      "2018-05-23T03:45:49.781917: step 3813, loss 0.393737, acc 0.828125\n",
      "2018-05-23T03:45:50.112033: step 3814, loss 0.500398, acc 0.765625\n",
      "2018-05-23T03:45:50.433173: step 3815, loss 0.469366, acc 0.8125\n",
      "2018-05-23T03:45:50.766283: step 3816, loss 0.403274, acc 0.796875\n",
      "2018-05-23T03:45:51.086596: step 3817, loss 0.465906, acc 0.765625\n",
      "2018-05-23T03:45:51.414740: step 3818, loss 0.468033, acc 0.78125\n",
      "2018-05-23T03:45:51.745856: step 3819, loss 0.417643, acc 0.828125\n",
      "2018-05-23T03:45:52.074976: step 3820, loss 0.390514, acc 0.796875\n",
      "2018-05-23T03:45:52.405092: step 3821, loss 0.388813, acc 0.8125\n",
      "2018-05-23T03:45:52.757150: step 3822, loss 0.515875, acc 0.78125\n",
      "2018-05-23T03:45:53.086271: step 3823, loss 0.469639, acc 0.796875\n",
      "2018-05-23T03:45:53.413395: step 3824, loss 0.473971, acc 0.75\n",
      "2018-05-23T03:45:53.744509: step 3825, loss 0.405501, acc 0.796875\n",
      "2018-05-23T03:45:54.068641: step 3826, loss 0.484279, acc 0.734375\n",
      "2018-05-23T03:45:54.401751: step 3827, loss 0.362381, acc 0.84375\n",
      "2018-05-23T03:45:54.738847: step 3828, loss 0.591574, acc 0.75\n",
      "2018-05-23T03:45:55.071957: step 3829, loss 0.360496, acc 0.859375\n",
      "2018-05-23T03:45:55.392099: step 3830, loss 0.502094, acc 0.796875\n",
      "2018-05-23T03:45:55.721219: step 3831, loss 0.400992, acc 0.78125\n",
      "2018-05-23T03:45:56.057320: step 3832, loss 0.348227, acc 0.890625\n",
      "2018-05-23T03:45:56.384447: step 3833, loss 0.560174, acc 0.78125\n",
      "2018-05-23T03:45:56.719549: step 3834, loss 0.483072, acc 0.71875\n",
      "2018-05-23T03:45:57.048670: step 3835, loss 0.475093, acc 0.796875\n",
      "2018-05-23T03:45:57.381779: step 3836, loss 0.581755, acc 0.75\n",
      "2018-05-23T03:45:57.719873: step 3837, loss 0.394329, acc 0.84375\n",
      "2018-05-23T03:45:58.048994: step 3838, loss 0.365917, acc 0.796875\n",
      "2018-05-23T03:45:58.375120: step 3839, loss 0.672191, acc 0.703125\n",
      "2018-05-23T03:45:58.714212: step 3840, loss 0.57709, acc 0.6875\n",
      "2018-05-23T03:45:59.044330: step 3841, loss 0.523868, acc 0.75\n",
      "2018-05-23T03:45:59.366470: step 3842, loss 0.615137, acc 0.71875\n",
      "2018-05-23T03:45:59.705560: step 3843, loss 0.447501, acc 0.796875\n",
      "2018-05-23T03:46:00.039670: step 3844, loss 0.468148, acc 0.8125\n",
      "2018-05-23T03:46:00.369786: step 3845, loss 0.39876, acc 0.796875\n",
      "2018-05-23T03:46:00.781683: step 3846, loss 0.461128, acc 0.765625\n",
      "2018-05-23T03:46:01.122772: step 3847, loss 0.410073, acc 0.796875\n",
      "2018-05-23T03:46:01.473831: step 3848, loss 0.470352, acc 0.765625\n",
      "2018-05-23T03:46:01.882738: step 3849, loss 0.366942, acc 0.796875\n",
      "2018-05-23T03:46:02.218837: step 3850, loss 0.434499, acc 0.734375\n",
      "2018-05-23T03:46:02.563913: step 3851, loss 0.44089, acc 0.765625\n",
      "2018-05-23T03:46:02.947887: step 3852, loss 0.408223, acc 0.78125\n",
      "2018-05-23T03:46:03.287978: step 3853, loss 0.438334, acc 0.78125\n",
      "2018-05-23T03:46:03.627069: step 3854, loss 0.428731, acc 0.796875\n",
      "2018-05-23T03:46:03.968159: step 3855, loss 0.311538, acc 0.890625\n",
      "2018-05-23T03:46:04.295282: step 3856, loss 0.374086, acc 0.828125\n",
      "2018-05-23T03:46:04.622406: step 3857, loss 0.494543, acc 0.8125\n",
      "2018-05-23T03:46:04.957512: step 3858, loss 0.536693, acc 0.71875\n",
      "2018-05-23T03:46:05.289623: step 3859, loss 0.408129, acc 0.828125\n",
      "2018-05-23T03:46:05.629714: step 3860, loss 0.449803, acc 0.75\n",
      "2018-05-23T03:46:05.959831: step 3861, loss 0.325246, acc 0.90625\n",
      "2018-05-23T03:46:06.297925: step 3862, loss 0.481081, acc 0.765625\n",
      "2018-05-23T03:46:06.627044: step 3863, loss 0.318691, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:46:06.970126: step 3864, loss 0.415252, acc 0.828125\n",
      "2018-05-23T03:46:07.302241: step 3865, loss 0.412682, acc 0.828125\n",
      "2018-05-23T03:46:07.626372: step 3866, loss 0.452674, acc 0.78125\n",
      "2018-05-23T03:46:07.962474: step 3867, loss 0.454657, acc 0.75\n",
      "2018-05-23T03:46:08.286605: step 3868, loss 0.482854, acc 0.796875\n",
      "2018-05-23T03:46:08.612734: step 3869, loss 0.309162, acc 0.84375\n",
      "2018-05-23T03:46:08.954818: step 3870, loss 0.328885, acc 0.875\n",
      "2018-05-23T03:46:09.277952: step 3871, loss 0.491158, acc 0.75\n",
      "2018-05-23T03:46:09.605077: step 3872, loss 0.511005, acc 0.8125\n",
      "2018-05-23T03:46:09.933200: step 3873, loss 0.257037, acc 0.921875\n",
      "2018-05-23T03:46:10.283267: step 3874, loss 0.402446, acc 0.796875\n",
      "2018-05-23T03:46:10.606398: step 3875, loss 0.473217, acc 0.734375\n",
      "2018-05-23T03:46:10.933523: step 3876, loss 0.501666, acc 0.75\n",
      "2018-05-23T03:46:11.264638: step 3877, loss 0.328437, acc 0.828125\n",
      "2018-05-23T03:46:11.590768: step 3878, loss 0.580997, acc 0.71875\n",
      "2018-05-23T03:46:11.924872: step 3879, loss 0.437507, acc 0.8125\n",
      "2018-05-23T03:46:12.258977: step 3880, loss 0.374711, acc 0.859375\n",
      "2018-05-23T03:46:12.588100: step 3881, loss 0.393599, acc 0.828125\n",
      "2018-05-23T03:46:12.917217: step 3882, loss 0.364876, acc 0.828125\n",
      "2018-05-23T03:46:13.251322: step 3883, loss 0.47293, acc 0.78125\n",
      "2018-05-23T03:46:13.575458: step 3884, loss 0.437178, acc 0.796875\n",
      "2018-05-23T03:46:13.916544: step 3885, loss 0.427144, acc 0.8125\n",
      "2018-05-23T03:46:14.247657: step 3886, loss 0.408747, acc 0.78125\n",
      "2018-05-23T03:46:14.572788: step 3887, loss 0.335148, acc 0.859375\n",
      "2018-05-23T03:46:14.911882: step 3888, loss 0.441651, acc 0.78125\n",
      "2018-05-23T03:46:15.247981: step 3889, loss 0.338356, acc 0.890625\n",
      "2018-05-23T03:46:15.575108: step 3890, loss 0.452241, acc 0.796875\n",
      "2018-05-23T03:46:15.958084: step 3891, loss 0.619604, acc 0.734375\n",
      "2018-05-23T03:46:16.301163: step 3892, loss 0.428597, acc 0.828125\n",
      "2018-05-23T03:46:16.630285: step 3893, loss 0.383617, acc 0.796875\n",
      "2018-05-23T03:46:16.964392: step 3894, loss 0.471925, acc 0.765625\n",
      "2018-05-23T03:46:17.289520: step 3895, loss 0.481283, acc 0.796875\n",
      "2018-05-23T03:46:17.612657: step 3896, loss 0.443894, acc 0.796875\n",
      "2018-05-23T03:46:17.956737: step 3897, loss 0.444056, acc 0.828125\n",
      "2018-05-23T03:46:18.282864: step 3898, loss 0.355486, acc 0.8125\n",
      "2018-05-23T03:46:18.609989: step 3899, loss 0.365919, acc 0.84375\n",
      "2018-05-23T03:46:18.939109: step 3900, loss 0.3862, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:46:23.332354: step 3900, loss 0.528478, acc 0.737391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-3900\n",
      "\n",
      "2018-05-23T03:46:24.491255: step 3901, loss 0.490635, acc 0.765625\n",
      "2018-05-23T03:46:25.027818: step 3902, loss 0.423314, acc 0.765625\n",
      "2018-05-23T03:46:25.392844: step 3903, loss 0.661216, acc 0.71875\n",
      "2018-05-23T03:46:25.754873: step 3904, loss 0.430169, acc 0.828125\n",
      "2018-05-23T03:46:26.082997: step 3905, loss 0.340892, acc 0.890625\n",
      "2018-05-23T03:46:26.411119: step 3906, loss 0.526374, acc 0.75\n",
      "2018-05-23T03:46:26.753205: step 3907, loss 0.406805, acc 0.84375\n",
      "2018-05-23T03:46:27.076341: step 3908, loss 0.445004, acc 0.796875\n",
      "2018-05-23T03:46:27.400472: step 3909, loss 0.496973, acc 0.765625\n",
      "2018-05-23T03:46:27.734578: step 3910, loss 0.466806, acc 0.8125\n",
      "2018-05-23T03:46:28.059707: step 3911, loss 0.376467, acc 0.84375\n",
      "2018-05-23T03:46:28.382844: step 3912, loss 0.464142, acc 0.8125\n",
      "2018-05-23T03:46:28.726923: step 3913, loss 0.44954, acc 0.796875\n",
      "2018-05-23T03:46:29.059036: step 3914, loss 0.466652, acc 0.75\n",
      "2018-05-23T03:46:29.389190: step 3915, loss 0.465712, acc 0.828125\n",
      "2018-05-23T03:46:29.724296: step 3916, loss 0.372507, acc 0.859375\n",
      "2018-05-23T03:46:30.054410: step 3917, loss 0.409152, acc 0.828125\n",
      "2018-05-23T03:46:30.383529: step 3918, loss 0.466172, acc 0.8125\n",
      "2018-05-23T03:46:30.716639: step 3919, loss 0.47697, acc 0.703125\n",
      "2018-05-23T03:46:31.095624: step 3920, loss 0.538417, acc 0.8125\n",
      "2018-05-23T03:46:31.419758: step 3921, loss 0.447686, acc 0.828125\n",
      "2018-05-23T03:46:31.748877: step 3922, loss 0.374089, acc 0.859375\n",
      "2018-05-23T03:46:32.078994: step 3923, loss 0.451059, acc 0.796875\n",
      "2018-05-23T03:46:32.409111: step 3924, loss 0.496625, acc 0.6875\n",
      "2018-05-23T03:46:32.744214: step 3925, loss 0.492647, acc 0.703125\n",
      "2018-05-23T03:46:33.070342: step 3926, loss 0.382515, acc 0.828125\n",
      "2018-05-23T03:46:33.402455: step 3927, loss 0.513722, acc 0.75\n",
      "2018-05-23T03:46:33.741546: step 3928, loss 0.463521, acc 0.796875\n",
      "2018-05-23T03:46:34.075654: step 3929, loss 0.573219, acc 0.75\n",
      "2018-05-23T03:46:34.406767: step 3930, loss 0.491982, acc 0.765625\n",
      "2018-05-23T03:46:34.733891: step 3931, loss 0.374599, acc 0.84375\n",
      "2018-05-23T03:46:35.064008: step 3932, loss 0.449966, acc 0.796875\n",
      "2018-05-23T03:46:35.396122: step 3933, loss 0.495337, acc 0.78125\n",
      "2018-05-23T03:46:35.729230: step 3934, loss 0.409243, acc 0.84375\n",
      "2018-05-23T03:46:36.060346: step 3935, loss 0.567079, acc 0.65625\n",
      "2018-05-23T03:46:36.382482: step 3936, loss 0.293409, acc 0.921875\n",
      "2018-05-23T03:46:36.712601: step 3937, loss 0.576202, acc 0.6875\n",
      "2018-05-23T03:46:37.047736: step 3938, loss 0.462882, acc 0.71875\n",
      "2018-05-23T03:46:37.371835: step 3939, loss 0.41951, acc 0.828125\n",
      "2018-05-23T03:46:37.604214: step 3940, loss 0.432167, acc 0.764706\n",
      "2018-05-23T03:46:37.947298: step 3941, loss 0.434891, acc 0.765625\n",
      "2018-05-23T03:46:38.281401: step 3942, loss 0.214816, acc 0.890625\n",
      "2018-05-23T03:46:38.610521: step 3943, loss 0.32644, acc 0.859375\n",
      "2018-05-23T03:46:38.943631: step 3944, loss 0.417334, acc 0.8125\n",
      "2018-05-23T03:46:39.276739: step 3945, loss 0.303508, acc 0.875\n",
      "2018-05-23T03:46:39.597880: step 3946, loss 0.282739, acc 0.921875\n",
      "2018-05-23T03:46:39.937970: step 3947, loss 0.298663, acc 0.90625\n",
      "2018-05-23T03:46:40.270081: step 3948, loss 0.390509, acc 0.828125\n",
      "2018-05-23T03:46:40.592219: step 3949, loss 0.292004, acc 0.859375\n",
      "2018-05-23T03:46:40.925328: step 3950, loss 0.30829, acc 0.90625\n",
      "2018-05-23T03:46:41.255445: step 3951, loss 0.27487, acc 0.890625\n",
      "2018-05-23T03:46:41.585562: step 3952, loss 0.446526, acc 0.765625\n",
      "2018-05-23T03:46:41.916677: step 3953, loss 0.398258, acc 0.828125\n",
      "2018-05-23T03:46:42.249785: step 3954, loss 0.557073, acc 0.734375\n",
      "2018-05-23T03:46:42.576914: step 3955, loss 0.520868, acc 0.734375\n",
      "2018-05-23T03:46:42.912014: step 3956, loss 0.337372, acc 0.828125\n",
      "2018-05-23T03:46:43.243130: step 3957, loss 0.360788, acc 0.828125\n",
      "2018-05-23T03:46:43.576237: step 3958, loss 0.345752, acc 0.84375\n",
      "2018-05-23T03:46:43.930289: step 3959, loss 0.436197, acc 0.78125\n",
      "2018-05-23T03:46:44.266391: step 3960, loss 0.459395, acc 0.75\n",
      "2018-05-23T03:46:44.596508: step 3961, loss 0.420662, acc 0.765625\n",
      "2018-05-23T03:46:44.932608: step 3962, loss 0.320841, acc 0.875\n",
      "2018-05-23T03:46:45.263724: step 3963, loss 0.310147, acc 0.890625\n",
      "2018-05-23T03:46:45.593841: step 3964, loss 0.396351, acc 0.859375\n",
      "2018-05-23T03:46:45.953876: step 3965, loss 0.397518, acc 0.8125\n",
      "2018-05-23T03:46:46.282996: step 3966, loss 0.28599, acc 0.859375\n",
      "2018-05-23T03:46:46.615108: step 3967, loss 0.338445, acc 0.84375\n",
      "2018-05-23T03:46:46.945225: step 3968, loss 0.305372, acc 0.890625\n",
      "2018-05-23T03:46:47.279331: step 3969, loss 0.318154, acc 0.875\n",
      "2018-05-23T03:46:47.613437: step 3970, loss 0.398915, acc 0.84375\n",
      "2018-05-23T03:46:47.943554: step 3971, loss 0.394447, acc 0.828125\n",
      "2018-05-23T03:46:48.279654: step 3972, loss 0.421379, acc 0.78125\n",
      "2018-05-23T03:46:48.604784: step 3973, loss 0.446751, acc 0.828125\n",
      "2018-05-23T03:46:48.938893: step 3974, loss 0.282968, acc 0.890625\n",
      "2018-05-23T03:46:49.267014: step 3975, loss 0.422672, acc 0.765625\n",
      "2018-05-23T03:46:49.598128: step 3976, loss 0.486294, acc 0.765625\n",
      "2018-05-23T03:46:49.934230: step 3977, loss 0.344128, acc 0.890625\n",
      "2018-05-23T03:46:50.269331: step 3978, loss 0.31789, acc 0.875\n",
      "2018-05-23T03:46:50.595474: step 3979, loss 0.467792, acc 0.75\n",
      "2018-05-23T03:46:50.919592: step 3980, loss 0.266982, acc 0.921875\n",
      "2018-05-23T03:46:51.250449: step 3981, loss 0.400454, acc 0.796875\n",
      "2018-05-23T03:46:51.580565: step 3982, loss 0.368223, acc 0.796875\n",
      "2018-05-23T03:46:51.904699: step 3983, loss 0.329604, acc 0.875\n",
      "2018-05-23T03:46:52.233816: step 3984, loss 0.343256, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:46:52.567923: step 3985, loss 0.337035, acc 0.828125\n",
      "2018-05-23T03:46:52.903028: step 3986, loss 0.347748, acc 0.84375\n",
      "2018-05-23T03:46:53.228159: step 3987, loss 0.473097, acc 0.796875\n",
      "2018-05-23T03:46:53.560268: step 3988, loss 0.502748, acc 0.765625\n",
      "2018-05-23T03:46:53.891384: step 3989, loss 0.390001, acc 0.859375\n",
      "2018-05-23T03:46:54.216512: step 3990, loss 0.430067, acc 0.765625\n",
      "2018-05-23T03:46:54.551617: step 3991, loss 0.397792, acc 0.828125\n",
      "2018-05-23T03:46:54.883727: step 3992, loss 0.385864, acc 0.84375\n",
      "2018-05-23T03:46:55.220826: step 3993, loss 0.268869, acc 0.921875\n",
      "2018-05-23T03:46:55.553934: step 3994, loss 0.325762, acc 0.875\n",
      "2018-05-23T03:46:55.882057: step 3995, loss 0.472853, acc 0.78125\n",
      "2018-05-23T03:46:56.213740: step 3996, loss 0.354788, acc 0.890625\n",
      "2018-05-23T03:46:56.546850: step 3997, loss 0.277732, acc 0.84375\n",
      "2018-05-23T03:46:56.880957: step 3998, loss 0.445574, acc 0.796875\n",
      "2018-05-23T03:46:57.203096: step 3999, loss 0.327556, acc 0.84375\n",
      "2018-05-23T03:46:57.530218: step 4000, loss 0.324586, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:47:02.105977: step 4000, loss 0.545752, acc 0.744249\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4000\n",
      "\n",
      "2018-05-23T03:47:03.432524: step 4001, loss 0.274754, acc 0.890625\n",
      "2018-05-23T03:47:03.769624: step 4002, loss 0.382688, acc 0.828125\n",
      "2018-05-23T03:47:04.100738: step 4003, loss 0.508081, acc 0.796875\n",
      "2018-05-23T03:47:04.441827: step 4004, loss 0.320955, acc 0.859375\n",
      "2018-05-23T03:47:04.807847: step 4005, loss 0.365533, acc 0.875\n",
      "2018-05-23T03:47:05.134970: step 4006, loss 0.416466, acc 0.796875\n",
      "2018-05-23T03:47:05.471073: step 4007, loss 0.341606, acc 0.890625\n",
      "2018-05-23T03:47:05.817145: step 4008, loss 0.374328, acc 0.828125\n",
      "2018-05-23T03:47:06.143272: step 4009, loss 0.389594, acc 0.859375\n",
      "2018-05-23T03:47:06.472394: step 4010, loss 0.326215, acc 0.890625\n",
      "2018-05-23T03:47:06.811484: step 4011, loss 0.346948, acc 0.84375\n",
      "2018-05-23T03:47:07.139608: step 4012, loss 0.415362, acc 0.84375\n",
      "2018-05-23T03:47:07.459752: step 4013, loss 0.449967, acc 0.828125\n",
      "2018-05-23T03:47:07.789868: step 4014, loss 0.626638, acc 0.765625\n",
      "2018-05-23T03:47:08.120981: step 4015, loss 0.289574, acc 0.875\n",
      "2018-05-23T03:47:08.447110: step 4016, loss 0.471341, acc 0.8125\n",
      "2018-05-23T03:47:08.773239: step 4017, loss 0.347352, acc 0.8125\n",
      "2018-05-23T03:47:09.109338: step 4018, loss 0.406547, acc 0.78125\n",
      "2018-05-23T03:47:09.438459: step 4019, loss 0.474521, acc 0.734375\n",
      "2018-05-23T03:47:09.763590: step 4020, loss 0.366959, acc 0.859375\n",
      "2018-05-23T03:47:10.092708: step 4021, loss 0.332945, acc 0.890625\n",
      "2018-05-23T03:47:10.420829: step 4022, loss 0.346398, acc 0.84375\n",
      "2018-05-23T03:47:10.750946: step 4023, loss 0.276156, acc 0.890625\n",
      "2018-05-23T03:47:11.087048: step 4024, loss 0.313232, acc 0.859375\n",
      "2018-05-23T03:47:11.413175: step 4025, loss 0.381402, acc 0.828125\n",
      "2018-05-23T03:47:11.745287: step 4026, loss 0.453039, acc 0.828125\n",
      "2018-05-23T03:47:12.082386: step 4027, loss 0.259723, acc 0.890625\n",
      "2018-05-23T03:47:12.410508: step 4028, loss 0.377131, acc 0.828125\n",
      "2018-05-23T03:47:12.739627: step 4029, loss 0.373401, acc 0.828125\n",
      "2018-05-23T03:47:13.071740: step 4030, loss 0.346977, acc 0.859375\n",
      "2018-05-23T03:47:13.400858: step 4031, loss 0.30452, acc 0.890625\n",
      "2018-05-23T03:47:13.737957: step 4032, loss 0.271556, acc 0.875\n",
      "2018-05-23T03:47:14.065081: step 4033, loss 0.373896, acc 0.8125\n",
      "2018-05-23T03:47:14.396194: step 4034, loss 0.398289, acc 0.84375\n",
      "2018-05-23T03:47:14.728306: step 4035, loss 0.293109, acc 0.875\n",
      "2018-05-23T03:47:15.068396: step 4036, loss 0.400229, acc 0.8125\n",
      "2018-05-23T03:47:15.397518: step 4037, loss 0.441537, acc 0.765625\n",
      "2018-05-23T03:47:15.740599: step 4038, loss 0.269733, acc 0.84375\n",
      "2018-05-23T03:47:16.113602: step 4039, loss 0.473078, acc 0.765625\n",
      "2018-05-23T03:47:16.442719: step 4040, loss 0.311877, acc 0.875\n",
      "2018-05-23T03:47:16.808741: step 4041, loss 0.234983, acc 0.921875\n",
      "2018-05-23T03:47:17.144842: step 4042, loss 0.30539, acc 0.890625\n",
      "2018-05-23T03:47:17.473963: step 4043, loss 0.253737, acc 0.890625\n",
      "2018-05-23T03:47:17.807070: step 4044, loss 0.340652, acc 0.859375\n",
      "2018-05-23T03:47:18.135192: step 4045, loss 0.419722, acc 0.828125\n",
      "2018-05-23T03:47:18.458328: step 4046, loss 0.491519, acc 0.765625\n",
      "2018-05-23T03:47:18.795428: step 4047, loss 0.432949, acc 0.84375\n",
      "2018-05-23T03:47:19.129531: step 4048, loss 0.363237, acc 0.859375\n",
      "2018-05-23T03:47:19.463637: step 4049, loss 0.327889, acc 0.859375\n",
      "2018-05-23T03:47:19.793754: step 4050, loss 0.374611, acc 0.8125\n",
      "2018-05-23T03:47:20.122874: step 4051, loss 0.338725, acc 0.796875\n",
      "2018-05-23T03:47:20.452992: step 4052, loss 0.330529, acc 0.84375\n",
      "2018-05-23T03:47:20.782113: step 4053, loss 0.465016, acc 0.796875\n",
      "2018-05-23T03:47:21.110233: step 4054, loss 0.221543, acc 0.953125\n",
      "2018-05-23T03:47:21.445371: step 4055, loss 0.411537, acc 0.796875\n",
      "2018-05-23T03:47:21.774458: step 4056, loss 0.334108, acc 0.859375\n",
      "2018-05-23T03:47:22.100584: step 4057, loss 0.368878, acc 0.8125\n",
      "2018-05-23T03:47:22.432696: step 4058, loss 0.462431, acc 0.8125\n",
      "2018-05-23T03:47:22.765805: step 4059, loss 0.29712, acc 0.875\n",
      "2018-05-23T03:47:23.096919: step 4060, loss 0.42994, acc 0.71875\n",
      "2018-05-23T03:47:23.429031: step 4061, loss 0.343975, acc 0.828125\n",
      "2018-05-23T03:47:23.764134: step 4062, loss 0.376461, acc 0.8125\n",
      "2018-05-23T03:47:24.093256: step 4063, loss 0.28358, acc 0.875\n",
      "2018-05-23T03:47:24.428357: step 4064, loss 0.321317, acc 0.875\n",
      "2018-05-23T03:47:24.756478: step 4065, loss 0.376827, acc 0.859375\n",
      "2018-05-23T03:47:25.083605: step 4066, loss 0.38218, acc 0.796875\n",
      "2018-05-23T03:47:25.449627: step 4067, loss 0.314192, acc 0.84375\n",
      "2018-05-23T03:47:25.783731: step 4068, loss 0.341966, acc 0.8125\n",
      "2018-05-23T03:47:26.120869: step 4069, loss 0.347703, acc 0.84375\n",
      "2018-05-23T03:47:26.452980: step 4070, loss 0.315758, acc 0.828125\n",
      "2018-05-23T03:47:26.785094: step 4071, loss 0.347557, acc 0.890625\n",
      "2018-05-23T03:47:27.111220: step 4072, loss 0.384321, acc 0.828125\n",
      "2018-05-23T03:47:27.447320: step 4073, loss 0.344259, acc 0.84375\n",
      "2018-05-23T03:47:27.783421: step 4074, loss 0.382792, acc 0.859375\n",
      "2018-05-23T03:47:28.117529: step 4075, loss 0.387762, acc 0.828125\n",
      "2018-05-23T03:47:28.451634: step 4076, loss 0.512706, acc 0.765625\n",
      "2018-05-23T03:47:28.803694: step 4077, loss 0.340215, acc 0.859375\n",
      "2018-05-23T03:47:29.129822: step 4078, loss 0.369838, acc 0.859375\n",
      "2018-05-23T03:47:29.459937: step 4079, loss 0.489521, acc 0.78125\n",
      "2018-05-23T03:47:29.791050: step 4080, loss 0.411554, acc 0.8125\n",
      "2018-05-23T03:47:30.126154: step 4081, loss 0.368142, acc 0.84375\n",
      "2018-05-23T03:47:30.449290: step 4082, loss 0.44715, acc 0.78125\n",
      "2018-05-23T03:47:30.782399: step 4083, loss 0.419954, acc 0.78125\n",
      "2018-05-23T03:47:31.120496: step 4084, loss 0.313133, acc 0.84375\n",
      "2018-05-23T03:47:31.445627: step 4085, loss 0.333694, acc 0.859375\n",
      "2018-05-23T03:47:31.781727: step 4086, loss 0.444788, acc 0.71875\n",
      "2018-05-23T03:47:32.114836: step 4087, loss 0.270057, acc 0.875\n",
      "2018-05-23T03:47:32.446948: step 4088, loss 0.314736, acc 0.828125\n",
      "2018-05-23T03:47:32.785043: step 4089, loss 0.355526, acc 0.875\n",
      "2018-05-23T03:47:33.120147: step 4090, loss 0.413886, acc 0.796875\n",
      "2018-05-23T03:47:33.445309: step 4091, loss 0.277158, acc 0.84375\n",
      "2018-05-23T03:47:33.777386: step 4092, loss 0.450206, acc 0.828125\n",
      "2018-05-23T03:47:34.110529: step 4093, loss 0.327007, acc 0.859375\n",
      "2018-05-23T03:47:34.441611: step 4094, loss 0.360171, acc 0.84375\n",
      "2018-05-23T03:47:34.781701: step 4095, loss 0.393557, acc 0.796875\n",
      "2018-05-23T03:47:35.108827: step 4096, loss 0.270372, acc 0.84375\n",
      "2018-05-23T03:47:35.440938: step 4097, loss 0.311163, acc 0.890625\n",
      "2018-05-23T03:47:35.773050: step 4098, loss 0.359188, acc 0.875\n",
      "2018-05-23T03:47:36.096185: step 4099, loss 0.294063, acc 0.859375\n",
      "2018-05-23T03:47:36.427299: step 4100, loss 0.368908, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:47:40.831596: step 4100, loss 0.561821, acc 0.740249\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4100\n",
      "\n",
      "2018-05-23T03:47:42.104838: step 4101, loss 0.312572, acc 0.84375\n",
      "2018-05-23T03:47:42.472851: step 4102, loss 0.404119, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:47:42.822914: step 4103, loss 0.440075, acc 0.8125\n",
      "2018-05-23T03:47:43.221844: step 4104, loss 0.401248, acc 0.84375\n",
      "2018-05-23T03:47:43.550965: step 4105, loss 0.374489, acc 0.859375\n",
      "2018-05-23T03:47:43.885073: step 4106, loss 0.314043, acc 0.875\n",
      "2018-05-23T03:47:44.221173: step 4107, loss 0.381791, acc 0.828125\n",
      "2018-05-23T03:47:44.549295: step 4108, loss 0.324828, acc 0.84375\n",
      "2018-05-23T03:47:44.882404: step 4109, loss 0.270389, acc 0.90625\n",
      "2018-05-23T03:47:45.217541: step 4110, loss 0.514947, acc 0.71875\n",
      "2018-05-23T03:47:45.547622: step 4111, loss 0.463882, acc 0.796875\n",
      "2018-05-23T03:47:45.885721: step 4112, loss 0.41384, acc 0.78125\n",
      "2018-05-23T03:47:46.218827: step 4113, loss 0.371967, acc 0.796875\n",
      "2018-05-23T03:47:46.536976: step 4114, loss 0.332424, acc 0.828125\n",
      "2018-05-23T03:47:46.864103: step 4115, loss 0.251275, acc 0.890625\n",
      "2018-05-23T03:47:47.194220: step 4116, loss 0.290673, acc 0.859375\n",
      "2018-05-23T03:47:47.521343: step 4117, loss 0.274801, acc 0.890625\n",
      "2018-05-23T03:47:47.854453: step 4118, loss 0.339289, acc 0.890625\n",
      "2018-05-23T03:47:48.189557: step 4119, loss 0.317772, acc 0.859375\n",
      "2018-05-23T03:47:48.524659: step 4120, loss 0.358681, acc 0.828125\n",
      "2018-05-23T03:47:48.854777: step 4121, loss 0.472212, acc 0.796875\n",
      "2018-05-23T03:47:49.178910: step 4122, loss 0.355987, acc 0.84375\n",
      "2018-05-23T03:47:49.499054: step 4123, loss 0.325796, acc 0.859375\n",
      "2018-05-23T03:47:49.826177: step 4124, loss 0.410245, acc 0.84375\n",
      "2018-05-23T03:47:50.153302: step 4125, loss 0.597244, acc 0.703125\n",
      "2018-05-23T03:47:50.482421: step 4126, loss 0.365988, acc 0.828125\n",
      "2018-05-23T03:47:50.811577: step 4127, loss 0.392661, acc 0.796875\n",
      "2018-05-23T03:47:51.139664: step 4128, loss 0.470179, acc 0.796875\n",
      "2018-05-23T03:47:51.475764: step 4129, loss 0.325026, acc 0.84375\n",
      "2018-05-23T03:47:51.803888: step 4130, loss 0.482586, acc 0.84375\n",
      "2018-05-23T03:47:52.136995: step 4131, loss 0.436696, acc 0.84375\n",
      "2018-05-23T03:47:52.467111: step 4132, loss 0.449334, acc 0.796875\n",
      "2018-05-23T03:47:52.794239: step 4133, loss 0.308228, acc 0.859375\n",
      "2018-05-23T03:47:53.121362: step 4134, loss 0.436444, acc 0.78125\n",
      "2018-05-23T03:47:53.456465: step 4135, loss 0.364519, acc 0.859375\n",
      "2018-05-23T03:47:53.785587: step 4136, loss 0.489768, acc 0.8125\n",
      "2018-05-23T03:47:54.111714: step 4137, loss 0.396894, acc 0.796875\n",
      "2018-05-23T03:47:54.448813: step 4138, loss 0.340019, acc 0.84375\n",
      "2018-05-23T03:47:54.790898: step 4139, loss 0.266685, acc 0.890625\n",
      "2018-05-23T03:47:55.123008: step 4140, loss 0.443403, acc 0.796875\n",
      "2018-05-23T03:47:55.453125: step 4141, loss 0.287466, acc 0.90625\n",
      "2018-05-23T03:47:55.779252: step 4142, loss 0.348256, acc 0.84375\n",
      "2018-05-23T03:47:56.106377: step 4143, loss 0.305483, acc 0.875\n",
      "2018-05-23T03:47:56.442478: step 4144, loss 0.324674, acc 0.859375\n",
      "2018-05-23T03:47:56.780575: step 4145, loss 0.26787, acc 0.859375\n",
      "2018-05-23T03:47:57.103710: step 4146, loss 0.290903, acc 0.859375\n",
      "2018-05-23T03:47:57.430835: step 4147, loss 0.435543, acc 0.828125\n",
      "2018-05-23T03:47:57.760951: step 4148, loss 0.36079, acc 0.875\n",
      "2018-05-23T03:47:58.092064: step 4149, loss 0.361826, acc 0.84375\n",
      "2018-05-23T03:47:58.419189: step 4150, loss 0.318592, acc 0.84375\n",
      "2018-05-23T03:47:58.753296: step 4151, loss 0.34866, acc 0.828125\n",
      "2018-05-23T03:47:59.076431: step 4152, loss 0.3342, acc 0.859375\n",
      "2018-05-23T03:47:59.402558: step 4153, loss 0.32413, acc 0.828125\n",
      "2018-05-23T03:47:59.733675: step 4154, loss 0.356871, acc 0.84375\n",
      "2018-05-23T03:48:00.063790: step 4155, loss 0.348277, acc 0.84375\n",
      "2018-05-23T03:48:00.398894: step 4156, loss 0.399636, acc 0.8125\n",
      "2018-05-23T03:48:00.814781: step 4157, loss 0.365851, acc 0.84375\n",
      "2018-05-23T03:48:01.200748: step 4158, loss 0.479341, acc 0.765625\n",
      "2018-05-23T03:48:01.590704: step 4159, loss 0.359325, acc 0.859375\n",
      "2018-05-23T03:48:01.984652: step 4160, loss 0.342106, acc 0.875\n",
      "2018-05-23T03:48:02.325738: step 4161, loss 0.416795, acc 0.859375\n",
      "2018-05-23T03:48:02.656853: step 4162, loss 0.360791, acc 0.859375\n",
      "2018-05-23T03:48:03.045812: step 4163, loss 0.459235, acc 0.78125\n",
      "2018-05-23T03:48:03.402858: step 4164, loss 0.479004, acc 0.828125\n",
      "2018-05-23T03:48:03.742947: step 4165, loss 0.388982, acc 0.78125\n",
      "2018-05-23T03:48:04.076058: step 4166, loss 0.403106, acc 0.796875\n",
      "2018-05-23T03:48:04.401188: step 4167, loss 0.548604, acc 0.75\n",
      "2018-05-23T03:48:04.732300: step 4168, loss 0.355236, acc 0.84375\n",
      "2018-05-23T03:48:05.058428: step 4169, loss 0.362498, acc 0.8125\n",
      "2018-05-23T03:48:05.389544: step 4170, loss 0.310867, acc 0.875\n",
      "2018-05-23T03:48:05.717665: step 4171, loss 0.364764, acc 0.78125\n",
      "2018-05-23T03:48:06.051772: step 4172, loss 0.343805, acc 0.875\n",
      "2018-05-23T03:48:06.382886: step 4173, loss 0.394444, acc 0.8125\n",
      "2018-05-23T03:48:06.710009: step 4174, loss 0.310058, acc 0.875\n",
      "2018-05-23T03:48:07.046111: step 4175, loss 0.344198, acc 0.875\n",
      "2018-05-23T03:48:07.377226: step 4176, loss 0.389987, acc 0.828125\n",
      "2018-05-23T03:48:07.711331: step 4177, loss 0.568208, acc 0.796875\n",
      "2018-05-23T03:48:08.042444: step 4178, loss 0.489502, acc 0.78125\n",
      "2018-05-23T03:48:08.372561: step 4179, loss 0.357261, acc 0.84375\n",
      "2018-05-23T03:48:08.698690: step 4180, loss 0.341866, acc 0.84375\n",
      "2018-05-23T03:48:09.030803: step 4181, loss 0.430991, acc 0.75\n",
      "2018-05-23T03:48:09.356931: step 4182, loss 0.413005, acc 0.8125\n",
      "2018-05-23T03:48:09.685050: step 4183, loss 0.43594, acc 0.796875\n",
      "2018-05-23T03:48:10.016164: step 4184, loss 0.364951, acc 0.84375\n",
      "2018-05-23T03:48:10.344288: step 4185, loss 0.360728, acc 0.828125\n",
      "2018-05-23T03:48:10.671412: step 4186, loss 0.492256, acc 0.8125\n",
      "2018-05-23T03:48:10.998537: step 4187, loss 0.341969, acc 0.859375\n",
      "2018-05-23T03:48:11.335636: step 4188, loss 0.40436, acc 0.859375\n",
      "2018-05-23T03:48:11.665754: step 4189, loss 0.360227, acc 0.8125\n",
      "2018-05-23T03:48:11.989885: step 4190, loss 0.538868, acc 0.796875\n",
      "2018-05-23T03:48:12.338953: step 4191, loss 0.309403, acc 0.890625\n",
      "2018-05-23T03:48:12.680040: step 4192, loss 0.486892, acc 0.75\n",
      "2018-05-23T03:48:13.008161: step 4193, loss 0.323346, acc 0.8125\n",
      "2018-05-23T03:48:13.351875: step 4194, loss 0.536259, acc 0.765625\n",
      "2018-05-23T03:48:13.687975: step 4195, loss 0.283293, acc 0.890625\n",
      "2018-05-23T03:48:14.041030: step 4196, loss 0.370411, acc 0.84375\n",
      "2018-05-23T03:48:14.385147: step 4197, loss 0.411163, acc 0.796875\n",
      "2018-05-23T03:48:14.714228: step 4198, loss 0.431459, acc 0.796875\n",
      "2018-05-23T03:48:15.046342: step 4199, loss 0.308242, acc 0.84375\n",
      "2018-05-23T03:48:15.377455: step 4200, loss 0.241501, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:48:19.755742: step 4200, loss 0.558218, acc 0.740249\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4200\n",
      "\n",
      "2018-05-23T03:48:20.851811: step 4201, loss 0.41971, acc 0.78125\n",
      "2018-05-23T03:48:21.326539: step 4202, loss 0.406007, acc 0.84375\n",
      "2018-05-23T03:48:21.687574: step 4203, loss 0.259786, acc 0.859375\n",
      "2018-05-23T03:48:22.079524: step 4204, loss 0.425494, acc 0.828125\n",
      "2018-05-23T03:48:22.416622: step 4205, loss 0.380231, acc 0.75\n",
      "2018-05-23T03:48:22.758708: step 4206, loss 0.348131, acc 0.859375\n",
      "2018-05-23T03:48:23.096803: step 4207, loss 0.415663, acc 0.84375\n",
      "2018-05-23T03:48:23.426920: step 4208, loss 0.32381, acc 0.875\n",
      "2018-05-23T03:48:23.763021: step 4209, loss 0.388978, acc 0.828125\n",
      "2018-05-23T03:48:24.095132: step 4210, loss 0.346875, acc 0.84375\n",
      "2018-05-23T03:48:24.421262: step 4211, loss 0.366675, acc 0.859375\n",
      "2018-05-23T03:48:24.756364: step 4212, loss 0.349888, acc 0.859375\n",
      "2018-05-23T03:48:25.089473: step 4213, loss 0.690425, acc 0.71875\n",
      "2018-05-23T03:48:25.413605: step 4214, loss 0.468902, acc 0.734375\n",
      "2018-05-23T03:48:25.755690: step 4215, loss 0.43789, acc 0.828125\n",
      "2018-05-23T03:48:26.084809: step 4216, loss 0.238562, acc 0.90625\n",
      "2018-05-23T03:48:26.421908: step 4217, loss 0.471013, acc 0.71875\n",
      "2018-05-23T03:48:26.760004: step 4218, loss 0.498613, acc 0.859375\n",
      "2018-05-23T03:48:27.094110: step 4219, loss 0.47624, acc 0.75\n",
      "2018-05-23T03:48:27.428215: step 4220, loss 0.471625, acc 0.796875\n",
      "2018-05-23T03:48:27.762333: step 4221, loss 0.253357, acc 0.890625\n",
      "2018-05-23T03:48:28.087466: step 4222, loss 0.474042, acc 0.78125\n",
      "2018-05-23T03:48:28.417582: step 4223, loss 0.279757, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:48:28.777618: step 4224, loss 0.36419, acc 0.84375\n",
      "2018-05-23T03:48:29.109729: step 4225, loss 0.403798, acc 0.8125\n",
      "2018-05-23T03:48:29.433864: step 4226, loss 0.373209, acc 0.828125\n",
      "2018-05-23T03:48:29.763016: step 4227, loss 0.406926, acc 0.828125\n",
      "2018-05-23T03:48:30.105066: step 4228, loss 0.382199, acc 0.828125\n",
      "2018-05-23T03:48:30.431194: step 4229, loss 0.426267, acc 0.828125\n",
      "2018-05-23T03:48:30.760315: step 4230, loss 0.385533, acc 0.859375\n",
      "2018-05-23T03:48:31.115376: step 4231, loss 0.286358, acc 0.828125\n",
      "2018-05-23T03:48:31.445481: step 4232, loss 0.288115, acc 0.90625\n",
      "2018-05-23T03:48:31.773603: step 4233, loss 0.396031, acc 0.859375\n",
      "2018-05-23T03:48:32.109705: step 4234, loss 0.37926, acc 0.828125\n",
      "2018-05-23T03:48:32.445804: step 4235, loss 0.59303, acc 0.765625\n",
      "2018-05-23T03:48:32.781907: step 4236, loss 0.413349, acc 0.8125\n",
      "2018-05-23T03:48:33.111026: step 4237, loss 0.577664, acc 0.765625\n",
      "2018-05-23T03:48:33.438150: step 4238, loss 0.29584, acc 0.90625\n",
      "2018-05-23T03:48:33.771260: step 4239, loss 0.390256, acc 0.859375\n",
      "2018-05-23T03:48:34.100379: step 4240, loss 0.325293, acc 0.875\n",
      "2018-05-23T03:48:34.428502: step 4241, loss 0.439936, acc 0.84375\n",
      "2018-05-23T03:48:34.761609: step 4242, loss 0.312184, acc 0.828125\n",
      "2018-05-23T03:48:35.088736: step 4243, loss 0.440016, acc 0.734375\n",
      "2018-05-23T03:48:35.417854: step 4244, loss 0.335007, acc 0.84375\n",
      "2018-05-23T03:48:35.753954: step 4245, loss 0.407295, acc 0.84375\n",
      "2018-05-23T03:48:36.081081: step 4246, loss 0.352959, acc 0.859375\n",
      "2018-05-23T03:48:36.411198: step 4247, loss 0.439057, acc 0.84375\n",
      "2018-05-23T03:48:36.742312: step 4248, loss 0.341716, acc 0.890625\n",
      "2018-05-23T03:48:37.077414: step 4249, loss 0.527548, acc 0.75\n",
      "2018-05-23T03:48:37.404540: step 4250, loss 0.422431, acc 0.78125\n",
      "2018-05-23T03:48:37.733659: step 4251, loss 0.237715, acc 0.90625\n",
      "2018-05-23T03:48:38.057793: step 4252, loss 0.376071, acc 0.84375\n",
      "2018-05-23T03:48:38.388908: step 4253, loss 0.5012, acc 0.78125\n",
      "2018-05-23T03:48:38.727000: step 4254, loss 0.463154, acc 0.796875\n",
      "2018-05-23T03:48:39.057117: step 4255, loss 0.33026, acc 0.859375\n",
      "2018-05-23T03:48:39.390227: step 4256, loss 0.455827, acc 0.859375\n",
      "2018-05-23T03:48:39.718351: step 4257, loss 0.303385, acc 0.890625\n",
      "2018-05-23T03:48:40.049465: step 4258, loss 0.537233, acc 0.8125\n",
      "2018-05-23T03:48:40.385564: step 4259, loss 0.397287, acc 0.796875\n",
      "2018-05-23T03:48:40.719671: step 4260, loss 0.498385, acc 0.765625\n",
      "2018-05-23T03:48:41.049787: step 4261, loss 0.451634, acc 0.734375\n",
      "2018-05-23T03:48:41.385889: step 4262, loss 0.446026, acc 0.796875\n",
      "2018-05-23T03:48:41.713014: step 4263, loss 0.341473, acc 0.859375\n",
      "2018-05-23T03:48:42.051110: step 4264, loss 0.356718, acc 0.875\n",
      "2018-05-23T03:48:42.385214: step 4265, loss 0.442555, acc 0.78125\n",
      "2018-05-23T03:48:42.713338: step 4266, loss 0.420972, acc 0.75\n",
      "2018-05-23T03:48:43.043454: step 4267, loss 0.48237, acc 0.828125\n",
      "2018-05-23T03:48:43.375567: step 4268, loss 0.292501, acc 0.84375\n",
      "2018-05-23T03:48:43.715655: step 4269, loss 0.250547, acc 0.9375\n",
      "2018-05-23T03:48:44.083672: step 4270, loss 0.424654, acc 0.75\n",
      "2018-05-23T03:48:44.415784: step 4271, loss 0.321754, acc 0.84375\n",
      "2018-05-23T03:48:44.744902: step 4272, loss 0.414544, acc 0.796875\n",
      "2018-05-23T03:48:45.081005: step 4273, loss 0.46906, acc 0.75\n",
      "2018-05-23T03:48:45.409127: step 4274, loss 0.416674, acc 0.796875\n",
      "2018-05-23T03:48:45.740240: step 4275, loss 0.352472, acc 0.84375\n",
      "2018-05-23T03:48:46.120223: step 4276, loss 0.529117, acc 0.71875\n",
      "2018-05-23T03:48:46.448344: step 4277, loss 0.297773, acc 0.875\n",
      "2018-05-23T03:48:46.779459: step 4278, loss 0.390046, acc 0.75\n",
      "2018-05-23T03:48:47.108580: step 4279, loss 0.313978, acc 0.859375\n",
      "2018-05-23T03:48:47.440691: step 4280, loss 0.356664, acc 0.828125\n",
      "2018-05-23T03:48:47.770809: step 4281, loss 0.27351, acc 0.859375\n",
      "2018-05-23T03:48:48.103917: step 4282, loss 0.516947, acc 0.734375\n",
      "2018-05-23T03:48:48.443009: step 4283, loss 0.353956, acc 0.84375\n",
      "2018-05-23T03:48:48.774122: step 4284, loss 0.343183, acc 0.875\n",
      "2018-05-23T03:48:49.100252: step 4285, loss 0.27073, acc 0.890625\n",
      "2018-05-23T03:48:49.428373: step 4286, loss 0.428987, acc 0.78125\n",
      "2018-05-23T03:48:49.772454: step 4287, loss 0.327173, acc 0.859375\n",
      "2018-05-23T03:48:50.103567: step 4288, loss 0.39746, acc 0.84375\n",
      "2018-05-23T03:48:50.430693: step 4289, loss 0.360987, acc 0.796875\n",
      "2018-05-23T03:48:50.771778: step 4290, loss 0.532301, acc 0.78125\n",
      "2018-05-23T03:48:51.096909: step 4291, loss 0.370162, acc 0.828125\n",
      "2018-05-23T03:48:51.429022: step 4292, loss 0.500735, acc 0.75\n",
      "2018-05-23T03:48:51.768113: step 4293, loss 0.411146, acc 0.828125\n",
      "2018-05-23T03:48:52.101224: step 4294, loss 0.337553, acc 0.875\n",
      "2018-05-23T03:48:52.429346: step 4295, loss 0.343767, acc 0.78125\n",
      "2018-05-23T03:48:52.771429: step 4296, loss 0.555211, acc 0.703125\n",
      "2018-05-23T03:48:53.100548: step 4297, loss 0.272107, acc 0.875\n",
      "2018-05-23T03:48:53.442636: step 4298, loss 0.311872, acc 0.859375\n",
      "2018-05-23T03:48:53.773750: step 4299, loss 0.412401, acc 0.8125\n",
      "2018-05-23T03:48:54.106859: step 4300, loss 0.319435, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:48:58.494121: step 4300, loss 0.563051, acc 0.732533\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4300\n",
      "\n",
      "2018-05-23T03:48:59.859087: step 4301, loss 0.444805, acc 0.765625\n",
      "2018-05-23T03:49:00.240067: step 4302, loss 0.351061, acc 0.828125\n",
      "2018-05-23T03:49:00.636148: step 4303, loss 0.531179, acc 0.796875\n",
      "2018-05-23T03:49:01.000176: step 4304, loss 0.305498, acc 0.84375\n",
      "2018-05-23T03:49:01.332669: step 4305, loss 0.358738, acc 0.828125\n",
      "2018-05-23T03:49:01.702680: step 4306, loss 0.40523, acc 0.796875\n",
      "2018-05-23T03:49:02.044764: step 4307, loss 0.493165, acc 0.796875\n",
      "2018-05-23T03:49:02.400811: step 4308, loss 0.421449, acc 0.796875\n",
      "2018-05-23T03:49:02.746886: step 4309, loss 0.266747, acc 0.890625\n",
      "2018-05-23T03:49:03.081991: step 4310, loss 0.384902, acc 0.828125\n",
      "2018-05-23T03:49:03.422080: step 4311, loss 0.308342, acc 0.890625\n",
      "2018-05-23T03:49:03.810042: step 4312, loss 0.327577, acc 0.875\n",
      "2018-05-23T03:49:04.139163: step 4313, loss 0.341697, acc 0.84375\n",
      "2018-05-23T03:49:04.466286: step 4314, loss 0.35391, acc 0.859375\n",
      "2018-05-23T03:49:04.797402: step 4315, loss 0.529037, acc 0.765625\n",
      "2018-05-23T03:49:05.128516: step 4316, loss 0.519174, acc 0.71875\n",
      "2018-05-23T03:49:05.452648: step 4317, loss 0.455706, acc 0.796875\n",
      "2018-05-23T03:49:05.783764: step 4318, loss 0.22539, acc 0.921875\n",
      "2018-05-23T03:49:06.113878: step 4319, loss 0.346563, acc 0.859375\n",
      "2018-05-23T03:49:06.446987: step 4320, loss 0.521416, acc 0.765625\n",
      "2018-05-23T03:49:06.781093: step 4321, loss 0.418391, acc 0.8125\n",
      "2018-05-23T03:49:07.114202: step 4322, loss 0.325627, acc 0.875\n",
      "2018-05-23T03:49:07.437339: step 4323, loss 0.353488, acc 0.859375\n",
      "2018-05-23T03:49:07.770446: step 4324, loss 0.361493, acc 0.828125\n",
      "2018-05-23T03:49:08.103554: step 4325, loss 0.419632, acc 0.78125\n",
      "2018-05-23T03:49:08.443645: step 4326, loss 0.251451, acc 0.875\n",
      "2018-05-23T03:49:08.783735: step 4327, loss 0.432376, acc 0.765625\n",
      "2018-05-23T03:49:09.116844: step 4328, loss 0.238818, acc 0.921875\n",
      "2018-05-23T03:49:09.450951: step 4329, loss 0.458897, acc 0.8125\n",
      "2018-05-23T03:49:09.785059: step 4330, loss 0.434476, acc 0.796875\n",
      "2018-05-23T03:49:10.117171: step 4331, loss 0.391946, acc 0.8125\n",
      "2018-05-23T03:49:10.462245: step 4332, loss 0.344621, acc 0.796875\n",
      "2018-05-23T03:49:10.798346: step 4333, loss 0.436628, acc 0.75\n",
      "2018-05-23T03:49:11.130460: step 4334, loss 0.339473, acc 0.828125\n",
      "2018-05-23T03:49:11.465561: step 4335, loss 0.363172, acc 0.84375\n",
      "2018-05-23T03:49:11.791689: step 4336, loss 0.390283, acc 0.859375\n",
      "2018-05-23T03:49:12.123803: step 4337, loss 0.412752, acc 0.84375\n",
      "2018-05-23T03:49:12.456909: step 4338, loss 0.29765, acc 0.890625\n",
      "2018-05-23T03:49:12.787026: step 4339, loss 0.314428, acc 0.84375\n",
      "2018-05-23T03:49:13.122129: step 4340, loss 0.425509, acc 0.75\n",
      "2018-05-23T03:49:13.456238: step 4341, loss 0.324349, acc 0.875\n",
      "2018-05-23T03:49:13.792338: step 4342, loss 0.413898, acc 0.84375\n",
      "2018-05-23T03:49:14.133424: step 4343, loss 0.388694, acc 0.796875\n",
      "2018-05-23T03:49:14.468551: step 4344, loss 0.380138, acc 0.859375\n",
      "2018-05-23T03:49:14.808617: step 4345, loss 0.318204, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:49:15.143721: step 4346, loss 0.353965, acc 0.84375\n",
      "2018-05-23T03:49:15.474173: step 4347, loss 0.272651, acc 0.875\n",
      "2018-05-23T03:49:15.800302: step 4348, loss 0.409325, acc 0.8125\n",
      "2018-05-23T03:49:16.134407: step 4349, loss 0.488757, acc 0.765625\n",
      "2018-05-23T03:49:16.501425: step 4350, loss 0.353051, acc 0.8125\n",
      "2018-05-23T03:49:16.834534: step 4351, loss 0.449141, acc 0.78125\n",
      "2018-05-23T03:49:17.219505: step 4352, loss 0.350583, acc 0.90625\n",
      "2018-05-23T03:49:17.623425: step 4353, loss 0.330155, acc 0.828125\n",
      "2018-05-23T03:49:18.034324: step 4354, loss 0.358873, acc 0.875\n",
      "2018-05-23T03:49:18.456196: step 4355, loss 0.455693, acc 0.84375\n",
      "2018-05-23T03:49:18.797282: step 4356, loss 0.345589, acc 0.828125\n",
      "2018-05-23T03:49:19.125406: step 4357, loss 0.407168, acc 0.78125\n",
      "2018-05-23T03:49:19.455522: step 4358, loss 0.384596, acc 0.859375\n",
      "2018-05-23T03:49:19.793617: step 4359, loss 0.361271, acc 0.875\n",
      "2018-05-23T03:49:20.121740: step 4360, loss 0.365812, acc 0.875\n",
      "2018-05-23T03:49:20.491751: step 4361, loss 0.339744, acc 0.875\n",
      "2018-05-23T03:49:20.827851: step 4362, loss 0.43493, acc 0.796875\n",
      "2018-05-23T03:49:21.158967: step 4363, loss 0.475478, acc 0.84375\n",
      "2018-05-23T03:49:21.486092: step 4364, loss 0.280192, acc 0.890625\n",
      "2018-05-23T03:49:21.828176: step 4365, loss 0.267201, acc 0.890625\n",
      "2018-05-23T03:49:22.162703: step 4366, loss 0.545672, acc 0.65625\n",
      "2018-05-23T03:49:22.495813: step 4367, loss 0.275898, acc 0.859375\n",
      "2018-05-23T03:49:22.821939: step 4368, loss 0.342989, acc 0.828125\n",
      "2018-05-23T03:49:23.153055: step 4369, loss 0.261024, acc 0.875\n",
      "2018-05-23T03:49:23.485166: step 4370, loss 0.347734, acc 0.796875\n",
      "2018-05-23T03:49:23.824260: step 4371, loss 0.34295, acc 0.875\n",
      "2018-05-23T03:49:24.153377: step 4372, loss 0.466478, acc 0.765625\n",
      "2018-05-23T03:49:24.483494: step 4373, loss 0.252176, acc 0.90625\n",
      "2018-05-23T03:49:24.816603: step 4374, loss 0.300751, acc 0.859375\n",
      "2018-05-23T03:49:25.150709: step 4375, loss 0.409838, acc 0.78125\n",
      "2018-05-23T03:49:25.486811: step 4376, loss 0.39264, acc 0.890625\n",
      "2018-05-23T03:49:25.817924: step 4377, loss 0.299809, acc 0.890625\n",
      "2018-05-23T03:49:26.150038: step 4378, loss 0.459938, acc 0.765625\n",
      "2018-05-23T03:49:26.478162: step 4379, loss 0.350114, acc 0.859375\n",
      "2018-05-23T03:49:26.812265: step 4380, loss 0.424652, acc 0.828125\n",
      "2018-05-23T03:49:27.145374: step 4381, loss 0.386697, acc 0.828125\n",
      "2018-05-23T03:49:27.475492: step 4382, loss 0.278346, acc 0.859375\n",
      "2018-05-23T03:49:27.807602: step 4383, loss 0.334922, acc 0.875\n",
      "2018-05-23T03:49:28.134727: step 4384, loss 0.48252, acc 0.75\n",
      "2018-05-23T03:49:28.457862: step 4385, loss 0.257095, acc 0.890625\n",
      "2018-05-23T03:49:28.801945: step 4386, loss 0.32091, acc 0.859375\n",
      "2018-05-23T03:49:29.135050: step 4387, loss 0.379357, acc 0.8125\n",
      "2018-05-23T03:49:29.459183: step 4388, loss 0.334752, acc 0.828125\n",
      "2018-05-23T03:49:29.788303: step 4389, loss 0.211916, acc 0.9375\n",
      "2018-05-23T03:49:30.129390: step 4390, loss 0.355812, acc 0.875\n",
      "2018-05-23T03:49:30.457513: step 4391, loss 0.266359, acc 0.90625\n",
      "2018-05-23T03:49:30.785635: step 4392, loss 0.444738, acc 0.75\n",
      "2018-05-23T03:49:31.139690: step 4393, loss 0.354684, acc 0.765625\n",
      "2018-05-23T03:49:31.468810: step 4394, loss 0.233705, acc 0.921875\n",
      "2018-05-23T03:49:31.796932: step 4395, loss 0.263197, acc 0.90625\n",
      "2018-05-23T03:49:32.124057: step 4396, loss 0.348777, acc 0.8125\n",
      "2018-05-23T03:49:32.452177: step 4397, loss 0.308965, acc 0.859375\n",
      "2018-05-23T03:49:32.792269: step 4398, loss 0.336404, acc 0.890625\n",
      "2018-05-23T03:49:33.122383: step 4399, loss 0.428117, acc 0.78125\n",
      "2018-05-23T03:49:33.463472: step 4400, loss 0.481252, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:49:37.871679: step 4400, loss 0.608939, acc 0.727818\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4400\n",
      "\n",
      "2018-05-23T03:49:39.070472: step 4401, loss 0.364598, acc 0.84375\n",
      "2018-05-23T03:49:39.494337: step 4402, loss 0.25739, acc 0.875\n",
      "2018-05-23T03:49:39.845401: step 4403, loss 0.357191, acc 0.84375\n",
      "2018-05-23T03:49:40.251314: step 4404, loss 0.398409, acc 0.828125\n",
      "2018-05-23T03:49:40.590405: step 4405, loss 0.24433, acc 0.875\n",
      "2018-05-23T03:49:40.926507: step 4406, loss 0.373951, acc 0.859375\n",
      "2018-05-23T03:49:41.256623: step 4407, loss 0.306269, acc 0.90625\n",
      "2018-05-23T03:49:41.593721: step 4408, loss 0.316401, acc 0.828125\n",
      "2018-05-23T03:49:41.934808: step 4409, loss 0.289175, acc 0.890625\n",
      "2018-05-23T03:49:42.261935: step 4410, loss 0.388314, acc 0.828125\n",
      "2018-05-23T03:49:42.600029: step 4411, loss 0.390179, acc 0.78125\n",
      "2018-05-23T03:49:42.932141: step 4412, loss 0.343934, acc 0.859375\n",
      "2018-05-23T03:49:43.257271: step 4413, loss 0.351479, acc 0.84375\n",
      "2018-05-23T03:49:43.587390: step 4414, loss 0.365395, acc 0.859375\n",
      "2018-05-23T03:49:43.936454: step 4415, loss 0.430457, acc 0.796875\n",
      "2018-05-23T03:49:44.261584: step 4416, loss 0.460854, acc 0.796875\n",
      "2018-05-23T03:49:44.594693: step 4417, loss 0.278682, acc 0.875\n",
      "2018-05-23T03:49:44.927803: step 4418, loss 0.479095, acc 0.828125\n",
      "2018-05-23T03:49:45.255924: step 4419, loss 0.315898, acc 0.828125\n",
      "2018-05-23T03:49:45.590030: step 4420, loss 0.388374, acc 0.78125\n",
      "2018-05-23T03:49:45.926131: step 4421, loss 0.308385, acc 0.84375\n",
      "2018-05-23T03:49:46.315091: step 4422, loss 0.254465, acc 0.90625\n",
      "2018-05-23T03:49:46.658173: step 4423, loss 0.364565, acc 0.890625\n",
      "2018-05-23T03:49:46.992280: step 4424, loss 0.502042, acc 0.765625\n",
      "2018-05-23T03:49:47.318406: step 4425, loss 0.39128, acc 0.859375\n",
      "2018-05-23T03:49:47.658500: step 4426, loss 0.446404, acc 0.875\n",
      "2018-05-23T03:49:47.989613: step 4427, loss 0.547052, acc 0.75\n",
      "2018-05-23T03:49:48.321725: step 4428, loss 0.439658, acc 0.796875\n",
      "2018-05-23T03:49:48.650843: step 4429, loss 0.343157, acc 0.875\n",
      "2018-05-23T03:49:48.984948: step 4430, loss 0.325801, acc 0.859375\n",
      "2018-05-23T03:49:49.310079: step 4431, loss 0.348939, acc 0.859375\n",
      "2018-05-23T03:49:49.636208: step 4432, loss 0.442338, acc 0.75\n",
      "2018-05-23T03:49:49.969316: step 4433, loss 0.40034, acc 0.796875\n",
      "2018-05-23T03:49:50.294483: step 4434, loss 0.337146, acc 0.84375\n",
      "2018-05-23T03:49:50.636530: step 4435, loss 0.387079, acc 0.828125\n",
      "2018-05-23T03:49:50.968642: step 4436, loss 0.372573, acc 0.84375\n",
      "2018-05-23T03:49:51.300753: step 4437, loss 0.384555, acc 0.84375\n",
      "2018-05-23T03:49:51.628875: step 4438, loss 0.391912, acc 0.828125\n",
      "2018-05-23T03:49:51.972956: step 4439, loss 0.327123, acc 0.84375\n",
      "2018-05-23T03:49:52.304069: step 4440, loss 0.316666, acc 0.859375\n",
      "2018-05-23T03:49:52.638175: step 4441, loss 0.410191, acc 0.78125\n",
      "2018-05-23T03:49:52.972282: step 4442, loss 0.342622, acc 0.90625\n",
      "2018-05-23T03:49:53.299406: step 4443, loss 0.375105, acc 0.78125\n",
      "2018-05-23T03:49:53.631520: step 4444, loss 0.295045, acc 0.796875\n",
      "2018-05-23T03:49:53.968616: step 4445, loss 0.394235, acc 0.8125\n",
      "2018-05-23T03:49:54.300761: step 4446, loss 0.45307, acc 0.828125\n",
      "2018-05-23T03:49:54.636829: step 4447, loss 0.542774, acc 0.75\n",
      "2018-05-23T03:49:54.975924: step 4448, loss 0.398508, acc 0.8125\n",
      "2018-05-23T03:49:55.306038: step 4449, loss 0.338283, acc 0.84375\n",
      "2018-05-23T03:49:55.630171: step 4450, loss 0.350262, acc 0.828125\n",
      "2018-05-23T03:49:55.970261: step 4451, loss 0.282383, acc 0.875\n",
      "2018-05-23T03:49:56.296396: step 4452, loss 0.432927, acc 0.765625\n",
      "2018-05-23T03:49:56.621520: step 4453, loss 0.331289, acc 0.84375\n",
      "2018-05-23T03:49:56.960612: step 4454, loss 0.349179, acc 0.859375\n",
      "2018-05-23T03:49:57.284747: step 4455, loss 0.391849, acc 0.796875\n",
      "2018-05-23T03:49:57.618851: step 4456, loss 0.304771, acc 0.90625\n",
      "2018-05-23T03:49:57.959938: step 4457, loss 0.550567, acc 0.71875\n",
      "2018-05-23T03:49:58.303020: step 4458, loss 0.346442, acc 0.859375\n",
      "2018-05-23T03:49:58.636130: step 4459, loss 0.456721, acc 0.734375\n",
      "2018-05-23T03:49:59.027083: step 4460, loss 0.397036, acc 0.765625\n",
      "2018-05-23T03:49:59.358198: step 4461, loss 0.40752, acc 0.8125\n",
      "2018-05-23T03:49:59.691308: step 4462, loss 0.432499, acc 0.8125\n",
      "2018-05-23T03:50:00.041370: step 4463, loss 0.244817, acc 0.921875\n",
      "2018-05-23T03:50:00.368495: step 4464, loss 0.43151, acc 0.765625\n",
      "2018-05-23T03:50:00.748480: step 4465, loss 0.457435, acc 0.78125\n",
      "2018-05-23T03:50:01.150406: step 4466, loss 0.435209, acc 0.78125\n",
      "2018-05-23T03:50:01.512435: step 4467, loss 0.590578, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:50:01.892419: step 4468, loss 0.258022, acc 0.890625\n",
      "2018-05-23T03:50:02.242483: step 4469, loss 0.281765, acc 0.875\n",
      "2018-05-23T03:50:02.601521: step 4470, loss 0.447915, acc 0.859375\n",
      "2018-05-23T03:50:02.966545: step 4471, loss 0.384823, acc 0.84375\n",
      "2018-05-23T03:50:03.336556: step 4472, loss 0.448959, acc 0.828125\n",
      "2018-05-23T03:50:03.679638: step 4473, loss 0.527109, acc 0.765625\n",
      "2018-05-23T03:50:04.016735: step 4474, loss 0.423281, acc 0.765625\n",
      "2018-05-23T03:50:04.375775: step 4475, loss 0.3688, acc 0.84375\n",
      "2018-05-23T03:50:04.705893: step 4476, loss 0.401904, acc 0.796875\n",
      "2018-05-23T03:50:05.036009: step 4477, loss 0.421075, acc 0.796875\n",
      "2018-05-23T03:50:05.371112: step 4478, loss 0.320168, acc 0.84375\n",
      "2018-05-23T03:50:05.695245: step 4479, loss 0.305631, acc 0.859375\n",
      "2018-05-23T03:50:06.034338: step 4480, loss 0.498786, acc 0.796875\n",
      "2018-05-23T03:50:06.370438: step 4481, loss 0.443758, acc 0.78125\n",
      "2018-05-23T03:50:06.702550: step 4482, loss 0.412368, acc 0.828125\n",
      "2018-05-23T03:50:07.028679: step 4483, loss 0.233909, acc 0.90625\n",
      "2018-05-23T03:50:07.364779: step 4484, loss 0.483215, acc 0.765625\n",
      "2018-05-23T03:50:07.692902: step 4485, loss 0.352119, acc 0.875\n",
      "2018-05-23T03:50:08.023019: step 4486, loss 0.314429, acc 0.84375\n",
      "2018-05-23T03:50:08.352139: step 4487, loss 0.370729, acc 0.8125\n",
      "2018-05-23T03:50:08.679262: step 4488, loss 0.428306, acc 0.828125\n",
      "2018-05-23T03:50:09.004393: step 4489, loss 0.316965, acc 0.84375\n",
      "2018-05-23T03:50:09.334510: step 4490, loss 0.332321, acc 0.796875\n",
      "2018-05-23T03:50:09.669614: step 4491, loss 0.271997, acc 0.859375\n",
      "2018-05-23T03:50:09.990754: step 4492, loss 0.269085, acc 0.875\n",
      "2018-05-23T03:50:10.319873: step 4493, loss 0.29859, acc 0.890625\n",
      "2018-05-23T03:50:10.648993: step 4494, loss 0.334612, acc 0.890625\n",
      "2018-05-23T03:50:10.978112: step 4495, loss 0.28009, acc 0.90625\n",
      "2018-05-23T03:50:11.304240: step 4496, loss 0.480684, acc 0.765625\n",
      "2018-05-23T03:50:11.647322: step 4497, loss 0.323146, acc 0.828125\n",
      "2018-05-23T03:50:11.981428: step 4498, loss 0.391846, acc 0.765625\n",
      "2018-05-23T03:50:12.311548: step 4499, loss 0.55837, acc 0.75\n",
      "2018-05-23T03:50:12.646648: step 4500, loss 0.429547, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:50:17.084776: step 4500, loss 0.56263, acc 0.738391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4500\n",
      "\n",
      "2018-05-23T03:50:18.185830: step 4501, loss 0.525961, acc 0.8125\n",
      "2018-05-23T03:50:18.664549: step 4502, loss 0.41553, acc 0.84375\n",
      "2018-05-23T03:50:19.023594: step 4503, loss 0.32057, acc 0.796875\n",
      "2018-05-23T03:50:19.413546: step 4504, loss 0.276798, acc 0.859375\n",
      "2018-05-23T03:50:19.750645: step 4505, loss 0.431322, acc 0.828125\n",
      "2018-05-23T03:50:20.094724: step 4506, loss 0.285477, acc 0.90625\n",
      "2018-05-23T03:50:20.416862: step 4507, loss 0.411022, acc 0.78125\n",
      "2018-05-23T03:50:20.745981: step 4508, loss 0.530673, acc 0.8125\n",
      "2018-05-23T03:50:21.079093: step 4509, loss 0.255157, acc 0.953125\n",
      "2018-05-23T03:50:21.404222: step 4510, loss 0.311488, acc 0.859375\n",
      "2018-05-23T03:50:21.730351: step 4511, loss 0.427359, acc 0.75\n",
      "2018-05-23T03:50:22.059467: step 4512, loss 0.48341, acc 0.8125\n",
      "2018-05-23T03:50:22.388587: step 4513, loss 0.334582, acc 0.859375\n",
      "2018-05-23T03:50:22.714714: step 4514, loss 0.394874, acc 0.765625\n",
      "2018-05-23T03:50:23.045864: step 4515, loss 0.351538, acc 0.890625\n",
      "2018-05-23T03:50:23.378938: step 4516, loss 0.283256, acc 0.890625\n",
      "2018-05-23T03:50:23.704068: step 4517, loss 0.545036, acc 0.765625\n",
      "2018-05-23T03:50:24.039172: step 4518, loss 0.330348, acc 0.875\n",
      "2018-05-23T03:50:24.376272: step 4519, loss 0.322164, acc 0.859375\n",
      "2018-05-23T03:50:24.705747: step 4520, loss 0.502923, acc 0.75\n",
      "2018-05-23T03:50:25.039855: step 4521, loss 0.356287, acc 0.84375\n",
      "2018-05-23T03:50:25.380942: step 4522, loss 0.444322, acc 0.78125\n",
      "2018-05-23T03:50:25.710063: step 4523, loss 0.33753, acc 0.828125\n",
      "2018-05-23T03:50:26.036188: step 4524, loss 0.305068, acc 0.921875\n",
      "2018-05-23T03:50:26.370294: step 4525, loss 0.441923, acc 0.78125\n",
      "2018-05-23T03:50:26.695427: step 4526, loss 0.434752, acc 0.78125\n",
      "2018-05-23T03:50:27.029533: step 4527, loss 0.261168, acc 0.875\n",
      "2018-05-23T03:50:27.363938: step 4528, loss 0.50159, acc 0.78125\n",
      "2018-05-23T03:50:27.689069: step 4529, loss 0.384723, acc 0.796875\n",
      "2018-05-23T03:50:28.015198: step 4530, loss 0.3833, acc 0.84375\n",
      "2018-05-23T03:50:28.342320: step 4531, loss 0.354878, acc 0.8125\n",
      "2018-05-23T03:50:28.667453: step 4532, loss 0.431988, acc 0.75\n",
      "2018-05-23T03:50:29.013525: step 4533, loss 0.36851, acc 0.84375\n",
      "2018-05-23T03:50:29.352617: step 4534, loss 0.35356, acc 0.84375\n",
      "2018-05-23T03:50:29.687721: step 4535, loss 0.384799, acc 0.765625\n",
      "2018-05-23T03:50:30.019816: step 4536, loss 0.448419, acc 0.796875\n",
      "2018-05-23T03:50:30.351928: step 4537, loss 0.396791, acc 0.828125\n",
      "2018-05-23T03:50:30.678056: step 4538, loss 0.302646, acc 0.890625\n",
      "2018-05-23T03:50:31.008171: step 4539, loss 0.442897, acc 0.78125\n",
      "2018-05-23T03:50:31.344273: step 4540, loss 0.348986, acc 0.859375\n",
      "2018-05-23T03:50:31.668405: step 4541, loss 0.454093, acc 0.828125\n",
      "2018-05-23T03:50:31.990545: step 4542, loss 0.42827, acc 0.78125\n",
      "2018-05-23T03:50:32.327643: step 4543, loss 0.512818, acc 0.765625\n",
      "2018-05-23T03:50:32.652771: step 4544, loss 0.354664, acc 0.84375\n",
      "2018-05-23T03:50:32.978898: step 4545, loss 0.349589, acc 0.890625\n",
      "2018-05-23T03:50:33.311012: step 4546, loss 0.406611, acc 0.875\n",
      "2018-05-23T03:50:33.638134: step 4547, loss 0.356743, acc 0.828125\n",
      "2018-05-23T03:50:33.969173: step 4548, loss 0.466357, acc 0.75\n",
      "2018-05-23T03:50:34.304277: step 4549, loss 0.476137, acc 0.84375\n",
      "2018-05-23T03:50:34.630405: step 4550, loss 0.352916, acc 0.796875\n",
      "2018-05-23T03:50:34.957527: step 4551, loss 0.434028, acc 0.765625\n",
      "2018-05-23T03:50:35.293629: step 4552, loss 0.408909, acc 0.859375\n",
      "2018-05-23T03:50:35.618758: step 4553, loss 0.433817, acc 0.78125\n",
      "2018-05-23T03:50:35.942891: step 4554, loss 0.291729, acc 0.90625\n",
      "2018-05-23T03:50:36.277998: step 4555, loss 0.387766, acc 0.8125\n",
      "2018-05-23T03:50:36.618085: step 4556, loss 0.41154, acc 0.8125\n",
      "2018-05-23T03:50:36.948202: step 4557, loss 0.28192, acc 0.921875\n",
      "2018-05-23T03:50:37.278319: step 4558, loss 0.451805, acc 0.765625\n",
      "2018-05-23T03:50:37.615417: step 4559, loss 0.464188, acc 0.796875\n",
      "2018-05-23T03:50:37.951116: step 4560, loss 0.592982, acc 0.75\n",
      "2018-05-23T03:50:38.335090: step 4561, loss 0.423075, acc 0.765625\n",
      "2018-05-23T03:50:38.705099: step 4562, loss 0.333017, acc 0.890625\n",
      "2018-05-23T03:50:39.039208: step 4563, loss 0.384445, acc 0.828125\n",
      "2018-05-23T03:50:39.374309: step 4564, loss 0.374664, acc 0.8125\n",
      "2018-05-23T03:50:39.720384: step 4565, loss 0.448751, acc 0.765625\n",
      "2018-05-23T03:50:40.054491: step 4566, loss 0.422247, acc 0.75\n",
      "2018-05-23T03:50:40.385605: step 4567, loss 0.467363, acc 0.765625\n",
      "2018-05-23T03:50:40.717715: step 4568, loss 0.449885, acc 0.78125\n",
      "2018-05-23T03:50:41.040853: step 4569, loss 0.331053, acc 0.84375\n",
      "2018-05-23T03:50:41.369973: step 4570, loss 0.524945, acc 0.75\n",
      "2018-05-23T03:50:41.704076: step 4571, loss 0.483437, acc 0.8125\n",
      "2018-05-23T03:50:42.037187: step 4572, loss 0.347212, acc 0.859375\n",
      "2018-05-23T03:50:42.375283: step 4573, loss 0.448879, acc 0.75\n",
      "2018-05-23T03:50:42.710386: step 4574, loss 0.361689, acc 0.828125\n",
      "2018-05-23T03:50:43.044491: step 4575, loss 0.521324, acc 0.765625\n",
      "2018-05-23T03:50:43.373611: step 4576, loss 0.441912, acc 0.78125\n",
      "2018-05-23T03:50:43.705763: step 4577, loss 0.506183, acc 0.71875\n",
      "2018-05-23T03:50:44.044854: step 4578, loss 0.309673, acc 0.875\n",
      "2018-05-23T03:50:44.375969: step 4579, loss 0.418189, acc 0.8125\n",
      "2018-05-23T03:50:44.706086: step 4580, loss 0.504965, acc 0.8125\n",
      "2018-05-23T03:50:45.040192: step 4581, loss 0.292714, acc 0.8125\n",
      "2018-05-23T03:50:45.372303: step 4582, loss 0.345971, acc 0.828125\n",
      "2018-05-23T03:50:45.706410: step 4583, loss 0.414945, acc 0.796875\n",
      "2018-05-23T03:50:46.073429: step 4584, loss 0.348362, acc 0.84375\n",
      "2018-05-23T03:50:46.403544: step 4585, loss 0.450684, acc 0.84375\n",
      "2018-05-23T03:50:46.737651: step 4586, loss 0.44989, acc 0.828125\n",
      "2018-05-23T03:50:47.068765: step 4587, loss 0.320703, acc 0.84375\n",
      "2018-05-23T03:50:47.401874: step 4588, loss 0.39766, acc 0.84375\n",
      "2018-05-23T03:50:47.740969: step 4589, loss 0.510939, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:50:48.073081: step 4590, loss 0.323773, acc 0.859375\n",
      "2018-05-23T03:50:48.400203: step 4591, loss 0.529848, acc 0.78125\n",
      "2018-05-23T03:50:48.724336: step 4592, loss 0.384268, acc 0.8125\n",
      "2018-05-23T03:50:49.059477: step 4593, loss 0.367231, acc 0.8125\n",
      "2018-05-23T03:50:49.394543: step 4594, loss 0.530935, acc 0.734375\n",
      "2018-05-23T03:50:49.718678: step 4595, loss 0.421028, acc 0.796875\n",
      "2018-05-23T03:50:50.055776: step 4596, loss 0.343598, acc 0.84375\n",
      "2018-05-23T03:50:50.387886: step 4597, loss 0.354073, acc 0.84375\n",
      "2018-05-23T03:50:50.710024: step 4598, loss 0.411474, acc 0.796875\n",
      "2018-05-23T03:50:51.046125: step 4599, loss 0.434185, acc 0.765625\n",
      "2018-05-23T03:50:51.372253: step 4600, loss 0.488675, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:50:55.725607: step 4600, loss 0.557181, acc 0.735962\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4600\n",
      "\n",
      "2018-05-23T03:50:56.959308: step 4601, loss 0.379951, acc 0.84375\n",
      "2018-05-23T03:50:57.363225: step 4602, loss 0.509566, acc 0.765625\n",
      "2018-05-23T03:50:57.711329: step 4603, loss 0.301183, acc 0.921875\n",
      "2018-05-23T03:50:58.077315: step 4604, loss 0.264403, acc 0.90625\n",
      "2018-05-23T03:50:58.403442: step 4605, loss 0.475737, acc 0.8125\n",
      "2018-05-23T03:50:58.731565: step 4606, loss 0.471046, acc 0.796875\n",
      "2018-05-23T03:50:59.072653: step 4607, loss 0.37734, acc 0.796875\n",
      "2018-05-23T03:50:59.400776: step 4608, loss 0.46224, acc 0.78125\n",
      "2018-05-23T03:50:59.727902: step 4609, loss 0.4261, acc 0.78125\n",
      "2018-05-23T03:51:00.059014: step 4610, loss 0.489148, acc 0.75\n",
      "2018-05-23T03:51:00.393121: step 4611, loss 0.257836, acc 0.875\n",
      "2018-05-23T03:51:00.714262: step 4612, loss 0.427934, acc 0.8125\n",
      "2018-05-23T03:51:01.058342: step 4613, loss 0.443688, acc 0.796875\n",
      "2018-05-23T03:51:01.390453: step 4614, loss 0.433401, acc 0.796875\n",
      "2018-05-23T03:51:01.719571: step 4615, loss 0.402637, acc 0.828125\n",
      "2018-05-23T03:51:02.050687: step 4616, loss 0.446708, acc 0.75\n",
      "2018-05-23T03:51:02.371826: step 4617, loss 0.398559, acc 0.828125\n",
      "2018-05-23T03:51:02.695961: step 4618, loss 0.429696, acc 0.8125\n",
      "2018-05-23T03:51:03.030065: step 4619, loss 0.33822, acc 0.8125\n",
      "2018-05-23T03:51:03.367164: step 4620, loss 0.451144, acc 0.796875\n",
      "2018-05-23T03:51:03.705259: step 4621, loss 0.468154, acc 0.765625\n",
      "2018-05-23T03:51:04.042358: step 4622, loss 0.383841, acc 0.78125\n",
      "2018-05-23T03:51:04.369482: step 4623, loss 0.34325, acc 0.875\n",
      "2018-05-23T03:51:04.695610: step 4624, loss 0.351992, acc 0.84375\n",
      "2018-05-23T03:51:05.022735: step 4625, loss 0.472261, acc 0.796875\n",
      "2018-05-23T03:51:05.347868: step 4626, loss 0.469496, acc 0.796875\n",
      "2018-05-23T03:51:05.677982: step 4627, loss 0.421077, acc 0.765625\n",
      "2018-05-23T03:51:06.000120: step 4628, loss 0.351636, acc 0.78125\n",
      "2018-05-23T03:51:06.328242: step 4629, loss 0.385031, acc 0.84375\n",
      "2018-05-23T03:51:06.658359: step 4630, loss 0.502548, acc 0.84375\n",
      "2018-05-23T03:51:06.987479: step 4631, loss 0.468761, acc 0.8125\n",
      "2018-05-23T03:51:07.315894: step 4632, loss 0.346529, acc 0.875\n",
      "2018-05-23T03:51:07.640026: step 4633, loss 0.353246, acc 0.8125\n",
      "2018-05-23T03:51:07.973135: step 4634, loss 0.414825, acc 0.84375\n",
      "2018-05-23T03:51:08.303251: step 4635, loss 0.354484, acc 0.84375\n",
      "2018-05-23T03:51:08.630376: step 4636, loss 0.359961, acc 0.8125\n",
      "2018-05-23T03:51:08.960494: step 4637, loss 0.455192, acc 0.796875\n",
      "2018-05-23T03:51:09.284626: step 4638, loss 0.348723, acc 0.890625\n",
      "2018-05-23T03:51:09.620727: step 4639, loss 0.385767, acc 0.875\n",
      "2018-05-23T03:51:09.955831: step 4640, loss 0.348851, acc 0.859375\n",
      "2018-05-23T03:51:10.286946: step 4641, loss 0.297731, acc 0.875\n",
      "2018-05-23T03:51:10.615067: step 4642, loss 0.359439, acc 0.796875\n",
      "2018-05-23T03:51:10.945186: step 4643, loss 0.463974, acc 0.765625\n",
      "2018-05-23T03:51:11.272310: step 4644, loss 0.494593, acc 0.734375\n",
      "2018-05-23T03:51:11.601430: step 4645, loss 0.374228, acc 0.84375\n",
      "2018-05-23T03:51:11.928554: step 4646, loss 0.4224, acc 0.796875\n",
      "2018-05-23T03:51:12.256677: step 4647, loss 0.480645, acc 0.875\n",
      "2018-05-23T03:51:12.586792: step 4648, loss 0.35102, acc 0.8125\n",
      "2018-05-23T03:51:12.913918: step 4649, loss 0.460386, acc 0.828125\n",
      "2018-05-23T03:51:13.245033: step 4650, loss 0.531197, acc 0.828125\n",
      "2018-05-23T03:51:13.576148: step 4651, loss 0.384545, acc 0.78125\n",
      "2018-05-23T03:51:13.912247: step 4652, loss 0.410493, acc 0.8125\n",
      "2018-05-23T03:51:14.254331: step 4653, loss 0.391246, acc 0.828125\n",
      "2018-05-23T03:51:14.606389: step 4654, loss 0.550642, acc 0.734375\n",
      "2018-05-23T03:51:14.969420: step 4655, loss 0.376904, acc 0.859375\n",
      "2018-05-23T03:51:15.321477: step 4656, loss 0.353929, acc 0.859375\n",
      "2018-05-23T03:51:15.696473: step 4657, loss 0.396538, acc 0.828125\n",
      "2018-05-23T03:51:16.037563: step 4658, loss 0.288319, acc 0.90625\n",
      "2018-05-23T03:51:16.364688: step 4659, loss 0.34898, acc 0.796875\n",
      "2018-05-23T03:51:16.758633: step 4660, loss 0.473811, acc 0.765625\n",
      "2018-05-23T03:51:17.084762: step 4661, loss 0.514493, acc 0.765625\n",
      "2018-05-23T03:51:17.411884: step 4662, loss 0.426817, acc 0.796875\n",
      "2018-05-23T03:51:17.772921: step 4663, loss 0.276418, acc 0.890625\n",
      "2018-05-23T03:51:18.101043: step 4664, loss 0.369268, acc 0.84375\n",
      "2018-05-23T03:51:18.432155: step 4665, loss 0.374609, acc 0.796875\n",
      "2018-05-23T03:51:18.767260: step 4666, loss 0.379978, acc 0.859375\n",
      "2018-05-23T03:51:19.097375: step 4667, loss 0.384515, acc 0.796875\n",
      "2018-05-23T03:51:19.429487: step 4668, loss 0.27525, acc 0.84375\n",
      "2018-05-23T03:51:19.763594: step 4669, loss 0.361636, acc 0.828125\n",
      "2018-05-23T03:51:20.089720: step 4670, loss 0.322469, acc 0.875\n",
      "2018-05-23T03:51:20.418840: step 4671, loss 0.335506, acc 0.90625\n",
      "2018-05-23T03:51:20.745965: step 4672, loss 0.429259, acc 0.84375\n",
      "2018-05-23T03:51:21.074087: step 4673, loss 0.314479, acc 0.875\n",
      "2018-05-23T03:51:21.396227: step 4674, loss 0.412116, acc 0.796875\n",
      "2018-05-23T03:51:21.721358: step 4675, loss 0.396386, acc 0.828125\n",
      "2018-05-23T03:51:22.053469: step 4676, loss 0.274828, acc 0.859375\n",
      "2018-05-23T03:51:22.377602: step 4677, loss 0.456121, acc 0.84375\n",
      "2018-05-23T03:51:22.703728: step 4678, loss 0.371545, acc 0.875\n",
      "2018-05-23T03:51:23.037835: step 4679, loss 0.436799, acc 0.859375\n",
      "2018-05-23T03:51:23.367951: step 4680, loss 0.331965, acc 0.859375\n",
      "2018-05-23T03:51:23.693082: step 4681, loss 0.550088, acc 0.71875\n",
      "2018-05-23T03:51:24.026189: step 4682, loss 0.476445, acc 0.796875\n",
      "2018-05-23T03:51:24.354314: step 4683, loss 0.459959, acc 0.734375\n",
      "2018-05-23T03:51:24.681438: step 4684, loss 0.418814, acc 0.796875\n",
      "2018-05-23T03:51:25.009561: step 4685, loss 0.31977, acc 0.890625\n",
      "2018-05-23T03:51:25.341672: step 4686, loss 0.25316, acc 0.890625\n",
      "2018-05-23T03:51:25.672786: step 4687, loss 0.475758, acc 0.796875\n",
      "2018-05-23T03:51:26.003900: step 4688, loss 0.375963, acc 0.78125\n",
      "2018-05-23T03:51:26.329030: step 4689, loss 0.457054, acc 0.796875\n",
      "2018-05-23T03:51:26.651168: step 4690, loss 0.37143, acc 0.796875\n",
      "2018-05-23T03:51:26.982284: step 4691, loss 0.353262, acc 0.8125\n",
      "2018-05-23T03:51:27.310404: step 4692, loss 0.248939, acc 0.921875\n",
      "2018-05-23T03:51:27.642517: step 4693, loss 0.411633, acc 0.828125\n",
      "2018-05-23T03:51:27.977620: step 4694, loss 0.389435, acc 0.828125\n",
      "2018-05-23T03:51:28.314719: step 4695, loss 0.39349, acc 0.828125\n",
      "2018-05-23T03:51:28.642839: step 4696, loss 0.586061, acc 0.71875\n",
      "2018-05-23T03:51:29.036787: step 4697, loss 0.314951, acc 0.875\n",
      "2018-05-23T03:51:29.358923: step 4698, loss 0.289269, acc 0.875\n",
      "2018-05-23T03:51:29.685052: step 4699, loss 0.494, acc 0.71875\n",
      "2018-05-23T03:51:30.016167: step 4700, loss 0.462846, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:51:34.374506: step 4700, loss 0.565879, acc 0.732962\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4700\n",
      "\n",
      "2018-05-23T03:51:35.693451: step 4701, loss 0.45538, acc 0.75\n",
      "2018-05-23T03:51:36.103357: step 4702, loss 0.486735, acc 0.71875\n",
      "2018-05-23T03:51:36.446436: step 4703, loss 0.450993, acc 0.828125\n",
      "2018-05-23T03:51:36.790517: step 4704, loss 0.416654, acc 0.796875\n",
      "2018-05-23T03:51:37.120633: step 4705, loss 0.39106, acc 0.78125\n",
      "2018-05-23T03:51:37.448757: step 4706, loss 0.386499, acc 0.796875\n",
      "2018-05-23T03:51:37.777877: step 4707, loss 0.407825, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:51:38.106994: step 4708, loss 0.662176, acc 0.796875\n",
      "2018-05-23T03:51:38.436148: step 4709, loss 0.40914, acc 0.828125\n",
      "2018-05-23T03:51:38.763239: step 4710, loss 0.275975, acc 0.84375\n",
      "2018-05-23T03:51:39.096347: step 4711, loss 0.370166, acc 0.8125\n",
      "2018-05-23T03:51:39.431453: step 4712, loss 0.432678, acc 0.828125\n",
      "2018-05-23T03:51:39.757579: step 4713, loss 0.373744, acc 0.859375\n",
      "2018-05-23T03:51:40.085700: step 4714, loss 0.458256, acc 0.78125\n",
      "2018-05-23T03:51:40.414820: step 4715, loss 0.375783, acc 0.828125\n",
      "2018-05-23T03:51:40.744937: step 4716, loss 0.428146, acc 0.796875\n",
      "2018-05-23T03:51:41.072064: step 4717, loss 0.331307, acc 0.859375\n",
      "2018-05-23T03:51:41.424121: step 4718, loss 0.429958, acc 0.796875\n",
      "2018-05-23T03:51:41.759224: step 4719, loss 0.314982, acc 0.859375\n",
      "2018-05-23T03:51:42.089341: step 4720, loss 0.549115, acc 0.78125\n",
      "2018-05-23T03:51:42.425442: step 4721, loss 0.422942, acc 0.75\n",
      "2018-05-23T03:51:42.750573: step 4722, loss 0.314021, acc 0.875\n",
      "2018-05-23T03:51:43.074707: step 4723, loss 0.378089, acc 0.8125\n",
      "2018-05-23T03:51:43.409810: step 4724, loss 0.393206, acc 0.8125\n",
      "2018-05-23T03:51:43.742917: step 4725, loss 0.258912, acc 0.90625\n",
      "2018-05-23T03:51:44.079017: step 4726, loss 0.335373, acc 0.828125\n",
      "2018-05-23T03:51:44.417115: step 4727, loss 0.384961, acc 0.890625\n",
      "2018-05-23T03:51:44.743240: step 4728, loss 0.36953, acc 0.84375\n",
      "2018-05-23T03:51:45.072360: step 4729, loss 0.37352, acc 0.84375\n",
      "2018-05-23T03:51:45.403474: step 4730, loss 0.303811, acc 0.890625\n",
      "2018-05-23T03:51:45.727608: step 4731, loss 0.400003, acc 0.828125\n",
      "2018-05-23T03:51:46.079665: step 4732, loss 0.410121, acc 0.78125\n",
      "2018-05-23T03:51:46.412774: step 4733, loss 0.445125, acc 0.75\n",
      "2018-05-23T03:51:46.737907: step 4734, loss 0.313314, acc 0.859375\n",
      "2018-05-23T03:51:47.061042: step 4735, loss 0.364153, acc 0.828125\n",
      "2018-05-23T03:51:47.389163: step 4736, loss 0.369928, acc 0.796875\n",
      "2018-05-23T03:51:47.718329: step 4737, loss 0.384075, acc 0.84375\n",
      "2018-05-23T03:51:48.041465: step 4738, loss 0.366318, acc 0.859375\n",
      "2018-05-23T03:51:48.380559: step 4739, loss 0.393238, acc 0.765625\n",
      "2018-05-23T03:51:48.708679: step 4740, loss 0.506649, acc 0.734375\n",
      "2018-05-23T03:51:49.032811: step 4741, loss 0.337531, acc 0.859375\n",
      "2018-05-23T03:51:49.359938: step 4742, loss 0.442551, acc 0.75\n",
      "2018-05-23T03:51:49.691050: step 4743, loss 0.487679, acc 0.796875\n",
      "2018-05-23T03:51:50.019172: step 4744, loss 0.323803, acc 0.84375\n",
      "2018-05-23T03:51:50.354279: step 4745, loss 0.312348, acc 0.796875\n",
      "2018-05-23T03:51:50.691375: step 4746, loss 0.332265, acc 0.875\n",
      "2018-05-23T03:51:51.016505: step 4747, loss 0.248959, acc 0.875\n",
      "2018-05-23T03:51:51.336648: step 4748, loss 0.416324, acc 0.78125\n",
      "2018-05-23T03:51:51.675741: step 4749, loss 0.368138, acc 0.8125\n",
      "2018-05-23T03:51:52.004860: step 4750, loss 0.435597, acc 0.765625\n",
      "2018-05-23T03:51:52.333981: step 4751, loss 0.443854, acc 0.8125\n",
      "2018-05-23T03:51:52.663101: step 4752, loss 0.426423, acc 0.765625\n",
      "2018-05-23T03:51:52.991222: step 4753, loss 0.381913, acc 0.859375\n",
      "2018-05-23T03:51:53.324332: step 4754, loss 0.40112, acc 0.84375\n",
      "2018-05-23T03:51:53.646469: step 4755, loss 0.363454, acc 0.859375\n",
      "2018-05-23T03:51:53.970603: step 4756, loss 0.312099, acc 0.875\n",
      "2018-05-23T03:51:54.296729: step 4757, loss 0.393631, acc 0.796875\n",
      "2018-05-23T03:51:54.625850: step 4758, loss 0.484526, acc 0.8125\n",
      "2018-05-23T03:51:54.958959: step 4759, loss 0.383379, acc 0.828125\n",
      "2018-05-23T03:51:55.285086: step 4760, loss 0.339179, acc 0.84375\n",
      "2018-05-23T03:51:55.614205: step 4761, loss 0.274754, acc 0.875\n",
      "2018-05-23T03:51:55.943324: step 4762, loss 0.448133, acc 0.796875\n",
      "2018-05-23T03:51:56.275436: step 4763, loss 0.376218, acc 0.84375\n",
      "2018-05-23T03:51:56.607548: step 4764, loss 0.439906, acc 0.765625\n",
      "2018-05-23T03:51:56.945643: step 4765, loss 0.492286, acc 0.75\n",
      "2018-05-23T03:51:57.276759: step 4766, loss 0.299108, acc 0.84375\n",
      "2018-05-23T03:51:57.605877: step 4767, loss 0.595458, acc 0.78125\n",
      "2018-05-23T03:51:57.935993: step 4768, loss 0.401609, acc 0.8125\n",
      "2018-05-23T03:51:58.268105: step 4769, loss 0.46078, acc 0.75\n",
      "2018-05-23T03:51:58.596227: step 4770, loss 0.322458, acc 0.8125\n",
      "2018-05-23T03:51:58.964242: step 4771, loss 0.361942, acc 0.8125\n",
      "2018-05-23T03:51:59.289373: step 4772, loss 0.353854, acc 0.84375\n",
      "2018-05-23T03:51:59.613506: step 4773, loss 0.393502, acc 0.84375\n",
      "2018-05-23T03:51:59.944620: step 4774, loss 0.425738, acc 0.765625\n",
      "2018-05-23T03:52:00.289697: step 4775, loss 0.369832, acc 0.796875\n",
      "2018-05-23T03:52:00.671675: step 4776, loss 0.583534, acc 0.703125\n",
      "2018-05-23T03:52:01.036701: step 4777, loss 0.364969, acc 0.84375\n",
      "2018-05-23T03:52:01.381775: step 4778, loss 0.455731, acc 0.828125\n",
      "2018-05-23T03:52:01.771732: step 4779, loss 0.341221, acc 0.828125\n",
      "2018-05-23T03:52:02.125788: step 4780, loss 0.363913, acc 0.84375\n",
      "2018-05-23T03:52:02.450915: step 4781, loss 0.422851, acc 0.8125\n",
      "2018-05-23T03:52:02.808957: step 4782, loss 0.406822, acc 0.796875\n",
      "2018-05-23T03:52:03.154036: step 4783, loss 0.450216, acc 0.828125\n",
      "2018-05-23T03:52:03.502103: step 4784, loss 0.381916, acc 0.828125\n",
      "2018-05-23T03:52:03.830225: step 4785, loss 0.51729, acc 0.703125\n",
      "2018-05-23T03:52:04.166326: step 4786, loss 0.308914, acc 0.859375\n",
      "2018-05-23T03:52:04.492454: step 4787, loss 0.383048, acc 0.875\n",
      "2018-05-23T03:52:04.823569: step 4788, loss 0.491253, acc 0.765625\n",
      "2018-05-23T03:52:05.161666: step 4789, loss 0.323412, acc 0.84375\n",
      "2018-05-23T03:52:05.488791: step 4790, loss 0.362405, acc 0.828125\n",
      "2018-05-23T03:52:05.818905: step 4791, loss 0.378461, acc 0.84375\n",
      "2018-05-23T03:52:06.157004: step 4792, loss 0.371898, acc 0.828125\n",
      "2018-05-23T03:52:06.486121: step 4793, loss 0.361305, acc 0.875\n",
      "2018-05-23T03:52:06.817235: step 4794, loss 0.386605, acc 0.859375\n",
      "2018-05-23T03:52:07.141367: step 4795, loss 0.351764, acc 0.875\n",
      "2018-05-23T03:52:07.469490: step 4796, loss 0.374545, acc 0.8125\n",
      "2018-05-23T03:52:07.798612: step 4797, loss 0.436478, acc 0.796875\n",
      "2018-05-23T03:52:08.129728: step 4798, loss 0.34213, acc 0.84375\n",
      "2018-05-23T03:52:08.478791: step 4799, loss 0.362865, acc 0.84375\n",
      "2018-05-23T03:52:08.801925: step 4800, loss 0.412778, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:52:13.149295: step 4800, loss 0.559435, acc 0.739677\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4800\n",
      "\n",
      "2018-05-23T03:52:14.349725: step 4801, loss 0.380969, acc 0.828125\n",
      "2018-05-23T03:52:14.857366: step 4802, loss 0.387851, acc 0.875\n",
      "2018-05-23T03:52:15.212416: step 4803, loss 0.465972, acc 0.765625\n",
      "2018-05-23T03:52:15.569461: step 4804, loss 0.409486, acc 0.84375\n",
      "2018-05-23T03:52:15.900575: step 4805, loss 0.358097, acc 0.859375\n",
      "2018-05-23T03:52:16.249642: step 4806, loss 0.605853, acc 0.703125\n",
      "2018-05-23T03:52:16.588736: step 4807, loss 0.287983, acc 0.875\n",
      "2018-05-23T03:52:16.917854: step 4808, loss 0.271607, acc 0.875\n",
      "2018-05-23T03:52:17.262930: step 4809, loss 0.496539, acc 0.828125\n",
      "2018-05-23T03:52:17.596039: step 4810, loss 0.595385, acc 0.6875\n",
      "2018-05-23T03:52:17.925162: step 4811, loss 0.393676, acc 0.796875\n",
      "2018-05-23T03:52:18.250291: step 4812, loss 0.370825, acc 0.765625\n",
      "2018-05-23T03:52:18.587390: step 4813, loss 0.449943, acc 0.78125\n",
      "2018-05-23T03:52:18.917504: step 4814, loss 0.386088, acc 0.796875\n",
      "2018-05-23T03:52:19.245627: step 4815, loss 0.290587, acc 0.859375\n",
      "2018-05-23T03:52:19.582728: step 4816, loss 0.434103, acc 0.796875\n",
      "2018-05-23T03:52:19.905862: step 4817, loss 0.536771, acc 0.796875\n",
      "2018-05-23T03:52:20.238971: step 4818, loss 0.413369, acc 0.796875\n",
      "2018-05-23T03:52:20.575071: step 4819, loss 0.406147, acc 0.765625\n",
      "2018-05-23T03:52:20.917156: step 4820, loss 0.325906, acc 0.859375\n",
      "2018-05-23T03:52:21.246274: step 4821, loss 0.330298, acc 0.890625\n",
      "2018-05-23T03:52:21.573400: step 4822, loss 0.336443, acc 0.828125\n",
      "2018-05-23T03:52:21.896535: step 4823, loss 0.407845, acc 0.78125\n",
      "2018-05-23T03:52:22.227650: step 4824, loss 0.383192, acc 0.8125\n",
      "2018-05-23T03:52:22.554774: step 4825, loss 0.335218, acc 0.84375\n",
      "2018-05-23T03:52:22.882896: step 4826, loss 0.444246, acc 0.78125\n",
      "2018-05-23T03:52:23.217003: step 4827, loss 0.420275, acc 0.8125\n",
      "2018-05-23T03:52:23.555101: step 4828, loss 0.447136, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:52:23.884217: step 4829, loss 0.403096, acc 0.796875\n",
      "2018-05-23T03:52:24.210347: step 4830, loss 0.442524, acc 0.796875\n",
      "2018-05-23T03:52:24.540461: step 4831, loss 0.376605, acc 0.796875\n",
      "2018-05-23T03:52:24.876562: step 4832, loss 0.358255, acc 0.84375\n",
      "2018-05-23T03:52:25.199699: step 4833, loss 0.365336, acc 0.796875\n",
      "2018-05-23T03:52:25.566716: step 4834, loss 0.356768, acc 0.828125\n",
      "2018-05-23T03:52:25.904815: step 4835, loss 0.397609, acc 0.84375\n",
      "2018-05-23T03:52:26.245899: step 4836, loss 0.335403, acc 0.859375\n",
      "2018-05-23T03:52:26.588984: step 4837, loss 0.305621, acc 0.875\n",
      "2018-05-23T03:52:26.923088: step 4838, loss 0.507734, acc 0.75\n",
      "2018-05-23T03:52:27.252208: step 4839, loss 0.448804, acc 0.828125\n",
      "2018-05-23T03:52:27.579333: step 4840, loss 0.343462, acc 0.84375\n",
      "2018-05-23T03:52:27.904465: step 4841, loss 0.460698, acc 0.828125\n",
      "2018-05-23T03:52:28.224606: step 4842, loss 0.39577, acc 0.84375\n",
      "2018-05-23T03:52:28.556718: step 4843, loss 0.372927, acc 0.8125\n",
      "2018-05-23T03:52:28.895811: step 4844, loss 0.427081, acc 0.796875\n",
      "2018-05-23T03:52:29.223935: step 4845, loss 0.368165, acc 0.796875\n",
      "2018-05-23T03:52:29.550061: step 4846, loss 0.299961, acc 0.875\n",
      "2018-05-23T03:52:29.879180: step 4847, loss 0.377446, acc 0.890625\n",
      "2018-05-23T03:52:30.208300: step 4848, loss 0.384444, acc 0.828125\n",
      "2018-05-23T03:52:30.537756: step 4849, loss 0.438163, acc 0.765625\n",
      "2018-05-23T03:52:30.866875: step 4850, loss 0.267727, acc 0.890625\n",
      "2018-05-23T03:52:31.205969: step 4851, loss 0.390655, acc 0.796875\n",
      "2018-05-23T03:52:31.539076: step 4852, loss 0.305651, acc 0.90625\n",
      "2018-05-23T03:52:31.865203: step 4853, loss 0.247432, acc 0.90625\n",
      "2018-05-23T03:52:32.192329: step 4854, loss 0.337774, acc 0.875\n",
      "2018-05-23T03:52:32.529426: step 4855, loss 0.271137, acc 0.875\n",
      "2018-05-23T03:52:32.855556: step 4856, loss 0.279119, acc 0.859375\n",
      "2018-05-23T03:52:33.188663: step 4857, loss 0.292975, acc 0.90625\n",
      "2018-05-23T03:52:33.510801: step 4858, loss 0.447258, acc 0.8125\n",
      "2018-05-23T03:52:33.840918: step 4859, loss 0.506243, acc 0.75\n",
      "2018-05-23T03:52:34.181010: step 4860, loss 0.486694, acc 0.8125\n",
      "2018-05-23T03:52:34.509130: step 4861, loss 0.446165, acc 0.796875\n",
      "2018-05-23T03:52:34.836255: step 4862, loss 0.386842, acc 0.875\n",
      "2018-05-23T03:52:35.166373: step 4863, loss 0.329086, acc 0.890625\n",
      "2018-05-23T03:52:35.490505: step 4864, loss 0.339855, acc 0.859375\n",
      "2018-05-23T03:52:35.816634: step 4865, loss 0.489715, acc 0.78125\n",
      "2018-05-23T03:52:36.145754: step 4866, loss 0.547127, acc 0.71875\n",
      "2018-05-23T03:52:36.471879: step 4867, loss 0.420263, acc 0.796875\n",
      "2018-05-23T03:52:36.800002: step 4868, loss 0.391392, acc 0.859375\n",
      "2018-05-23T03:52:37.130119: step 4869, loss 0.455705, acc 0.78125\n",
      "2018-05-23T03:52:37.454251: step 4870, loss 0.402711, acc 0.78125\n",
      "2018-05-23T03:52:37.782375: step 4871, loss 0.339882, acc 0.890625\n",
      "2018-05-23T03:52:38.111494: step 4872, loss 0.401456, acc 0.84375\n",
      "2018-05-23T03:52:38.439617: step 4873, loss 0.457011, acc 0.75\n",
      "2018-05-23T03:52:38.764748: step 4874, loss 0.400992, acc 0.8125\n",
      "2018-05-23T03:52:39.090875: step 4875, loss 0.433887, acc 0.796875\n",
      "2018-05-23T03:52:39.423983: step 4876, loss 0.27002, acc 0.84375\n",
      "2018-05-23T03:52:39.750110: step 4877, loss 0.387308, acc 0.84375\n",
      "2018-05-23T03:52:40.077235: step 4878, loss 0.272801, acc 0.890625\n",
      "2018-05-23T03:52:40.413337: step 4879, loss 0.377187, acc 0.828125\n",
      "2018-05-23T03:52:40.736506: step 4880, loss 0.349346, acc 0.859375\n",
      "2018-05-23T03:52:41.062598: step 4881, loss 0.33841, acc 0.859375\n",
      "2018-05-23T03:52:41.394711: step 4882, loss 0.399948, acc 0.796875\n",
      "2018-05-23T03:52:41.717845: step 4883, loss 0.449201, acc 0.765625\n",
      "2018-05-23T03:52:42.050955: step 4884, loss 0.307339, acc 0.84375\n",
      "2018-05-23T03:52:42.387056: step 4885, loss 0.304261, acc 0.90625\n",
      "2018-05-23T03:52:42.719169: step 4886, loss 0.410637, acc 0.765625\n",
      "2018-05-23T03:52:43.047291: step 4887, loss 0.413019, acc 0.8125\n",
      "2018-05-23T03:52:43.378403: step 4888, loss 0.456163, acc 0.796875\n",
      "2018-05-23T03:52:43.706526: step 4889, loss 0.351619, acc 0.875\n",
      "2018-05-23T03:52:44.075051: step 4890, loss 0.377063, acc 0.84375\n",
      "2018-05-23T03:52:44.412149: step 4891, loss 0.421411, acc 0.734375\n",
      "2018-05-23T03:52:44.747253: step 4892, loss 0.378039, acc 0.828125\n",
      "2018-05-23T03:52:45.072383: step 4893, loss 0.476854, acc 0.765625\n",
      "2018-05-23T03:52:45.395517: step 4894, loss 0.389309, acc 0.828125\n",
      "2018-05-23T03:52:45.720648: step 4895, loss 0.440434, acc 0.828125\n",
      "2018-05-23T03:52:46.054754: step 4896, loss 0.443006, acc 0.78125\n",
      "2018-05-23T03:52:46.399832: step 4897, loss 0.42222, acc 0.796875\n",
      "2018-05-23T03:52:46.728952: step 4898, loss 0.353969, acc 0.84375\n",
      "2018-05-23T03:52:47.060104: step 4899, loss 0.386058, acc 0.78125\n",
      "2018-05-23T03:52:47.386191: step 4900, loss 0.535327, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:52:51.755503: step 4900, loss 0.557272, acc 0.736248\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-4900\n",
      "\n",
      "2018-05-23T03:52:53.204720: step 4901, loss 0.336868, acc 0.875\n",
      "2018-05-23T03:52:53.555780: step 4902, loss 0.319696, acc 0.875\n",
      "2018-05-23T03:52:53.901853: step 4903, loss 0.358251, acc 0.84375\n",
      "2018-05-23T03:52:54.259896: step 4904, loss 0.404109, acc 0.78125\n",
      "2018-05-23T03:52:54.699720: step 4905, loss 0.401525, acc 0.78125\n",
      "2018-05-23T03:52:55.049783: step 4906, loss 0.477516, acc 0.796875\n",
      "2018-05-23T03:52:55.379901: step 4907, loss 0.343322, acc 0.875\n",
      "2018-05-23T03:52:55.722981: step 4908, loss 0.358147, acc 0.8125\n",
      "2018-05-23T03:52:56.107952: step 4909, loss 0.327876, acc 0.84375\n",
      "2018-05-23T03:52:56.454027: step 4910, loss 0.312077, acc 0.8125\n",
      "2018-05-23T03:52:56.782150: step 4911, loss 0.32505, acc 0.921875\n",
      "2018-05-23T03:52:57.119246: step 4912, loss 0.453939, acc 0.703125\n",
      "2018-05-23T03:52:57.469309: step 4913, loss 0.439404, acc 0.765625\n",
      "2018-05-23T03:52:57.831341: step 4914, loss 0.372491, acc 0.828125\n",
      "2018-05-23T03:52:58.182404: step 4915, loss 0.500072, acc 0.71875\n",
      "2018-05-23T03:52:58.549419: step 4916, loss 0.27195, acc 0.90625\n",
      "2018-05-23T03:52:58.928406: step 4917, loss 0.34811, acc 0.84375\n",
      "2018-05-23T03:52:59.386182: step 4918, loss 0.446234, acc 0.765625\n",
      "2018-05-23T03:53:00.210975: step 4919, loss 0.425373, acc 0.8125\n",
      "2018-05-23T03:53:00.829320: step 4920, loss 0.34722, acc 0.890625\n",
      "2018-05-23T03:53:01.330979: step 4921, loss 0.465687, acc 0.765625\n",
      "2018-05-23T03:53:01.773794: step 4922, loss 0.392532, acc 0.859375\n",
      "2018-05-23T03:53:02.209628: step 4923, loss 0.416629, acc 0.796875\n",
      "2018-05-23T03:53:02.571659: step 4924, loss 0.387379, acc 0.796875\n",
      "2018-05-23T03:53:02.826977: step 4925, loss 0.468123, acc 0.764706\n",
      "2018-05-23T03:53:03.192000: step 4926, loss 0.237434, acc 0.921875\n",
      "2018-05-23T03:53:03.556027: step 4927, loss 0.333817, acc 0.84375\n",
      "2018-05-23T03:53:03.912075: step 4928, loss 0.242222, acc 0.90625\n",
      "2018-05-23T03:53:04.297044: step 4929, loss 0.301841, acc 0.890625\n",
      "2018-05-23T03:53:04.677027: step 4930, loss 0.243756, acc 0.90625\n",
      "2018-05-23T03:53:05.027091: step 4931, loss 0.269079, acc 0.921875\n",
      "2018-05-23T03:53:05.388124: step 4932, loss 0.260756, acc 0.890625\n",
      "2018-05-23T03:53:05.746169: step 4933, loss 0.321933, acc 0.890625\n",
      "2018-05-23T03:53:06.099223: step 4934, loss 0.328122, acc 0.828125\n",
      "2018-05-23T03:53:06.507131: step 4935, loss 0.232648, acc 0.96875\n",
      "2018-05-23T03:53:06.878139: step 4936, loss 0.24627, acc 0.90625\n",
      "2018-05-23T03:53:07.253136: step 4937, loss 0.280518, acc 0.859375\n",
      "2018-05-23T03:53:07.598212: step 4938, loss 0.293582, acc 0.84375\n",
      "2018-05-23T03:53:07.955260: step 4939, loss 0.26438, acc 0.859375\n",
      "2018-05-23T03:53:08.311306: step 4940, loss 0.242887, acc 0.90625\n",
      "2018-05-23T03:53:08.663363: step 4941, loss 0.324741, acc 0.859375\n",
      "2018-05-23T03:53:09.006448: step 4942, loss 0.343238, acc 0.859375\n",
      "2018-05-23T03:53:09.387426: step 4943, loss 0.423491, acc 0.828125\n",
      "2018-05-23T03:53:09.738489: step 4944, loss 0.361145, acc 0.859375\n",
      "2018-05-23T03:53:10.077581: step 4945, loss 0.380574, acc 0.84375\n",
      "2018-05-23T03:53:10.412684: step 4946, loss 0.319011, acc 0.859375\n",
      "2018-05-23T03:53:10.733861: step 4947, loss 0.368467, acc 0.84375\n",
      "2018-05-23T03:53:11.053007: step 4948, loss 0.253371, acc 0.9375\n",
      "2018-05-23T03:53:11.372153: step 4949, loss 0.303636, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T03:53:11.688307: step 4950, loss 0.293949, acc 0.875\n",
      "2018-05-23T03:53:12.003464: step 4951, loss 0.386322, acc 0.859375\n",
      "2018-05-23T03:53:12.321616: step 4952, loss 0.312477, acc 0.890625\n",
      "2018-05-23T03:53:12.644837: step 4953, loss 0.329407, acc 0.84375\n",
      "2018-05-23T03:53:12.958997: step 4954, loss 0.260984, acc 0.890625\n",
      "2018-05-23T03:53:13.278146: step 4955, loss 0.351536, acc 0.796875\n",
      "2018-05-23T03:53:13.602276: step 4956, loss 0.294412, acc 0.890625\n",
      "2018-05-23T03:53:13.940371: step 4957, loss 0.310089, acc 0.875\n",
      "2018-05-23T03:53:14.254531: step 4958, loss 0.383662, acc 0.90625\n",
      "2018-05-23T03:53:14.573677: step 4959, loss 0.294386, acc 0.890625\n",
      "2018-05-23T03:53:14.885842: step 4960, loss 0.2761, acc 0.875\n",
      "2018-05-23T03:53:15.204989: step 4961, loss 0.339124, acc 0.859375\n",
      "2018-05-23T03:53:15.523137: step 4962, loss 0.380805, acc 0.8125\n",
      "2018-05-23T03:53:15.842285: step 4963, loss 0.41021, acc 0.84375\n",
      "2018-05-23T03:53:16.178385: step 4964, loss 0.250629, acc 0.890625\n",
      "2018-05-23T03:53:16.499526: step 4965, loss 0.244809, acc 0.90625\n",
      "2018-05-23T03:53:16.846597: step 4966, loss 0.317762, acc 0.90625\n",
      "2018-05-23T03:53:17.168735: step 4967, loss 0.30784, acc 0.890625\n",
      "2018-05-23T03:53:17.484889: step 4968, loss 0.357013, acc 0.828125\n",
      "2018-05-23T03:53:17.808027: step 4969, loss 0.255036, acc 0.859375\n",
      "2018-05-23T03:53:18.123183: step 4970, loss 0.395784, acc 0.84375\n",
      "2018-05-23T03:53:18.433352: step 4971, loss 0.325575, acc 0.84375\n",
      "2018-05-23T03:53:18.757486: step 4972, loss 0.216123, acc 0.90625\n",
      "2018-05-23T03:53:19.073640: step 4973, loss 0.419504, acc 0.765625\n",
      "2018-05-23T03:53:19.391788: step 4974, loss 0.327877, acc 0.90625\n",
      "2018-05-23T03:53:19.715922: step 4975, loss 0.205262, acc 0.921875\n",
      "2018-05-23T03:53:20.031078: step 4976, loss 0.458234, acc 0.84375\n",
      "2018-05-23T03:53:20.342245: step 4977, loss 0.32904, acc 0.859375\n",
      "2018-05-23T03:53:20.722229: step 4978, loss 0.311821, acc 0.875\n",
      "2018-05-23T03:53:21.066311: step 4979, loss 0.272131, acc 0.90625\n",
      "2018-05-23T03:53:21.390441: step 4980, loss 0.275121, acc 0.890625\n",
      "2018-05-23T03:53:21.718563: step 4981, loss 0.258744, acc 0.921875\n",
      "2018-05-23T03:53:22.036712: step 4982, loss 0.257627, acc 0.875\n",
      "2018-05-23T03:53:22.353865: step 4983, loss 0.333761, acc 0.828125\n",
      "2018-05-23T03:53:22.677999: step 4984, loss 0.209167, acc 0.90625\n",
      "2018-05-23T03:53:22.997145: step 4985, loss 0.244663, acc 0.890625\n",
      "2018-05-23T03:53:23.318285: step 4986, loss 0.274969, acc 0.875\n",
      "2018-05-23T03:53:23.635435: step 4987, loss 0.277165, acc 0.890625\n",
      "2018-05-23T03:53:23.958570: step 4988, loss 0.294044, acc 0.875\n",
      "2018-05-23T03:53:24.272732: step 4989, loss 0.263883, acc 0.875\n",
      "2018-05-23T03:53:24.645733: step 4990, loss 0.301943, acc 0.875\n",
      "2018-05-23T03:53:26.199204: step 4991, loss 0.259386, acc 0.875\n",
      "2018-05-23T03:53:26.689891: step 4992, loss 0.302229, acc 0.921875\n",
      "2018-05-23T03:53:27.255378: step 4993, loss 0.385496, acc 0.8125\n",
      "2018-05-23T03:53:27.796930: step 4994, loss 0.261177, acc 0.90625\n",
      "2018-05-23T03:53:28.295595: step 4995, loss 0.22737, acc 0.90625\n",
      "2018-05-23T03:53:28.829170: step 4996, loss 0.408877, acc 0.765625\n",
      "2018-05-23T03:53:29.322846: step 4997, loss 0.315658, acc 0.921875\n",
      "2018-05-23T03:53:29.801566: step 4998, loss 0.300985, acc 0.875\n",
      "2018-05-23T03:53:30.320178: step 4999, loss 0.433805, acc 0.8125\n",
      "2018-05-23T03:53:30.807874: step 5000, loss 0.339767, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T03:53:36.623316: step 5000, loss 0.595482, acc 0.734248\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-5000\n",
      "\n",
      "2018-05-23T03:53:38.384605: step 5001, loss 0.235686, acc 0.921875\n",
      "2018-05-23T03:53:38.871304: step 5002, loss 0.367401, acc 0.765625\n",
      "2018-05-23T03:53:39.345035: step 5003, loss 0.356373, acc 0.875\n",
      "2018-05-23T03:53:39.863648: step 5004, loss 0.310739, acc 0.9375\n",
      "2018-05-23T03:53:40.331397: step 5005, loss 0.289327, acc 0.859375\n",
      "2018-05-23T03:53:40.813109: step 5006, loss 0.259774, acc 0.875\n",
      "2018-05-23T13:01:42.678927: step 5767, loss 0.55252, acc 0.796875\n",
      "2018-05-23T13:01:43.253389: step 5768, loss 0.365953, acc 0.875\n",
      "2018-05-23T13:01:43.889688: step 5769, loss 0.310152, acc 0.84375\n",
      "2018-05-23T13:01:44.602780: step 5770, loss 0.243656, acc 0.890625\n",
      "2018-05-23T13:01:45.258027: step 5771, loss 0.276442, acc 0.90625\n",
      "2018-05-23T13:01:45.856426: step 5772, loss 0.402678, acc 0.8125\n",
      "2018-05-23T13:01:46.457817: step 5773, loss 0.327993, acc 0.875\n",
      "2018-05-23T13:01:47.105085: step 5774, loss 0.239022, acc 0.90625\n",
      "2018-05-23T13:01:47.632675: step 5775, loss 0.323877, acc 0.890625\n",
      "2018-05-23T13:01:48.109399: step 5776, loss 0.464499, acc 0.84375\n",
      "2018-05-23T13:01:48.504343: step 5777, loss 0.452109, acc 0.78125\n",
      "2018-05-23T13:01:48.936186: step 5778, loss 0.351134, acc 0.859375\n",
      "2018-05-23T13:01:49.381994: step 5779, loss 0.328773, acc 0.859375\n",
      "2018-05-23T13:01:49.949477: step 5780, loss 0.317712, acc 0.875\n",
      "2018-05-23T13:01:50.447145: step 5781, loss 0.369632, acc 0.8125\n",
      "2018-05-23T13:01:50.931849: step 5782, loss 0.26196, acc 0.90625\n",
      "2018-05-23T13:01:51.329783: step 5783, loss 0.472137, acc 0.765625\n",
      "2018-05-23T13:01:51.768610: step 5784, loss 0.337945, acc 0.8125\n",
      "2018-05-23T13:01:52.295201: step 5785, loss 0.243672, acc 0.890625\n",
      "2018-05-23T13:01:52.838747: step 5786, loss 0.451097, acc 0.796875\n",
      "2018-05-23T13:01:53.325444: step 5787, loss 0.273056, acc 0.875\n",
      "2018-05-23T13:01:53.818127: step 5788, loss 0.431028, acc 0.828125\n",
      "2018-05-23T13:01:54.428495: step 5789, loss 0.347243, acc 0.828125\n",
      "2018-05-23T13:01:54.908211: step 5790, loss 0.380916, acc 0.78125\n",
      "2018-05-23T13:01:55.421837: step 5791, loss 0.401368, acc 0.84375\n",
      "2018-05-23T13:01:55.857670: step 5792, loss 0.400643, acc 0.828125\n",
      "2018-05-23T13:01:56.289514: step 5793, loss 0.265498, acc 0.890625\n",
      "2018-05-23T13:01:56.802143: step 5794, loss 0.353348, acc 0.859375\n",
      "2018-05-23T13:01:57.355663: step 5795, loss 0.438053, acc 0.84375\n",
      "2018-05-23T13:01:57.758586: step 5796, loss 0.259461, acc 0.890625\n",
      "2018-05-23T13:01:58.232318: step 5797, loss 0.415106, acc 0.78125\n",
      "2018-05-23T13:01:58.751928: step 5798, loss 0.238072, acc 0.890625\n",
      "2018-05-23T13:01:59.203719: step 5799, loss 0.483309, acc 0.765625\n",
      "2018-05-23T13:01:59.785163: step 5800, loss 0.445763, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:02:05.424078: step 5800, loss 0.637323, acc 0.723675\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-5800\n",
      "\n",
      "2018-05-23T13:02:07.359901: step 5801, loss 0.381739, acc 0.859375\n",
      "2018-05-23T13:02:07.815679: step 5802, loss 0.30738, acc 0.875\n",
      "2018-05-23T13:02:08.204642: step 5803, loss 0.33346, acc 0.84375\n",
      "2018-05-23T13:02:08.554704: step 5804, loss 0.372569, acc 0.875\n",
      "2018-05-23T13:02:08.903770: step 5805, loss 0.400706, acc 0.8125\n",
      "2018-05-23T13:02:09.315668: step 5806, loss 0.35162, acc 0.8125\n",
      "2018-05-23T13:02:09.770451: step 5807, loss 0.312239, acc 0.84375\n",
      "2018-05-23T13:02:10.217256: step 5808, loss 0.268625, acc 0.875\n",
      "2018-05-23T13:02:10.732878: step 5809, loss 0.227056, acc 0.890625\n",
      "2018-05-23T13:02:11.290386: step 5810, loss 0.457334, acc 0.765625\n",
      "2018-05-23T13:02:11.796032: step 5811, loss 0.340821, acc 0.90625\n",
      "2018-05-23T13:02:12.245828: step 5812, loss 0.383541, acc 0.859375\n",
      "2018-05-23T13:02:12.609855: step 5813, loss 0.394055, acc 0.828125\n",
      "2018-05-23T13:02:12.981859: step 5814, loss 0.361992, acc 0.828125\n",
      "2018-05-23T13:02:13.314970: step 5815, loss 0.263433, acc 0.890625\n",
      "2018-05-23T13:02:13.651069: step 5816, loss 0.297674, acc 0.875\n",
      "2018-05-23T13:02:14.011107: step 5817, loss 0.392285, acc 0.84375\n",
      "2018-05-23T13:02:14.341223: step 5818, loss 0.274839, acc 0.875\n",
      "2018-05-23T13:02:14.678321: step 5819, loss 0.386481, acc 0.78125\n",
      "2018-05-23T13:02:15.002454: step 5820, loss 0.350114, acc 0.765625\n",
      "2018-05-23T13:02:15.334565: step 5821, loss 0.336474, acc 0.84375\n",
      "2018-05-23T13:02:15.662688: step 5822, loss 0.462344, acc 0.78125\n",
      "2018-05-23T13:02:15.989813: step 5823, loss 0.285275, acc 0.828125\n",
      "2018-05-23T13:02:16.315940: step 5824, loss 0.233448, acc 0.890625\n",
      "2018-05-23T13:02:16.646060: step 5825, loss 0.280884, acc 0.890625\n",
      "2018-05-23T13:02:16.967198: step 5826, loss 0.498769, acc 0.765625\n",
      "2018-05-23T13:02:17.305294: step 5827, loss 0.375638, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:02:17.658349: step 5828, loss 0.47258, acc 0.8125\n",
      "2018-05-23T13:02:17.976499: step 5829, loss 0.335041, acc 0.828125\n",
      "2018-05-23T13:02:18.332546: step 5830, loss 0.388161, acc 0.84375\n",
      "2018-05-23T13:02:18.653687: step 5831, loss 0.424578, acc 0.8125\n",
      "2018-05-23T13:02:18.968843: step 5832, loss 0.365243, acc 0.828125\n",
      "2018-05-23T13:02:19.291979: step 5833, loss 0.258138, acc 0.875\n",
      "2018-05-23T13:02:19.605141: step 5834, loss 0.326929, acc 0.875\n",
      "2018-05-23T13:02:19.921295: step 5835, loss 0.510097, acc 0.75\n",
      "2018-05-23T13:02:20.258394: step 5836, loss 0.293498, acc 0.796875\n",
      "2018-05-23T13:02:20.585519: step 5837, loss 0.288701, acc 0.890625\n",
      "2018-05-23T13:02:20.933588: step 5838, loss 0.271145, acc 0.875\n",
      "2018-05-23T13:02:21.350472: step 5839, loss 0.236444, acc 0.9375\n",
      "2018-05-23T13:02:21.761373: step 5840, loss 0.470891, acc 0.796875\n",
      "2018-05-23T13:02:22.143355: step 5841, loss 0.517272, acc 0.78125\n",
      "2018-05-23T13:02:22.541288: step 5842, loss 0.267085, acc 0.859375\n",
      "2018-05-23T13:02:22.863426: step 5843, loss 0.316051, acc 0.84375\n",
      "2018-05-23T13:02:23.272331: step 5844, loss 0.382362, acc 0.828125\n",
      "2018-05-23T13:02:23.681237: step 5845, loss 0.350004, acc 0.84375\n",
      "2018-05-23T13:02:24.021326: step 5846, loss 0.281151, acc 0.890625\n",
      "2018-05-23T13:02:24.414276: step 5847, loss 0.262376, acc 0.890625\n",
      "2018-05-23T13:02:24.775311: step 5848, loss 0.256777, acc 0.890625\n",
      "2018-05-23T13:02:25.091464: step 5849, loss 0.385673, acc 0.8125\n",
      "2018-05-23T13:02:25.400637: step 5850, loss 0.290127, acc 0.890625\n",
      "2018-05-23T13:02:25.717788: step 5851, loss 0.328802, acc 0.84375\n",
      "2018-05-23T13:02:26.035937: step 5852, loss 0.322572, acc 0.84375\n",
      "2018-05-23T13:02:26.352092: step 5853, loss 0.424055, acc 0.859375\n",
      "2018-05-23T13:02:26.682208: step 5854, loss 0.359001, acc 0.8125\n",
      "2018-05-23T13:02:27.078150: step 5855, loss 0.423968, acc 0.8125\n",
      "2018-05-23T13:02:27.466111: step 5856, loss 0.322867, acc 0.859375\n",
      "2018-05-23T13:02:27.836122: step 5857, loss 0.315307, acc 0.859375\n",
      "2018-05-23T13:02:28.166239: step 5858, loss 0.370137, acc 0.890625\n",
      "2018-05-23T13:02:28.486382: step 5859, loss 0.272032, acc 0.828125\n",
      "2018-05-23T13:02:28.809518: step 5860, loss 0.346795, acc 0.84375\n",
      "2018-05-23T13:02:29.118691: step 5861, loss 0.346024, acc 0.8125\n",
      "2018-05-23T13:02:29.436839: step 5862, loss 0.374743, acc 0.84375\n",
      "2018-05-23T13:02:29.824801: step 5863, loss 0.270749, acc 0.890625\n",
      "2018-05-23T13:02:30.146940: step 5864, loss 0.294949, acc 0.90625\n",
      "2018-05-23T13:02:30.522934: step 5865, loss 0.419576, acc 0.84375\n",
      "2018-05-23T13:02:30.864022: step 5866, loss 0.407041, acc 0.796875\n",
      "2018-05-23T13:02:31.245002: step 5867, loss 0.456556, acc 0.84375\n",
      "2018-05-23T13:02:31.600054: step 5868, loss 0.35974, acc 0.828125\n",
      "2018-05-23T13:02:31.956101: step 5869, loss 0.374731, acc 0.828125\n",
      "2018-05-23T13:02:32.380966: step 5870, loss 0.348098, acc 0.859375\n",
      "2018-05-23T13:02:32.763939: step 5871, loss 0.332856, acc 0.890625\n",
      "2018-05-23T13:02:33.079095: step 5872, loss 0.24114, acc 0.90625\n",
      "2018-05-23T13:02:33.383282: step 5873, loss 0.186285, acc 0.90625\n",
      "2018-05-23T13:02:33.694451: step 5874, loss 0.332638, acc 0.890625\n",
      "2018-05-23T13:02:34.009606: step 5875, loss 0.350879, acc 0.875\n",
      "2018-05-23T13:02:34.323766: step 5876, loss 0.252484, acc 0.875\n",
      "2018-05-23T13:02:34.633937: step 5877, loss 0.273417, acc 0.890625\n",
      "2018-05-23T13:02:34.953083: step 5878, loss 0.361466, acc 0.84375\n",
      "2018-05-23T13:02:35.263253: step 5879, loss 0.327223, acc 0.828125\n",
      "2018-05-23T13:02:35.613316: step 5880, loss 0.380865, acc 0.8125\n",
      "2018-05-23T13:02:35.928473: step 5881, loss 0.375115, acc 0.84375\n",
      "2018-05-23T13:02:36.238644: step 5882, loss 0.374031, acc 0.890625\n",
      "2018-05-23T13:02:36.539837: step 5883, loss 0.291074, acc 0.84375\n",
      "2018-05-23T13:02:36.906856: step 5884, loss 0.323524, acc 0.859375\n",
      "2018-05-23T13:02:37.294818: step 5885, loss 0.234231, acc 0.90625\n",
      "2018-05-23T13:02:37.677794: step 5886, loss 0.36554, acc 0.828125\n",
      "2018-05-23T13:02:38.071739: step 5887, loss 0.349065, acc 0.828125\n",
      "2018-05-23T13:02:38.384902: step 5888, loss 0.371654, acc 0.828125\n",
      "2018-05-23T13:02:38.701058: step 5889, loss 0.329152, acc 0.828125\n",
      "2018-05-23T13:02:39.015215: step 5890, loss 0.250079, acc 0.890625\n",
      "2018-05-23T13:02:39.394202: step 5891, loss 0.39665, acc 0.8125\n",
      "2018-05-23T13:02:39.702377: step 5892, loss 0.593882, acc 0.71875\n",
      "2018-05-23T13:02:40.013546: step 5893, loss 0.418938, acc 0.796875\n",
      "2018-05-23T13:02:40.336681: step 5894, loss 0.350664, acc 0.84375\n",
      "2018-05-23T13:02:40.692728: step 5895, loss 0.365515, acc 0.8125\n",
      "2018-05-23T13:02:41.001900: step 5896, loss 0.356166, acc 0.859375\n",
      "2018-05-23T13:02:41.326034: step 5897, loss 0.342353, acc 0.84375\n",
      "2018-05-23T13:02:41.729953: step 5898, loss 0.322665, acc 0.859375\n",
      "2018-05-23T13:02:42.081013: step 5899, loss 0.184947, acc 0.96875\n",
      "2018-05-23T13:02:42.493911: step 5900, loss 0.335048, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:02:46.451323: step 5900, loss 0.619187, acc 0.735105\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-5900\n",
      "\n",
      "2018-05-23T13:02:47.511487: step 5901, loss 0.474091, acc 0.796875\n",
      "2018-05-23T13:02:47.924382: step 5902, loss 0.370265, acc 0.8125\n",
      "2018-05-23T13:02:48.267466: step 5903, loss 0.221952, acc 0.890625\n",
      "2018-05-23T13:02:48.587607: step 5904, loss 0.361387, acc 0.890625\n",
      "2018-05-23T13:02:48.908748: step 5905, loss 0.309033, acc 0.859375\n",
      "2018-05-23T13:02:49.226897: step 5906, loss 0.215735, acc 0.90625\n",
      "2018-05-23T13:02:49.540060: step 5907, loss 0.381653, acc 0.875\n",
      "2018-05-23T13:02:49.847238: step 5908, loss 0.386305, acc 0.8125\n",
      "2018-05-23T13:02:50.164392: step 5909, loss 0.309974, acc 0.890625\n",
      "2018-05-23T13:02:50.398762: step 5910, loss 0.345319, acc 0.882353\n",
      "2018-05-23T13:02:50.724890: step 5911, loss 0.337726, acc 0.828125\n",
      "2018-05-23T13:02:51.047029: step 5912, loss 0.187657, acc 0.96875\n",
      "2018-05-23T13:02:51.370164: step 5913, loss 0.24404, acc 0.890625\n",
      "2018-05-23T13:02:51.693299: step 5914, loss 0.319346, acc 0.875\n",
      "2018-05-23T13:02:52.084253: step 5915, loss 0.308395, acc 0.875\n",
      "2018-05-23T13:02:52.442296: step 5916, loss 0.169597, acc 0.90625\n",
      "2018-05-23T13:02:52.841228: step 5917, loss 0.333965, acc 0.84375\n",
      "2018-05-23T13:02:53.236172: step 5918, loss 0.191036, acc 0.9375\n",
      "2018-05-23T13:02:53.650064: step 5919, loss 0.241082, acc 0.890625\n",
      "2018-05-23T13:02:54.034037: step 5920, loss 0.29984, acc 0.84375\n",
      "2018-05-23T13:02:54.426986: step 5921, loss 0.343805, acc 0.828125\n",
      "2018-05-23T13:02:54.750122: step 5922, loss 0.296184, acc 0.859375\n",
      "2018-05-23T13:02:55.074254: step 5923, loss 0.191563, acc 0.921875\n",
      "2018-05-23T13:02:55.384424: step 5924, loss 0.183632, acc 0.953125\n",
      "2018-05-23T13:02:55.705565: step 5925, loss 0.206654, acc 0.921875\n",
      "2018-05-23T13:02:56.069592: step 5926, loss 0.35672, acc 0.875\n",
      "2018-05-23T13:02:56.388737: step 5927, loss 0.316211, acc 0.875\n",
      "2018-05-23T13:02:56.694919: step 5928, loss 0.261449, acc 0.90625\n",
      "2018-05-23T13:02:57.016060: step 5929, loss 0.201894, acc 0.9375\n",
      "2018-05-23T13:02:57.371111: step 5930, loss 0.272764, acc 0.859375\n",
      "2018-05-23T13:02:57.683276: step 5931, loss 0.166549, acc 0.953125\n",
      "2018-05-23T13:02:57.997434: step 5932, loss 0.419721, acc 0.796875\n",
      "2018-05-23T13:02:58.311594: step 5933, loss 0.245541, acc 0.890625\n",
      "2018-05-23T13:02:58.638718: step 5934, loss 0.406557, acc 0.8125\n",
      "2018-05-23T13:02:58.977813: step 5935, loss 0.259524, acc 0.84375\n",
      "2018-05-23T13:02:59.310921: step 5936, loss 0.234419, acc 0.90625\n",
      "2018-05-23T13:02:59.618100: step 5937, loss 0.210189, acc 0.90625\n",
      "2018-05-23T13:02:59.942233: step 5938, loss 0.27092, acc 0.90625\n",
      "2018-05-23T13:03:00.269356: step 5939, loss 0.232917, acc 0.890625\n",
      "2018-05-23T13:03:00.594488: step 5940, loss 0.3168, acc 0.875\n",
      "2018-05-23T13:03:00.909643: step 5941, loss 0.252874, acc 0.890625\n",
      "2018-05-23T13:03:01.236769: step 5942, loss 0.40285, acc 0.875\n",
      "2018-05-23T13:03:01.556912: step 5943, loss 0.218391, acc 0.90625\n",
      "2018-05-23T13:03:01.860101: step 5944, loss 0.257, acc 0.90625\n",
      "2018-05-23T13:03:02.178251: step 5945, loss 0.210562, acc 0.90625\n",
      "2018-05-23T13:03:02.513353: step 5946, loss 0.286333, acc 0.90625\n",
      "2018-05-23T13:03:02.850452: step 5947, loss 0.275802, acc 0.890625\n",
      "2018-05-23T13:03:03.203507: step 5948, loss 0.308579, acc 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:03:03.518664: step 5949, loss 0.189762, acc 0.953125\n",
      "2018-05-23T13:03:03.831826: step 5950, loss 0.348961, acc 0.875\n",
      "2018-05-23T13:03:04.147981: step 5951, loss 0.546478, acc 0.796875\n",
      "2018-05-23T13:03:04.465132: step 5952, loss 0.24515, acc 0.890625\n",
      "2018-05-23T13:03:04.780290: step 5953, loss 0.188111, acc 0.9375\n",
      "2018-05-23T13:03:05.097441: step 5954, loss 0.146967, acc 0.96875\n",
      "2018-05-23T13:03:05.424566: step 5955, loss 0.306031, acc 0.875\n",
      "2018-05-23T13:03:05.735733: step 5956, loss 0.221048, acc 0.90625\n",
      "2018-05-23T13:03:06.055876: step 5957, loss 0.410154, acc 0.78125\n",
      "2018-05-23T13:03:06.380009: step 5958, loss 0.275946, acc 0.875\n",
      "2018-05-23T13:03:06.689182: step 5959, loss 0.348796, acc 0.796875\n",
      "2018-05-23T13:03:07.009326: step 5960, loss 0.206702, acc 0.90625\n",
      "2018-05-23T13:03:07.314509: step 5961, loss 0.186102, acc 0.921875\n",
      "2018-05-23T13:03:07.633658: step 5962, loss 0.317603, acc 0.828125\n",
      "2018-05-23T13:03:07.946818: step 5963, loss 0.272568, acc 0.875\n",
      "2018-05-23T13:03:08.259980: step 5964, loss 0.25128, acc 0.84375\n",
      "2018-05-23T13:03:08.569153: step 5965, loss 0.288382, acc 0.875\n",
      "2018-05-23T13:03:08.896278: step 5966, loss 0.235246, acc 0.921875\n",
      "2018-05-23T13:03:09.222406: step 5967, loss 0.248263, acc 0.890625\n",
      "2018-05-23T13:03:09.547536: step 5968, loss 0.282876, acc 0.8125\n",
      "2018-05-23T13:03:09.860698: step 5969, loss 0.204046, acc 0.921875\n",
      "2018-05-23T13:03:10.176852: step 5970, loss 0.348157, acc 0.859375\n",
      "2018-05-23T13:03:10.489017: step 5971, loss 0.337324, acc 0.859375\n",
      "2018-05-23T13:03:10.805171: step 5972, loss 0.287999, acc 0.8125\n",
      "2018-05-23T13:03:11.123320: step 5973, loss 0.306367, acc 0.859375\n",
      "2018-05-23T13:03:11.446456: step 5974, loss 0.182, acc 0.9375\n",
      "2018-05-23T13:03:11.761613: step 5975, loss 0.147757, acc 0.984375\n",
      "2018-05-23T13:03:12.070786: step 5976, loss 0.374381, acc 0.828125\n",
      "2018-05-23T13:03:12.379958: step 5977, loss 0.302302, acc 0.921875\n",
      "2018-05-23T13:03:12.705089: step 5978, loss 0.302574, acc 0.859375\n",
      "2018-05-23T13:03:13.043185: step 5979, loss 0.258755, acc 0.9375\n",
      "2018-05-23T13:03:13.363327: step 5980, loss 0.253074, acc 0.90625\n",
      "2018-05-23T13:03:13.678484: step 5981, loss 0.291318, acc 0.875\n",
      "2018-05-23T13:03:13.989652: step 5982, loss 0.385019, acc 0.78125\n",
      "2018-05-23T13:03:14.299823: step 5983, loss 0.290976, acc 0.875\n",
      "2018-05-23T13:03:14.609993: step 5984, loss 0.314146, acc 0.859375\n",
      "2018-05-23T13:03:14.923156: step 5985, loss 0.186139, acc 0.953125\n",
      "2018-05-23T13:03:15.239309: step 5986, loss 0.29247, acc 0.890625\n",
      "2018-05-23T13:03:15.575410: step 5987, loss 0.293938, acc 0.921875\n",
      "2018-05-23T13:03:15.917496: step 5988, loss 0.241605, acc 0.921875\n",
      "2018-05-23T13:03:16.223675: step 5989, loss 0.308401, acc 0.859375\n",
      "2018-05-23T13:03:16.536838: step 5990, loss 0.300553, acc 0.828125\n",
      "2018-05-23T13:03:16.848005: step 5991, loss 0.309757, acc 0.90625\n",
      "2018-05-23T13:03:17.157179: step 5992, loss 0.228637, acc 0.84375\n",
      "2018-05-23T13:03:17.465355: step 5993, loss 0.270655, acc 0.890625\n",
      "2018-05-23T13:03:17.778516: step 5994, loss 0.602446, acc 0.765625\n",
      "2018-05-23T13:03:18.102650: step 5995, loss 0.221418, acc 0.90625\n",
      "2018-05-23T13:03:18.410825: step 5996, loss 0.289785, acc 0.890625\n",
      "2018-05-23T13:03:18.730968: step 5997, loss 0.183777, acc 0.953125\n",
      "2018-05-23T13:03:19.051111: step 5998, loss 0.254495, acc 0.890625\n",
      "2018-05-23T13:03:19.363276: step 5999, loss 0.213912, acc 0.9375\n",
      "2018-05-23T13:03:19.679432: step 6000, loss 0.285808, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:03:23.501207: step 6000, loss 0.652507, acc 0.727104\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6000\n",
      "\n",
      "2018-05-23T13:03:24.800029: step 6001, loss 0.284392, acc 0.890625\n",
      "2018-05-23T13:03:25.135131: step 6002, loss 0.248769, acc 0.921875\n",
      "2018-05-23T13:03:25.451286: step 6003, loss 0.267558, acc 0.875\n",
      "2018-05-23T13:03:25.762453: step 6004, loss 0.178821, acc 0.921875\n",
      "2018-05-23T13:03:26.081600: step 6005, loss 0.25194, acc 0.890625\n",
      "2018-05-23T13:03:26.406731: step 6006, loss 0.199613, acc 0.9375\n",
      "2018-05-23T13:03:26.716900: step 6007, loss 0.297733, acc 0.890625\n",
      "2018-05-23T13:03:27.054996: step 6008, loss 0.255791, acc 0.953125\n",
      "2018-05-23T13:03:27.364168: step 6009, loss 0.23984, acc 0.921875\n",
      "2018-05-23T13:03:27.674340: step 6010, loss 0.191988, acc 0.9375\n",
      "2018-05-23T13:03:27.994482: step 6011, loss 0.217436, acc 0.890625\n",
      "2018-05-23T13:03:28.306647: step 6012, loss 0.171398, acc 0.90625\n",
      "2018-05-23T13:03:28.648733: step 6013, loss 0.314136, acc 0.859375\n",
      "2018-05-23T13:03:28.962891: step 6014, loss 0.299645, acc 0.921875\n",
      "2018-05-23T13:03:29.263088: step 6015, loss 0.335298, acc 0.84375\n",
      "2018-05-23T13:03:29.554311: step 6016, loss 0.314888, acc 0.859375\n",
      "2018-05-23T13:03:29.851514: step 6017, loss 0.181755, acc 0.96875\n",
      "2018-05-23T13:03:30.144729: step 6018, loss 0.27691, acc 0.859375\n",
      "2018-05-23T13:03:30.432959: step 6019, loss 0.292575, acc 0.859375\n",
      "2018-05-23T13:03:30.719192: step 6020, loss 0.268727, acc 0.875\n",
      "2018-05-23T13:03:31.012408: step 6021, loss 0.256576, acc 0.890625\n",
      "2018-05-23T13:03:31.303629: step 6022, loss 0.447003, acc 0.84375\n",
      "2018-05-23T13:03:31.594851: step 6023, loss 0.315524, acc 0.875\n",
      "2018-05-23T13:03:31.897041: step 6024, loss 0.213758, acc 0.921875\n",
      "2018-05-23T13:03:32.199234: step 6025, loss 0.404871, acc 0.828125\n",
      "2018-05-23T13:03:32.505414: step 6026, loss 0.253469, acc 0.890625\n",
      "2018-05-23T13:03:32.801621: step 6027, loss 0.218258, acc 0.921875\n",
      "2018-05-23T13:03:33.114787: step 6028, loss 0.271145, acc 0.875\n",
      "2018-05-23T13:03:33.408001: step 6029, loss 0.236345, acc 0.84375\n",
      "2018-05-23T13:03:33.714182: step 6030, loss 0.252919, acc 0.875\n",
      "2018-05-23T13:03:34.014378: step 6031, loss 0.21043, acc 0.921875\n",
      "2018-05-23T13:03:34.316570: step 6032, loss 0.212142, acc 0.921875\n",
      "2018-05-23T13:03:34.620762: step 6033, loss 0.267773, acc 0.890625\n",
      "2018-05-23T13:03:34.921949: step 6034, loss 0.192985, acc 0.921875\n",
      "2018-05-23T13:03:35.224141: step 6035, loss 0.293139, acc 0.84375\n",
      "2018-05-23T13:03:35.534311: step 6036, loss 0.246123, acc 0.859375\n",
      "2018-05-23T13:03:35.824536: step 6037, loss 0.18004, acc 0.921875\n",
      "2018-05-23T13:03:36.136699: step 6038, loss 0.384532, acc 0.859375\n",
      "2018-05-23T13:03:36.427920: step 6039, loss 0.315189, acc 0.875\n",
      "2018-05-23T13:03:36.733157: step 6040, loss 0.360252, acc 0.859375\n",
      "2018-05-23T13:03:37.032358: step 6041, loss 0.423405, acc 0.796875\n",
      "2018-05-23T13:03:37.338537: step 6042, loss 0.205703, acc 0.921875\n",
      "2018-05-23T13:03:37.637738: step 6043, loss 0.265622, acc 0.890625\n",
      "2018-05-23T13:03:37.934944: step 6044, loss 0.243612, acc 0.9375\n",
      "2018-05-23T13:03:38.246111: step 6045, loss 0.379093, acc 0.84375\n",
      "2018-05-23T13:03:38.547303: step 6046, loss 0.452239, acc 0.765625\n",
      "2018-05-23T13:03:38.849495: step 6047, loss 0.287274, acc 0.890625\n",
      "2018-05-23T13:03:39.143708: step 6048, loss 0.251856, acc 0.90625\n",
      "2018-05-23T13:03:39.443907: step 6049, loss 0.279949, acc 0.875\n",
      "2018-05-23T13:03:39.744102: step 6050, loss 0.294783, acc 0.875\n",
      "2018-05-23T13:03:40.047291: step 6051, loss 0.417624, acc 0.8125\n",
      "2018-05-23T13:03:40.348485: step 6052, loss 0.39695, acc 0.796875\n",
      "2018-05-23T13:03:40.642697: step 6053, loss 0.171073, acc 0.9375\n",
      "2018-05-23T13:03:40.942896: step 6054, loss 0.463329, acc 0.78125\n",
      "2018-05-23T13:03:41.242097: step 6055, loss 0.340306, acc 0.875\n",
      "2018-05-23T13:03:41.546280: step 6056, loss 0.315627, acc 0.875\n",
      "2018-05-23T13:03:41.851464: step 6057, loss 0.247956, acc 0.921875\n",
      "2018-05-23T13:03:42.155650: step 6058, loss 0.309162, acc 0.859375\n",
      "2018-05-23T13:03:42.462829: step 6059, loss 0.2842, acc 0.875\n",
      "2018-05-23T13:03:42.762028: step 6060, loss 0.333097, acc 0.84375\n",
      "2018-05-23T13:03:43.065217: step 6061, loss 0.342986, acc 0.828125\n",
      "2018-05-23T13:03:43.360427: step 6062, loss 0.275878, acc 0.875\n",
      "2018-05-23T13:03:43.667605: step 6063, loss 0.279664, acc 0.890625\n",
      "2018-05-23T13:03:43.970796: step 6064, loss 0.289438, acc 0.828125\n",
      "2018-05-23T13:03:44.267002: step 6065, loss 0.214607, acc 0.921875\n",
      "2018-05-23T13:03:44.572186: step 6066, loss 0.192875, acc 0.90625\n",
      "2018-05-23T13:03:44.873381: step 6067, loss 0.184842, acc 0.921875\n",
      "2018-05-23T13:03:45.171128: step 6068, loss 0.235566, acc 0.890625\n",
      "2018-05-23T13:03:45.466339: step 6069, loss 0.413491, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:03:45.768528: step 6070, loss 0.236187, acc 0.890625\n",
      "2018-05-23T13:03:46.062741: step 6071, loss 0.303365, acc 0.828125\n",
      "2018-05-23T13:03:46.356956: step 6072, loss 0.187358, acc 0.921875\n",
      "2018-05-23T13:03:46.650170: step 6073, loss 0.354268, acc 0.859375\n",
      "2018-05-23T13:03:46.948374: step 6074, loss 0.312724, acc 0.859375\n",
      "2018-05-23T13:03:47.248571: step 6075, loss 0.200412, acc 0.90625\n",
      "2018-05-23T13:03:47.550762: step 6076, loss 0.278805, acc 0.890625\n",
      "2018-05-23T13:03:47.853990: step 6077, loss 0.412029, acc 0.765625\n",
      "2018-05-23T13:03:48.155144: step 6078, loss 0.305737, acc 0.859375\n",
      "2018-05-23T13:03:48.447363: step 6079, loss 0.341277, acc 0.84375\n",
      "2018-05-23T13:03:48.738584: step 6080, loss 0.339239, acc 0.859375\n",
      "2018-05-23T13:03:49.037785: step 6081, loss 0.336105, acc 0.828125\n",
      "2018-05-23T13:03:49.323019: step 6082, loss 0.258372, acc 0.90625\n",
      "2018-05-23T13:03:49.619267: step 6083, loss 0.197699, acc 0.9375\n",
      "2018-05-23T13:03:49.910449: step 6084, loss 0.271787, acc 0.859375\n",
      "2018-05-23T13:03:50.205659: step 6085, loss 0.401157, acc 0.8125\n",
      "2018-05-23T13:03:50.501866: step 6086, loss 0.224207, acc 0.921875\n",
      "2018-05-23T13:03:50.794125: step 6087, loss 0.210795, acc 0.90625\n",
      "2018-05-23T13:03:51.091329: step 6088, loss 0.33287, acc 0.84375\n",
      "2018-05-23T13:03:51.385543: step 6089, loss 0.343497, acc 0.828125\n",
      "2018-05-23T13:03:51.673771: step 6090, loss 0.289703, acc 0.90625\n",
      "2018-05-23T13:03:51.970977: step 6091, loss 0.192864, acc 0.921875\n",
      "2018-05-23T13:03:52.263197: step 6092, loss 0.189897, acc 0.90625\n",
      "2018-05-23T13:03:52.557410: step 6093, loss 0.308001, acc 0.84375\n",
      "2018-05-23T13:03:52.853616: step 6094, loss 0.309717, acc 0.890625\n",
      "2018-05-23T13:03:53.138853: step 6095, loss 0.301021, acc 0.875\n",
      "2018-05-23T13:03:53.429075: step 6096, loss 0.260684, acc 0.90625\n",
      "2018-05-23T13:03:53.720298: step 6097, loss 0.264028, acc 0.890625\n",
      "2018-05-23T13:03:54.018499: step 6098, loss 0.267098, acc 0.875\n",
      "2018-05-23T13:03:54.313710: step 6099, loss 0.312905, acc 0.859375\n",
      "2018-05-23T13:03:54.601940: step 6100, loss 0.305754, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:03:58.345923: step 6100, loss 0.655881, acc 0.730819\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6100\n",
      "\n",
      "2018-05-23T13:03:59.420454: step 6101, loss 0.227536, acc 0.9375\n",
      "2018-05-23T13:03:59.907153: step 6102, loss 0.240307, acc 0.90625\n",
      "2018-05-23T13:04:00.304091: step 6103, loss 0.269191, acc 0.875\n",
      "2018-05-23T13:04:00.752890: step 6104, loss 0.2866, acc 0.84375\n",
      "2018-05-23T13:04:01.156810: step 6105, loss 0.252699, acc 0.890625\n",
      "2018-05-23T13:04:01.499891: step 6106, loss 0.369111, acc 0.859375\n",
      "2018-05-23T13:04:01.859928: step 6107, loss 0.321015, acc 0.859375\n",
      "2018-05-23T13:04:02.194034: step 6108, loss 0.173244, acc 0.953125\n",
      "2018-05-23T13:04:02.510189: step 6109, loss 0.189778, acc 0.90625\n",
      "2018-05-23T13:04:02.851275: step 6110, loss 0.223483, acc 0.890625\n",
      "2018-05-23T13:04:03.170422: step 6111, loss 0.240702, acc 0.90625\n",
      "2018-05-23T13:04:03.483584: step 6112, loss 0.301315, acc 0.859375\n",
      "2018-05-23T13:04:03.795749: step 6113, loss 0.222394, acc 0.921875\n",
      "2018-05-23T13:04:04.105919: step 6114, loss 0.198788, acc 0.953125\n",
      "2018-05-23T13:04:04.419081: step 6115, loss 0.245817, acc 0.84375\n",
      "2018-05-23T13:04:04.741220: step 6116, loss 0.261737, acc 0.859375\n",
      "2018-05-23T13:04:05.064356: step 6117, loss 0.206568, acc 0.921875\n",
      "2018-05-23T13:04:05.392477: step 6118, loss 0.204707, acc 0.90625\n",
      "2018-05-23T13:04:05.724589: step 6119, loss 0.290793, acc 0.859375\n",
      "2018-05-23T13:04:06.069665: step 6120, loss 0.315783, acc 0.828125\n",
      "2018-05-23T13:04:06.392802: step 6121, loss 0.279913, acc 0.859375\n",
      "2018-05-23T13:04:06.721921: step 6122, loss 0.300367, acc 0.890625\n",
      "2018-05-23T13:04:07.049580: step 6123, loss 0.17291, acc 0.9375\n",
      "2018-05-23T13:04:07.379697: step 6124, loss 0.248324, acc 0.875\n",
      "2018-05-23T13:04:07.700700: step 6125, loss 0.265081, acc 0.859375\n",
      "2018-05-23T13:04:08.023359: step 6126, loss 0.239307, acc 0.875\n",
      "2018-05-23T13:04:08.341525: step 6127, loss 0.279573, acc 0.875\n",
      "2018-05-23T13:04:08.658185: step 6128, loss 0.274646, acc 0.890625\n",
      "2018-05-23T13:04:08.981339: step 6129, loss 0.24655, acc 0.921875\n",
      "2018-05-23T13:04:09.312962: step 6130, loss 0.233595, acc 0.890625\n",
      "2018-05-23T13:04:09.642081: step 6131, loss 0.312082, acc 0.859375\n",
      "2018-05-23T13:04:09.957488: step 6132, loss 0.24043, acc 0.890625\n",
      "2018-05-23T13:04:10.276144: step 6133, loss 0.293468, acc 0.875\n",
      "2018-05-23T13:04:10.602272: step 6134, loss 0.275345, acc 0.84375\n",
      "2018-05-23T13:04:10.974276: step 6135, loss 0.330292, acc 0.84375\n",
      "2018-05-23T13:04:11.307894: step 6136, loss 0.222001, acc 0.921875\n",
      "2018-05-23T13:04:11.636031: step 6137, loss 0.284266, acc 0.828125\n",
      "2018-05-23T13:04:11.963690: step 6138, loss 0.23538, acc 0.90625\n",
      "2018-05-23T13:04:12.297840: step 6139, loss 0.312224, acc 0.90625\n",
      "2018-05-23T13:04:12.624965: step 6140, loss 0.287018, acc 0.890625\n",
      "2018-05-23T13:04:12.941121: step 6141, loss 0.312649, acc 0.875\n",
      "2018-05-23T13:04:13.254190: step 6142, loss 0.287516, acc 0.875\n",
      "2018-05-23T13:04:13.574351: step 6143, loss 0.250495, acc 0.90625\n",
      "2018-05-23T13:04:13.901981: step 6144, loss 0.191518, acc 0.90625\n",
      "2018-05-23T13:04:14.226621: step 6145, loss 0.153289, acc 0.953125\n",
      "2018-05-23T13:04:14.542793: step 6146, loss 0.270517, acc 0.875\n",
      "2018-05-23T13:04:14.848990: step 6147, loss 0.262809, acc 0.859375\n",
      "2018-05-23T13:04:15.170195: step 6148, loss 0.353072, acc 0.828125\n",
      "2018-05-23T13:04:15.496337: step 6149, loss 0.452766, acc 0.8125\n",
      "2018-05-23T13:04:15.823462: step 6150, loss 0.304924, acc 0.859375\n",
      "2018-05-23T13:04:16.144245: step 6151, loss 0.228234, acc 0.890625\n",
      "2018-05-23T13:04:16.455413: step 6152, loss 0.24845, acc 0.890625\n",
      "2018-05-23T13:04:16.770570: step 6153, loss 0.310775, acc 0.9375\n",
      "2018-05-23T13:04:17.121632: step 6154, loss 0.310194, acc 0.84375\n",
      "2018-05-23T13:04:17.506602: step 6155, loss 0.149986, acc 0.953125\n",
      "2018-05-23T13:04:17.842701: step 6156, loss 0.40114, acc 0.796875\n",
      "2018-05-23T13:04:18.165837: step 6157, loss 0.20853, acc 0.921875\n",
      "2018-05-23T13:04:18.483986: step 6158, loss 0.326057, acc 0.875\n",
      "2018-05-23T13:04:18.831058: step 6159, loss 0.320069, acc 0.875\n",
      "2018-05-23T13:04:19.146214: step 6160, loss 0.476824, acc 0.78125\n",
      "2018-05-23T13:04:19.471344: step 6161, loss 0.20112, acc 0.9375\n",
      "2018-05-23T13:04:19.777526: step 6162, loss 0.275229, acc 0.890625\n",
      "2018-05-23T13:04:20.090692: step 6163, loss 0.354812, acc 0.859375\n",
      "2018-05-23T13:04:20.417813: step 6164, loss 0.375638, acc 0.84375\n",
      "2018-05-23T13:04:20.730976: step 6165, loss 0.356235, acc 0.828125\n",
      "2018-05-23T13:04:21.058100: step 6166, loss 0.133117, acc 0.953125\n",
      "2018-05-23T13:04:21.375252: step 6167, loss 0.235009, acc 0.875\n",
      "2018-05-23T13:04:21.680435: step 6168, loss 0.213833, acc 0.921875\n",
      "2018-05-23T13:04:22.004568: step 6169, loss 0.223835, acc 0.90625\n",
      "2018-05-23T13:04:22.315736: step 6170, loss 0.270794, acc 0.890625\n",
      "2018-05-23T13:04:22.625906: step 6171, loss 0.409255, acc 0.796875\n",
      "2018-05-23T13:04:22.940065: step 6172, loss 0.283385, acc 0.90625\n",
      "2018-05-23T13:04:23.251232: step 6173, loss 0.213117, acc 0.890625\n",
      "2018-05-23T13:04:23.561403: step 6174, loss 0.369728, acc 0.84375\n",
      "2018-05-23T13:04:23.881546: step 6175, loss 0.389784, acc 0.84375\n",
      "2018-05-23T13:04:24.188725: step 6176, loss 0.279052, acc 0.875\n",
      "2018-05-23T13:04:24.505876: step 6177, loss 0.281927, acc 0.890625\n",
      "2018-05-23T13:04:24.826020: step 6178, loss 0.340205, acc 0.828125\n",
      "2018-05-23T13:04:25.157134: step 6179, loss 0.282038, acc 0.90625\n",
      "2018-05-23T13:04:25.468301: step 6180, loss 0.314505, acc 0.875\n",
      "2018-05-23T13:04:25.782461: step 6181, loss 0.215452, acc 0.90625\n",
      "2018-05-23T13:04:26.104599: step 6182, loss 0.402731, acc 0.859375\n",
      "2018-05-23T13:04:26.425741: step 6183, loss 0.243931, acc 0.859375\n",
      "2018-05-23T13:04:26.741895: step 6184, loss 0.302813, acc 0.875\n",
      "2018-05-23T13:04:27.072011: step 6185, loss 0.265623, acc 0.859375\n",
      "2018-05-23T13:04:27.386171: step 6186, loss 0.238639, acc 0.921875\n",
      "2018-05-23T13:04:27.699333: step 6187, loss 0.14928, acc 0.953125\n",
      "2018-05-23T13:04:28.015487: step 6188, loss 0.268041, acc 0.84375\n",
      "2018-05-23T13:04:28.334633: step 6189, loss 0.223774, acc 0.90625\n",
      "2018-05-23T13:04:28.646798: step 6190, loss 0.28913, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:04:28.972926: step 6191, loss 0.334186, acc 0.90625\n",
      "2018-05-23T13:04:29.286088: step 6192, loss 0.291165, acc 0.875\n",
      "2018-05-23T13:04:29.598253: step 6193, loss 0.340245, acc 0.875\n",
      "2018-05-23T13:04:29.912413: step 6194, loss 0.231824, acc 0.875\n",
      "2018-05-23T13:04:30.218593: step 6195, loss 0.223753, acc 0.875\n",
      "2018-05-23T13:04:30.536742: step 6196, loss 0.364413, acc 0.859375\n",
      "2018-05-23T13:04:30.844918: step 6197, loss 0.245881, acc 0.875\n",
      "2018-05-23T13:04:31.149104: step 6198, loss 0.28727, acc 0.90625\n",
      "2018-05-23T13:04:31.461270: step 6199, loss 0.162458, acc 0.9375\n",
      "2018-05-23T13:04:31.754486: step 6200, loss 0.293337, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:04:35.524401: step 6200, loss 0.673799, acc 0.729676\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6200\n",
      "\n",
      "2018-05-23T13:04:36.449023: step 6201, loss 0.353062, acc 0.84375\n",
      "2018-05-23T13:04:36.831001: step 6202, loss 0.276304, acc 0.859375\n",
      "2018-05-23T13:04:37.126212: step 6203, loss 0.226492, acc 0.890625\n",
      "2018-05-23T13:04:37.429400: step 6204, loss 0.260107, acc 0.828125\n",
      "2018-05-23T13:04:37.725608: step 6205, loss 0.405476, acc 0.859375\n",
      "2018-05-23T13:04:38.020818: step 6206, loss 0.254324, acc 0.890625\n",
      "2018-05-23T13:04:38.314036: step 6207, loss 0.229867, acc 0.921875\n",
      "2018-05-23T13:04:38.605257: step 6208, loss 0.241907, acc 0.890625\n",
      "2018-05-23T13:04:38.894481: step 6209, loss 0.354276, acc 0.8125\n",
      "2018-05-23T13:04:39.186701: step 6210, loss 0.247119, acc 0.9375\n",
      "2018-05-23T13:04:39.476925: step 6211, loss 0.19959, acc 0.953125\n",
      "2018-05-23T13:04:39.772133: step 6212, loss 0.278798, acc 0.890625\n",
      "2018-05-23T13:04:40.073328: step 6213, loss 0.346943, acc 0.859375\n",
      "2018-05-23T13:04:40.363553: step 6214, loss 0.266704, acc 0.875\n",
      "2018-05-23T13:04:40.658761: step 6215, loss 0.344069, acc 0.84375\n",
      "2018-05-23T13:04:40.945994: step 6216, loss 0.291598, acc 0.84375\n",
      "2018-05-23T13:04:41.239210: step 6217, loss 0.34981, acc 0.875\n",
      "2018-05-23T13:04:41.528436: step 6218, loss 0.292749, acc 0.90625\n",
      "2018-05-23T13:04:41.823645: step 6219, loss 0.359519, acc 0.828125\n",
      "2018-05-23T13:04:42.120852: step 6220, loss 0.220279, acc 0.90625\n",
      "2018-05-23T13:04:42.409078: step 6221, loss 0.312029, acc 0.875\n",
      "2018-05-23T13:04:42.701299: step 6222, loss 0.323761, acc 0.828125\n",
      "2018-05-23T13:04:42.992518: step 6223, loss 0.296989, acc 0.90625\n",
      "2018-05-23T13:04:43.283741: step 6224, loss 0.303071, acc 0.875\n",
      "2018-05-23T13:04:43.581941: step 6225, loss 0.227549, acc 0.90625\n",
      "2018-05-23T13:04:43.874159: step 6226, loss 0.350013, acc 0.859375\n",
      "2018-05-23T13:04:44.174356: step 6227, loss 0.294763, acc 0.84375\n",
      "2018-05-23T13:04:44.464580: step 6228, loss 0.310096, acc 0.859375\n",
      "2018-05-23T13:04:44.755802: step 6229, loss 0.229321, acc 0.890625\n",
      "2018-05-23T13:04:45.050014: step 6230, loss 0.247326, acc 0.890625\n",
      "2018-05-23T13:04:45.351208: step 6231, loss 0.388053, acc 0.8125\n",
      "2018-05-23T13:04:45.639438: step 6232, loss 0.225745, acc 0.875\n",
      "2018-05-23T13:04:45.938636: step 6233, loss 0.273418, acc 0.921875\n",
      "2018-05-23T13:04:46.231852: step 6234, loss 0.317299, acc 0.875\n",
      "2018-05-23T13:04:46.520081: step 6235, loss 0.330601, acc 0.84375\n",
      "2018-05-23T13:04:46.810304: step 6236, loss 0.347159, acc 0.890625\n",
      "2018-05-23T13:04:47.102523: step 6237, loss 0.375398, acc 0.828125\n",
      "2018-05-23T13:04:47.396745: step 6238, loss 0.215837, acc 0.921875\n",
      "2018-05-23T13:04:47.688954: step 6239, loss 0.320585, acc 0.84375\n",
      "2018-05-23T13:04:47.990148: step 6240, loss 0.248073, acc 0.890625\n",
      "2018-05-23T13:04:48.339215: step 6241, loss 0.569828, acc 0.796875\n",
      "2018-05-23T13:04:48.634426: step 6242, loss 0.240045, acc 0.84375\n",
      "2018-05-23T13:04:48.925645: step 6243, loss 0.240119, acc 0.90625\n",
      "2018-05-23T13:04:49.214871: step 6244, loss 0.164094, acc 0.96875\n",
      "2018-05-23T13:04:49.501106: step 6245, loss 0.22256, acc 0.90625\n",
      "2018-05-23T13:04:49.787340: step 6246, loss 0.327878, acc 0.8125\n",
      "2018-05-23T13:04:50.084546: step 6247, loss 0.26041, acc 0.890625\n",
      "2018-05-23T13:04:50.379756: step 6248, loss 0.267141, acc 0.890625\n",
      "2018-05-23T13:04:50.678955: step 6249, loss 0.236048, acc 0.9375\n",
      "2018-05-23T13:04:50.984139: step 6250, loss 0.355979, acc 0.875\n",
      "2018-05-23T13:04:51.273365: step 6251, loss 0.337592, acc 0.859375\n",
      "2018-05-23T13:04:51.562591: step 6252, loss 0.348526, acc 0.8125\n",
      "2018-05-23T13:04:51.864783: step 6253, loss 0.208674, acc 0.921875\n",
      "2018-05-23T13:04:52.155007: step 6254, loss 0.323028, acc 0.84375\n",
      "2018-05-23T13:04:52.445230: step 6255, loss 0.205339, acc 0.90625\n",
      "2018-05-23T13:04:52.735453: step 6256, loss 0.247746, acc 0.890625\n",
      "2018-05-23T13:04:53.035650: step 6257, loss 0.383443, acc 0.859375\n",
      "2018-05-23T13:04:53.333852: step 6258, loss 0.282571, acc 0.90625\n",
      "2018-05-23T13:04:53.624076: step 6259, loss 0.231021, acc 0.921875\n",
      "2018-05-23T13:04:53.918289: step 6260, loss 0.253094, acc 0.84375\n",
      "2018-05-23T13:04:54.210507: step 6261, loss 0.405665, acc 0.8125\n",
      "2018-05-23T13:04:54.497739: step 6262, loss 0.205008, acc 0.9375\n",
      "2018-05-23T13:04:54.791952: step 6263, loss 0.410782, acc 0.828125\n",
      "2018-05-23T13:04:55.091152: step 6264, loss 0.213713, acc 0.921875\n",
      "2018-05-23T13:04:55.381375: step 6265, loss 0.282129, acc 0.859375\n",
      "2018-05-23T13:04:55.675590: step 6266, loss 0.268463, acc 0.84375\n",
      "2018-05-23T13:04:55.972793: step 6267, loss 0.366515, acc 0.796875\n",
      "2018-05-23T13:04:56.261022: step 6268, loss 0.329755, acc 0.875\n",
      "2018-05-23T13:04:56.547257: step 6269, loss 0.313057, acc 0.875\n",
      "2018-05-23T13:04:56.840472: step 6270, loss 0.315334, acc 0.828125\n",
      "2018-05-23T13:04:57.142664: step 6271, loss 0.312578, acc 0.84375\n",
      "2018-05-23T13:04:57.437874: step 6272, loss 0.305448, acc 0.8125\n",
      "2018-05-23T13:04:57.727101: step 6273, loss 0.247501, acc 0.90625\n",
      "2018-05-23T13:04:58.029293: step 6274, loss 0.319405, acc 0.828125\n",
      "2018-05-23T13:04:58.319515: step 6275, loss 0.325936, acc 0.8125\n",
      "2018-05-23T13:04:58.612731: step 6276, loss 0.22956, acc 0.953125\n",
      "2018-05-23T13:04:58.902955: step 6277, loss 0.382268, acc 0.796875\n",
      "2018-05-23T13:04:59.197169: step 6278, loss 0.33782, acc 0.890625\n",
      "2018-05-23T13:04:59.485397: step 6279, loss 0.388725, acc 0.84375\n",
      "2018-05-23T13:04:59.782601: step 6280, loss 0.264268, acc 0.890625\n",
      "2018-05-23T13:05:00.095763: step 6281, loss 0.323383, acc 0.828125\n",
      "2018-05-23T13:05:00.400947: step 6282, loss 0.343795, acc 0.890625\n",
      "2018-05-23T13:05:00.689178: step 6283, loss 0.369925, acc 0.890625\n",
      "2018-05-23T13:05:00.979399: step 6284, loss 0.284813, acc 0.921875\n",
      "2018-05-23T13:05:01.284585: step 6285, loss 0.314933, acc 0.828125\n",
      "2018-05-23T13:05:01.574807: step 6286, loss 0.246135, acc 0.90625\n",
      "2018-05-23T13:05:01.868024: step 6287, loss 0.2362, acc 0.890625\n",
      "2018-05-23T13:05:02.168221: step 6288, loss 0.277367, acc 0.875\n",
      "2018-05-23T13:05:02.463429: step 6289, loss 0.253497, acc 0.875\n",
      "2018-05-23T13:05:02.754650: step 6290, loss 0.260907, acc 0.90625\n",
      "2018-05-23T13:05:03.047867: step 6291, loss 0.207251, acc 0.90625\n",
      "2018-05-23T13:05:03.344073: step 6292, loss 0.149483, acc 0.953125\n",
      "2018-05-23T13:05:03.633300: step 6293, loss 0.325903, acc 0.859375\n",
      "2018-05-23T13:05:03.928510: step 6294, loss 0.317058, acc 0.84375\n",
      "2018-05-23T13:05:04.227710: step 6295, loss 0.21969, acc 0.921875\n",
      "2018-05-23T13:05:04.520927: step 6296, loss 0.349469, acc 0.875\n",
      "2018-05-23T13:05:04.812146: step 6297, loss 0.33742, acc 0.859375\n",
      "2018-05-23T13:05:05.104365: step 6298, loss 0.371966, acc 0.8125\n",
      "2018-05-23T13:05:05.406555: step 6299, loss 0.216553, acc 0.921875\n",
      "2018-05-23T13:05:05.697779: step 6300, loss 0.195051, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:05:09.464700: step 6300, loss 0.675923, acc 0.729819\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6300\n",
      "\n",
      "2018-05-23T13:05:10.799013: step 6301, loss 0.244734, acc 0.90625\n",
      "2018-05-23T13:05:11.101205: step 6302, loss 0.389963, acc 0.828125\n",
      "2018-05-23T13:05:11.402399: step 6303, loss 0.28902, acc 0.84375\n",
      "2018-05-23T13:05:11.699605: step 6304, loss 0.271819, acc 0.875\n",
      "2018-05-23T13:05:11.998044: step 6305, loss 0.255146, acc 0.859375\n",
      "2018-05-23T13:05:12.294251: step 6306, loss 0.319716, acc 0.859375\n",
      "2018-05-23T13:05:12.591455: step 6307, loss 0.277906, acc 0.921875\n",
      "2018-05-23T13:05:12.880681: step 6308, loss 0.253034, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:05:13.169908: step 6309, loss 0.147083, acc 0.96875\n",
      "2018-05-23T13:05:13.464122: step 6310, loss 0.225186, acc 0.875\n",
      "2018-05-23T13:05:13.755342: step 6311, loss 0.293034, acc 0.875\n",
      "2018-05-23T13:05:14.050551: step 6312, loss 0.224574, acc 0.90625\n",
      "2018-05-23T13:05:14.338781: step 6313, loss 0.264776, acc 0.859375\n",
      "2018-05-23T13:05:14.636985: step 6314, loss 0.23026, acc 0.890625\n",
      "2018-05-23T13:05:14.930199: step 6315, loss 0.301748, acc 0.84375\n",
      "2018-05-23T13:05:15.218429: step 6316, loss 0.273944, acc 0.890625\n",
      "2018-05-23T13:05:15.505659: step 6317, loss 0.345364, acc 0.921875\n",
      "2018-05-23T13:05:15.797878: step 6318, loss 0.290499, acc 0.90625\n",
      "2018-05-23T13:05:16.099071: step 6319, loss 0.258111, acc 0.890625\n",
      "2018-05-23T13:05:16.396278: step 6320, loss 0.275019, acc 0.875\n",
      "2018-05-23T13:05:16.717418: step 6321, loss 0.21323, acc 0.90625\n",
      "2018-05-23T13:05:17.012629: step 6322, loss 0.171464, acc 0.9375\n",
      "2018-05-23T13:05:17.302852: step 6323, loss 0.25159, acc 0.859375\n",
      "2018-05-23T13:05:17.598062: step 6324, loss 0.312036, acc 0.890625\n",
      "2018-05-23T13:05:17.884340: step 6325, loss 0.253972, acc 0.9375\n",
      "2018-05-23T13:05:18.172570: step 6326, loss 0.255127, acc 0.90625\n",
      "2018-05-23T13:05:18.465786: step 6327, loss 0.228395, acc 0.921875\n",
      "2018-05-23T13:05:18.757005: step 6328, loss 0.309305, acc 0.875\n",
      "2018-05-23T13:05:19.054212: step 6329, loss 0.236969, acc 0.875\n",
      "2018-05-23T13:05:19.341442: step 6330, loss 0.349328, acc 0.828125\n",
      "2018-05-23T13:05:19.637650: step 6331, loss 0.285216, acc 0.875\n",
      "2018-05-23T13:05:19.928870: step 6332, loss 0.227176, acc 0.9375\n",
      "2018-05-23T13:05:20.216107: step 6333, loss 0.449896, acc 0.796875\n",
      "2018-05-23T13:05:20.515303: step 6334, loss 0.315242, acc 0.8125\n",
      "2018-05-23T13:05:20.812510: step 6335, loss 0.261947, acc 0.875\n",
      "2018-05-23T13:05:21.104726: step 6336, loss 0.265782, acc 0.921875\n",
      "2018-05-23T13:05:21.401929: step 6337, loss 0.229769, acc 0.875\n",
      "2018-05-23T13:05:21.707113: step 6338, loss 0.231844, acc 0.90625\n",
      "2018-05-23T13:05:22.008308: step 6339, loss 0.243798, acc 0.890625\n",
      "2018-05-23T13:05:22.299529: step 6340, loss 0.47111, acc 0.828125\n",
      "2018-05-23T13:05:22.587757: step 6341, loss 0.335284, acc 0.875\n",
      "2018-05-23T13:05:22.879976: step 6342, loss 0.369089, acc 0.828125\n",
      "2018-05-23T13:05:23.172194: step 6343, loss 0.360322, acc 0.8125\n",
      "2018-05-23T13:05:23.467404: step 6344, loss 0.275447, acc 0.859375\n",
      "2018-05-23T13:05:23.756631: step 6345, loss 0.27071, acc 0.890625\n",
      "2018-05-23T13:05:24.053835: step 6346, loss 0.294881, acc 0.84375\n",
      "2018-05-23T13:05:24.346053: step 6347, loss 0.229059, acc 0.890625\n",
      "2018-05-23T13:05:24.633285: step 6348, loss 0.321401, acc 0.8125\n",
      "2018-05-23T13:05:24.931487: step 6349, loss 0.216522, acc 0.875\n",
      "2018-05-23T13:05:25.224703: step 6350, loss 0.363943, acc 0.890625\n",
      "2018-05-23T13:05:25.517920: step 6351, loss 0.286834, acc 0.859375\n",
      "2018-05-23T13:05:25.810137: step 6352, loss 0.27834, acc 0.890625\n",
      "2018-05-23T13:05:26.103352: step 6353, loss 0.274402, acc 0.890625\n",
      "2018-05-23T13:05:26.390584: step 6354, loss 0.226209, acc 0.9375\n",
      "2018-05-23T13:05:26.677815: step 6355, loss 0.231346, acc 0.90625\n",
      "2018-05-23T13:05:26.979011: step 6356, loss 0.309449, acc 0.828125\n",
      "2018-05-23T13:05:27.266242: step 6357, loss 0.282967, acc 0.890625\n",
      "2018-05-23T13:05:27.559698: step 6358, loss 0.274084, acc 0.859375\n",
      "2018-05-23T13:05:27.851917: step 6359, loss 0.202999, acc 0.953125\n",
      "2018-05-23T13:05:28.144136: step 6360, loss 0.199898, acc 0.921875\n",
      "2018-05-23T13:05:28.435355: step 6361, loss 0.258744, acc 0.921875\n",
      "2018-05-23T13:05:28.726578: step 6362, loss 0.194641, acc 0.9375\n",
      "2018-05-23T13:05:29.029147: step 6363, loss 0.375838, acc 0.84375\n",
      "2018-05-23T13:05:29.317374: step 6364, loss 0.217087, acc 0.921875\n",
      "2018-05-23T13:05:29.610589: step 6365, loss 0.306895, acc 0.84375\n",
      "2018-05-23T13:05:29.904803: step 6366, loss 0.358206, acc 0.859375\n",
      "2018-05-23T13:05:30.197022: step 6367, loss 0.240282, acc 0.890625\n",
      "2018-05-23T13:05:30.485250: step 6368, loss 0.350328, acc 0.875\n",
      "2018-05-23T13:05:30.773480: step 6369, loss 0.267864, acc 0.875\n",
      "2018-05-23T13:05:31.069686: step 6370, loss 0.255328, acc 0.890625\n",
      "2018-05-23T13:05:31.361905: step 6371, loss 0.211961, acc 0.921875\n",
      "2018-05-23T13:05:31.652129: step 6372, loss 0.160568, acc 0.921875\n",
      "2018-05-23T13:05:31.956314: step 6373, loss 0.226685, acc 0.890625\n",
      "2018-05-23T13:05:32.247536: step 6374, loss 0.317685, acc 0.890625\n",
      "2018-05-23T13:05:32.539754: step 6375, loss 0.336878, acc 0.859375\n",
      "2018-05-23T13:05:32.833969: step 6376, loss 0.194544, acc 0.921875\n",
      "2018-05-23T13:05:33.133208: step 6377, loss 0.359558, acc 0.828125\n",
      "2018-05-23T13:05:33.432366: step 6378, loss 0.306002, acc 0.859375\n",
      "2018-05-23T13:05:33.721592: step 6379, loss 0.34453, acc 0.796875\n",
      "2018-05-23T13:05:34.019795: step 6380, loss 0.194806, acc 0.90625\n",
      "2018-05-23T13:05:34.305031: step 6381, loss 0.298956, acc 0.828125\n",
      "2018-05-23T13:05:34.598247: step 6382, loss 0.435535, acc 0.75\n",
      "2018-05-23T13:05:34.887473: step 6383, loss 0.358377, acc 0.84375\n",
      "2018-05-23T13:05:35.180690: step 6384, loss 0.204523, acc 0.921875\n",
      "2018-05-23T13:05:35.514795: step 6385, loss 0.271469, acc 0.890625\n",
      "2018-05-23T13:05:35.805018: step 6386, loss 0.260459, acc 0.921875\n",
      "2018-05-23T13:05:36.108207: step 6387, loss 0.292821, acc 0.859375\n",
      "2018-05-23T13:05:36.397434: step 6388, loss 0.219326, acc 0.890625\n",
      "2018-05-23T13:05:36.689652: step 6389, loss 0.173828, acc 0.9375\n",
      "2018-05-23T13:05:37.007801: step 6390, loss 0.333095, acc 0.859375\n",
      "2018-05-23T13:05:37.307999: step 6391, loss 0.252343, acc 0.890625\n",
      "2018-05-23T13:05:37.600217: step 6392, loss 0.437959, acc 0.859375\n",
      "2018-05-23T13:05:37.894431: step 6393, loss 0.367699, acc 0.859375\n",
      "2018-05-23T13:05:38.186648: step 6394, loss 0.201955, acc 0.90625\n",
      "2018-05-23T13:05:38.473882: step 6395, loss 0.333806, acc 0.828125\n",
      "2018-05-23T13:05:38.763104: step 6396, loss 0.320911, acc 0.84375\n",
      "2018-05-23T13:05:39.056321: step 6397, loss 0.409326, acc 0.84375\n",
      "2018-05-23T13:05:39.352528: step 6398, loss 0.274877, acc 0.921875\n",
      "2018-05-23T13:05:39.643749: step 6399, loss 0.263019, acc 0.890625\n",
      "2018-05-23T13:05:39.940954: step 6400, loss 0.430542, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:05:43.666986: step 6400, loss 0.684507, acc 0.731962\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6400\n",
      "\n",
      "2018-05-23T13:05:44.733347: step 6401, loss 0.305976, acc 0.875\n",
      "2018-05-23T13:05:45.126295: step 6402, loss 0.237723, acc 0.9375\n",
      "2018-05-23T13:05:45.435468: step 6403, loss 0.31913, acc 0.859375\n",
      "2018-05-23T13:05:45.726688: step 6404, loss 0.312146, acc 0.875\n",
      "2018-05-23T13:05:46.014918: step 6405, loss 0.331941, acc 0.859375\n",
      "2018-05-23T13:05:46.308133: step 6406, loss 0.446081, acc 0.796875\n",
      "2018-05-23T13:05:46.607334: step 6407, loss 0.275603, acc 0.921875\n",
      "2018-05-23T13:05:46.902545: step 6408, loss 0.262485, acc 0.890625\n",
      "2018-05-23T13:05:47.196758: step 6409, loss 0.256953, acc 0.875\n",
      "2018-05-23T13:05:47.488975: step 6410, loss 0.284069, acc 0.875\n",
      "2018-05-23T13:05:47.781192: step 6411, loss 0.192528, acc 0.953125\n",
      "2018-05-23T13:05:48.078398: step 6412, loss 0.322992, acc 0.859375\n",
      "2018-05-23T13:05:48.382585: step 6413, loss 0.244301, acc 0.9375\n",
      "2018-05-23T13:05:48.678792: step 6414, loss 0.192225, acc 0.96875\n",
      "2018-05-23T13:05:48.974003: step 6415, loss 0.223555, acc 0.859375\n",
      "2018-05-23T13:05:49.267218: step 6416, loss 0.249592, acc 0.890625\n",
      "2018-05-23T13:05:49.562428: step 6417, loss 0.398494, acc 0.78125\n",
      "2018-05-23T13:05:49.854646: step 6418, loss 0.259944, acc 0.875\n",
      "2018-05-23T13:05:50.140777: step 6419, loss 0.343711, acc 0.8125\n",
      "2018-05-23T13:05:50.475881: step 6420, loss 0.305946, acc 0.84375\n",
      "2018-05-23T13:05:50.772089: step 6421, loss 0.417988, acc 0.828125\n",
      "2018-05-23T13:05:51.062313: step 6422, loss 0.196806, acc 0.921875\n",
      "2018-05-23T13:05:51.347550: step 6423, loss 0.234199, acc 0.890625\n",
      "2018-05-23T13:05:51.644754: step 6424, loss 0.276455, acc 0.890625\n",
      "2018-05-23T13:05:51.934978: step 6425, loss 0.149312, acc 0.96875\n",
      "2018-05-23T13:05:52.230188: step 6426, loss 0.277853, acc 0.859375\n",
      "2018-05-23T13:05:52.515427: step 6427, loss 0.270419, acc 0.890625\n",
      "2018-05-23T13:05:52.815622: step 6428, loss 0.234293, acc 0.9375\n",
      "2018-05-23T13:05:53.104848: step 6429, loss 0.324394, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:05:53.398064: step 6430, loss 0.275656, acc 0.859375\n",
      "2018-05-23T13:05:53.692277: step 6431, loss 0.229982, acc 0.890625\n",
      "2018-05-23T13:05:53.984495: step 6432, loss 0.377562, acc 0.78125\n",
      "2018-05-23T13:05:54.277710: step 6433, loss 0.338595, acc 0.859375\n",
      "2018-05-23T13:05:54.562949: step 6434, loss 0.295111, acc 0.859375\n",
      "2018-05-23T13:05:54.862148: step 6435, loss 0.220717, acc 0.890625\n",
      "2018-05-23T13:05:55.146387: step 6436, loss 0.18684, acc 0.953125\n",
      "2018-05-23T13:05:55.433618: step 6437, loss 0.180715, acc 0.921875\n",
      "2018-05-23T13:05:55.730824: step 6438, loss 0.325393, acc 0.84375\n",
      "2018-05-23T13:05:56.026033: step 6439, loss 0.387403, acc 0.859375\n",
      "2018-05-23T13:05:56.316257: step 6440, loss 0.419647, acc 0.828125\n",
      "2018-05-23T13:05:56.605484: step 6441, loss 0.367927, acc 0.875\n",
      "2018-05-23T13:05:56.902689: step 6442, loss 0.232781, acc 0.921875\n",
      "2018-05-23T13:05:57.189920: step 6443, loss 0.373243, acc 0.890625\n",
      "2018-05-23T13:05:57.480143: step 6444, loss 0.263306, acc 0.890625\n",
      "2018-05-23T13:05:57.770367: step 6445, loss 0.145139, acc 0.953125\n",
      "2018-05-23T13:05:58.063583: step 6446, loss 0.350115, acc 0.859375\n",
      "2018-05-23T13:05:58.348820: step 6447, loss 0.170025, acc 0.921875\n",
      "2018-05-23T13:05:58.636052: step 6448, loss 0.204974, acc 0.921875\n",
      "2018-05-23T13:05:58.932203: step 6449, loss 0.427912, acc 0.796875\n",
      "2018-05-23T13:05:59.222426: step 6450, loss 0.31151, acc 0.796875\n",
      "2018-05-23T13:05:59.508662: step 6451, loss 0.409492, acc 0.84375\n",
      "2018-05-23T13:05:59.799883: step 6452, loss 0.28081, acc 0.875\n",
      "2018-05-23T13:06:00.109055: step 6453, loss 0.193597, acc 0.90625\n",
      "2018-05-23T13:06:00.412201: step 6454, loss 0.2421, acc 0.875\n",
      "2018-05-23T13:06:00.756278: step 6455, loss 0.295414, acc 0.84375\n",
      "2018-05-23T13:06:01.052486: step 6456, loss 0.320411, acc 0.859375\n",
      "2018-05-23T13:06:01.352683: step 6457, loss 0.430393, acc 0.8125\n",
      "2018-05-23T13:06:01.684795: step 6458, loss 0.279672, acc 0.84375\n",
      "2018-05-23T13:06:02.005937: step 6459, loss 0.280328, acc 0.890625\n",
      "2018-05-23T13:06:02.315110: step 6460, loss 0.389566, acc 0.84375\n",
      "2018-05-23T13:06:02.603338: step 6461, loss 0.212997, acc 0.890625\n",
      "2018-05-23T13:06:02.929465: step 6462, loss 0.393994, acc 0.78125\n",
      "2018-05-23T13:06:03.243624: step 6463, loss 0.168605, acc 0.953125\n",
      "2018-05-23T13:06:03.533848: step 6464, loss 0.232049, acc 0.859375\n",
      "2018-05-23T13:06:03.824071: step 6465, loss 0.19403, acc 0.890625\n",
      "2018-05-23T13:06:04.122276: step 6466, loss 0.302033, acc 0.828125\n",
      "2018-05-23T13:06:04.408510: step 6467, loss 0.337117, acc 0.84375\n",
      "2018-05-23T13:06:04.705715: step 6468, loss 0.355962, acc 0.875\n",
      "2018-05-23T13:06:05.003915: step 6469, loss 0.327227, acc 0.90625\n",
      "2018-05-23T13:06:05.301121: step 6470, loss 0.27295, acc 0.859375\n",
      "2018-05-23T13:06:05.644202: step 6471, loss 0.30734, acc 0.859375\n",
      "2018-05-23T13:06:05.936420: step 6472, loss 0.324297, acc 0.859375\n",
      "2018-05-23T13:06:06.228639: step 6473, loss 0.331161, acc 0.859375\n",
      "2018-05-23T13:06:06.519860: step 6474, loss 0.337743, acc 0.875\n",
      "2018-05-23T13:06:06.810086: step 6475, loss 0.241097, acc 0.859375\n",
      "2018-05-23T13:06:07.103299: step 6476, loss 0.346329, acc 0.8125\n",
      "2018-05-23T13:06:07.398511: step 6477, loss 0.315085, acc 0.859375\n",
      "2018-05-23T13:06:07.690728: step 6478, loss 0.32482, acc 0.828125\n",
      "2018-05-23T13:06:07.984943: step 6479, loss 0.31608, acc 0.84375\n",
      "2018-05-23T13:06:08.279156: step 6480, loss 0.287067, acc 0.890625\n",
      "2018-05-23T13:06:08.573366: step 6481, loss 0.3704, acc 0.796875\n",
      "2018-05-23T13:06:08.865584: step 6482, loss 0.255002, acc 0.890625\n",
      "2018-05-23T13:06:09.163787: step 6483, loss 0.427624, acc 0.8125\n",
      "2018-05-23T13:06:09.460994: step 6484, loss 0.289765, acc 0.859375\n",
      "2018-05-23T13:06:09.760192: step 6485, loss 0.19935, acc 0.9375\n",
      "2018-05-23T13:06:10.055401: step 6486, loss 0.208853, acc 0.9375\n",
      "2018-05-23T13:06:10.342638: step 6487, loss 0.310429, acc 0.828125\n",
      "2018-05-23T13:06:10.629865: step 6488, loss 0.219657, acc 0.90625\n",
      "2018-05-23T13:06:10.931059: step 6489, loss 0.354758, acc 0.84375\n",
      "2018-05-23T13:06:11.226270: step 6490, loss 0.333969, acc 0.828125\n",
      "2018-05-23T13:06:11.524471: step 6491, loss 0.224321, acc 0.890625\n",
      "2018-05-23T13:06:11.812702: step 6492, loss 0.304077, acc 0.890625\n",
      "2018-05-23T13:06:12.106913: step 6493, loss 0.317492, acc 0.84375\n",
      "2018-05-23T13:06:12.403121: step 6494, loss 0.283623, acc 0.859375\n",
      "2018-05-23T13:06:12.696336: step 6495, loss 0.346244, acc 0.890625\n",
      "2018-05-23T13:06:12.987557: step 6496, loss 0.443702, acc 0.796875\n",
      "2018-05-23T13:06:13.275787: step 6497, loss 0.297451, acc 0.84375\n",
      "2018-05-23T13:06:13.585957: step 6498, loss 0.246951, acc 0.9375\n",
      "2018-05-23T13:06:13.883161: step 6499, loss 0.258008, acc 0.921875\n",
      "2018-05-23T13:06:14.172388: step 6500, loss 0.469608, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:06:17.890442: step 6500, loss 0.67606, acc 0.72939\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6500\n",
      "\n",
      "2018-05-23T13:06:18.848876: step 6501, loss 0.190206, acc 0.90625\n",
      "2018-05-23T13:06:19.253793: step 6502, loss 0.428185, acc 0.921875\n",
      "2018-05-23T13:06:19.545014: step 6503, loss 0.212339, acc 0.921875\n",
      "2018-05-23T13:06:19.854186: step 6504, loss 0.20259, acc 0.921875\n",
      "2018-05-23T13:06:20.147404: step 6505, loss 0.232314, acc 0.921875\n",
      "2018-05-23T13:06:20.448597: step 6506, loss 0.406807, acc 0.828125\n",
      "2018-05-23T13:06:20.740814: step 6507, loss 0.3077, acc 0.859375\n",
      "2018-05-23T13:06:21.031040: step 6508, loss 0.267041, acc 0.890625\n",
      "2018-05-23T13:06:21.318270: step 6509, loss 0.240889, acc 0.921875\n",
      "2018-05-23T13:06:21.612485: step 6510, loss 0.258842, acc 0.890625\n",
      "2018-05-23T13:06:21.919663: step 6511, loss 0.261132, acc 0.875\n",
      "2018-05-23T13:06:22.211880: step 6512, loss 0.399376, acc 0.84375\n",
      "2018-05-23T13:06:22.509087: step 6513, loss 0.461962, acc 0.78125\n",
      "2018-05-23T13:06:22.804294: step 6514, loss 0.428979, acc 0.828125\n",
      "2018-05-23T13:06:23.100504: step 6515, loss 0.262564, acc 0.84375\n",
      "2018-05-23T13:06:23.386737: step 6516, loss 0.394465, acc 0.890625\n",
      "2018-05-23T13:06:23.677958: step 6517, loss 0.297483, acc 0.859375\n",
      "2018-05-23T13:06:23.980151: step 6518, loss 0.287937, acc 0.828125\n",
      "2018-05-23T13:06:24.279349: step 6519, loss 0.0925562, acc 0.984375\n",
      "2018-05-23T13:06:24.569574: step 6520, loss 0.257187, acc 0.859375\n",
      "2018-05-23T13:06:24.868772: step 6521, loss 0.291788, acc 0.859375\n",
      "2018-05-23T13:06:25.159992: step 6522, loss 0.225514, acc 0.875\n",
      "2018-05-23T13:06:25.450217: step 6523, loss 0.341738, acc 0.859375\n",
      "2018-05-23T13:06:25.736452: step 6524, loss 0.189364, acc 0.921875\n",
      "2018-05-23T13:06:26.033655: step 6525, loss 0.28401, acc 0.90625\n",
      "2018-05-23T13:06:26.326870: step 6526, loss 0.306891, acc 0.84375\n",
      "2018-05-23T13:06:26.615101: step 6527, loss 0.234749, acc 0.90625\n",
      "2018-05-23T13:06:26.913303: step 6528, loss 0.216307, acc 0.9375\n",
      "2018-05-23T13:06:27.205520: step 6529, loss 0.313233, acc 0.859375\n",
      "2018-05-23T13:06:27.492752: step 6530, loss 0.347184, acc 0.90625\n",
      "2018-05-23T13:06:27.789998: step 6531, loss 0.243112, acc 0.890625\n",
      "2018-05-23T13:06:28.087162: step 6532, loss 0.3715, acc 0.84375\n",
      "2018-05-23T13:06:28.379382: step 6533, loss 0.132937, acc 0.953125\n",
      "2018-05-23T13:06:28.670603: step 6534, loss 0.379117, acc 0.765625\n",
      "2018-05-23T13:06:28.968803: step 6535, loss 0.26454, acc 0.859375\n",
      "2018-05-23T13:06:29.261023: step 6536, loss 0.364598, acc 0.875\n",
      "2018-05-23T13:06:29.572191: step 6537, loss 0.322832, acc 0.859375\n",
      "2018-05-23T13:06:29.875379: step 6538, loss 0.2115, acc 0.953125\n",
      "2018-05-23T13:06:30.171587: step 6539, loss 0.520879, acc 0.703125\n",
      "2018-05-23T13:06:30.468792: step 6540, loss 0.233902, acc 0.890625\n",
      "2018-05-23T13:06:30.758017: step 6541, loss 0.275281, acc 0.84375\n",
      "2018-05-23T13:06:31.058214: step 6542, loss 0.397851, acc 0.828125\n",
      "2018-05-23T13:06:31.346445: step 6543, loss 0.278347, acc 0.9375\n",
      "2018-05-23T13:06:31.638662: step 6544, loss 0.400597, acc 0.875\n",
      "2018-05-23T13:06:31.934870: step 6545, loss 0.239298, acc 0.84375\n",
      "2018-05-23T13:06:32.232074: step 6546, loss 0.288482, acc 0.859375\n",
      "2018-05-23T13:06:32.590117: step 6547, loss 0.163162, acc 0.953125\n",
      "2018-05-23T13:06:32.907268: step 6548, loss 0.353621, acc 0.875\n",
      "2018-05-23T13:06:33.228408: step 6549, loss 0.23581, acc 0.90625\n",
      "2018-05-23T13:06:33.525614: step 6550, loss 0.260202, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:06:33.819826: step 6551, loss 0.222199, acc 0.921875\n",
      "2018-05-23T13:06:34.119027: step 6552, loss 0.378632, acc 0.8125\n",
      "2018-05-23T13:06:34.415233: step 6553, loss 0.215342, acc 0.9375\n",
      "2018-05-23T13:06:34.715431: step 6554, loss 0.220911, acc 0.890625\n",
      "2018-05-23T13:06:35.017621: step 6555, loss 0.38626, acc 0.859375\n",
      "2018-05-23T13:06:35.318818: step 6556, loss 0.312041, acc 0.828125\n",
      "2018-05-23T13:06:35.637962: step 6557, loss 0.291962, acc 0.859375\n",
      "2018-05-23T13:06:35.945141: step 6558, loss 0.125964, acc 0.9375\n",
      "2018-05-23T13:06:36.241348: step 6559, loss 0.328831, acc 0.84375\n",
      "2018-05-23T13:06:36.533566: step 6560, loss 0.260492, acc 0.875\n",
      "2018-05-23T13:06:36.832767: step 6561, loss 0.346844, acc 0.828125\n",
      "2018-05-23T13:06:37.135956: step 6562, loss 0.217514, acc 0.9375\n",
      "2018-05-23T13:06:37.438147: step 6563, loss 0.287202, acc 0.890625\n",
      "2018-05-23T13:06:37.733357: step 6564, loss 0.388497, acc 0.8125\n",
      "2018-05-23T13:06:38.030561: step 6565, loss 0.291245, acc 0.84375\n",
      "2018-05-23T13:06:38.334749: step 6566, loss 0.217905, acc 0.90625\n",
      "2018-05-23T13:06:38.631952: step 6567, loss 0.267645, acc 0.8125\n",
      "2018-05-23T13:06:38.933146: step 6568, loss 0.334117, acc 0.8125\n",
      "2018-05-23T13:06:39.231349: step 6569, loss 0.300465, acc 0.78125\n",
      "2018-05-23T13:06:39.537529: step 6570, loss 0.32028, acc 0.859375\n",
      "2018-05-23T13:06:39.840719: step 6571, loss 0.164318, acc 0.921875\n",
      "2018-05-23T13:06:40.141913: step 6572, loss 0.398294, acc 0.828125\n",
      "2018-05-23T13:06:40.441112: step 6573, loss 0.26365, acc 0.828125\n",
      "2018-05-23T13:06:40.737337: step 6574, loss 0.237923, acc 0.875\n",
      "2018-05-23T13:06:41.039572: step 6575, loss 0.374558, acc 0.828125\n",
      "2018-05-23T13:06:41.337714: step 6576, loss 0.294829, acc 0.859375\n",
      "2018-05-23T13:06:41.638909: step 6577, loss 0.310969, acc 0.890625\n",
      "2018-05-23T13:06:41.937112: step 6578, loss 0.417133, acc 0.765625\n",
      "2018-05-23T13:06:42.233319: step 6579, loss 0.463997, acc 0.78125\n",
      "2018-05-23T13:06:42.536509: step 6580, loss 0.174372, acc 0.9375\n",
      "2018-05-23T13:06:42.835706: step 6581, loss 0.274454, acc 0.890625\n",
      "2018-05-23T13:06:43.136902: step 6582, loss 0.195377, acc 0.953125\n",
      "2018-05-23T13:06:43.433889: step 6583, loss 0.251991, acc 0.859375\n",
      "2018-05-23T13:06:43.740071: step 6584, loss 0.412051, acc 0.859375\n",
      "2018-05-23T13:06:44.035282: step 6585, loss 0.269179, acc 0.875\n",
      "2018-05-23T13:06:44.331488: step 6586, loss 0.182003, acc 0.921875\n",
      "2018-05-23T13:06:44.630688: step 6587, loss 0.210065, acc 0.90625\n",
      "2018-05-23T13:06:44.931882: step 6588, loss 0.333988, acc 0.796875\n",
      "2018-05-23T13:06:45.228090: step 6589, loss 0.380812, acc 0.875\n",
      "2018-05-23T13:06:45.524299: step 6590, loss 0.296025, acc 0.828125\n",
      "2018-05-23T13:06:45.829482: step 6591, loss 0.221443, acc 0.90625\n",
      "2018-05-23T13:06:46.133667: step 6592, loss 0.193536, acc 0.890625\n",
      "2018-05-23T13:06:46.431871: step 6593, loss 0.350635, acc 0.84375\n",
      "2018-05-23T13:06:46.747026: step 6594, loss 0.458481, acc 0.84375\n",
      "2018-05-23T13:06:47.046226: step 6595, loss 0.241938, acc 0.875\n",
      "2018-05-23T13:06:47.339444: step 6596, loss 0.321573, acc 0.828125\n",
      "2018-05-23T13:06:47.649613: step 6597, loss 0.252943, acc 0.859375\n",
      "2018-05-23T13:06:47.945820: step 6598, loss 0.269899, acc 0.859375\n",
      "2018-05-23T13:06:48.246018: step 6599, loss 0.378122, acc 0.859375\n",
      "2018-05-23T13:06:48.540229: step 6600, loss 0.220862, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:06:52.333083: step 6600, loss 0.678824, acc 0.731104\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6600\n",
      "\n",
      "2018-05-23T13:06:53.389682: step 6601, loss 0.426522, acc 0.796875\n",
      "2018-05-23T13:06:53.821526: step 6602, loss 0.339804, acc 0.84375\n",
      "2018-05-23T13:06:54.130700: step 6603, loss 0.36807, acc 0.890625\n",
      "2018-05-23T13:06:54.430897: step 6604, loss 0.179983, acc 0.921875\n",
      "2018-05-23T13:06:54.725109: step 6605, loss 0.351234, acc 0.859375\n",
      "2018-05-23T13:06:55.028297: step 6606, loss 0.273913, acc 0.84375\n",
      "2018-05-23T13:06:55.322510: step 6607, loss 0.334444, acc 0.875\n",
      "2018-05-23T13:06:55.619715: step 6608, loss 0.262256, acc 0.875\n",
      "2018-05-23T13:06:55.916920: step 6609, loss 0.247204, acc 0.890625\n",
      "2018-05-23T13:06:56.214127: step 6610, loss 0.313037, acc 0.90625\n",
      "2018-05-23T13:06:56.511332: step 6611, loss 0.361114, acc 0.90625\n",
      "2018-05-23T13:06:56.804547: step 6612, loss 0.243929, acc 0.890625\n",
      "2018-05-23T13:06:57.110726: step 6613, loss 0.293033, acc 0.90625\n",
      "2018-05-23T13:06:57.409927: step 6614, loss 0.203051, acc 0.9375\n",
      "2018-05-23T13:06:57.711121: step 6615, loss 0.337602, acc 0.8125\n",
      "2018-05-23T13:06:58.015308: step 6616, loss 0.280196, acc 0.875\n",
      "2018-05-23T13:06:58.315347: step 6617, loss 0.220417, acc 0.875\n",
      "2018-05-23T13:06:58.614548: step 6618, loss 0.298634, acc 0.84375\n",
      "2018-05-23T13:06:58.917737: step 6619, loss 0.309014, acc 0.890625\n",
      "2018-05-23T13:06:59.219927: step 6620, loss 0.227044, acc 0.859375\n",
      "2018-05-23T13:06:59.521122: step 6621, loss 0.117956, acc 0.953125\n",
      "2018-05-23T13:06:59.893126: step 6622, loss 0.26013, acc 0.90625\n",
      "2018-05-23T13:07:00.219254: step 6623, loss 0.270309, acc 0.859375\n",
      "2018-05-23T13:07:00.523440: step 6624, loss 0.250428, acc 0.953125\n",
      "2018-05-23T13:07:00.832613: step 6625, loss 0.32417, acc 0.875\n",
      "2018-05-23T13:07:01.145775: step 6626, loss 0.320012, acc 0.875\n",
      "2018-05-23T13:07:01.439989: step 6627, loss 0.315632, acc 0.875\n",
      "2018-05-23T13:07:01.736195: step 6628, loss 0.237283, acc 0.859375\n",
      "2018-05-23T13:07:02.047365: step 6629, loss 0.286916, acc 0.859375\n",
      "2018-05-23T13:07:02.351550: step 6630, loss 0.313458, acc 0.828125\n",
      "2018-05-23T13:07:02.643767: step 6631, loss 0.396678, acc 0.8125\n",
      "2018-05-23T13:07:02.942967: step 6632, loss 0.326043, acc 0.828125\n",
      "2018-05-23T13:07:03.244161: step 6633, loss 0.283688, acc 0.921875\n",
      "2018-05-23T13:07:03.546353: step 6634, loss 0.328602, acc 0.859375\n",
      "2018-05-23T13:07:03.846569: step 6635, loss 0.322037, acc 0.796875\n",
      "2018-05-23T13:07:04.145771: step 6636, loss 0.171313, acc 0.96875\n",
      "2018-05-23T13:07:04.442975: step 6637, loss 0.307832, acc 0.875\n",
      "2018-05-23T13:07:04.742175: step 6638, loss 0.426634, acc 0.78125\n",
      "2018-05-23T13:07:05.041375: step 6639, loss 0.28485, acc 0.859375\n",
      "2018-05-23T13:07:05.338578: step 6640, loss 0.288547, acc 0.8125\n",
      "2018-05-23T13:07:05.650743: step 6641, loss 0.217045, acc 0.921875\n",
      "2018-05-23T13:07:05.951937: step 6642, loss 0.378307, acc 0.828125\n",
      "2018-05-23T13:07:06.243158: step 6643, loss 0.238265, acc 0.875\n",
      "2018-05-23T13:07:06.548342: step 6644, loss 0.280778, acc 0.859375\n",
      "2018-05-23T13:07:06.850533: step 6645, loss 0.24358, acc 0.9375\n",
      "2018-05-23T13:07:07.145744: step 6646, loss 0.201228, acc 0.921875\n",
      "2018-05-23T13:07:07.445941: step 6647, loss 0.331331, acc 0.90625\n",
      "2018-05-23T13:07:07.750129: step 6648, loss 0.30701, acc 0.875\n",
      "2018-05-23T13:07:08.053316: step 6649, loss 0.388482, acc 0.84375\n",
      "2018-05-23T13:07:08.345535: step 6650, loss 0.335283, acc 0.8125\n",
      "2018-05-23T13:07:08.644735: step 6651, loss 0.309826, acc 0.859375\n",
      "2018-05-23T13:07:08.944931: step 6652, loss 0.266915, acc 0.859375\n",
      "2018-05-23T13:07:09.235154: step 6653, loss 0.231633, acc 0.9375\n",
      "2018-05-23T13:07:09.535352: step 6654, loss 0.284433, acc 0.875\n",
      "2018-05-23T13:07:09.829566: step 6655, loss 0.379149, acc 0.84375\n",
      "2018-05-23T13:07:10.132753: step 6656, loss 0.256775, acc 0.890625\n",
      "2018-05-23T13:07:10.430955: step 6657, loss 0.282737, acc 0.859375\n",
      "2018-05-23T13:07:10.732151: step 6658, loss 0.2515, acc 0.90625\n",
      "2018-05-23T13:07:11.037332: step 6659, loss 0.372541, acc 0.875\n",
      "2018-05-23T13:07:11.331548: step 6660, loss 0.250309, acc 0.890625\n",
      "2018-05-23T13:07:11.630745: step 6661, loss 0.28442, acc 0.921875\n",
      "2018-05-23T13:07:11.926954: step 6662, loss 0.474452, acc 0.765625\n",
      "2018-05-23T13:07:12.223162: step 6663, loss 0.27678, acc 0.859375\n",
      "2018-05-23T13:07:12.537320: step 6664, loss 0.263646, acc 0.90625\n",
      "2018-05-23T13:07:12.842503: step 6665, loss 0.134364, acc 0.9375\n",
      "2018-05-23T13:07:13.140706: step 6666, loss 0.295854, acc 0.921875\n",
      "2018-05-23T13:07:13.438908: step 6667, loss 0.31091, acc 0.859375\n",
      "2018-05-23T13:07:13.737112: step 6668, loss 0.35269, acc 0.859375\n",
      "2018-05-23T13:07:14.044289: step 6669, loss 0.27845, acc 0.90625\n",
      "2018-05-23T13:07:14.342491: step 6670, loss 0.479989, acc 0.796875\n",
      "2018-05-23T13:07:14.642689: step 6671, loss 0.282281, acc 0.90625\n",
      "2018-05-23T13:07:14.938896: step 6672, loss 0.250676, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:07:15.234105: step 6673, loss 0.37001, acc 0.78125\n",
      "2018-05-23T13:07:15.529317: step 6674, loss 0.409663, acc 0.78125\n",
      "2018-05-23T13:07:15.837491: step 6675, loss 0.322945, acc 0.828125\n",
      "2018-05-23T13:07:16.145668: step 6676, loss 0.232359, acc 0.890625\n",
      "2018-05-23T13:07:16.441876: step 6677, loss 0.287614, acc 0.890625\n",
      "2018-05-23T13:07:16.768999: step 6678, loss 0.246626, acc 0.890625\n",
      "2018-05-23T13:07:17.070195: step 6679, loss 0.409083, acc 0.828125\n",
      "2018-05-23T13:07:17.367399: step 6680, loss 0.271788, acc 0.859375\n",
      "2018-05-23T13:07:17.668595: step 6681, loss 0.303753, acc 0.875\n",
      "2018-05-23T13:07:17.972779: step 6682, loss 0.240942, acc 0.921875\n",
      "2018-05-23T13:07:18.313867: step 6683, loss 0.308648, acc 0.84375\n",
      "2018-05-23T13:07:18.610075: step 6684, loss 0.292865, acc 0.859375\n",
      "2018-05-23T13:07:18.911270: step 6685, loss 0.347054, acc 0.859375\n",
      "2018-05-23T13:07:19.209472: step 6686, loss 0.418246, acc 0.84375\n",
      "2018-05-23T13:07:19.504681: step 6687, loss 0.278533, acc 0.890625\n",
      "2018-05-23T13:07:19.800890: step 6688, loss 0.230124, acc 0.921875\n",
      "2018-05-23T13:07:20.109065: step 6689, loss 0.355115, acc 0.796875\n",
      "2018-05-23T13:07:20.418240: step 6690, loss 0.359405, acc 0.84375\n",
      "2018-05-23T13:07:20.719432: step 6691, loss 0.348701, acc 0.875\n",
      "2018-05-23T13:07:21.031598: step 6692, loss 0.373991, acc 0.8125\n",
      "2018-05-23T13:07:21.330798: step 6693, loss 0.300601, acc 0.90625\n",
      "2018-05-23T13:07:21.626007: step 6694, loss 0.171321, acc 0.9375\n",
      "2018-05-23T13:07:21.935235: step 6695, loss 0.275016, acc 0.859375\n",
      "2018-05-23T13:07:22.231487: step 6696, loss 0.431147, acc 0.8125\n",
      "2018-05-23T13:07:22.526653: step 6697, loss 0.243123, acc 0.890625\n",
      "2018-05-23T13:07:22.815881: step 6698, loss 0.465354, acc 0.8125\n",
      "2018-05-23T13:07:23.119072: step 6699, loss 0.215283, acc 0.90625\n",
      "2018-05-23T13:07:23.418268: step 6700, loss 0.344399, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:07:27.587116: step 6700, loss 0.664788, acc 0.733391\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6700\n",
      "\n",
      "2018-05-23T13:07:29.385295: step 6701, loss 0.308282, acc 0.890625\n",
      "2018-05-23T13:07:29.701450: step 6702, loss 0.336706, acc 0.859375\n",
      "2018-05-23T13:07:30.036552: step 6703, loss 0.374491, acc 0.828125\n",
      "2018-05-23T13:07:30.383624: step 6704, loss 0.312539, acc 0.890625\n",
      "2018-05-23T13:07:30.713750: step 6705, loss 0.29912, acc 0.890625\n",
      "2018-05-23T13:07:31.042861: step 6706, loss 0.276172, acc 0.90625\n",
      "2018-05-23T13:07:31.381953: step 6707, loss 0.277725, acc 0.8125\n",
      "2018-05-23T13:07:31.776897: step 6708, loss 0.335764, acc 0.859375\n",
      "2018-05-23T13:07:32.128955: step 6709, loss 0.149034, acc 0.96875\n",
      "2018-05-23T13:07:32.472040: step 6710, loss 0.50984, acc 0.8125\n",
      "2018-05-23T13:07:32.804148: step 6711, loss 0.448976, acc 0.859375\n",
      "2018-05-23T13:07:33.135263: step 6712, loss 0.265744, acc 0.90625\n",
      "2018-05-23T13:07:33.499289: step 6713, loss 0.286728, acc 0.90625\n",
      "2018-05-23T13:07:33.830404: step 6714, loss 0.306244, acc 0.890625\n",
      "2018-05-23T13:07:34.155536: step 6715, loss 0.195462, acc 0.921875\n",
      "2018-05-23T13:07:34.489642: step 6716, loss 0.227556, acc 0.90625\n",
      "2018-05-23T13:07:34.820754: step 6717, loss 0.278587, acc 0.875\n",
      "2018-05-23T13:07:35.154862: step 6718, loss 0.259843, acc 0.890625\n",
      "2018-05-23T13:07:35.492956: step 6719, loss 0.329699, acc 0.84375\n",
      "2018-05-23T13:07:35.822075: step 6720, loss 0.265371, acc 0.859375\n",
      "2018-05-23T13:07:36.158176: step 6721, loss 0.376856, acc 0.828125\n",
      "2018-05-23T13:07:36.490289: step 6722, loss 0.355738, acc 0.8125\n",
      "2018-05-23T13:07:36.823398: step 6723, loss 0.253817, acc 0.875\n",
      "2018-05-23T13:07:37.163582: step 6724, loss 0.315859, acc 0.84375\n",
      "2018-05-23T13:07:37.501677: step 6725, loss 0.235654, acc 0.9375\n",
      "2018-05-23T13:07:37.833789: step 6726, loss 0.457048, acc 0.84375\n",
      "2018-05-23T13:07:38.160912: step 6727, loss 0.181451, acc 0.9375\n",
      "2018-05-23T13:07:38.487042: step 6728, loss 0.300003, acc 0.859375\n",
      "2018-05-23T13:07:38.816161: step 6729, loss 0.262068, acc 0.875\n",
      "2018-05-23T13:07:39.149301: step 6730, loss 0.433057, acc 0.828125\n",
      "2018-05-23T13:07:39.476393: step 6731, loss 0.314734, acc 0.8125\n",
      "2018-05-23T13:07:39.807507: step 6732, loss 0.261482, acc 0.921875\n",
      "2018-05-23T13:07:40.132640: step 6733, loss 0.28243, acc 0.84375\n",
      "2018-05-23T13:07:40.452782: step 6734, loss 0.366132, acc 0.84375\n",
      "2018-05-23T13:07:40.775917: step 6735, loss 0.392431, acc 0.828125\n",
      "2018-05-23T13:07:41.106034: step 6736, loss 0.365384, acc 0.84375\n",
      "2018-05-23T13:07:41.435153: step 6737, loss 0.346336, acc 0.84375\n",
      "2018-05-23T13:07:41.764275: step 6738, loss 0.358139, acc 0.796875\n",
      "2018-05-23T13:07:42.088406: step 6739, loss 0.334077, acc 0.875\n",
      "2018-05-23T13:07:42.411541: step 6740, loss 0.353937, acc 0.875\n",
      "2018-05-23T13:07:42.737671: step 6741, loss 0.406907, acc 0.765625\n",
      "2018-05-23T13:07:43.063798: step 6742, loss 0.403753, acc 0.78125\n",
      "2018-05-23T13:07:43.386932: step 6743, loss 0.272958, acc 0.875\n",
      "2018-05-23T13:07:43.706078: step 6744, loss 0.30854, acc 0.890625\n",
      "2018-05-23T13:07:44.037192: step 6745, loss 0.380961, acc 0.84375\n",
      "2018-05-23T13:07:44.362322: step 6746, loss 0.239228, acc 0.90625\n",
      "2018-05-23T13:07:44.685458: step 6747, loss 0.341481, acc 0.828125\n",
      "2018-05-23T13:07:45.012583: step 6748, loss 0.338099, acc 0.875\n",
      "2018-05-23T13:07:45.339709: step 6749, loss 0.181794, acc 0.9375\n",
      "2018-05-23T13:07:45.672819: step 6750, loss 0.221453, acc 0.90625\n",
      "2018-05-23T13:07:46.003931: step 6751, loss 0.289403, acc 0.84375\n",
      "2018-05-23T13:07:46.329063: step 6752, loss 0.405051, acc 0.859375\n",
      "2018-05-23T13:07:46.650202: step 6753, loss 0.217517, acc 0.875\n",
      "2018-05-23T13:07:46.980319: step 6754, loss 0.493815, acc 0.828125\n",
      "2018-05-23T13:07:47.316421: step 6755, loss 0.222768, acc 0.90625\n",
      "2018-05-23T13:07:47.638559: step 6756, loss 0.306479, acc 0.859375\n",
      "2018-05-23T13:07:47.964685: step 6757, loss 0.337652, acc 0.84375\n",
      "2018-05-23T13:07:48.354642: step 6758, loss 0.318344, acc 0.890625\n",
      "2018-05-23T13:07:48.684761: step 6759, loss 0.383275, acc 0.828125\n",
      "2018-05-23T13:07:49.016870: step 6760, loss 0.320796, acc 0.875\n",
      "2018-05-23T13:07:49.335019: step 6761, loss 0.278689, acc 0.828125\n",
      "2018-05-23T13:07:49.663143: step 6762, loss 0.310351, acc 0.828125\n",
      "2018-05-23T13:07:49.990266: step 6763, loss 0.336808, acc 0.859375\n",
      "2018-05-23T13:07:50.309413: step 6764, loss 0.492067, acc 0.796875\n",
      "2018-05-23T13:07:50.636538: step 6765, loss 0.398571, acc 0.828125\n",
      "2018-05-23T13:07:50.957680: step 6766, loss 0.184571, acc 0.921875\n",
      "2018-05-23T13:07:51.283806: step 6767, loss 0.351533, acc 0.828125\n",
      "2018-05-23T13:07:51.600958: step 6768, loss 0.389053, acc 0.859375\n",
      "2018-05-23T13:07:51.923096: step 6769, loss 0.349115, acc 0.828125\n",
      "2018-05-23T13:07:52.247229: step 6770, loss 0.206751, acc 0.9375\n",
      "2018-05-23T13:07:52.572361: step 6771, loss 0.268605, acc 0.859375\n",
      "2018-05-23T13:07:52.891508: step 6772, loss 0.318008, acc 0.859375\n",
      "2018-05-23T13:07:53.215638: step 6773, loss 0.239461, acc 0.859375\n",
      "2018-05-23T13:07:53.527803: step 6774, loss 0.21686, acc 0.890625\n",
      "2018-05-23T13:07:53.852936: step 6775, loss 0.285074, acc 0.828125\n",
      "2018-05-23T13:07:54.181055: step 6776, loss 0.368377, acc 0.8125\n",
      "2018-05-23T13:07:54.503193: step 6777, loss 0.323638, acc 0.84375\n",
      "2018-05-23T13:07:54.827327: step 6778, loss 0.294104, acc 0.875\n",
      "2018-05-23T13:07:55.156446: step 6779, loss 0.459983, acc 0.78125\n",
      "2018-05-23T13:07:55.481576: step 6780, loss 0.225745, acc 0.90625\n",
      "2018-05-23T13:07:55.795737: step 6781, loss 0.323363, acc 0.890625\n",
      "2018-05-23T13:07:56.119870: step 6782, loss 0.257559, acc 0.90625\n",
      "2018-05-23T13:07:56.435025: step 6783, loss 0.284172, acc 0.875\n",
      "2018-05-23T13:07:56.756167: step 6784, loss 0.431389, acc 0.859375\n",
      "2018-05-23T13:07:57.086285: step 6785, loss 0.278298, acc 0.859375\n",
      "2018-05-23T13:07:57.419393: step 6786, loss 0.254704, acc 0.90625\n",
      "2018-05-23T13:07:57.736545: step 6787, loss 0.216821, acc 0.96875\n",
      "2018-05-23T13:07:58.069652: step 6788, loss 0.32787, acc 0.8125\n",
      "2018-05-23T13:07:58.388799: step 6789, loss 0.251502, acc 0.921875\n",
      "2018-05-23T13:07:58.707945: step 6790, loss 0.25418, acc 0.84375\n",
      "2018-05-23T13:07:59.044048: step 6791, loss 0.246096, acc 0.875\n",
      "2018-05-23T13:07:59.366185: step 6792, loss 0.261194, acc 0.890625\n",
      "2018-05-23T13:07:59.687326: step 6793, loss 0.267401, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:08:00.038388: step 6794, loss 0.405164, acc 0.84375\n",
      "2018-05-23T13:08:00.359529: step 6795, loss 0.256681, acc 0.890625\n",
      "2018-05-23T13:08:00.727542: step 6796, loss 0.273999, acc 0.859375\n",
      "2018-05-23T13:08:01.066635: step 6797, loss 0.231274, acc 0.875\n",
      "2018-05-23T13:08:01.402736: step 6798, loss 0.303067, acc 0.796875\n",
      "2018-05-23T13:08:01.779727: step 6799, loss 0.283115, acc 0.859375\n",
      "2018-05-23T13:08:02.114832: step 6800, loss 0.294047, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:08:06.464196: step 6800, loss 0.661148, acc 0.72739\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6800\n",
      "\n",
      "2018-05-23T13:08:07.590990: step 6801, loss 0.37824, acc 0.796875\n",
      "2018-05-23T13:08:08.053750: step 6802, loss 0.318017, acc 0.875\n",
      "2018-05-23T13:08:08.396867: step 6803, loss 0.319254, acc 0.8125\n",
      "2018-05-23T13:08:08.724955: step 6804, loss 0.253982, acc 0.859375\n",
      "2018-05-23T13:08:09.040111: step 6805, loss 0.356998, acc 0.828125\n",
      "2018-05-23T13:08:09.359291: step 6806, loss 0.215714, acc 0.875\n",
      "2018-05-23T13:08:09.671422: step 6807, loss 0.280681, acc 0.875\n",
      "2018-05-23T13:08:09.982589: step 6808, loss 0.316374, acc 0.875\n",
      "2018-05-23T13:08:10.299741: step 6809, loss 0.255426, acc 0.890625\n",
      "2018-05-23T13:08:10.608917: step 6810, loss 0.275983, acc 0.875\n",
      "2018-05-23T13:08:10.925071: step 6811, loss 0.326814, acc 0.84375\n",
      "2018-05-23T13:08:11.238233: step 6812, loss 0.280087, acc 0.921875\n",
      "2018-05-23T13:08:11.553387: step 6813, loss 0.387065, acc 0.859375\n",
      "2018-05-23T13:08:11.872534: step 6814, loss 0.218472, acc 0.90625\n",
      "2018-05-23T13:08:12.192678: step 6815, loss 0.371414, acc 0.859375\n",
      "2018-05-23T13:08:12.509830: step 6816, loss 0.245619, acc 0.890625\n",
      "2018-05-23T13:08:12.818004: step 6817, loss 0.370991, acc 0.796875\n",
      "2018-05-23T13:08:13.128177: step 6818, loss 0.229799, acc 0.890625\n",
      "2018-05-23T13:08:13.441337: step 6819, loss 0.292307, acc 0.875\n",
      "2018-05-23T13:08:13.747518: step 6820, loss 0.297863, acc 0.875\n",
      "2018-05-23T13:08:14.056693: step 6821, loss 0.189384, acc 0.90625\n",
      "2018-05-23T13:08:14.367859: step 6822, loss 0.267353, acc 0.875\n",
      "2018-05-23T13:08:14.681021: step 6823, loss 0.464923, acc 0.75\n",
      "2018-05-23T13:08:14.986204: step 6824, loss 0.451948, acc 0.78125\n",
      "2018-05-23T13:08:15.297372: step 6825, loss 0.227339, acc 0.890625\n",
      "2018-05-23T13:08:15.610534: step 6826, loss 0.289399, acc 0.875\n",
      "2018-05-23T13:08:15.920704: step 6827, loss 0.341288, acc 0.890625\n",
      "2018-05-23T13:08:16.244838: step 6828, loss 0.247938, acc 0.890625\n",
      "2018-05-23T13:08:16.557003: step 6829, loss 0.209285, acc 0.90625\n",
      "2018-05-23T13:08:16.866177: step 6830, loss 0.208025, acc 0.921875\n",
      "2018-05-23T13:08:17.175347: step 6831, loss 0.301744, acc 0.890625\n",
      "2018-05-23T13:08:17.490506: step 6832, loss 0.412707, acc 0.84375\n",
      "2018-05-23T13:08:17.811647: step 6833, loss 0.314936, acc 0.8125\n",
      "2018-05-23T13:08:18.124808: step 6834, loss 0.230961, acc 0.890625\n",
      "2018-05-23T13:08:18.449938: step 6835, loss 0.179671, acc 0.859375\n",
      "2018-05-23T13:08:18.759111: step 6836, loss 0.281472, acc 0.875\n",
      "2018-05-23T13:08:19.066289: step 6837, loss 0.241349, acc 0.890625\n",
      "2018-05-23T13:08:19.377457: step 6838, loss 0.35347, acc 0.875\n",
      "2018-05-23T13:08:19.688626: step 6839, loss 0.209715, acc 0.921875\n",
      "2018-05-23T13:08:19.996802: step 6840, loss 0.358791, acc 0.875\n",
      "2018-05-23T13:08:20.310961: step 6841, loss 0.345355, acc 0.828125\n",
      "2018-05-23T13:08:20.624124: step 6842, loss 0.427593, acc 0.8125\n",
      "2018-05-23T13:08:20.942272: step 6843, loss 0.307292, acc 0.875\n",
      "2018-05-23T13:08:21.248452: step 6844, loss 0.349909, acc 0.84375\n",
      "2018-05-23T13:08:21.552640: step 6845, loss 0.326918, acc 0.859375\n",
      "2018-05-23T13:08:21.865803: step 6846, loss 0.293902, acc 0.8125\n",
      "2018-05-23T13:08:22.177967: step 6847, loss 0.248257, acc 0.859375\n",
      "2018-05-23T13:08:22.493121: step 6848, loss 0.521717, acc 0.796875\n",
      "2018-05-23T13:08:22.806286: step 6849, loss 0.285908, acc 0.84375\n",
      "2018-05-23T13:08:23.117453: step 6850, loss 0.321209, acc 0.84375\n",
      "2018-05-23T13:08:23.424631: step 6851, loss 0.284634, acc 0.890625\n",
      "2018-05-23T13:08:23.731808: step 6852, loss 0.356241, acc 0.8125\n",
      "2018-05-23T13:08:24.050954: step 6853, loss 0.204507, acc 0.90625\n",
      "2018-05-23T13:08:24.360129: step 6854, loss 0.376524, acc 0.859375\n",
      "2018-05-23T13:08:24.673289: step 6855, loss 0.309032, acc 0.859375\n",
      "2018-05-23T13:08:24.984458: step 6856, loss 0.227286, acc 0.890625\n",
      "2018-05-23T13:08:25.290640: step 6857, loss 0.24968, acc 0.890625\n",
      "2018-05-23T13:08:25.602803: step 6858, loss 0.375952, acc 0.84375\n",
      "2018-05-23T13:08:25.907987: step 6859, loss 0.318804, acc 0.828125\n",
      "2018-05-23T13:08:26.220152: step 6860, loss 0.344796, acc 0.84375\n",
      "2018-05-23T13:08:26.528327: step 6861, loss 0.323722, acc 0.828125\n",
      "2018-05-23T13:08:26.847473: step 6862, loss 0.470433, acc 0.78125\n",
      "2018-05-23T13:08:27.163630: step 6863, loss 0.374647, acc 0.890625\n",
      "2018-05-23T13:08:27.473798: step 6864, loss 0.343078, acc 0.875\n",
      "2018-05-23T13:08:27.784965: step 6865, loss 0.293578, acc 0.875\n",
      "2018-05-23T13:08:28.099126: step 6866, loss 0.277375, acc 0.859375\n",
      "2018-05-23T13:08:28.410293: step 6867, loss 0.362557, acc 0.828125\n",
      "2018-05-23T13:08:28.718467: step 6868, loss 0.335876, acc 0.84375\n",
      "2018-05-23T13:08:29.039609: step 6869, loss 0.314465, acc 0.875\n",
      "2018-05-23T13:08:29.344792: step 6870, loss 0.374076, acc 0.78125\n",
      "2018-05-23T13:08:29.651973: step 6871, loss 0.258711, acc 0.90625\n",
      "2018-05-23T13:08:29.968126: step 6872, loss 0.355132, acc 0.8125\n",
      "2018-05-23T13:08:30.290266: step 6873, loss 0.251739, acc 0.953125\n",
      "2018-05-23T13:08:30.602430: step 6874, loss 0.257705, acc 0.890625\n",
      "2018-05-23T13:08:30.910606: step 6875, loss 0.254417, acc 0.890625\n",
      "2018-05-23T13:08:31.228753: step 6876, loss 0.35972, acc 0.8125\n",
      "2018-05-23T13:08:31.541915: step 6877, loss 0.360123, acc 0.875\n",
      "2018-05-23T13:08:31.854082: step 6878, loss 0.210347, acc 0.921875\n",
      "2018-05-23T13:08:32.175220: step 6879, loss 0.228951, acc 0.875\n",
      "2018-05-23T13:08:32.490378: step 6880, loss 0.245539, acc 0.90625\n",
      "2018-05-23T13:08:32.807529: step 6881, loss 0.169894, acc 0.921875\n",
      "2018-05-23T13:08:33.129670: step 6882, loss 0.340636, acc 0.828125\n",
      "2018-05-23T13:08:33.463776: step 6883, loss 0.177855, acc 0.953125\n",
      "2018-05-23T13:08:33.790899: step 6884, loss 0.249394, acc 0.921875\n",
      "2018-05-23T13:08:34.116028: step 6885, loss 0.197745, acc 0.90625\n",
      "2018-05-23T13:08:34.454124: step 6886, loss 0.263916, acc 0.875\n",
      "2018-05-23T13:08:34.784242: step 6887, loss 0.12401, acc 0.984375\n",
      "2018-05-23T13:08:35.113362: step 6888, loss 0.231175, acc 0.890625\n",
      "2018-05-23T13:08:35.451456: step 6889, loss 0.27801, acc 0.90625\n",
      "2018-05-23T13:08:35.784566: step 6890, loss 0.27237, acc 0.859375\n",
      "2018-05-23T13:08:36.116677: step 6891, loss 0.249156, acc 0.9375\n",
      "2018-05-23T13:08:36.449785: step 6892, loss 0.229474, acc 0.890625\n",
      "2018-05-23T13:08:36.778905: step 6893, loss 0.236983, acc 0.890625\n",
      "2018-05-23T13:08:37.110020: step 6894, loss 0.232447, acc 0.890625\n",
      "2018-05-23T13:08:37.342401: step 6895, loss 0.462206, acc 0.882353\n",
      "2018-05-23T13:08:37.681490: step 6896, loss 0.183171, acc 0.953125\n",
      "2018-05-23T13:08:38.010610: step 6897, loss 0.197223, acc 0.921875\n",
      "2018-05-23T13:08:38.339730: step 6898, loss 0.231762, acc 0.9375\n",
      "2018-05-23T13:08:38.673838: step 6899, loss 0.253012, acc 0.875\n",
      "2018-05-23T13:08:39.000960: step 6900, loss 0.197192, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:08:43.293478: step 6900, loss 0.673772, acc 0.730104\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-6900\n",
      "\n",
      "2018-05-23T13:08:44.455438: step 6901, loss 0.193816, acc 0.96875\n",
      "2018-05-23T13:08:44.901247: step 6902, loss 0.27226, acc 0.890625\n",
      "2018-05-23T13:08:45.238345: step 6903, loss 0.193903, acc 0.921875\n",
      "2018-05-23T13:08:45.562476: step 6904, loss 0.200933, acc 0.921875\n",
      "2018-05-23T13:08:45.892593: step 6905, loss 0.253647, acc 0.859375\n",
      "2018-05-23T13:08:46.215729: step 6906, loss 0.264743, acc 0.90625\n",
      "2018-05-23T13:08:46.545846: step 6907, loss 0.19598, acc 0.96875\n",
      "2018-05-23T13:08:46.873969: step 6908, loss 0.112504, acc 0.96875\n",
      "2018-05-23T13:08:47.202092: step 6909, loss 0.272187, acc 0.859375\n",
      "2018-05-23T13:08:47.524228: step 6910, loss 0.234673, acc 0.859375\n",
      "2018-05-23T13:08:47.851355: step 6911, loss 0.21859, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:08:48.188451: step 6912, loss 0.327326, acc 0.828125\n",
      "2018-05-23T13:08:48.538515: step 6913, loss 0.246649, acc 0.890625\n",
      "2018-05-23T13:08:48.860654: step 6914, loss 0.244951, acc 0.90625\n",
      "2018-05-23T13:08:49.190769: step 6915, loss 0.176532, acc 0.921875\n",
      "2018-05-23T13:08:49.511911: step 6916, loss 0.200976, acc 0.921875\n",
      "2018-05-23T13:08:49.838039: step 6917, loss 0.225442, acc 0.9375\n",
      "2018-05-23T13:08:50.170150: step 6918, loss 0.175426, acc 0.90625\n",
      "2018-05-23T13:08:50.496279: step 6919, loss 0.19148, acc 0.875\n",
      "2018-05-23T13:08:50.820411: step 6920, loss 0.357368, acc 0.84375\n",
      "2018-05-23T13:08:51.137563: step 6921, loss 0.183363, acc 0.90625\n",
      "2018-05-23T13:08:51.466681: step 6922, loss 0.311233, acc 0.828125\n",
      "2018-05-23T13:08:51.786825: step 6923, loss 0.269986, acc 0.875\n",
      "2018-05-23T13:08:52.105973: step 6924, loss 0.274458, acc 0.84375\n",
      "2018-05-23T13:08:52.434093: step 6925, loss 0.152295, acc 0.921875\n",
      "2018-05-23T13:08:52.750247: step 6926, loss 0.24149, acc 0.859375\n",
      "2018-05-23T13:08:53.078349: step 6927, loss 0.224303, acc 0.921875\n",
      "2018-05-23T13:08:53.401487: step 6928, loss 0.199873, acc 0.875\n",
      "2018-05-23T13:08:53.717675: step 6929, loss 0.263041, acc 0.875\n",
      "2018-05-23T13:08:54.046759: step 6930, loss 0.187167, acc 0.9375\n",
      "2018-05-23T13:08:54.365905: step 6931, loss 0.21502, acc 0.890625\n",
      "2018-05-23T13:08:54.686049: step 6932, loss 0.250915, acc 0.859375\n",
      "2018-05-23T13:08:55.010182: step 6933, loss 0.209882, acc 0.9375\n",
      "2018-05-23T13:08:55.329328: step 6934, loss 0.172314, acc 0.921875\n",
      "2018-05-23T13:08:55.652463: step 6935, loss 0.142075, acc 0.953125\n",
      "2018-05-23T13:08:55.976596: step 6936, loss 0.230513, acc 0.875\n",
      "2018-05-23T13:08:56.301726: step 6937, loss 0.130039, acc 0.953125\n",
      "2018-05-23T13:08:56.617881: step 6938, loss 0.211154, acc 0.90625\n",
      "2018-05-23T13:08:56.949993: step 6939, loss 0.122279, acc 0.96875\n",
      "2018-05-23T13:08:57.276120: step 6940, loss 0.125424, acc 0.953125\n",
      "2018-05-23T13:08:57.596263: step 6941, loss 0.220045, acc 0.9375\n",
      "2018-05-23T13:08:57.918847: step 6942, loss 0.286932, acc 0.875\n",
      "2018-05-23T13:08:58.249962: step 6943, loss 0.214257, acc 0.890625\n",
      "2018-05-23T13:08:58.574093: step 6944, loss 0.272897, acc 0.859375\n",
      "2018-05-23T13:08:58.897230: step 6945, loss 0.244757, acc 0.890625\n",
      "2018-05-23T13:08:59.229342: step 6946, loss 0.23425, acc 0.921875\n",
      "2018-05-23T13:08:59.549486: step 6947, loss 0.231359, acc 0.90625\n",
      "2018-05-23T13:08:59.877607: step 6948, loss 0.143137, acc 0.953125\n",
      "2018-05-23T13:09:00.227671: step 6949, loss 0.257026, acc 0.890625\n",
      "2018-05-23T13:09:00.558786: step 6950, loss 0.247382, acc 0.90625\n",
      "2018-05-23T13:09:00.882916: step 6951, loss 0.211824, acc 0.890625\n",
      "2018-05-23T13:09:01.211041: step 6952, loss 0.231851, acc 0.859375\n",
      "2018-05-23T13:09:01.534176: step 6953, loss 0.186338, acc 0.9375\n",
      "2018-05-23T13:09:01.852323: step 6954, loss 0.317904, acc 0.890625\n",
      "2018-05-23T13:09:02.191417: step 6955, loss 0.339591, acc 0.890625\n",
      "2018-05-23T13:09:02.517546: step 6956, loss 0.198647, acc 0.90625\n",
      "2018-05-23T13:09:02.837690: step 6957, loss 0.295857, acc 0.828125\n",
      "2018-05-23T13:09:03.165811: step 6958, loss 0.27076, acc 0.890625\n",
      "2018-05-23T13:09:03.499916: step 6959, loss 0.145276, acc 0.96875\n",
      "2018-05-23T13:09:03.824048: step 6960, loss 0.18003, acc 0.953125\n",
      "2018-05-23T13:09:04.149179: step 6961, loss 0.170663, acc 0.890625\n",
      "2018-05-23T13:09:04.501237: step 6962, loss 0.144179, acc 0.953125\n",
      "2018-05-23T13:09:04.850303: step 6963, loss 0.165137, acc 0.90625\n",
      "2018-05-23T13:09:05.195380: step 6964, loss 0.179664, acc 0.9375\n",
      "2018-05-23T13:09:05.533477: step 6965, loss 0.287197, acc 0.84375\n",
      "2018-05-23T13:09:05.854617: step 6966, loss 0.28674, acc 0.875\n",
      "2018-05-23T13:09:06.180745: step 6967, loss 0.188609, acc 0.90625\n",
      "2018-05-23T13:09:06.512857: step 6968, loss 0.182466, acc 0.921875\n",
      "2018-05-23T13:09:06.836991: step 6969, loss 0.187158, acc 0.9375\n",
      "2018-05-23T13:09:07.169101: step 6970, loss 0.122643, acc 0.953125\n",
      "2018-05-23T13:09:07.498220: step 6971, loss 0.347406, acc 0.890625\n",
      "2018-05-23T13:09:07.818364: step 6972, loss 0.171251, acc 0.9375\n",
      "2018-05-23T13:09:08.143495: step 6973, loss 0.317353, acc 0.859375\n",
      "2018-05-23T13:09:08.467664: step 6974, loss 0.225281, acc 0.90625\n",
      "2018-05-23T13:09:08.790762: step 6975, loss 0.216717, acc 0.90625\n",
      "2018-05-23T13:09:09.116891: step 6976, loss 0.175716, acc 0.9375\n",
      "2018-05-23T13:09:09.447007: step 6977, loss 0.247192, acc 0.890625\n",
      "2018-05-23T13:09:09.765155: step 6978, loss 0.18069, acc 0.9375\n",
      "2018-05-23T13:09:10.088292: step 6979, loss 0.258466, acc 0.875\n",
      "2018-05-23T13:09:10.413421: step 6980, loss 0.177923, acc 0.921875\n",
      "2018-05-23T13:09:10.735559: step 6981, loss 0.256276, acc 0.84375\n",
      "2018-05-23T13:09:11.059693: step 6982, loss 0.174347, acc 0.9375\n",
      "2018-05-23T13:09:11.388811: step 6983, loss 0.152312, acc 0.9375\n",
      "2018-05-23T13:09:11.718928: step 6984, loss 0.210505, acc 0.890625\n",
      "2018-05-23T13:09:12.044059: step 6985, loss 0.201691, acc 0.9375\n",
      "2018-05-23T13:09:12.367196: step 6986, loss 0.210903, acc 0.890625\n",
      "2018-05-23T13:09:12.698316: step 6987, loss 0.267418, acc 0.859375\n",
      "2018-05-23T13:09:13.028427: step 6988, loss 0.185108, acc 0.921875\n",
      "2018-05-23T13:09:13.355551: step 6989, loss 0.193955, acc 0.921875\n",
      "2018-05-23T13:09:13.680682: step 6990, loss 0.168174, acc 0.96875\n",
      "2018-05-23T13:09:14.005813: step 6991, loss 0.158558, acc 0.921875\n",
      "2018-05-23T13:09:14.322963: step 6992, loss 0.216314, acc 0.90625\n",
      "2018-05-23T13:09:14.651084: step 6993, loss 0.241831, acc 0.890625\n",
      "2018-05-23T13:09:14.983196: step 6994, loss 0.250358, acc 0.875\n",
      "2018-05-23T13:09:15.303340: step 6995, loss 0.320348, acc 0.84375\n",
      "2018-05-23T13:09:15.630467: step 6996, loss 0.327956, acc 0.890625\n",
      "2018-05-23T13:09:15.962575: step 6997, loss 0.196792, acc 0.921875\n",
      "2018-05-23T13:09:16.290698: step 6998, loss 0.174237, acc 0.921875\n",
      "2018-05-23T13:09:16.636773: step 6999, loss 0.200084, acc 0.890625\n",
      "2018-05-23T13:09:16.966891: step 7000, loss 0.299846, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:09:21.395043: step 7000, loss 0.72482, acc 0.726961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7000\n",
      "\n",
      "2018-05-23T13:09:22.666642: step 7001, loss 0.270343, acc 0.859375\n",
      "2018-05-23T13:09:23.020696: step 7002, loss 0.274087, acc 0.890625\n",
      "2018-05-23T13:09:23.359787: step 7003, loss 0.249723, acc 0.9375\n",
      "2018-05-23T13:09:23.680930: step 7004, loss 0.141272, acc 0.9375\n",
      "2018-05-23T13:09:24.008053: step 7005, loss 0.250246, acc 0.875\n",
      "2018-05-23T13:09:24.326204: step 7006, loss 0.30823, acc 0.859375\n",
      "2018-05-23T13:09:24.646347: step 7007, loss 0.283821, acc 0.890625\n",
      "2018-05-23T13:09:24.982447: step 7008, loss 0.231315, acc 0.90625\n",
      "2018-05-23T13:09:25.314559: step 7009, loss 0.257083, acc 0.890625\n",
      "2018-05-23T13:09:25.633704: step 7010, loss 0.247011, acc 0.859375\n",
      "2018-05-23T13:09:25.959833: step 7011, loss 0.162134, acc 0.9375\n",
      "2018-05-23T13:09:26.285960: step 7012, loss 0.151141, acc 0.953125\n",
      "2018-05-23T13:09:26.612086: step 7013, loss 0.273482, acc 0.890625\n",
      "2018-05-23T13:09:26.944200: step 7014, loss 0.291284, acc 0.890625\n",
      "2018-05-23T13:09:27.273319: step 7015, loss 0.25049, acc 0.90625\n",
      "2018-05-23T13:09:27.598449: step 7016, loss 0.203812, acc 0.921875\n",
      "2018-05-23T13:09:27.930559: step 7017, loss 0.279173, acc 0.84375\n",
      "2018-05-23T13:09:28.256688: step 7018, loss 0.199692, acc 0.9375\n",
      "2018-05-23T13:09:28.581819: step 7019, loss 0.229254, acc 0.890625\n",
      "2018-05-23T13:09:28.905952: step 7020, loss 0.108983, acc 0.984375\n",
      "2018-05-23T13:09:29.246041: step 7021, loss 0.224759, acc 0.859375\n",
      "2018-05-23T13:09:29.572168: step 7022, loss 0.181382, acc 0.9375\n",
      "2018-05-23T13:09:29.895304: step 7023, loss 0.267042, acc 0.875\n",
      "2018-05-23T13:09:30.224076: step 7024, loss 0.153542, acc 0.921875\n",
      "2018-05-23T13:09:30.556188: step 7025, loss 0.200475, acc 0.921875\n",
      "2018-05-23T13:09:30.880322: step 7026, loss 0.126349, acc 0.96875\n",
      "2018-05-23T13:09:31.216421: step 7027, loss 0.267254, acc 0.859375\n",
      "2018-05-23T13:09:31.538561: step 7028, loss 0.19759, acc 0.921875\n",
      "2018-05-23T13:09:31.868678: step 7029, loss 0.187256, acc 0.921875\n",
      "2018-05-23T13:09:32.196798: step 7030, loss 0.245922, acc 0.875\n",
      "2018-05-23T13:09:32.524920: step 7031, loss 0.266305, acc 0.90625\n",
      "2018-05-23T13:09:32.851048: step 7032, loss 0.229363, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:09:33.183160: step 7033, loss 0.147805, acc 0.96875\n",
      "2018-05-23T13:09:33.534221: step 7034, loss 0.258368, acc 0.90625\n",
      "2018-05-23T13:09:33.842396: step 7035, loss 0.254859, acc 0.890625\n",
      "2018-05-23T13:09:34.162717: step 7036, loss 0.220645, acc 0.921875\n",
      "2018-05-23T13:09:34.475885: step 7037, loss 0.152082, acc 0.96875\n",
      "2018-05-23T13:09:34.789047: step 7038, loss 0.269763, acc 0.875\n",
      "2018-05-23T13:09:35.106199: step 7039, loss 0.218734, acc 0.90625\n",
      "2018-05-23T13:09:35.420358: step 7040, loss 0.269315, acc 0.84375\n",
      "2018-05-23T13:09:35.739504: step 7041, loss 0.164982, acc 0.9375\n",
      "2018-05-23T13:09:36.049674: step 7042, loss 0.331456, acc 0.875\n",
      "2018-05-23T13:09:36.361840: step 7043, loss 0.173863, acc 0.953125\n",
      "2018-05-23T13:09:36.673006: step 7044, loss 0.185177, acc 0.921875\n",
      "2018-05-23T13:09:37.001128: step 7045, loss 0.118606, acc 0.953125\n",
      "2018-05-23T13:09:37.319277: step 7046, loss 0.26227, acc 0.90625\n",
      "2018-05-23T13:09:37.625460: step 7047, loss 0.148945, acc 0.9375\n",
      "2018-05-23T13:09:37.939618: step 7048, loss 0.178731, acc 0.890625\n",
      "2018-05-23T13:09:38.253777: step 7049, loss 0.265656, acc 0.921875\n",
      "2018-05-23T13:09:38.563948: step 7050, loss 0.308856, acc 0.859375\n",
      "2018-05-23T13:09:38.870129: step 7051, loss 0.263122, acc 0.890625\n",
      "2018-05-23T13:09:39.181297: step 7052, loss 0.246154, acc 0.890625\n",
      "2018-05-23T13:09:39.493461: step 7053, loss 0.257897, acc 0.890625\n",
      "2018-05-23T13:09:39.800639: step 7054, loss 0.177836, acc 0.890625\n",
      "2018-05-23T13:09:40.103827: step 7055, loss 0.209562, acc 0.9375\n",
      "2018-05-23T13:09:40.413998: step 7056, loss 0.349385, acc 0.859375\n",
      "2018-05-23T13:09:40.721177: step 7057, loss 0.216581, acc 0.90625\n",
      "2018-05-23T13:09:41.028356: step 7058, loss 0.201855, acc 0.921875\n",
      "2018-05-23T13:09:41.333538: step 7059, loss 0.225815, acc 0.875\n",
      "2018-05-23T13:09:41.646700: step 7060, loss 0.121106, acc 0.96875\n",
      "2018-05-23T13:09:41.950887: step 7061, loss 0.214698, acc 0.921875\n",
      "2018-05-23T13:09:42.258065: step 7062, loss 0.227496, acc 0.90625\n",
      "2018-05-23T13:09:42.585189: step 7063, loss 0.152292, acc 0.90625\n",
      "2018-05-23T13:09:42.895360: step 7064, loss 0.217611, acc 0.875\n",
      "2018-05-23T13:09:43.206530: step 7065, loss 0.147529, acc 0.90625\n",
      "2018-05-23T13:09:43.512709: step 7066, loss 0.246126, acc 0.90625\n",
      "2018-05-23T13:09:43.823878: step 7067, loss 0.223176, acc 0.9375\n",
      "2018-05-23T13:09:44.138068: step 7068, loss 0.208606, acc 0.90625\n",
      "2018-05-23T13:09:44.442224: step 7069, loss 0.216892, acc 0.9375\n",
      "2018-05-23T13:09:44.755384: step 7070, loss 0.269602, acc 0.875\n",
      "2018-05-23T13:09:45.070541: step 7071, loss 0.169428, acc 0.90625\n",
      "2018-05-23T13:09:45.385700: step 7072, loss 0.366469, acc 0.859375\n",
      "2018-05-23T13:09:45.693874: step 7073, loss 0.186785, acc 0.890625\n",
      "2018-05-23T13:09:46.002049: step 7074, loss 0.270629, acc 0.828125\n",
      "2018-05-23T13:09:46.324188: step 7075, loss 0.171723, acc 0.9375\n",
      "2018-05-23T13:09:46.633361: step 7076, loss 0.257583, acc 0.90625\n",
      "2018-05-23T13:09:46.948520: step 7077, loss 0.311766, acc 0.890625\n",
      "2018-05-23T13:09:47.263677: step 7078, loss 0.245446, acc 0.859375\n",
      "2018-05-23T13:09:47.571850: step 7079, loss 0.273001, acc 0.890625\n",
      "2018-05-23T13:09:47.886010: step 7080, loss 0.184763, acc 0.921875\n",
      "2018-05-23T13:09:48.196179: step 7081, loss 0.297572, acc 0.859375\n",
      "2018-05-23T13:09:48.553227: step 7082, loss 0.207155, acc 0.890625\n",
      "2018-05-23T13:09:48.867384: step 7083, loss 0.244714, acc 0.921875\n",
      "2018-05-23T13:09:49.186531: step 7084, loss 0.207142, acc 0.875\n",
      "2018-05-23T13:09:49.495704: step 7085, loss 0.241923, acc 0.890625\n",
      "2018-05-23T13:09:49.814849: step 7086, loss 0.186531, acc 0.9375\n",
      "2018-05-23T13:09:50.137986: step 7087, loss 0.143189, acc 0.9375\n",
      "2018-05-23T13:09:50.453184: step 7088, loss 0.231827, acc 0.921875\n",
      "2018-05-23T13:09:50.767344: step 7089, loss 0.158065, acc 0.921875\n",
      "2018-05-23T13:09:51.083497: step 7090, loss 0.13592, acc 0.9375\n",
      "2018-05-23T13:09:51.393436: step 7091, loss 0.260081, acc 0.828125\n",
      "2018-05-23T13:09:51.712583: step 7092, loss 0.2903, acc 0.875\n",
      "2018-05-23T13:09:52.034765: step 7093, loss 0.32719, acc 0.828125\n",
      "2018-05-23T13:09:52.343937: step 7094, loss 0.21853, acc 0.90625\n",
      "2018-05-23T13:09:52.657100: step 7095, loss 0.166015, acc 0.9375\n",
      "2018-05-23T13:09:52.975248: step 7096, loss 0.197272, acc 0.921875\n",
      "2018-05-23T13:09:53.287414: step 7097, loss 0.212769, acc 0.953125\n",
      "2018-05-23T13:09:53.603568: step 7098, loss 0.276812, acc 0.890625\n",
      "2018-05-23T13:09:53.925706: step 7099, loss 0.258152, acc 0.890625\n",
      "2018-05-23T13:09:54.243854: step 7100, loss 0.175827, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:09:58.445615: step 7100, loss 0.751603, acc 0.722532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7100\n",
      "\n",
      "2018-05-23T13:09:59.563641: step 7101, loss 0.190703, acc 0.921875\n",
      "2018-05-23T13:10:00.016431: step 7102, loss 0.256325, acc 0.859375\n",
      "2018-05-23T13:10:00.365498: step 7103, loss 0.306229, acc 0.890625\n",
      "2018-05-23T13:10:00.756451: step 7104, loss 0.310592, acc 0.890625\n",
      "2018-05-23T13:10:01.081583: step 7105, loss 0.163496, acc 0.9375\n",
      "2018-05-23T13:10:01.425661: step 7106, loss 0.215387, acc 0.875\n",
      "2018-05-23T13:10:01.803650: step 7107, loss 0.179994, acc 0.921875\n",
      "2018-05-23T13:10:02.171665: step 7108, loss 0.197636, acc 0.953125\n",
      "2018-05-23T13:10:02.502780: step 7109, loss 0.0925942, acc 0.96875\n",
      "2018-05-23T13:10:02.856833: step 7110, loss 0.240162, acc 0.921875\n",
      "2018-05-23T13:10:03.186950: step 7111, loss 0.189993, acc 0.96875\n",
      "2018-05-23T13:10:03.539006: step 7112, loss 0.171202, acc 0.890625\n",
      "2018-05-23T13:10:03.868126: step 7113, loss 0.245065, acc 0.90625\n",
      "2018-05-23T13:10:04.191262: step 7114, loss 0.191953, acc 0.9375\n",
      "2018-05-23T13:10:04.511406: step 7115, loss 0.201066, acc 0.921875\n",
      "2018-05-23T13:10:04.832548: step 7116, loss 0.152411, acc 0.9375\n",
      "2018-05-23T13:10:05.152692: step 7117, loss 0.239996, acc 0.890625\n",
      "2018-05-23T13:10:05.474830: step 7118, loss 0.315913, acc 0.84375\n",
      "2018-05-23T13:10:05.798961: step 7119, loss 0.216917, acc 0.890625\n",
      "2018-05-23T13:10:06.116113: step 7120, loss 0.205285, acc 0.90625\n",
      "2018-05-23T13:10:06.437253: step 7121, loss 0.309947, acc 0.875\n",
      "2018-05-23T13:10:06.749419: step 7122, loss 0.313004, acc 0.90625\n",
      "2018-05-23T13:10:07.058591: step 7123, loss 0.215851, acc 0.84375\n",
      "2018-05-23T13:10:07.374747: step 7124, loss 0.182292, acc 0.90625\n",
      "2018-05-23T13:10:07.687908: step 7125, loss 0.217365, acc 0.90625\n",
      "2018-05-23T13:10:07.999075: step 7126, loss 0.183193, acc 0.953125\n",
      "2018-05-23T13:10:08.315231: step 7127, loss 0.30079, acc 0.84375\n",
      "2018-05-23T13:10:08.638367: step 7128, loss 0.24419, acc 0.859375\n",
      "2018-05-23T13:10:08.947539: step 7129, loss 0.20631, acc 0.953125\n",
      "2018-05-23T13:10:09.268680: step 7130, loss 0.148228, acc 0.9375\n",
      "2018-05-23T13:10:09.587827: step 7131, loss 0.191587, acc 0.90625\n",
      "2018-05-23T13:10:09.896002: step 7132, loss 0.206723, acc 0.921875\n",
      "2018-05-23T13:10:10.213154: step 7133, loss 0.256948, acc 0.875\n",
      "2018-05-23T13:10:10.534295: step 7134, loss 0.311956, acc 0.859375\n",
      "2018-05-23T13:10:10.849450: step 7135, loss 0.290644, acc 0.921875\n",
      "2018-05-23T13:10:11.161615: step 7136, loss 0.412336, acc 0.796875\n",
      "2018-05-23T13:10:11.477770: step 7137, loss 0.240794, acc 0.859375\n",
      "2018-05-23T13:10:11.802901: step 7138, loss 0.306018, acc 0.875\n",
      "2018-05-23T13:10:12.122046: step 7139, loss 0.364665, acc 0.859375\n",
      "2018-05-23T13:10:12.434212: step 7140, loss 0.208968, acc 0.90625\n",
      "2018-05-23T13:10:12.751364: step 7141, loss 0.355356, acc 0.859375\n",
      "2018-05-23T13:10:13.061534: step 7142, loss 0.181022, acc 0.9375\n",
      "2018-05-23T13:10:13.379681: step 7143, loss 0.254761, acc 0.875\n",
      "2018-05-23T13:10:13.690850: step 7144, loss 0.203464, acc 0.90625\n",
      "2018-05-23T13:10:14.012987: step 7145, loss 0.331211, acc 0.875\n",
      "2018-05-23T13:10:14.324155: step 7146, loss 0.190322, acc 0.9375\n",
      "2018-05-23T13:10:14.642708: step 7147, loss 0.250044, acc 0.859375\n",
      "2018-05-23T13:10:14.952877: step 7148, loss 0.176391, acc 0.9375\n",
      "2018-05-23T13:10:15.265044: step 7149, loss 0.156319, acc 0.953125\n",
      "2018-05-23T13:10:15.574216: step 7150, loss 0.224325, acc 0.953125\n",
      "2018-05-23T13:10:15.893362: step 7151, loss 0.354067, acc 0.859375\n",
      "2018-05-23T13:10:16.203533: step 7152, loss 0.403156, acc 0.84375\n",
      "2018-05-23T13:10:16.513703: step 7153, loss 0.192358, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:10:16.833848: step 7154, loss 0.167984, acc 0.9375\n",
      "2018-05-23T13:10:17.146009: step 7155, loss 0.135884, acc 0.9375\n",
      "2018-05-23T13:10:17.457177: step 7156, loss 0.182692, acc 0.90625\n",
      "2018-05-23T13:10:17.775328: step 7157, loss 0.296379, acc 0.921875\n",
      "2018-05-23T13:10:18.088491: step 7158, loss 0.196283, acc 0.890625\n",
      "2018-05-23T13:10:18.449523: step 7159, loss 0.254517, acc 0.859375\n",
      "2018-05-23T13:10:18.764679: step 7160, loss 0.246428, acc 0.890625\n",
      "2018-05-23T13:10:19.082829: step 7161, loss 0.117234, acc 0.953125\n",
      "2018-05-23T13:10:19.393996: step 7162, loss 0.165532, acc 0.921875\n",
      "2018-05-23T13:10:19.707158: step 7163, loss 0.327375, acc 0.828125\n",
      "2018-05-23T13:10:20.025307: step 7164, loss 0.238461, acc 0.890625\n",
      "2018-05-23T13:10:20.346448: step 7165, loss 0.245745, acc 0.890625\n",
      "2018-05-23T13:10:20.683548: step 7166, loss 0.256965, acc 0.90625\n",
      "2018-05-23T13:10:21.005687: step 7167, loss 0.157387, acc 0.9375\n",
      "2018-05-23T13:10:21.318850: step 7168, loss 0.180148, acc 0.9375\n",
      "2018-05-23T13:10:21.639990: step 7169, loss 0.255824, acc 0.890625\n",
      "2018-05-23T13:10:21.949163: step 7170, loss 0.244044, acc 0.921875\n",
      "2018-05-23T13:10:22.262322: step 7171, loss 0.239879, acc 0.875\n",
      "2018-05-23T13:10:22.578477: step 7172, loss 0.2705, acc 0.90625\n",
      "2018-05-23T13:10:22.887652: step 7173, loss 0.130085, acc 0.96875\n",
      "2018-05-23T13:10:23.207796: step 7174, loss 0.249009, acc 0.859375\n",
      "2018-05-23T13:10:23.522952: step 7175, loss 0.132155, acc 0.96875\n",
      "2018-05-23T13:10:23.834117: step 7176, loss 0.28096, acc 0.875\n",
      "2018-05-23T13:10:24.154264: step 7177, loss 0.243977, acc 0.890625\n",
      "2018-05-23T13:10:24.463436: step 7178, loss 0.24844, acc 0.90625\n",
      "2018-05-23T13:10:24.778592: step 7179, loss 0.238096, acc 0.90625\n",
      "2018-05-23T13:10:25.091755: step 7180, loss 0.199618, acc 0.921875\n",
      "2018-05-23T13:10:25.401923: step 7181, loss 0.350541, acc 0.84375\n",
      "2018-05-23T13:10:25.713094: step 7182, loss 0.203462, acc 0.9375\n",
      "2018-05-23T13:10:26.018275: step 7183, loss 0.265478, acc 0.890625\n",
      "2018-05-23T13:10:26.335427: step 7184, loss 0.169533, acc 0.9375\n",
      "2018-05-23T13:10:26.649586: step 7185, loss 0.215855, acc 0.890625\n",
      "2018-05-23T13:10:26.964743: step 7186, loss 0.170842, acc 0.921875\n",
      "2018-05-23T13:10:27.279563: step 7187, loss 0.150681, acc 0.984375\n",
      "2018-05-23T13:10:27.588737: step 7188, loss 0.39122, acc 0.78125\n",
      "2018-05-23T13:10:27.902896: step 7189, loss 0.192912, acc 0.921875\n",
      "2018-05-23T13:10:28.224037: step 7190, loss 0.324649, acc 0.84375\n",
      "2018-05-23T13:10:28.542187: step 7191, loss 0.152252, acc 0.953125\n",
      "2018-05-23T13:10:28.852358: step 7192, loss 0.323529, acc 0.890625\n",
      "2018-05-23T13:10:29.165520: step 7193, loss 0.198579, acc 0.890625\n",
      "2018-05-23T13:10:29.482670: step 7194, loss 0.461912, acc 0.828125\n",
      "2018-05-23T13:10:29.788852: step 7195, loss 0.29503, acc 0.875\n",
      "2018-05-23T13:10:30.105006: step 7196, loss 0.351891, acc 0.84375\n",
      "2018-05-23T13:10:30.423154: step 7197, loss 0.175707, acc 0.921875\n",
      "2018-05-23T13:10:30.734323: step 7198, loss 0.234216, acc 0.921875\n",
      "2018-05-23T13:10:31.047483: step 7199, loss 0.267, acc 0.875\n",
      "2018-05-23T13:10:31.365635: step 7200, loss 0.260329, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:10:35.502566: step 7200, loss 0.753118, acc 0.723961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7200\n",
      "\n",
      "2018-05-23T13:10:36.696789: step 7201, loss 0.184355, acc 0.875\n",
      "2018-05-23T13:10:37.067796: step 7202, loss 0.360459, acc 0.859375\n",
      "2018-05-23T13:10:37.402902: step 7203, loss 0.175697, acc 0.953125\n",
      "2018-05-23T13:10:37.736008: step 7204, loss 0.224099, acc 0.890625\n",
      "2018-05-23T13:10:38.057151: step 7205, loss 0.24991, acc 0.890625\n",
      "2018-05-23T13:10:38.378291: step 7206, loss 0.267491, acc 0.90625\n",
      "2018-05-23T13:10:38.687464: step 7207, loss 0.250147, acc 0.859375\n",
      "2018-05-23T13:10:39.013590: step 7208, loss 0.277454, acc 0.875\n",
      "2018-05-23T13:10:39.317779: step 7209, loss 0.365012, acc 0.828125\n",
      "2018-05-23T13:10:39.630940: step 7210, loss 0.349671, acc 0.875\n",
      "2018-05-23T13:10:39.946096: step 7211, loss 0.0903682, acc 0.984375\n",
      "2018-05-23T13:10:40.261290: step 7212, loss 0.190721, acc 0.90625\n",
      "2018-05-23T13:10:40.572421: step 7213, loss 0.182256, acc 0.90625\n",
      "2018-05-23T13:10:40.891570: step 7214, loss 0.278017, acc 0.890625\n",
      "2018-05-23T13:10:41.207721: step 7215, loss 0.228852, acc 0.921875\n",
      "2018-05-23T13:10:41.511910: step 7216, loss 0.232847, acc 0.90625\n",
      "2018-05-23T13:10:41.828063: step 7217, loss 0.186687, acc 0.921875\n",
      "2018-05-23T13:10:42.141225: step 7218, loss 0.245356, acc 0.90625\n",
      "2018-05-23T13:10:42.451394: step 7219, loss 0.200481, acc 0.875\n",
      "2018-05-23T13:10:42.764557: step 7220, loss 0.182606, acc 0.953125\n",
      "2018-05-23T13:10:43.080711: step 7221, loss 0.285941, acc 0.890625\n",
      "2018-05-23T13:10:43.391880: step 7222, loss 0.157329, acc 0.953125\n",
      "2018-05-23T13:10:43.705040: step 7223, loss 0.274524, acc 0.9375\n",
      "2018-05-23T13:10:44.011221: step 7224, loss 0.0906061, acc 0.984375\n",
      "2018-05-23T13:10:44.330368: step 7225, loss 0.197538, acc 0.953125\n",
      "2018-05-23T13:10:44.641538: step 7226, loss 0.20861, acc 0.90625\n",
      "2018-05-23T13:10:44.954697: step 7227, loss 0.197199, acc 0.875\n",
      "2018-05-23T13:10:45.275838: step 7228, loss 0.295659, acc 0.890625\n",
      "2018-05-23T13:10:45.594985: step 7229, loss 0.252933, acc 0.90625\n",
      "2018-05-23T13:10:45.911138: step 7230, loss 0.163668, acc 0.9375\n",
      "2018-05-23T13:10:46.223304: step 7231, loss 0.297516, acc 0.875\n",
      "2018-05-23T13:10:46.532476: step 7232, loss 0.158053, acc 0.96875\n",
      "2018-05-23T13:10:46.840653: step 7233, loss 0.257286, acc 0.90625\n",
      "2018-05-23T13:10:47.162792: step 7234, loss 0.311636, acc 0.921875\n",
      "2018-05-23T13:10:47.468972: step 7235, loss 0.17205, acc 0.921875\n",
      "2018-05-23T13:10:47.779144: step 7236, loss 0.220028, acc 0.921875\n",
      "2018-05-23T13:10:48.097290: step 7237, loss 0.219525, acc 0.890625\n",
      "2018-05-23T13:10:48.424416: step 7238, loss 0.274775, acc 0.921875\n",
      "2018-05-23T13:10:48.735583: step 7239, loss 0.212735, acc 0.890625\n",
      "2018-05-23T13:10:49.058721: step 7240, loss 0.215838, acc 0.875\n",
      "2018-05-23T13:10:49.373876: step 7241, loss 0.187676, acc 0.921875\n",
      "2018-05-23T13:10:49.695017: step 7242, loss 0.285586, acc 0.828125\n",
      "2018-05-23T13:10:50.014163: step 7243, loss 0.244015, acc 0.875\n",
      "2018-05-23T13:10:50.335304: step 7244, loss 0.156363, acc 0.953125\n",
      "2018-05-23T13:10:50.703319: step 7245, loss 0.312647, acc 0.875\n",
      "2018-05-23T13:10:51.008502: step 7246, loss 0.280068, acc 0.859375\n",
      "2018-05-23T13:10:51.323660: step 7247, loss 0.255837, acc 0.859375\n",
      "2018-05-23T13:10:51.640811: step 7248, loss 0.357306, acc 0.84375\n",
      "2018-05-23T13:10:51.962949: step 7249, loss 0.262496, acc 0.875\n",
      "2018-05-23T13:10:52.280101: step 7250, loss 0.208214, acc 0.90625\n",
      "2018-05-23T13:10:52.596255: step 7251, loss 0.269118, acc 0.859375\n",
      "2018-05-23T13:10:52.909417: step 7252, loss 0.201011, acc 0.9375\n",
      "2018-05-23T13:10:53.238537: step 7253, loss 0.176491, acc 0.90625\n",
      "2018-05-23T13:10:53.553694: step 7254, loss 0.233999, acc 0.90625\n",
      "2018-05-23T13:10:53.862866: step 7255, loss 0.215414, acc 0.890625\n",
      "2018-05-23T13:10:54.183011: step 7256, loss 0.149696, acc 0.9375\n",
      "2018-05-23T13:10:54.493180: step 7257, loss 0.274565, acc 0.921875\n",
      "2018-05-23T13:10:54.803350: step 7258, loss 0.199428, acc 0.90625\n",
      "2018-05-23T13:10:55.123496: step 7259, loss 0.275752, acc 0.875\n",
      "2018-05-23T13:10:55.435660: step 7260, loss 0.298145, acc 0.890625\n",
      "2018-05-23T13:10:55.750816: step 7261, loss 0.249433, acc 0.84375\n",
      "2018-05-23T13:10:56.062980: step 7262, loss 0.261344, acc 0.890625\n",
      "2018-05-23T13:10:56.381129: step 7263, loss 0.137114, acc 0.9375\n",
      "2018-05-23T13:10:56.690304: step 7264, loss 0.336236, acc 0.875\n",
      "2018-05-23T13:10:56.996483: step 7265, loss 0.242931, acc 0.890625\n",
      "2018-05-23T13:10:57.306653: step 7266, loss 0.230688, acc 0.859375\n",
      "2018-05-23T13:10:57.624802: step 7267, loss 0.160919, acc 0.921875\n",
      "2018-05-23T13:10:57.940957: step 7268, loss 0.337452, acc 0.890625\n",
      "2018-05-23T13:10:58.258109: step 7269, loss 0.0893648, acc 0.984375\n",
      "2018-05-23T13:10:58.570273: step 7270, loss 0.371633, acc 0.828125\n",
      "2018-05-23T13:10:58.891417: step 7271, loss 0.159537, acc 0.953125\n",
      "2018-05-23T13:10:59.209563: step 7272, loss 0.223881, acc 0.921875\n",
      "2018-05-23T13:10:59.520730: step 7273, loss 0.127604, acc 0.953125\n",
      "2018-05-23T13:10:59.844863: step 7274, loss 0.23557, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:11:00.185951: step 7275, loss 0.193316, acc 0.9375\n",
      "2018-05-23T13:11:00.503103: step 7276, loss 0.245311, acc 0.921875\n",
      "2018-05-23T13:11:00.823567: step 7277, loss 0.285083, acc 0.859375\n",
      "2018-05-23T13:11:01.137727: step 7278, loss 0.237975, acc 0.90625\n",
      "2018-05-23T13:11:01.452883: step 7279, loss 0.251721, acc 0.84375\n",
      "2018-05-23T13:11:01.760062: step 7280, loss 0.245889, acc 0.9375\n",
      "2018-05-23T13:11:02.075218: step 7281, loss 0.277684, acc 0.90625\n",
      "2018-05-23T13:11:02.386387: step 7282, loss 0.242476, acc 0.875\n",
      "2018-05-23T13:11:02.704537: step 7283, loss 0.260163, acc 0.90625\n",
      "2018-05-23T13:11:03.017697: step 7284, loss 0.190826, acc 0.921875\n",
      "2018-05-23T13:11:03.336843: step 7285, loss 0.27321, acc 0.90625\n",
      "2018-05-23T13:11:03.651005: step 7286, loss 0.222874, acc 0.90625\n",
      "2018-05-23T13:11:03.966162: step 7287, loss 0.307841, acc 0.859375\n",
      "2018-05-23T13:11:04.281317: step 7288, loss 0.254283, acc 0.921875\n",
      "2018-05-23T13:11:04.594479: step 7289, loss 0.3639, acc 0.90625\n",
      "2018-05-23T13:11:04.904649: step 7290, loss 0.16199, acc 0.921875\n",
      "2018-05-23T13:11:05.220805: step 7291, loss 0.339442, acc 0.828125\n",
      "2018-05-23T13:11:05.538953: step 7292, loss 0.182927, acc 0.890625\n",
      "2018-05-23T13:11:05.851117: step 7293, loss 0.297171, acc 0.890625\n",
      "2018-05-23T13:11:06.169266: step 7294, loss 0.268491, acc 0.90625\n",
      "2018-05-23T13:11:06.482428: step 7295, loss 0.42615, acc 0.84375\n",
      "2018-05-23T13:11:06.791603: step 7296, loss 0.21457, acc 0.921875\n",
      "2018-05-23T13:11:07.108753: step 7297, loss 0.234667, acc 0.90625\n",
      "2018-05-23T13:11:07.444855: step 7298, loss 0.251772, acc 0.890625\n",
      "2018-05-23T13:11:07.817857: step 7299, loss 0.16208, acc 0.953125\n",
      "2018-05-23T13:11:08.163930: step 7300, loss 0.155623, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:11:16.538307: step 7300, loss 0.76314, acc 0.719817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7300\n",
      "\n",
      "2018-05-23T13:11:18.266683: step 7301, loss 0.263694, acc 0.90625\n",
      "2018-05-23T13:11:18.838270: step 7302, loss 0.197279, acc 0.890625\n",
      "2018-05-23T13:11:19.424222: step 7303, loss 0.233996, acc 0.921875\n",
      "2018-05-23T13:11:19.865036: step 7304, loss 0.228152, acc 0.859375\n",
      "2018-05-23T13:11:20.385646: step 7305, loss 0.139988, acc 0.96875\n",
      "2018-05-23T13:11:20.942156: step 7306, loss 0.206026, acc 0.921875\n",
      "2018-05-23T13:11:21.407911: step 7307, loss 0.359703, acc 0.796875\n",
      "2018-05-23T13:11:21.825794: step 7308, loss 0.271635, acc 0.890625\n",
      "2018-05-23T13:11:22.321469: step 7309, loss 0.215491, acc 0.859375\n",
      "2018-05-23T13:11:22.755305: step 7310, loss 0.277617, acc 0.875\n",
      "2018-05-23T13:11:23.099387: step 7311, loss 0.289413, acc 0.890625\n",
      "2018-05-23T13:11:23.441505: step 7312, loss 0.20631, acc 0.890625\n",
      "2018-05-23T13:11:23.779564: step 7313, loss 0.147435, acc 0.9375\n",
      "2018-05-23T13:11:24.119655: step 7314, loss 0.273895, acc 0.890625\n",
      "2018-05-23T13:11:24.447785: step 7315, loss 0.222488, acc 0.859375\n",
      "2018-05-23T13:11:24.761936: step 7316, loss 0.226949, acc 0.875\n",
      "2018-05-23T13:11:25.091056: step 7317, loss 0.190851, acc 0.9375\n",
      "2018-05-23T13:11:25.429152: step 7318, loss 0.177856, acc 0.921875\n",
      "2018-05-23T13:11:25.757274: step 7319, loss 0.203199, acc 0.90625\n",
      "2018-05-23T13:11:26.089385: step 7320, loss 0.246052, acc 0.859375\n",
      "2018-05-23T13:11:26.419504: step 7321, loss 0.281528, acc 0.890625\n",
      "2018-05-23T13:11:26.746627: step 7322, loss 0.189501, acc 0.90625\n",
      "2018-05-23T13:11:27.075749: step 7323, loss 0.294842, acc 0.90625\n",
      "2018-05-23T13:11:27.398883: step 7324, loss 0.141164, acc 0.953125\n",
      "2018-05-23T13:11:27.717032: step 7325, loss 0.186543, acc 0.953125\n",
      "2018-05-23T13:11:28.049145: step 7326, loss 0.376541, acc 0.828125\n",
      "2018-05-23T13:11:28.377266: step 7327, loss 0.176825, acc 0.921875\n",
      "2018-05-23T13:11:28.705390: step 7328, loss 0.162598, acc 0.921875\n",
      "2018-05-23T13:11:29.040492: step 7329, loss 0.187181, acc 0.921875\n",
      "2018-05-23T13:11:29.377589: step 7330, loss 0.153029, acc 0.953125\n",
      "2018-05-23T13:11:29.721668: step 7331, loss 0.233269, acc 0.9375\n",
      "2018-05-23T13:11:30.057770: step 7332, loss 0.251143, acc 0.90625\n",
      "2018-05-23T13:11:30.381903: step 7333, loss 0.506284, acc 0.6875\n",
      "2018-05-23T13:11:30.709028: step 7334, loss 0.285506, acc 0.859375\n",
      "2018-05-23T13:11:31.030168: step 7335, loss 0.150731, acc 0.921875\n",
      "2018-05-23T13:11:31.353304: step 7336, loss 0.249088, acc 0.890625\n",
      "2018-05-23T13:11:31.741268: step 7337, loss 0.371673, acc 0.859375\n",
      "2018-05-23T13:11:32.133218: step 7338, loss 0.193766, acc 0.890625\n",
      "2018-05-23T13:11:32.464331: step 7339, loss 0.286321, acc 0.84375\n",
      "2018-05-23T13:11:32.802427: step 7340, loss 0.228781, acc 0.9375\n",
      "2018-05-23T13:11:33.133541: step 7341, loss 0.266271, acc 0.90625\n",
      "2018-05-23T13:11:33.484601: step 7342, loss 0.156057, acc 0.953125\n",
      "2018-05-23T13:11:33.841647: step 7343, loss 0.249592, acc 0.890625\n",
      "2018-05-23T13:11:34.225619: step 7344, loss 0.189941, acc 0.953125\n",
      "2018-05-23T13:11:34.559726: step 7345, loss 0.214171, acc 0.90625\n",
      "2018-05-23T13:11:34.891837: step 7346, loss 0.33017, acc 0.84375\n",
      "2018-05-23T13:11:35.218961: step 7347, loss 0.234303, acc 0.90625\n",
      "2018-05-23T13:11:35.557058: step 7348, loss 0.308919, acc 0.859375\n",
      "2018-05-23T13:11:35.963971: step 7349, loss 0.112449, acc 0.96875\n",
      "2018-05-23T13:11:36.316027: step 7350, loss 0.161044, acc 0.890625\n",
      "2018-05-23T13:11:36.643154: step 7351, loss 0.21349, acc 0.90625\n",
      "2018-05-23T13:11:36.980250: step 7352, loss 0.239191, acc 0.90625\n",
      "2018-05-23T13:11:37.311364: step 7353, loss 0.192834, acc 0.921875\n",
      "2018-05-23T13:11:37.648463: step 7354, loss 0.427222, acc 0.859375\n",
      "2018-05-23T13:11:37.995535: step 7355, loss 0.280385, acc 0.84375\n",
      "2018-05-23T13:11:38.437352: step 7356, loss 0.196662, acc 0.890625\n",
      "2018-05-23T13:11:38.895179: step 7357, loss 0.137354, acc 0.96875\n",
      "2018-05-23T13:11:39.361880: step 7358, loss 0.274875, acc 0.84375\n",
      "2018-05-23T13:11:39.748856: step 7359, loss 0.277731, acc 0.84375\n",
      "2018-05-23T13:11:40.212606: step 7360, loss 0.26518, acc 0.875\n",
      "2018-05-23T13:11:40.607548: step 7361, loss 0.302386, acc 0.890625\n",
      "2018-05-23T13:11:41.071307: step 7362, loss 0.162272, acc 0.9375\n",
      "2018-05-23T13:11:41.563989: step 7363, loss 0.201359, acc 0.90625\n",
      "2018-05-23T13:11:41.928015: step 7364, loss 0.208152, acc 0.96875\n",
      "2018-05-23T13:11:42.367837: step 7365, loss 0.233446, acc 0.890625\n",
      "2018-05-23T13:11:42.814677: step 7366, loss 0.212465, acc 0.90625\n",
      "2018-05-23T13:11:43.166703: step 7367, loss 0.196151, acc 0.9375\n",
      "2018-05-23T13:11:43.515768: step 7368, loss 0.210716, acc 0.890625\n",
      "2018-05-23T13:11:43.859847: step 7369, loss 0.292443, acc 0.859375\n",
      "2018-05-23T13:11:44.212901: step 7370, loss 0.414799, acc 0.796875\n",
      "2018-05-23T13:11:44.676661: step 7371, loss 0.173061, acc 0.9375\n",
      "2018-05-23T13:11:45.153387: step 7372, loss 0.378339, acc 0.84375\n",
      "2018-05-23T13:11:45.533369: step 7373, loss 0.284049, acc 0.890625\n",
      "2018-05-23T13:11:45.878446: step 7374, loss 0.358525, acc 0.875\n",
      "2018-05-23T13:11:46.226517: step 7375, loss 0.284339, acc 0.859375\n",
      "2018-05-23T13:11:46.562615: step 7376, loss 0.205729, acc 0.90625\n",
      "2018-05-23T13:11:46.896721: step 7377, loss 0.248294, acc 0.875\n",
      "2018-05-23T13:11:47.241798: step 7378, loss 0.289016, acc 0.875\n",
      "2018-05-23T13:11:47.579894: step 7379, loss 0.203975, acc 0.890625\n",
      "2018-05-23T13:11:47.916992: step 7380, loss 0.153893, acc 0.953125\n",
      "2018-05-23T13:11:48.263066: step 7381, loss 0.231287, acc 0.90625\n",
      "2018-05-23T13:11:48.623103: step 7382, loss 0.141056, acc 0.953125\n",
      "2018-05-23T13:11:48.957209: step 7383, loss 0.15824, acc 0.9375\n",
      "2018-05-23T13:11:49.301289: step 7384, loss 0.18225, acc 0.859375\n",
      "2018-05-23T13:11:49.639387: step 7385, loss 0.336063, acc 0.90625\n",
      "2018-05-23T13:11:49.974488: step 7386, loss 0.198012, acc 0.90625\n",
      "2018-05-23T13:11:50.316575: step 7387, loss 0.169988, acc 0.953125\n",
      "2018-05-23T13:11:50.696556: step 7388, loss 0.15685, acc 0.921875\n",
      "2018-05-23T13:11:51.040636: step 7389, loss 0.422454, acc 0.765625\n",
      "2018-05-23T13:11:51.388706: step 7390, loss 0.246932, acc 0.921875\n",
      "2018-05-23T13:11:51.774686: step 7391, loss 0.237255, acc 0.921875\n",
      "2018-05-23T13:11:52.200533: step 7392, loss 0.170656, acc 0.953125\n",
      "2018-05-23T13:11:52.550596: step 7393, loss 0.293952, acc 0.84375\n",
      "2018-05-23T13:11:52.893681: step 7394, loss 0.157848, acc 0.90625\n",
      "2018-05-23T13:11:53.229780: step 7395, loss 0.477528, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:11:53.568874: step 7396, loss 0.319482, acc 0.90625\n",
      "2018-05-23T13:11:53.963817: step 7397, loss 0.291581, acc 0.875\n",
      "2018-05-23T13:11:54.301911: step 7398, loss 0.267832, acc 0.875\n",
      "2018-05-23T13:11:54.641006: step 7399, loss 0.237318, acc 0.90625\n",
      "2018-05-23T13:11:54.975112: step 7400, loss 0.24609, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:11:59.400272: step 7400, loss 0.766693, acc 0.723103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7400\n",
      "\n",
      "2018-05-23T13:12:00.936166: step 7401, loss 0.232068, acc 0.953125\n",
      "2018-05-23T13:12:01.324125: step 7402, loss 0.234393, acc 0.921875\n",
      "2018-05-23T13:12:01.690147: step 7403, loss 0.190891, acc 0.921875\n",
      "2018-05-23T13:12:02.065143: step 7404, loss 0.124866, acc 0.96875\n",
      "2018-05-23T13:12:02.407229: step 7405, loss 0.362769, acc 0.859375\n",
      "2018-05-23T13:12:02.751308: step 7406, loss 0.241607, acc 0.90625\n",
      "2018-05-23T13:12:03.133319: step 7407, loss 0.272512, acc 0.90625\n",
      "2018-05-23T13:12:03.509280: step 7408, loss 0.287914, acc 0.90625\n",
      "2018-05-23T13:12:03.863333: step 7409, loss 0.347042, acc 0.859375\n",
      "2018-05-23T13:12:04.269247: step 7410, loss 0.128575, acc 0.953125\n",
      "2018-05-23T13:12:04.620311: step 7411, loss 0.305124, acc 0.84375\n",
      "2018-05-23T13:12:05.022233: step 7412, loss 0.256646, acc 0.890625\n",
      "2018-05-23T13:12:05.446100: step 7413, loss 0.287648, acc 0.875\n",
      "2018-05-23T13:12:05.992636: step 7414, loss 0.187157, acc 0.9375\n",
      "2018-05-23T13:12:06.411516: step 7415, loss 0.330994, acc 0.875\n",
      "2018-05-23T13:12:06.891235: step 7416, loss 0.226564, acc 0.90625\n",
      "2018-05-23T13:12:07.354992: step 7417, loss 0.230203, acc 0.921875\n",
      "2018-05-23T13:12:07.798806: step 7418, loss 0.315936, acc 0.875\n",
      "2018-05-23T13:12:08.220676: step 7419, loss 0.191056, acc 0.890625\n",
      "2018-05-23T13:12:08.688425: step 7420, loss 0.199271, acc 0.90625\n",
      "2018-05-23T13:12:09.168142: step 7421, loss 0.177306, acc 0.90625\n",
      "2018-05-23T13:12:09.564085: step 7422, loss 0.272563, acc 0.859375\n",
      "2018-05-23T13:12:09.961020: step 7423, loss 0.249238, acc 0.890625\n",
      "2018-05-23T13:12:10.308094: step 7424, loss 0.170454, acc 0.9375\n",
      "2018-05-23T13:12:10.649182: step 7425, loss 0.20135, acc 0.921875\n",
      "2018-05-23T13:12:10.990267: step 7426, loss 0.217774, acc 0.90625\n",
      "2018-05-23T13:12:11.331355: step 7427, loss 0.200269, acc 0.9375\n",
      "2018-05-23T13:12:11.662470: step 7428, loss 0.233199, acc 0.875\n",
      "2018-05-23T13:12:12.004553: step 7429, loss 0.1896, acc 0.9375\n",
      "2018-05-23T13:12:12.342649: step 7430, loss 0.262068, acc 0.890625\n",
      "2018-05-23T13:12:12.676755: step 7431, loss 0.225383, acc 0.921875\n",
      "2018-05-23T13:12:13.015851: step 7432, loss 0.313744, acc 0.84375\n",
      "2018-05-23T13:12:13.358930: step 7433, loss 0.23673, acc 0.90625\n",
      "2018-05-23T13:12:13.693038: step 7434, loss 0.244872, acc 0.875\n",
      "2018-05-23T13:12:14.027143: step 7435, loss 0.322095, acc 0.921875\n",
      "2018-05-23T13:12:14.367233: step 7436, loss 0.200778, acc 0.953125\n",
      "2018-05-23T13:12:14.700344: step 7437, loss 0.364383, acc 0.921875\n",
      "2018-05-23T13:12:15.040433: step 7438, loss 0.539892, acc 0.828125\n",
      "2018-05-23T13:12:15.382516: step 7439, loss 0.237708, acc 0.921875\n",
      "2018-05-23T13:12:15.723604: step 7440, loss 0.294547, acc 0.90625\n",
      "2018-05-23T13:12:16.055716: step 7441, loss 0.267869, acc 0.921875\n",
      "2018-05-23T13:12:16.390819: step 7442, loss 0.102505, acc 0.953125\n",
      "2018-05-23T13:12:16.729913: step 7443, loss 0.31062, acc 0.828125\n",
      "2018-05-23T13:12:17.063023: step 7444, loss 0.211726, acc 0.921875\n",
      "2018-05-23T13:12:17.405108: step 7445, loss 0.236504, acc 0.90625\n",
      "2018-05-23T13:12:17.744199: step 7446, loss 0.190062, acc 0.9375\n",
      "2018-05-23T13:12:18.083294: step 7447, loss 0.26291, acc 0.890625\n",
      "2018-05-23T13:12:18.444326: step 7448, loss 0.14888, acc 0.984375\n",
      "2018-05-23T13:12:18.782421: step 7449, loss 0.360615, acc 0.828125\n",
      "2018-05-23T13:12:19.125504: step 7450, loss 0.278249, acc 0.890625\n",
      "2018-05-23T13:12:19.471580: step 7451, loss 0.293166, acc 0.890625\n",
      "2018-05-23T13:12:19.806681: step 7452, loss 0.182392, acc 0.921875\n",
      "2018-05-23T13:12:20.149764: step 7453, loss 0.2849, acc 0.859375\n",
      "2018-05-23T13:12:20.499827: step 7454, loss 0.34318, acc 0.84375\n",
      "2018-05-23T13:12:20.837922: step 7455, loss 0.257605, acc 0.84375\n",
      "2018-05-23T13:12:21.174026: step 7456, loss 0.374765, acc 0.796875\n",
      "2018-05-23T13:12:21.513117: step 7457, loss 0.311126, acc 0.875\n",
      "2018-05-23T13:12:21.842236: step 7458, loss 0.319878, acc 0.859375\n",
      "2018-05-23T13:12:22.183324: step 7459, loss 0.22745, acc 0.90625\n",
      "2018-05-23T13:12:22.528400: step 7460, loss 0.37309, acc 0.84375\n",
      "2018-05-23T13:12:22.866495: step 7461, loss 0.1833, acc 0.921875\n",
      "2018-05-23T13:12:23.203629: step 7462, loss 0.294623, acc 0.84375\n",
      "2018-05-23T13:12:23.541689: step 7463, loss 0.241615, acc 0.921875\n",
      "2018-05-23T13:12:23.879787: step 7464, loss 0.240695, acc 0.90625\n",
      "2018-05-23T13:12:24.221873: step 7465, loss 0.261693, acc 0.90625\n",
      "2018-05-23T13:12:24.557970: step 7466, loss 0.240514, acc 0.875\n",
      "2018-05-23T13:12:24.894071: step 7467, loss 0.302092, acc 0.828125\n",
      "2018-05-23T13:12:25.229175: step 7468, loss 0.115868, acc 1\n",
      "2018-05-23T13:12:25.560292: step 7469, loss 0.217884, acc 0.90625\n",
      "2018-05-23T13:12:25.895393: step 7470, loss 0.301842, acc 0.84375\n",
      "2018-05-23T13:12:26.236481: step 7471, loss 0.234611, acc 0.90625\n",
      "2018-05-23T13:12:26.569590: step 7472, loss 0.193513, acc 0.890625\n",
      "2018-05-23T13:12:26.905691: step 7473, loss 0.341417, acc 0.8125\n",
      "2018-05-23T13:12:27.239796: step 7474, loss 0.322261, acc 0.875\n",
      "2018-05-23T13:12:27.585870: step 7475, loss 0.344217, acc 0.859375\n",
      "2018-05-23T13:12:27.915988: step 7476, loss 0.24975, acc 0.875\n",
      "2018-05-23T13:12:28.252088: step 7477, loss 0.206218, acc 0.90625\n",
      "2018-05-23T13:12:28.590217: step 7478, loss 0.174575, acc 0.90625\n",
      "2018-05-23T13:12:28.924290: step 7479, loss 0.199087, acc 0.921875\n",
      "2018-05-23T13:12:29.260393: step 7480, loss 0.190034, acc 0.953125\n",
      "2018-05-23T13:12:29.592503: step 7481, loss 0.243432, acc 0.90625\n",
      "2018-05-23T13:12:29.929603: step 7482, loss 0.222087, acc 0.875\n",
      "2018-05-23T13:12:30.265703: step 7483, loss 0.260559, acc 0.890625\n",
      "2018-05-23T13:12:30.601804: step 7484, loss 0.251425, acc 0.890625\n",
      "2018-05-23T13:12:30.937903: step 7485, loss 0.211991, acc 0.921875\n",
      "2018-05-23T13:12:31.275004: step 7486, loss 0.239533, acc 0.890625\n",
      "2018-05-23T13:12:31.608112: step 7487, loss 0.374845, acc 0.828125\n",
      "2018-05-23T13:12:31.938227: step 7488, loss 0.33165, acc 0.828125\n",
      "2018-05-23T13:12:32.277320: step 7489, loss 0.257252, acc 0.875\n",
      "2018-05-23T13:12:32.610431: step 7490, loss 0.138575, acc 0.953125\n",
      "2018-05-23T13:12:32.941543: step 7491, loss 0.174074, acc 0.921875\n",
      "2018-05-23T13:12:33.279640: step 7492, loss 0.21222, acc 0.921875\n",
      "2018-05-23T13:12:33.658625: step 7493, loss 0.319733, acc 0.796875\n",
      "2018-05-23T13:12:33.994726: step 7494, loss 0.159159, acc 0.921875\n",
      "2018-05-23T13:12:34.332821: step 7495, loss 0.2269, acc 0.921875\n",
      "2018-05-23T13:12:34.670917: step 7496, loss 0.315127, acc 0.875\n",
      "2018-05-23T13:12:35.005024: step 7497, loss 0.244, acc 0.90625\n",
      "2018-05-23T13:12:35.338134: step 7498, loss 0.263334, acc 0.875\n",
      "2018-05-23T13:12:35.706147: step 7499, loss 0.239177, acc 0.859375\n",
      "2018-05-23T13:12:36.038259: step 7500, loss 0.307011, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:12:40.274926: step 7500, loss 0.775142, acc 0.719674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7500\n",
      "\n",
      "2018-05-23T13:12:41.331100: step 7501, loss 0.231877, acc 0.890625\n",
      "2018-05-23T13:12:41.785883: step 7502, loss 0.131826, acc 0.96875\n",
      "2018-05-23T13:12:42.142930: step 7503, loss 0.227723, acc 0.9375\n",
      "2018-05-23T13:12:42.488005: step 7504, loss 0.236672, acc 0.921875\n",
      "2018-05-23T13:12:42.826100: step 7505, loss 0.27193, acc 0.890625\n",
      "2018-05-23T13:12:43.167190: step 7506, loss 0.25688, acc 0.890625\n",
      "2018-05-23T13:12:43.497304: step 7507, loss 0.212584, acc 0.90625\n",
      "2018-05-23T13:12:43.829416: step 7508, loss 0.147732, acc 0.90625\n",
      "2018-05-23T13:12:44.173496: step 7509, loss 0.172376, acc 0.9375\n",
      "2018-05-23T13:12:44.504612: step 7510, loss 0.278075, acc 0.875\n",
      "2018-05-23T13:12:44.833730: step 7511, loss 0.13407, acc 0.953125\n",
      "2018-05-23T13:12:45.179803: step 7512, loss 0.350166, acc 0.84375\n",
      "2018-05-23T13:12:45.514907: step 7513, loss 0.203086, acc 0.890625\n",
      "2018-05-23T13:12:45.851010: step 7514, loss 0.238743, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:12:46.196087: step 7515, loss 0.255466, acc 0.875\n",
      "2018-05-23T13:12:46.526201: step 7516, loss 0.234759, acc 0.921875\n",
      "2018-05-23T13:12:46.858314: step 7517, loss 0.227552, acc 0.90625\n",
      "2018-05-23T13:12:47.201395: step 7518, loss 0.228865, acc 0.890625\n",
      "2018-05-23T13:12:47.531514: step 7519, loss 0.321604, acc 0.859375\n",
      "2018-05-23T13:12:47.860632: step 7520, loss 0.170763, acc 0.921875\n",
      "2018-05-23T13:12:48.195736: step 7521, loss 0.153495, acc 0.921875\n",
      "2018-05-23T13:12:48.543807: step 7522, loss 0.174953, acc 0.875\n",
      "2018-05-23T13:12:48.871926: step 7523, loss 0.225429, acc 0.90625\n",
      "2018-05-23T13:12:49.206034: step 7524, loss 0.176078, acc 0.921875\n",
      "2018-05-23T13:12:49.532160: step 7525, loss 0.377526, acc 0.875\n",
      "2018-05-23T13:12:49.861279: step 7526, loss 0.363727, acc 0.875\n",
      "2018-05-23T13:12:50.188407: step 7527, loss 0.331029, acc 0.875\n",
      "2018-05-23T13:12:50.514532: step 7528, loss 0.321156, acc 0.84375\n",
      "2018-05-23T13:12:50.856617: step 7529, loss 0.234472, acc 0.875\n",
      "2018-05-23T13:12:51.185736: step 7530, loss 0.18532, acc 0.9375\n",
      "2018-05-23T13:12:51.518845: step 7531, loss 0.099216, acc 0.96875\n",
      "2018-05-23T13:12:51.849959: step 7532, loss 0.27311, acc 0.90625\n",
      "2018-05-23T13:12:52.195036: step 7533, loss 0.173206, acc 0.953125\n",
      "2018-05-23T13:12:52.528146: step 7534, loss 0.208061, acc 0.9375\n",
      "2018-05-23T13:12:52.857267: step 7535, loss 0.218751, acc 0.84375\n",
      "2018-05-23T13:12:53.190374: step 7536, loss 0.222901, acc 0.90625\n",
      "2018-05-23T13:12:53.526475: step 7537, loss 0.174455, acc 0.90625\n",
      "2018-05-23T13:12:53.853599: step 7538, loss 0.30946, acc 0.875\n",
      "2018-05-23T13:12:54.194687: step 7539, loss 0.151046, acc 0.96875\n",
      "2018-05-23T13:12:54.535774: step 7540, loss 0.251276, acc 0.90625\n",
      "2018-05-23T13:12:54.864895: step 7541, loss 0.275505, acc 0.859375\n",
      "2018-05-23T13:12:55.201992: step 7542, loss 0.258331, acc 0.875\n",
      "2018-05-23T13:12:55.535101: step 7543, loss 0.134427, acc 0.9375\n",
      "2018-05-23T13:12:55.870246: step 7544, loss 0.15768, acc 0.953125\n",
      "2018-05-23T13:12:56.203313: step 7545, loss 0.239279, acc 0.90625\n",
      "2018-05-23T13:12:56.537420: step 7546, loss 0.233187, acc 0.90625\n",
      "2018-05-23T13:12:56.865541: step 7547, loss 0.327327, acc 0.859375\n",
      "2018-05-23T13:12:57.202642: step 7548, loss 0.204488, acc 0.890625\n",
      "2018-05-23T13:12:57.536746: step 7549, loss 0.193997, acc 0.9375\n",
      "2018-05-23T13:12:57.864870: step 7550, loss 0.242, acc 0.875\n",
      "2018-05-23T13:12:58.195985: step 7551, loss 0.219054, acc 0.875\n",
      "2018-05-23T13:12:58.531088: step 7552, loss 0.400624, acc 0.765625\n",
      "2018-05-23T13:12:58.863198: step 7553, loss 0.223375, acc 0.875\n",
      "2018-05-23T13:12:59.203288: step 7554, loss 0.279851, acc 0.859375\n",
      "2018-05-23T13:12:59.537394: step 7555, loss 0.295035, acc 0.90625\n",
      "2018-05-23T13:12:59.866516: step 7556, loss 0.368525, acc 0.828125\n",
      "2018-05-23T13:13:00.208598: step 7557, loss 0.190017, acc 0.921875\n",
      "2018-05-23T13:13:00.544699: step 7558, loss 0.278452, acc 0.890625\n",
      "2018-05-23T13:13:00.879804: step 7559, loss 0.486062, acc 0.796875\n",
      "2018-05-23T13:13:01.216900: step 7560, loss 0.324207, acc 0.828125\n",
      "2018-05-23T13:13:01.547018: step 7561, loss 0.351234, acc 0.859375\n",
      "2018-05-23T13:13:01.872150: step 7562, loss 0.286689, acc 0.859375\n",
      "2018-05-23T13:13:02.204259: step 7563, loss 0.361166, acc 0.90625\n",
      "2018-05-23T13:13:02.533379: step 7564, loss 0.292925, acc 0.890625\n",
      "2018-05-23T13:13:02.862499: step 7565, loss 0.381325, acc 0.84375\n",
      "2018-05-23T13:13:03.193615: step 7566, loss 0.327187, acc 0.890625\n",
      "2018-05-23T13:13:03.562627: step 7567, loss 0.363148, acc 0.875\n",
      "2018-05-23T13:13:03.893739: step 7568, loss 0.320644, acc 0.875\n",
      "2018-05-23T13:13:04.224855: step 7569, loss 0.250554, acc 0.90625\n",
      "2018-05-23T13:13:04.559958: step 7570, loss 0.189802, acc 0.96875\n",
      "2018-05-23T13:13:04.891072: step 7571, loss 0.233159, acc 0.890625\n",
      "2018-05-23T13:13:05.221191: step 7572, loss 0.174093, acc 0.90625\n",
      "2018-05-23T13:13:05.554299: step 7573, loss 0.291525, acc 0.90625\n",
      "2018-05-23T13:13:05.906358: step 7574, loss 0.244583, acc 0.890625\n",
      "2018-05-23T13:13:06.238467: step 7575, loss 0.221837, acc 0.90625\n",
      "2018-05-23T13:13:06.573573: step 7576, loss 0.239292, acc 0.90625\n",
      "2018-05-23T13:13:06.904685: step 7577, loss 0.411417, acc 0.84375\n",
      "2018-05-23T13:13:07.234803: step 7578, loss 0.238163, acc 0.921875\n",
      "2018-05-23T13:13:07.565953: step 7579, loss 0.322044, acc 0.859375\n",
      "2018-05-23T13:13:07.892043: step 7580, loss 0.254263, acc 0.859375\n",
      "2018-05-23T13:13:08.235128: step 7581, loss 0.211481, acc 0.90625\n",
      "2018-05-23T13:13:08.565243: step 7582, loss 0.277449, acc 0.859375\n",
      "2018-05-23T13:13:08.903339: step 7583, loss 0.346178, acc 0.859375\n",
      "2018-05-23T13:13:09.242432: step 7584, loss 0.330243, acc 0.828125\n",
      "2018-05-23T13:13:09.571550: step 7585, loss 0.251236, acc 0.828125\n",
      "2018-05-23T13:13:09.906654: step 7586, loss 0.299012, acc 0.859375\n",
      "2018-05-23T13:13:10.240760: step 7587, loss 0.352297, acc 0.8125\n",
      "2018-05-23T13:13:10.569881: step 7588, loss 0.298352, acc 0.859375\n",
      "2018-05-23T13:13:10.905982: step 7589, loss 0.232654, acc 0.84375\n",
      "2018-05-23T13:13:11.234105: step 7590, loss 0.261368, acc 0.90625\n",
      "2018-05-23T13:13:11.561228: step 7591, loss 0.210807, acc 0.890625\n",
      "2018-05-23T13:13:11.901319: step 7592, loss 0.199233, acc 0.875\n",
      "2018-05-23T13:13:12.236421: step 7593, loss 0.294239, acc 0.875\n",
      "2018-05-23T13:13:12.571528: step 7594, loss 0.332612, acc 0.796875\n",
      "2018-05-23T13:13:12.910618: step 7595, loss 0.201755, acc 0.90625\n",
      "2018-05-23T13:13:13.244726: step 7596, loss 0.256069, acc 0.890625\n",
      "2018-05-23T13:13:13.583817: step 7597, loss 0.255869, acc 0.90625\n",
      "2018-05-23T13:13:13.919920: step 7598, loss 0.143871, acc 0.9375\n",
      "2018-05-23T13:13:14.255434: step 7599, loss 0.25201, acc 0.90625\n",
      "2018-05-23T13:13:14.583556: step 7600, loss 0.22421, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:13:18.790301: step 7600, loss 0.765185, acc 0.721675\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7600\n",
      "\n",
      "2018-05-23T13:13:19.992091: step 7601, loss 0.18837, acc 0.96875\n",
      "2018-05-23T13:13:20.401990: step 7602, loss 0.294523, acc 0.859375\n",
      "2018-05-23T13:13:20.749061: step 7603, loss 0.259765, acc 0.890625\n",
      "2018-05-23T13:13:21.077184: step 7604, loss 0.121035, acc 0.96875\n",
      "2018-05-23T13:13:21.396331: step 7605, loss 0.265385, acc 0.875\n",
      "2018-05-23T13:13:21.714480: step 7606, loss 0.492379, acc 0.78125\n",
      "2018-05-23T13:13:22.035618: step 7607, loss 0.261555, acc 0.875\n",
      "2018-05-23T13:13:22.353769: step 7608, loss 0.22754, acc 0.90625\n",
      "2018-05-23T13:13:22.665935: step 7609, loss 0.256923, acc 0.921875\n",
      "2018-05-23T13:13:22.980092: step 7610, loss 0.167452, acc 0.953125\n",
      "2018-05-23T13:13:23.304225: step 7611, loss 0.29882, acc 0.84375\n",
      "2018-05-23T13:13:23.622376: step 7612, loss 0.182756, acc 0.90625\n",
      "2018-05-23T13:13:23.946506: step 7613, loss 0.206393, acc 0.921875\n",
      "2018-05-23T13:13:24.271637: step 7614, loss 0.208217, acc 0.9375\n",
      "2018-05-23T13:13:24.582806: step 7615, loss 0.287804, acc 0.875\n",
      "2018-05-23T13:13:24.899998: step 7616, loss 0.16275, acc 0.921875\n",
      "2018-05-23T13:13:25.218147: step 7617, loss 0.211574, acc 0.9375\n",
      "2018-05-23T13:13:25.533304: step 7618, loss 0.261106, acc 0.90625\n",
      "2018-05-23T13:13:25.849456: step 7619, loss 0.260775, acc 0.90625\n",
      "2018-05-23T13:13:26.170599: step 7620, loss 0.215036, acc 0.953125\n",
      "2018-05-23T13:13:26.486753: step 7621, loss 0.192198, acc 0.921875\n",
      "2018-05-23T13:13:26.795927: step 7622, loss 0.294277, acc 0.875\n",
      "2018-05-23T13:13:27.115071: step 7623, loss 0.387118, acc 0.84375\n",
      "2018-05-23T13:13:27.432222: step 7624, loss 0.264574, acc 0.859375\n",
      "2018-05-23T13:13:27.741395: step 7625, loss 0.326698, acc 0.84375\n",
      "2018-05-23T13:13:28.056554: step 7626, loss 0.357281, acc 0.859375\n",
      "2018-05-23T13:13:28.372709: step 7627, loss 0.333832, acc 0.84375\n",
      "2018-05-23T13:13:28.687864: step 7628, loss 0.272867, acc 0.859375\n",
      "2018-05-23T13:13:28.999033: step 7629, loss 0.222734, acc 0.890625\n",
      "2018-05-23T13:13:29.323163: step 7630, loss 0.31098, acc 0.875\n",
      "2018-05-23T13:13:29.637326: step 7631, loss 0.247897, acc 0.875\n",
      "2018-05-23T13:13:29.951483: step 7632, loss 0.241716, acc 0.875\n",
      "2018-05-23T13:13:30.269633: step 7633, loss 0.103738, acc 0.953125\n",
      "2018-05-23T13:13:30.596759: step 7634, loss 0.235995, acc 0.921875\n",
      "2018-05-23T13:13:30.923882: step 7635, loss 0.17824, acc 0.921875\n",
      "2018-05-23T13:13:31.252005: step 7636, loss 0.254457, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:13:31.574143: step 7637, loss 0.237176, acc 0.9375\n",
      "2018-05-23T13:13:31.891294: step 7638, loss 0.208721, acc 0.921875\n",
      "2018-05-23T13:13:32.212435: step 7639, loss 0.159037, acc 0.9375\n",
      "2018-05-23T13:13:32.528591: step 7640, loss 0.221103, acc 0.890625\n",
      "2018-05-23T13:13:32.846738: step 7641, loss 0.221088, acc 0.9375\n",
      "2018-05-23T13:13:33.171867: step 7642, loss 0.258978, acc 0.875\n",
      "2018-05-23T13:13:33.497995: step 7643, loss 0.16176, acc 0.90625\n",
      "2018-05-23T13:13:33.819136: step 7644, loss 0.264004, acc 0.875\n",
      "2018-05-23T13:13:34.134296: step 7645, loss 0.225287, acc 0.90625\n",
      "2018-05-23T13:13:34.447455: step 7646, loss 0.203987, acc 0.9375\n",
      "2018-05-23T13:13:34.765605: step 7647, loss 0.233482, acc 0.90625\n",
      "2018-05-23T13:13:35.092729: step 7648, loss 0.208552, acc 0.90625\n",
      "2018-05-23T13:13:35.409881: step 7649, loss 0.226885, acc 0.90625\n",
      "2018-05-23T13:13:35.786872: step 7650, loss 0.154103, acc 0.953125\n",
      "2018-05-23T13:13:36.105021: step 7651, loss 0.232079, acc 0.90625\n",
      "2018-05-23T13:13:36.421176: step 7652, loss 0.236626, acc 0.890625\n",
      "2018-05-23T13:13:36.742317: step 7653, loss 0.284171, acc 0.890625\n",
      "2018-05-23T13:13:37.058470: step 7654, loss 0.271132, acc 0.859375\n",
      "2018-05-23T13:13:37.376621: step 7655, loss 0.115575, acc 0.96875\n",
      "2018-05-23T13:13:37.716710: step 7656, loss 0.20711, acc 0.90625\n",
      "2018-05-23T13:13:38.051814: step 7657, loss 0.284885, acc 0.84375\n",
      "2018-05-23T13:13:38.369963: step 7658, loss 0.215803, acc 0.890625\n",
      "2018-05-23T13:13:38.686846: step 7659, loss 0.435726, acc 0.78125\n",
      "2018-05-23T13:13:39.003998: step 7660, loss 0.261911, acc 0.875\n",
      "2018-05-23T13:13:39.327133: step 7661, loss 0.163539, acc 0.953125\n",
      "2018-05-23T13:13:39.641293: step 7662, loss 0.241497, acc 0.890625\n",
      "2018-05-23T13:13:39.962434: step 7663, loss 0.357788, acc 0.828125\n",
      "2018-05-23T13:13:40.284570: step 7664, loss 0.468321, acc 0.796875\n",
      "2018-05-23T13:13:40.600727: step 7665, loss 0.158712, acc 0.921875\n",
      "2018-05-23T13:13:40.924858: step 7666, loss 0.202217, acc 0.890625\n",
      "2018-05-23T13:13:41.248993: step 7667, loss 0.272339, acc 0.90625\n",
      "2018-05-23T13:13:41.563151: step 7668, loss 0.265527, acc 0.921875\n",
      "2018-05-23T13:13:41.880304: step 7669, loss 0.198752, acc 0.90625\n",
      "2018-05-23T13:13:42.211418: step 7670, loss 0.18923, acc 0.921875\n",
      "2018-05-23T13:13:42.533554: step 7671, loss 0.165321, acc 0.90625\n",
      "2018-05-23T13:13:42.853699: step 7672, loss 0.222809, acc 0.875\n",
      "2018-05-23T13:13:43.190796: step 7673, loss 0.321934, acc 0.84375\n",
      "2018-05-23T13:13:43.510942: step 7674, loss 0.325079, acc 0.859375\n",
      "2018-05-23T13:13:43.830086: step 7675, loss 0.237438, acc 0.9375\n",
      "2018-05-23T13:13:44.157211: step 7676, loss 0.184815, acc 0.921875\n",
      "2018-05-23T13:13:44.479349: step 7677, loss 0.186124, acc 0.921875\n",
      "2018-05-23T13:13:44.799493: step 7678, loss 0.234649, acc 0.890625\n",
      "2018-05-23T13:13:45.116644: step 7679, loss 0.274871, acc 0.859375\n",
      "2018-05-23T13:13:45.437787: step 7680, loss 0.214503, acc 0.9375\n",
      "2018-05-23T13:13:45.763913: step 7681, loss 0.369585, acc 0.859375\n",
      "2018-05-23T13:13:46.083060: step 7682, loss 0.39496, acc 0.84375\n",
      "2018-05-23T13:13:46.403204: step 7683, loss 0.250102, acc 0.921875\n",
      "2018-05-23T13:13:46.722350: step 7684, loss 0.328731, acc 0.859375\n",
      "2018-05-23T13:13:47.046484: step 7685, loss 0.350432, acc 0.8125\n",
      "2018-05-23T13:13:47.368619: step 7686, loss 0.276465, acc 0.890625\n",
      "2018-05-23T13:13:47.684776: step 7687, loss 0.175476, acc 0.90625\n",
      "2018-05-23T13:13:48.003922: step 7688, loss 0.270354, acc 0.9375\n",
      "2018-05-23T13:13:48.325062: step 7689, loss 0.241116, acc 0.921875\n",
      "2018-05-23T13:13:48.659168: step 7690, loss 0.302252, acc 0.84375\n",
      "2018-05-23T13:13:48.985294: step 7691, loss 0.219223, acc 0.890625\n",
      "2018-05-23T13:13:49.315412: step 7692, loss 0.229442, acc 0.875\n",
      "2018-05-23T13:13:49.641539: step 7693, loss 0.201283, acc 0.875\n",
      "2018-05-23T13:13:49.956696: step 7694, loss 0.380322, acc 0.890625\n",
      "2018-05-23T13:13:50.283822: step 7695, loss 0.287172, acc 0.859375\n",
      "2018-05-23T13:13:50.616930: step 7696, loss 0.220963, acc 0.90625\n",
      "2018-05-23T13:13:50.937074: step 7697, loss 0.401056, acc 0.84375\n",
      "2018-05-23T13:13:51.260210: step 7698, loss 0.22414, acc 0.890625\n",
      "2018-05-23T13:13:51.582348: step 7699, loss 0.341516, acc 0.859375\n",
      "2018-05-23T13:13:51.899499: step 7700, loss 0.206963, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:13:56.178052: step 7700, loss 0.776618, acc 0.723818\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7700\n",
      "\n",
      "2018-05-23T13:13:57.548353: step 7701, loss 0.27311, acc 0.859375\n",
      "2018-05-23T13:13:57.884455: step 7702, loss 0.239756, acc 0.859375\n",
      "2018-05-23T13:13:58.222549: step 7703, loss 0.235881, acc 0.921875\n",
      "2018-05-23T13:13:58.549674: step 7704, loss 0.227533, acc 0.90625\n",
      "2018-05-23T13:13:58.865860: step 7705, loss 0.266784, acc 0.90625\n",
      "2018-05-23T13:13:59.198936: step 7706, loss 0.281488, acc 0.875\n",
      "2018-05-23T13:13:59.522072: step 7707, loss 0.415494, acc 0.78125\n",
      "2018-05-23T13:13:59.845209: step 7708, loss 0.397551, acc 0.921875\n",
      "2018-05-23T13:14:00.180311: step 7709, loss 0.267819, acc 0.921875\n",
      "2018-05-23T13:14:00.522397: step 7710, loss 0.231413, acc 0.890625\n",
      "2018-05-23T13:14:00.923323: step 7711, loss 0.313298, acc 0.859375\n",
      "2018-05-23T13:14:01.279372: step 7712, loss 0.347463, acc 0.890625\n",
      "2018-05-23T13:14:01.634424: step 7713, loss 0.337128, acc 0.84375\n",
      "2018-05-23T13:14:01.961547: step 7714, loss 0.329104, acc 0.8125\n",
      "2018-05-23T13:14:02.298645: step 7715, loss 0.384019, acc 0.828125\n",
      "2018-05-23T13:14:02.622779: step 7716, loss 0.186906, acc 0.90625\n",
      "2018-05-23T13:14:03.001528: step 7717, loss 0.236619, acc 0.90625\n",
      "2018-05-23T13:14:03.339624: step 7718, loss 0.211195, acc 0.890625\n",
      "2018-05-23T13:14:03.713622: step 7719, loss 0.201019, acc 0.921875\n",
      "2018-05-23T13:14:04.030773: step 7720, loss 0.23587, acc 0.921875\n",
      "2018-05-23T13:14:04.357897: step 7721, loss 0.186674, acc 0.890625\n",
      "2018-05-23T13:14:04.685022: step 7722, loss 0.0812895, acc 0.984375\n",
      "2018-05-23T13:14:05.006165: step 7723, loss 0.312608, acc 0.890625\n",
      "2018-05-23T13:14:05.335283: step 7724, loss 0.276676, acc 0.921875\n",
      "2018-05-23T13:14:05.659416: step 7725, loss 0.245914, acc 0.890625\n",
      "2018-05-23T13:14:05.982550: step 7726, loss 0.262277, acc 0.890625\n",
      "2018-05-23T13:14:06.312668: step 7727, loss 0.312695, acc 0.875\n",
      "2018-05-23T13:14:06.635804: step 7728, loss 0.265394, acc 0.921875\n",
      "2018-05-23T13:14:06.954951: step 7729, loss 0.377012, acc 0.8125\n",
      "2018-05-23T13:14:07.281096: step 7730, loss 0.189906, acc 0.890625\n",
      "2018-05-23T13:14:07.600223: step 7731, loss 0.277923, acc 0.875\n",
      "2018-05-23T13:14:07.922407: step 7732, loss 0.158132, acc 0.921875\n",
      "2018-05-23T13:14:08.246543: step 7733, loss 0.286764, acc 0.84375\n",
      "2018-05-23T13:14:08.563691: step 7734, loss 0.29982, acc 0.875\n",
      "2018-05-23T13:14:08.883836: step 7735, loss 0.230663, acc 0.921875\n",
      "2018-05-23T13:14:09.212954: step 7736, loss 0.173617, acc 0.921875\n",
      "2018-05-23T13:14:09.539082: step 7737, loss 0.354749, acc 0.828125\n",
      "2018-05-23T13:14:09.862217: step 7738, loss 0.268413, acc 0.875\n",
      "2018-05-23T13:14:10.186353: step 7739, loss 0.211157, acc 0.90625\n",
      "2018-05-23T13:14:10.501508: step 7740, loss 0.175295, acc 0.90625\n",
      "2018-05-23T13:14:10.825641: step 7741, loss 0.241542, acc 0.90625\n",
      "2018-05-23T13:14:11.152767: step 7742, loss 0.235888, acc 0.921875\n",
      "2018-05-23T13:14:11.469916: step 7743, loss 0.278355, acc 0.84375\n",
      "2018-05-23T13:14:11.795047: step 7744, loss 0.188261, acc 0.9375\n",
      "2018-05-23T13:14:12.116190: step 7745, loss 0.344274, acc 0.875\n",
      "2018-05-23T13:14:12.436333: step 7746, loss 0.242746, acc 0.90625\n",
      "2018-05-23T13:14:12.765452: step 7747, loss 0.253699, acc 0.859375\n",
      "2018-05-23T13:14:13.083599: step 7748, loss 0.31283, acc 0.859375\n",
      "2018-05-23T13:14:13.402745: step 7749, loss 0.207378, acc 0.921875\n",
      "2018-05-23T13:14:13.731898: step 7750, loss 0.24251, acc 0.875\n",
      "2018-05-23T13:14:14.054003: step 7751, loss 0.204077, acc 0.890625\n",
      "2018-05-23T13:14:14.376142: step 7752, loss 0.271814, acc 0.90625\n",
      "2018-05-23T13:14:14.701274: step 7753, loss 0.25904, acc 0.890625\n",
      "2018-05-23T13:14:15.022413: step 7754, loss 0.206602, acc 0.921875\n",
      "2018-05-23T13:14:15.343556: step 7755, loss 0.293053, acc 0.859375\n",
      "2018-05-23T13:14:15.670678: step 7756, loss 0.32679, acc 0.84375\n",
      "2018-05-23T13:14:15.993814: step 7757, loss 0.236348, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:14:16.317947: step 7758, loss 0.205205, acc 0.90625\n",
      "2018-05-23T13:14:16.643077: step 7759, loss 0.160719, acc 0.9375\n",
      "2018-05-23T13:14:16.955242: step 7760, loss 0.236862, acc 0.90625\n",
      "2018-05-23T13:14:17.383099: step 7761, loss 0.328945, acc 0.84375\n",
      "2018-05-23T13:14:17.804970: step 7762, loss 0.192973, acc 0.890625\n",
      "2018-05-23T13:14:18.230830: step 7763, loss 0.238072, acc 0.890625\n",
      "2018-05-23T13:14:18.651705: step 7764, loss 0.221411, acc 0.90625\n",
      "2018-05-23T13:14:19.004759: step 7765, loss 0.273221, acc 0.84375\n",
      "2018-05-23T13:14:19.344849: step 7766, loss 0.296461, acc 0.875\n",
      "2018-05-23T13:14:19.681947: step 7767, loss 0.15117, acc 0.953125\n",
      "2018-05-23T13:14:20.029019: step 7768, loss 0.198278, acc 0.921875\n",
      "2018-05-23T13:14:20.367115: step 7769, loss 0.530094, acc 0.765625\n",
      "2018-05-23T13:14:20.729146: step 7770, loss 0.291838, acc 0.875\n",
      "2018-05-23T13:14:21.072228: step 7771, loss 0.203132, acc 0.90625\n",
      "2018-05-23T13:14:21.417305: step 7772, loss 0.262308, acc 0.90625\n",
      "2018-05-23T13:14:21.753408: step 7773, loss 0.158163, acc 0.953125\n",
      "2018-05-23T13:14:22.101476: step 7774, loss 0.283514, acc 0.859375\n",
      "2018-05-23T13:14:22.437578: step 7775, loss 0.226428, acc 0.890625\n",
      "2018-05-23T13:14:22.772679: step 7776, loss 0.171544, acc 0.953125\n",
      "2018-05-23T13:14:23.130721: step 7777, loss 0.2322, acc 0.859375\n",
      "2018-05-23T13:14:23.472806: step 7778, loss 0.232533, acc 0.90625\n",
      "2018-05-23T13:14:23.811899: step 7779, loss 0.17502, acc 0.9375\n",
      "2018-05-23T13:14:24.158970: step 7780, loss 0.243526, acc 0.9375\n",
      "2018-05-23T13:14:24.496070: step 7781, loss 0.316454, acc 0.875\n",
      "2018-05-23T13:14:24.830178: step 7782, loss 0.413837, acc 0.8125\n",
      "2018-05-23T13:14:25.179241: step 7783, loss 0.251229, acc 0.890625\n",
      "2018-05-23T13:14:25.514345: step 7784, loss 0.283165, acc 0.90625\n",
      "2018-05-23T13:14:25.858425: step 7785, loss 0.25826, acc 0.890625\n",
      "2018-05-23T13:14:26.207250: step 7786, loss 0.344426, acc 0.859375\n",
      "2018-05-23T13:14:26.540348: step 7787, loss 0.366064, acc 0.828125\n",
      "2018-05-23T13:14:26.877445: step 7788, loss 0.201609, acc 0.921875\n",
      "2018-05-23T13:14:27.226510: step 7789, loss 0.304608, acc 0.84375\n",
      "2018-05-23T13:14:27.567601: step 7790, loss 0.271881, acc 0.859375\n",
      "2018-05-23T13:14:27.906691: step 7791, loss 0.484865, acc 0.78125\n",
      "2018-05-23T13:14:28.245787: step 7792, loss 0.405084, acc 0.84375\n",
      "2018-05-23T13:14:28.579891: step 7793, loss 0.300083, acc 0.84375\n",
      "2018-05-23T13:14:28.921975: step 7794, loss 0.186975, acc 0.921875\n",
      "2018-05-23T13:14:29.267054: step 7795, loss 0.247099, acc 0.890625\n",
      "2018-05-23T13:14:29.611133: step 7796, loss 0.330456, acc 0.875\n",
      "2018-05-23T13:14:29.950224: step 7797, loss 0.433817, acc 0.796875\n",
      "2018-05-23T13:14:30.292309: step 7798, loss 0.234612, acc 0.890625\n",
      "2018-05-23T13:14:30.628410: step 7799, loss 0.32765, acc 0.859375\n",
      "2018-05-23T13:14:30.962517: step 7800, loss 0.272084, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:14:35.256030: step 7800, loss 0.82967, acc 0.704386\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7800\n",
      "\n",
      "2018-05-23T13:14:36.567523: step 7801, loss 0.40638, acc 0.828125\n",
      "2018-05-23T13:14:36.927559: step 7802, loss 0.337368, acc 0.796875\n",
      "2018-05-23T13:14:37.262664: step 7803, loss 0.285551, acc 0.84375\n",
      "2018-05-23T13:14:37.590786: step 7804, loss 0.23759, acc 0.921875\n",
      "2018-05-23T13:14:37.909931: step 7805, loss 0.277179, acc 0.859375\n",
      "2018-05-23T13:14:38.232069: step 7806, loss 0.250748, acc 0.90625\n",
      "2018-05-23T13:14:38.556201: step 7807, loss 0.244067, acc 0.875\n",
      "2018-05-23T13:14:38.867371: step 7808, loss 0.14065, acc 0.9375\n",
      "2018-05-23T13:14:39.193496: step 7809, loss 0.389401, acc 0.796875\n",
      "2018-05-23T13:14:39.511646: step 7810, loss 0.33765, acc 0.828125\n",
      "2018-05-23T13:14:39.826802: step 7811, loss 0.292418, acc 0.859375\n",
      "2018-05-23T13:14:40.147943: step 7812, loss 0.312602, acc 0.9375\n",
      "2018-05-23T13:14:40.463101: step 7813, loss 0.339954, acc 0.859375\n",
      "2018-05-23T13:14:40.780252: step 7814, loss 0.343125, acc 0.875\n",
      "2018-05-23T13:14:41.099400: step 7815, loss 0.520843, acc 0.765625\n",
      "2018-05-23T13:14:41.418547: step 7816, loss 0.351677, acc 0.859375\n",
      "2018-05-23T13:14:41.741681: step 7817, loss 0.160617, acc 0.96875\n",
      "2018-05-23T13:14:42.057835: step 7818, loss 0.372943, acc 0.8125\n",
      "2018-05-23T13:14:42.385958: step 7819, loss 0.317367, acc 0.875\n",
      "2018-05-23T13:14:42.705132: step 7820, loss 0.251247, acc 0.859375\n",
      "2018-05-23T13:14:43.023288: step 7821, loss 0.221632, acc 0.890625\n",
      "2018-05-23T13:14:43.342400: step 7822, loss 0.290053, acc 0.890625\n",
      "2018-05-23T13:14:43.663539: step 7823, loss 0.228143, acc 0.921875\n",
      "2018-05-23T13:14:43.985676: step 7824, loss 0.19064, acc 0.953125\n",
      "2018-05-23T13:14:44.308814: step 7825, loss 0.266737, acc 0.859375\n",
      "2018-05-23T13:14:44.625965: step 7826, loss 0.562834, acc 0.78125\n",
      "2018-05-23T13:14:44.942120: step 7827, loss 0.186104, acc 0.921875\n",
      "2018-05-23T13:14:45.267250: step 7828, loss 0.23543, acc 0.890625\n",
      "2018-05-23T13:14:45.586394: step 7829, loss 0.234906, acc 0.90625\n",
      "2018-05-23T13:14:45.911527: step 7830, loss 0.21888, acc 0.90625\n",
      "2018-05-23T13:14:46.233665: step 7831, loss 0.204874, acc 0.859375\n",
      "2018-05-23T13:14:46.554806: step 7832, loss 0.245233, acc 0.890625\n",
      "2018-05-23T13:14:46.875038: step 7833, loss 0.358704, acc 0.90625\n",
      "2018-05-23T13:14:47.202162: step 7834, loss 0.431611, acc 0.84375\n",
      "2018-05-23T13:14:47.537265: step 7835, loss 0.244225, acc 0.90625\n",
      "2018-05-23T13:14:47.864392: step 7836, loss 0.316676, acc 0.859375\n",
      "2018-05-23T13:14:48.192512: step 7837, loss 0.201293, acc 0.921875\n",
      "2018-05-23T13:14:48.505675: step 7838, loss 0.215681, acc 0.90625\n",
      "2018-05-23T13:14:48.837788: step 7839, loss 0.160783, acc 0.921875\n",
      "2018-05-23T13:14:49.165910: step 7840, loss 0.277723, acc 0.84375\n",
      "2018-05-23T13:14:49.485056: step 7841, loss 0.364109, acc 0.84375\n",
      "2018-05-23T13:14:49.804201: step 7842, loss 0.167355, acc 0.9375\n",
      "2018-05-23T13:14:50.126340: step 7843, loss 0.274725, acc 0.875\n",
      "2018-05-23T13:14:50.445488: step 7844, loss 0.227092, acc 0.90625\n",
      "2018-05-23T13:14:50.808516: step 7845, loss 0.257043, acc 0.875\n",
      "2018-05-23T13:14:51.139628: step 7846, loss 0.299228, acc 0.84375\n",
      "2018-05-23T13:14:51.461767: step 7847, loss 0.244492, acc 0.875\n",
      "2018-05-23T13:14:51.799865: step 7848, loss 0.247392, acc 0.890625\n",
      "2018-05-23T13:14:52.125992: step 7849, loss 0.260035, acc 0.921875\n",
      "2018-05-23T13:14:52.457439: step 7850, loss 0.429097, acc 0.765625\n",
      "2018-05-23T13:14:52.774590: step 7851, loss 0.244545, acc 0.90625\n",
      "2018-05-23T13:14:53.103712: step 7852, loss 0.215967, acc 0.921875\n",
      "2018-05-23T13:14:53.426846: step 7853, loss 0.309922, acc 0.859375\n",
      "2018-05-23T13:14:53.747989: step 7854, loss 0.323901, acc 0.828125\n",
      "2018-05-23T13:14:54.075113: step 7855, loss 0.29469, acc 0.90625\n",
      "2018-05-23T13:14:54.395256: step 7856, loss 0.156408, acc 0.96875\n",
      "2018-05-23T13:14:54.723379: step 7857, loss 0.434786, acc 0.84375\n",
      "2018-05-23T13:14:55.052498: step 7858, loss 0.405313, acc 0.8125\n",
      "2018-05-23T13:14:55.372641: step 7859, loss 0.237689, acc 0.890625\n",
      "2018-05-23T13:14:55.701760: step 7860, loss 0.294929, acc 0.890625\n",
      "2018-05-23T13:14:56.025892: step 7861, loss 0.210991, acc 0.90625\n",
      "2018-05-23T13:14:56.353019: step 7862, loss 0.213269, acc 0.890625\n",
      "2018-05-23T13:14:56.673163: step 7863, loss 0.17589, acc 0.890625\n",
      "2018-05-23T13:14:57.000286: step 7864, loss 0.281504, acc 0.90625\n",
      "2018-05-23T13:14:57.324418: step 7865, loss 0.290234, acc 0.890625\n",
      "2018-05-23T13:14:57.651545: step 7866, loss 0.208978, acc 0.921875\n",
      "2018-05-23T13:14:57.983657: step 7867, loss 0.241475, acc 0.90625\n",
      "2018-05-23T13:14:58.307788: step 7868, loss 0.323828, acc 0.84375\n",
      "2018-05-23T13:14:58.632918: step 7869, loss 0.186299, acc 0.90625\n",
      "2018-05-23T13:14:58.958051: step 7870, loss 0.309527, acc 0.890625\n",
      "2018-05-23T13:14:59.284178: step 7871, loss 0.351371, acc 0.84375\n",
      "2018-05-23T13:14:59.605321: step 7872, loss 0.244065, acc 0.890625\n",
      "2018-05-23T13:14:59.936433: step 7873, loss 0.201868, acc 0.921875\n",
      "2018-05-23T13:15:00.286496: step 7874, loss 0.22712, acc 0.890625\n",
      "2018-05-23T13:15:00.619606: step 7875, loss 0.268851, acc 0.796875\n",
      "2018-05-23T13:15:00.956704: step 7876, loss 0.254553, acc 0.875\n",
      "2018-05-23T13:15:01.299786: step 7877, loss 0.293156, acc 0.875\n",
      "2018-05-23T13:15:01.635885: step 7878, loss 0.291518, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:15:01.964007: step 7879, loss 0.227458, acc 0.890625\n",
      "2018-05-23T13:15:02.197384: step 7880, loss 0.198664, acc 0.941176\n",
      "2018-05-23T13:15:02.530494: step 7881, loss 0.267855, acc 0.84375\n",
      "2018-05-23T13:15:02.851634: step 7882, loss 0.168526, acc 0.953125\n",
      "2018-05-23T13:15:03.182749: step 7883, loss 0.139925, acc 0.9375\n",
      "2018-05-23T13:15:03.537798: step 7884, loss 0.255996, acc 0.921875\n",
      "2018-05-23T13:15:03.858938: step 7885, loss 0.161942, acc 0.921875\n",
      "2018-05-23T13:15:04.186064: step 7886, loss 0.183007, acc 0.90625\n",
      "2018-05-23T13:15:04.515184: step 7887, loss 0.231335, acc 0.890625\n",
      "2018-05-23T13:15:04.833333: step 7888, loss 0.277714, acc 0.84375\n",
      "2018-05-23T13:15:05.168436: step 7889, loss 0.38905, acc 0.90625\n",
      "2018-05-23T13:15:05.490573: step 7890, loss 0.210403, acc 0.921875\n",
      "2018-05-23T13:15:05.849613: step 7891, loss 0.258349, acc 0.875\n",
      "2018-05-23T13:15:06.181724: step 7892, loss 0.24754, acc 0.875\n",
      "2018-05-23T13:15:06.503862: step 7893, loss 0.131215, acc 0.953125\n",
      "2018-05-23T13:15:06.825004: step 7894, loss 0.202529, acc 0.953125\n",
      "2018-05-23T13:15:07.153125: step 7895, loss 0.165748, acc 0.921875\n",
      "2018-05-23T13:15:07.479253: step 7896, loss 0.284697, acc 0.875\n",
      "2018-05-23T13:15:07.798399: step 7897, loss 0.192015, acc 0.953125\n",
      "2018-05-23T13:15:08.123529: step 7898, loss 0.170479, acc 0.921875\n",
      "2018-05-23T13:15:08.443673: step 7899, loss 0.267396, acc 0.875\n",
      "2018-05-23T13:15:08.759827: step 7900, loss 0.275044, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:15:13.088247: step 7900, loss 0.754969, acc 0.728961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-7900\n",
      "\n",
      "2018-05-23T13:15:14.178883: step 7901, loss 0.171481, acc 0.953125\n",
      "2018-05-23T13:15:14.684533: step 7902, loss 0.16883, acc 0.90625\n",
      "2018-05-23T13:15:15.051548: step 7903, loss 0.128257, acc 0.953125\n",
      "2018-05-23T13:15:15.400615: step 7904, loss 0.204376, acc 0.90625\n",
      "2018-05-23T13:15:15.733724: step 7905, loss 0.224438, acc 0.890625\n",
      "2018-05-23T13:15:16.060851: step 7906, loss 0.220795, acc 0.9375\n",
      "2018-05-23T13:15:16.387976: step 7907, loss 0.238394, acc 0.90625\n",
      "2018-05-23T13:15:16.733051: step 7908, loss 0.301779, acc 0.859375\n",
      "2018-05-23T13:15:17.064891: step 7909, loss 0.166772, acc 0.9375\n",
      "2018-05-23T13:15:17.390022: step 7910, loss 0.153259, acc 0.9375\n",
      "2018-05-23T13:15:17.714155: step 7911, loss 0.202944, acc 0.9375\n",
      "2018-05-23T13:15:18.039286: step 7912, loss 0.221484, acc 0.90625\n",
      "2018-05-23T13:15:18.369402: step 7913, loss 0.238396, acc 0.90625\n",
      "2018-05-23T13:15:18.718468: step 7914, loss 0.168184, acc 0.9375\n",
      "2018-05-23T13:15:19.044596: step 7915, loss 0.236759, acc 0.890625\n",
      "2018-05-23T13:15:19.367731: step 7916, loss 0.142332, acc 0.9375\n",
      "2018-05-23T13:15:19.690868: step 7917, loss 0.214481, acc 0.890625\n",
      "2018-05-23T13:15:20.010014: step 7918, loss 0.131204, acc 0.953125\n",
      "2018-05-23T13:15:20.334146: step 7919, loss 0.377467, acc 0.828125\n",
      "2018-05-23T13:15:20.661272: step 7920, loss 0.290745, acc 0.828125\n",
      "2018-05-23T13:15:20.986401: step 7921, loss 0.18551, acc 0.90625\n",
      "2018-05-23T13:15:21.310534: step 7922, loss 0.228857, acc 0.890625\n",
      "2018-05-23T13:15:21.633672: step 7923, loss 0.262626, acc 0.90625\n",
      "2018-05-23T13:15:21.954812: step 7924, loss 0.202583, acc 0.9375\n",
      "2018-05-23T13:15:22.277946: step 7925, loss 0.144801, acc 0.96875\n",
      "2018-05-23T13:15:22.603079: step 7926, loss 0.27568, acc 0.859375\n",
      "2018-05-23T13:15:22.922222: step 7927, loss 0.428689, acc 0.8125\n",
      "2018-05-23T13:15:23.251342: step 7928, loss 0.14713, acc 0.9375\n",
      "2018-05-23T13:15:23.574480: step 7929, loss 0.130341, acc 0.9375\n",
      "2018-05-23T13:15:23.895360: step 7930, loss 0.345396, acc 0.875\n",
      "2018-05-23T13:15:24.220491: step 7931, loss 0.180888, acc 0.921875\n",
      "2018-05-23T13:15:24.546617: step 7932, loss 0.192979, acc 0.890625\n",
      "2018-05-23T13:15:24.870749: step 7933, loss 0.257447, acc 0.90625\n",
      "2018-05-23T13:15:25.196876: step 7934, loss 0.136067, acc 0.953125\n",
      "2018-05-23T13:15:25.519015: step 7935, loss 0.104144, acc 1\n",
      "2018-05-23T13:15:25.838162: step 7936, loss 0.230347, acc 0.921875\n",
      "2018-05-23T13:15:26.164288: step 7937, loss 0.164002, acc 0.921875\n",
      "2018-05-23T13:15:26.486428: step 7938, loss 0.281126, acc 0.921875\n",
      "2018-05-23T13:15:26.821531: step 7939, loss 0.124076, acc 0.9375\n",
      "2018-05-23T13:15:27.148654: step 7940, loss 0.179816, acc 0.890625\n",
      "2018-05-23T13:15:27.470795: step 7941, loss 0.116302, acc 0.984375\n",
      "2018-05-23T13:15:27.795923: step 7942, loss 0.182887, acc 0.921875\n",
      "2018-05-23T13:15:28.132025: step 7943, loss 0.217418, acc 0.890625\n",
      "2018-05-23T13:15:28.454163: step 7944, loss 0.276866, acc 0.875\n",
      "2018-05-23T13:15:28.777436: step 7945, loss 0.129681, acc 0.953125\n",
      "2018-05-23T13:15:29.096584: step 7946, loss 0.214113, acc 0.921875\n",
      "2018-05-23T13:15:29.421011: step 7947, loss 0.171765, acc 0.90625\n",
      "2018-05-23T13:15:29.745142: step 7948, loss 0.142671, acc 0.953125\n",
      "2018-05-23T13:15:30.065287: step 7949, loss 0.158933, acc 0.9375\n",
      "2018-05-23T13:15:30.391414: step 7950, loss 0.119526, acc 0.984375\n",
      "2018-05-23T13:15:30.718539: step 7951, loss 0.141168, acc 0.921875\n",
      "2018-05-23T13:15:31.042706: step 7952, loss 0.158117, acc 0.90625\n",
      "2018-05-23T13:15:31.360821: step 7953, loss 0.111914, acc 0.984375\n",
      "2018-05-23T13:15:31.686947: step 7954, loss 0.228561, acc 0.890625\n",
      "2018-05-23T13:15:32.018062: step 7955, loss 0.211338, acc 0.9375\n",
      "2018-05-23T13:15:32.340201: step 7956, loss 0.182134, acc 0.9375\n",
      "2018-05-23T13:15:32.666328: step 7957, loss 0.193779, acc 0.921875\n",
      "2018-05-23T13:15:32.996444: step 7958, loss 0.182946, acc 0.875\n",
      "2018-05-23T13:15:33.319579: step 7959, loss 0.176154, acc 0.90625\n",
      "2018-05-23T13:15:33.663659: step 7960, loss 0.202555, acc 0.90625\n",
      "2018-05-23T13:15:33.995772: step 7961, loss 0.321309, acc 0.84375\n",
      "2018-05-23T13:15:34.320903: step 7962, loss 0.163271, acc 0.921875\n",
      "2018-05-23T13:15:34.654012: step 7963, loss 0.11583, acc 0.984375\n",
      "2018-05-23T13:15:34.992105: step 7964, loss 0.120624, acc 0.953125\n",
      "2018-05-23T13:15:35.339178: step 7965, loss 0.197467, acc 0.90625\n",
      "2018-05-23T13:15:35.685253: step 7966, loss 0.239373, acc 0.859375\n",
      "2018-05-23T13:15:36.033319: step 7967, loss 0.233232, acc 0.890625\n",
      "2018-05-23T13:15:36.381390: step 7968, loss 0.0799497, acc 0.984375\n",
      "2018-05-23T13:15:36.735443: step 7969, loss 0.242098, acc 0.875\n",
      "2018-05-23T13:15:37.080518: step 7970, loss 0.27504, acc 0.890625\n",
      "2018-05-23T13:15:37.430583: step 7971, loss 0.151127, acc 0.921875\n",
      "2018-05-23T13:15:37.783637: step 7972, loss 0.107395, acc 0.96875\n",
      "2018-05-23T13:15:38.128715: step 7973, loss 0.217137, acc 0.890625\n",
      "2018-05-23T13:15:38.481770: step 7974, loss 0.191447, acc 0.9375\n",
      "2018-05-23T13:15:38.828843: step 7975, loss 0.281827, acc 0.859375\n",
      "2018-05-23T13:15:39.174915: step 7976, loss 0.170527, acc 0.921875\n",
      "2018-05-23T13:15:39.518000: step 7977, loss 0.201852, acc 0.9375\n",
      "2018-05-23T13:15:39.862077: step 7978, loss 0.262584, acc 0.859375\n",
      "2018-05-23T13:15:40.210148: step 7979, loss 0.265435, acc 0.859375\n",
      "2018-05-23T13:15:40.557218: step 7980, loss 0.216851, acc 0.921875\n",
      "2018-05-23T13:15:40.903291: step 7981, loss 0.187035, acc 0.890625\n",
      "2018-05-23T13:15:41.255351: step 7982, loss 0.156416, acc 0.890625\n",
      "2018-05-23T13:15:41.604417: step 7983, loss 0.232342, acc 0.921875\n",
      "2018-05-23T13:15:41.959466: step 7984, loss 0.186957, acc 0.921875\n",
      "2018-05-23T13:15:42.307536: step 7985, loss 0.177464, acc 0.9375\n",
      "2018-05-23T13:15:42.652614: step 7986, loss 0.239957, acc 0.875\n",
      "2018-05-23T13:15:43.008660: step 7987, loss 0.196057, acc 0.9375\n",
      "2018-05-23T13:15:43.352740: step 7988, loss 0.13929, acc 0.953125\n",
      "2018-05-23T13:15:43.694825: step 7989, loss 0.109344, acc 0.953125\n",
      "2018-05-23T13:15:44.045884: step 7990, loss 0.361013, acc 0.859375\n",
      "2018-05-23T13:15:44.387971: step 7991, loss 0.218847, acc 0.875\n",
      "2018-05-23T13:15:44.740028: step 7992, loss 0.140082, acc 0.921875\n",
      "2018-05-23T13:15:45.091088: step 7993, loss 0.1778, acc 0.921875\n",
      "2018-05-23T13:15:45.431179: step 7994, loss 0.175554, acc 0.9375\n",
      "2018-05-23T13:15:45.780246: step 7995, loss 0.138019, acc 0.9375\n",
      "2018-05-23T13:15:46.130310: step 7996, loss 0.227805, acc 0.953125\n",
      "2018-05-23T13:15:46.475386: step 7997, loss 0.241208, acc 0.875\n",
      "2018-05-23T13:15:46.819467: step 7998, loss 0.166279, acc 0.90625\n",
      "2018-05-23T13:15:47.162548: step 7999, loss 0.183635, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:15:47.508623: step 8000, loss 0.0964263, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:15:51.946748: step 8000, loss 0.805867, acc 0.727961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8000\n",
      "\n",
      "2018-05-23T13:15:53.234347: step 8001, loss 0.194229, acc 0.9375\n",
      "2018-05-23T13:15:53.637305: step 8002, loss 0.161507, acc 0.921875\n",
      "2018-05-23T13:15:53.988330: step 8003, loss 0.23937, acc 0.890625\n",
      "2018-05-23T13:15:54.323433: step 8004, loss 0.265136, acc 0.875\n",
      "2018-05-23T13:15:54.650557: step 8005, loss 0.235277, acc 0.890625\n",
      "2018-05-23T13:15:54.980677: step 8006, loss 0.144482, acc 0.96875\n",
      "2018-05-23T13:15:55.312788: step 8007, loss 0.279237, acc 0.84375\n",
      "2018-05-23T13:15:55.636920: step 8008, loss 0.132723, acc 0.9375\n",
      "2018-05-23T13:15:55.965042: step 8009, loss 0.26345, acc 0.890625\n",
      "2018-05-23T13:15:56.291169: step 8010, loss 0.1667, acc 0.921875\n",
      "2018-05-23T13:15:56.608320: step 8011, loss 0.289095, acc 0.875\n",
      "2018-05-23T13:15:56.929462: step 8012, loss 0.193467, acc 0.921875\n",
      "2018-05-23T13:15:57.251601: step 8013, loss 0.163764, acc 0.90625\n",
      "2018-05-23T13:15:57.571743: step 8014, loss 0.180511, acc 0.90625\n",
      "2018-05-23T13:15:57.891887: step 8015, loss 0.160099, acc 0.921875\n",
      "2018-05-23T13:15:58.218015: step 8016, loss 0.217813, acc 0.890625\n",
      "2018-05-23T13:15:58.538160: step 8017, loss 0.135255, acc 0.953125\n",
      "2018-05-23T13:15:58.855309: step 8018, loss 0.18857, acc 0.921875\n",
      "2018-05-23T13:15:59.184431: step 8019, loss 0.146744, acc 0.9375\n",
      "2018-05-23T13:15:59.503576: step 8020, loss 0.250329, acc 0.90625\n",
      "2018-05-23T13:15:59.818734: step 8021, loss 0.288052, acc 0.828125\n",
      "2018-05-23T13:16:00.162813: step 8022, loss 0.119261, acc 0.953125\n",
      "2018-05-23T13:16:00.503900: step 8023, loss 0.234231, acc 0.875\n",
      "2018-05-23T13:16:00.888870: step 8024, loss 0.177616, acc 0.90625\n",
      "2018-05-23T13:16:01.240928: step 8025, loss 0.201001, acc 0.921875\n",
      "2018-05-23T13:16:01.595979: step 8026, loss 0.211283, acc 0.921875\n",
      "2018-05-23T13:16:01.924099: step 8027, loss 0.222134, acc 0.921875\n",
      "2018-05-23T13:16:02.271173: step 8028, loss 0.133027, acc 0.953125\n",
      "2018-05-23T13:16:02.604282: step 8029, loss 0.306021, acc 0.890625\n",
      "2018-05-23T13:16:02.970302: step 8030, loss 0.129417, acc 0.953125\n",
      "2018-05-23T13:16:03.314383: step 8031, loss 0.170704, acc 0.9375\n",
      "2018-05-23T13:16:03.725283: step 8032, loss 0.192185, acc 0.875\n",
      "2018-05-23T13:16:04.055399: step 8033, loss 0.171164, acc 0.921875\n",
      "2018-05-23T13:16:04.382523: step 8034, loss 0.128208, acc 0.9375\n",
      "2018-05-23T13:16:04.703666: step 8035, loss 0.156044, acc 0.921875\n",
      "2018-05-23T13:16:05.031786: step 8036, loss 0.214976, acc 0.90625\n",
      "2018-05-23T13:16:05.366889: step 8037, loss 0.209746, acc 0.859375\n",
      "2018-05-23T13:16:05.698007: step 8038, loss 0.168227, acc 0.9375\n",
      "2018-05-23T13:16:06.024133: step 8039, loss 0.130958, acc 0.984375\n",
      "2018-05-23T13:16:06.351256: step 8040, loss 0.297886, acc 0.875\n",
      "2018-05-23T13:16:06.678381: step 8041, loss 0.288611, acc 0.90625\n",
      "2018-05-23T13:16:07.003513: step 8042, loss 0.170489, acc 0.90625\n",
      "2018-05-23T13:16:07.329639: step 8043, loss 0.394007, acc 0.828125\n",
      "2018-05-23T13:16:07.649783: step 8044, loss 0.26059, acc 0.890625\n",
      "2018-05-23T13:16:07.980896: step 8045, loss 0.108722, acc 0.96875\n",
      "2018-05-23T13:16:08.300044: step 8046, loss 0.13837, acc 0.953125\n",
      "2018-05-23T13:16:08.633153: step 8047, loss 0.26068, acc 0.9375\n",
      "2018-05-23T13:16:08.963271: step 8048, loss 0.173024, acc 0.921875\n",
      "2018-05-23T13:16:09.286405: step 8049, loss 0.176833, acc 0.90625\n",
      "2018-05-23T13:16:09.616989: step 8050, loss 0.145075, acc 0.953125\n",
      "2018-05-23T13:16:09.939129: step 8051, loss 0.206069, acc 0.90625\n",
      "2018-05-23T13:16:10.272263: step 8052, loss 0.254824, acc 0.875\n",
      "2018-05-23T13:16:10.645239: step 8053, loss 0.155559, acc 0.953125\n",
      "2018-05-23T13:16:11.010261: step 8054, loss 0.193779, acc 0.953125\n",
      "2018-05-23T13:16:11.339383: step 8055, loss 0.192126, acc 0.90625\n",
      "2018-05-23T13:16:11.671494: step 8056, loss 0.247666, acc 0.9375\n",
      "2018-05-23T13:16:11.998619: step 8057, loss 0.12194, acc 0.953125\n",
      "2018-05-23T13:16:12.324748: step 8058, loss 0.152429, acc 0.921875\n",
      "2018-05-23T13:16:12.650873: step 8059, loss 0.200422, acc 0.90625\n",
      "2018-05-23T13:16:12.972015: step 8060, loss 0.235438, acc 0.875\n",
      "2018-05-23T13:16:13.300136: step 8061, loss 0.208787, acc 0.890625\n",
      "2018-05-23T13:16:13.620282: step 8062, loss 0.162813, acc 0.9375\n",
      "2018-05-23T13:16:13.952391: step 8063, loss 0.187963, acc 0.90625\n",
      "2018-05-23T13:16:14.281545: step 8064, loss 0.175503, acc 0.9375\n",
      "2018-05-23T13:16:14.609635: step 8065, loss 0.2194, acc 0.875\n",
      "2018-05-23T13:16:14.931771: step 8066, loss 0.228553, acc 0.875\n",
      "2018-05-23T13:16:15.261893: step 8067, loss 0.170632, acc 0.953125\n",
      "2018-05-23T13:16:15.589014: step 8068, loss 0.172682, acc 0.9375\n",
      "2018-05-23T13:16:15.916138: step 8069, loss 0.187149, acc 0.90625\n",
      "2018-05-23T13:16:16.245258: step 8070, loss 0.158935, acc 0.921875\n",
      "2018-05-23T13:16:16.576372: step 8071, loss 0.27348, acc 0.859375\n",
      "2018-05-23T13:16:16.907486: step 8072, loss 0.155075, acc 0.921875\n",
      "2018-05-23T13:16:17.239599: step 8073, loss 0.222892, acc 0.9375\n",
      "2018-05-23T13:16:17.567722: step 8074, loss 0.328139, acc 0.90625\n",
      "2018-05-23T13:16:17.897837: step 8075, loss 0.228815, acc 0.859375\n",
      "2018-05-23T13:16:18.233938: step 8076, loss 0.0850435, acc 0.96875\n",
      "2018-05-23T13:16:18.617911: step 8077, loss 0.208601, acc 0.953125\n",
      "2018-05-23T13:16:18.951021: step 8078, loss 0.205174, acc 0.9375\n",
      "2018-05-23T13:16:19.282134: step 8079, loss 0.203066, acc 0.953125\n",
      "2018-05-23T13:16:19.612250: step 8080, loss 0.15992, acc 0.90625\n",
      "2018-05-23T13:16:19.949348: step 8081, loss 0.185295, acc 0.921875\n",
      "2018-05-23T13:16:20.275478: step 8082, loss 0.208252, acc 0.921875\n",
      "2018-05-23T13:16:20.595619: step 8083, loss 0.123837, acc 0.953125\n",
      "2018-05-23T13:16:20.973608: step 8084, loss 0.19143, acc 0.9375\n",
      "2018-05-23T13:16:21.302727: step 8085, loss 0.186522, acc 0.9375\n",
      "2018-05-23T13:16:21.628855: step 8086, loss 0.22742, acc 0.90625\n",
      "2018-05-23T13:16:21.955982: step 8087, loss 0.223124, acc 0.890625\n",
      "2018-05-23T13:16:22.288093: step 8088, loss 0.14533, acc 0.953125\n",
      "2018-05-23T13:16:22.614221: step 8089, loss 0.162979, acc 0.9375\n",
      "2018-05-23T13:16:22.952314: step 8090, loss 0.27724, acc 0.90625\n",
      "2018-05-23T13:16:23.289414: step 8091, loss 0.214159, acc 0.90625\n",
      "2018-05-23T13:16:23.615542: step 8092, loss 0.233718, acc 0.890625\n",
      "2018-05-23T13:16:23.943664: step 8093, loss 0.134096, acc 0.953125\n",
      "2018-05-23T13:16:24.276774: step 8094, loss 0.193055, acc 0.921875\n",
      "2018-05-23T13:16:24.603898: step 8095, loss 0.185787, acc 0.9375\n",
      "2018-05-23T13:16:24.936154: step 8096, loss 0.210991, acc 0.90625\n",
      "2018-05-23T13:16:25.266270: step 8097, loss 0.223569, acc 0.859375\n",
      "2018-05-23T13:16:25.592397: step 8098, loss 0.273182, acc 0.875\n",
      "2018-05-23T13:16:25.945453: step 8099, loss 0.146086, acc 0.953125\n",
      "2018-05-23T13:16:26.425170: step 8100, loss 0.18763, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:16:31.940416: step 8100, loss 0.830364, acc 0.724675\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8100\n",
      "\n",
      "2018-05-23T13:16:33.358134: step 8101, loss 0.0998231, acc 0.984375\n",
      "2018-05-23T13:16:33.781005: step 8102, loss 0.235431, acc 0.921875\n",
      "2018-05-23T13:16:34.117103: step 8103, loss 0.176752, acc 0.953125\n",
      "2018-05-23T13:16:34.446224: step 8104, loss 0.207491, acc 0.90625\n",
      "2018-05-23T13:16:34.772353: step 8105, loss 0.231153, acc 0.921875\n",
      "2018-05-23T13:16:35.102467: step 8106, loss 0.183931, acc 0.9375\n",
      "2018-05-23T13:16:35.437573: step 8107, loss 0.20721, acc 0.921875\n",
      "2018-05-23T13:16:35.931263: step 8108, loss 0.152752, acc 0.921875\n",
      "2018-05-23T13:16:36.451857: step 8109, loss 0.166483, acc 0.90625\n",
      "2018-05-23T13:16:36.850790: step 8110, loss 0.168131, acc 0.90625\n",
      "2018-05-23T13:16:37.211825: step 8111, loss 0.24393, acc 0.875\n",
      "2018-05-23T13:16:37.558896: step 8112, loss 0.258595, acc 0.90625\n",
      "2018-05-23T13:16:38.026646: step 8113, loss 0.245212, acc 0.890625\n",
      "2018-05-23T13:16:38.489408: step 8114, loss 0.131771, acc 0.953125\n",
      "2018-05-23T13:16:38.938208: step 8115, loss 0.164305, acc 0.921875\n",
      "2018-05-23T13:16:39.351104: step 8116, loss 0.243631, acc 0.875\n",
      "2018-05-23T13:16:39.714130: step 8117, loss 0.227786, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:16:40.062202: step 8118, loss 0.243213, acc 0.90625\n",
      "2018-05-23T13:16:40.394311: step 8119, loss 0.185341, acc 0.890625\n",
      "2018-05-23T13:16:40.816183: step 8120, loss 0.227121, acc 0.859375\n",
      "2018-05-23T13:16:41.242042: step 8121, loss 0.210328, acc 0.859375\n",
      "2018-05-23T13:16:41.665942: step 8122, loss 0.224663, acc 0.921875\n",
      "2018-05-23T13:16:42.085786: step 8123, loss 0.152304, acc 0.921875\n",
      "2018-05-23T13:16:42.516632: step 8124, loss 0.134541, acc 0.921875\n",
      "2018-05-23T13:16:42.948477: step 8125, loss 0.185143, acc 0.921875\n",
      "2018-05-23T13:16:43.361376: step 8126, loss 0.164533, acc 0.9375\n",
      "2018-05-23T13:16:43.785240: step 8127, loss 0.108378, acc 0.953125\n",
      "2018-05-23T13:16:44.216087: step 8128, loss 0.15883, acc 0.921875\n",
      "2018-05-23T13:16:44.610035: step 8129, loss 0.266502, acc 0.875\n",
      "2018-05-23T13:16:45.000987: step 8130, loss 0.278382, acc 0.90625\n",
      "2018-05-23T13:16:45.366011: step 8131, loss 0.242088, acc 0.890625\n",
      "2018-05-23T13:16:45.780902: step 8132, loss 0.184718, acc 0.921875\n",
      "2018-05-23T13:16:46.176843: step 8133, loss 0.212259, acc 0.90625\n",
      "2018-05-23T13:16:46.589737: step 8134, loss 0.196857, acc 0.921875\n",
      "2018-05-23T13:16:47.001638: step 8135, loss 0.161959, acc 0.90625\n",
      "2018-05-23T13:16:47.410574: step 8136, loss 0.158336, acc 0.921875\n",
      "2018-05-23T13:16:47.828422: step 8137, loss 0.235306, acc 0.90625\n",
      "2018-05-23T13:16:48.238326: step 8138, loss 0.100675, acc 0.96875\n",
      "2018-05-23T13:16:48.643243: step 8139, loss 0.152213, acc 0.9375\n",
      "2018-05-23T13:16:49.020235: step 8140, loss 0.15942, acc 0.96875\n",
      "2018-05-23T13:16:49.414182: step 8141, loss 0.154355, acc 0.9375\n",
      "2018-05-23T13:16:49.809126: step 8142, loss 0.168794, acc 0.921875\n",
      "2018-05-23T13:16:50.228004: step 8143, loss 0.177488, acc 0.890625\n",
      "2018-05-23T13:16:50.630925: step 8144, loss 0.251031, acc 0.875\n",
      "2018-05-23T13:16:51.030857: step 8145, loss 0.0907384, acc 1\n",
      "2018-05-23T13:16:51.398871: step 8146, loss 0.136057, acc 0.9375\n",
      "2018-05-23T13:16:51.768883: step 8147, loss 0.114314, acc 0.96875\n",
      "2018-05-23T13:16:52.134905: step 8148, loss 0.289505, acc 0.875\n",
      "2018-05-23T13:16:52.473000: step 8149, loss 0.203807, acc 0.921875\n",
      "2018-05-23T13:16:52.806107: step 8150, loss 0.184428, acc 0.9375\n",
      "2018-05-23T13:16:53.137223: step 8151, loss 0.128869, acc 0.9375\n",
      "2018-05-23T13:16:53.467337: step 8152, loss 0.255623, acc 0.875\n",
      "2018-05-23T13:16:53.798452: step 8153, loss 0.208917, acc 0.890625\n",
      "2018-05-23T13:16:54.134555: step 8154, loss 0.0765818, acc 0.984375\n",
      "2018-05-23T13:16:54.465667: step 8155, loss 0.314739, acc 0.890625\n",
      "2018-05-23T13:16:54.792793: step 8156, loss 0.27128, acc 0.90625\n",
      "2018-05-23T13:16:55.158813: step 8157, loss 0.175166, acc 0.953125\n",
      "2018-05-23T13:16:55.563729: step 8158, loss 0.210041, acc 0.921875\n",
      "2018-05-23T13:16:55.904817: step 8159, loss 0.169902, acc 0.9375\n",
      "2018-05-23T13:16:56.259867: step 8160, loss 0.196806, acc 0.90625\n",
      "2018-05-23T13:16:56.598960: step 8161, loss 0.18898, acc 0.921875\n",
      "2018-05-23T13:16:56.944037: step 8162, loss 0.190807, acc 0.9375\n",
      "2018-05-23T13:16:57.298092: step 8163, loss 0.161545, acc 0.953125\n",
      "2018-05-23T13:16:57.623220: step 8164, loss 0.355089, acc 0.828125\n",
      "2018-05-23T13:16:57.953338: step 8165, loss 0.181747, acc 0.921875\n",
      "2018-05-23T13:16:58.291433: step 8166, loss 0.169406, acc 0.953125\n",
      "2018-05-23T13:16:58.618557: step 8167, loss 0.225854, acc 0.90625\n",
      "2018-05-23T13:16:58.944685: step 8168, loss 0.250632, acc 0.859375\n",
      "2018-05-23T13:16:59.278794: step 8169, loss 0.194507, acc 0.921875\n",
      "2018-05-23T13:16:59.612898: step 8170, loss 0.128853, acc 0.9375\n",
      "2018-05-23T13:16:59.938030: step 8171, loss 0.258222, acc 0.890625\n",
      "2018-05-23T13:17:00.292081: step 8172, loss 0.251013, acc 0.890625\n",
      "2018-05-23T13:17:00.620202: step 8173, loss 0.0927246, acc 0.984375\n",
      "2018-05-23T13:17:00.951317: step 8174, loss 0.215849, acc 0.890625\n",
      "2018-05-23T13:17:01.337286: step 8175, loss 0.197871, acc 0.890625\n",
      "2018-05-23T13:17:01.714277: step 8176, loss 0.13429, acc 0.90625\n",
      "2018-05-23T13:17:02.109219: step 8177, loss 0.197122, acc 0.90625\n",
      "2018-05-23T13:17:02.492197: step 8178, loss 0.110746, acc 0.96875\n",
      "2018-05-23T13:17:02.850237: step 8179, loss 0.163048, acc 0.953125\n",
      "2018-05-23T13:17:03.231218: step 8180, loss 0.164692, acc 0.9375\n",
      "2018-05-23T13:17:03.770774: step 8181, loss 0.254172, acc 0.859375\n",
      "2018-05-23T13:17:04.162725: step 8182, loss 0.212616, acc 0.875\n",
      "2018-05-23T13:17:04.514786: step 8183, loss 0.283905, acc 0.875\n",
      "2018-05-23T13:17:04.854874: step 8184, loss 0.180532, acc 0.90625\n",
      "2018-05-23T13:17:05.202943: step 8185, loss 0.137949, acc 0.953125\n",
      "2018-05-23T13:17:05.550014: step 8186, loss 0.202451, acc 0.90625\n",
      "2018-05-23T13:17:06.036714: step 8187, loss 0.175572, acc 0.90625\n",
      "2018-05-23T13:17:06.521416: step 8188, loss 0.108219, acc 0.953125\n",
      "2018-05-23T13:17:06.943288: step 8189, loss 0.114506, acc 0.96875\n",
      "2018-05-23T13:17:07.358179: step 8190, loss 0.190346, acc 0.9375\n",
      "2018-05-23T13:17:07.707243: step 8191, loss 0.201647, acc 0.921875\n",
      "2018-05-23T13:17:08.049328: step 8192, loss 0.152918, acc 0.953125\n",
      "2018-05-23T13:17:08.395405: step 8193, loss 0.309091, acc 0.890625\n",
      "2018-05-23T13:17:08.786356: step 8194, loss 0.196563, acc 0.890625\n",
      "2018-05-23T13:17:09.132431: step 8195, loss 0.0680202, acc 1\n",
      "2018-05-23T13:17:09.465540: step 8196, loss 0.267674, acc 0.921875\n",
      "2018-05-23T13:17:09.801642: step 8197, loss 0.162362, acc 0.9375\n",
      "2018-05-23T13:17:10.142728: step 8198, loss 0.180751, acc 0.90625\n",
      "2018-05-23T13:17:10.477368: step 8199, loss 0.22878, acc 0.921875\n",
      "2018-05-23T13:17:10.809477: step 8200, loss 0.330797, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:17:15.605647: step 8200, loss 0.856677, acc 0.727818\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8200\n",
      "\n",
      "2018-05-23T13:17:16.865395: step 8201, loss 0.26931, acc 0.875\n",
      "2018-05-23T13:17:17.341123: step 8202, loss 0.226316, acc 0.90625\n",
      "2018-05-23T13:17:17.687195: step 8203, loss 0.239981, acc 0.875\n",
      "2018-05-23T13:17:18.064186: step 8204, loss 0.31033, acc 0.890625\n",
      "2018-05-23T13:17:18.395303: step 8205, loss 0.178247, acc 0.921875\n",
      "2018-05-23T13:17:18.837121: step 8206, loss 0.215651, acc 0.921875\n",
      "2018-05-23T13:17:19.174217: step 8207, loss 0.280871, acc 0.890625\n",
      "2018-05-23T13:17:19.497352: step 8208, loss 0.085875, acc 0.984375\n",
      "2018-05-23T13:17:19.826472: step 8209, loss 0.208253, acc 0.921875\n",
      "2018-05-23T13:17:20.158585: step 8210, loss 0.195241, acc 0.9375\n",
      "2018-05-23T13:17:20.483478: step 8211, loss 0.235936, acc 0.875\n",
      "2018-05-23T13:17:20.822571: step 8212, loss 0.162077, acc 0.921875\n",
      "2018-05-23T13:17:21.150694: step 8213, loss 0.273929, acc 0.875\n",
      "2018-05-23T13:17:21.481808: step 8214, loss 0.147232, acc 0.9375\n",
      "2018-05-23T13:17:21.811925: step 8215, loss 0.142126, acc 0.9375\n",
      "2018-05-23T13:17:22.151016: step 8216, loss 0.237864, acc 0.859375\n",
      "2018-05-23T13:17:22.479139: step 8217, loss 0.259929, acc 0.90625\n",
      "2018-05-23T13:17:22.812247: step 8218, loss 0.186664, acc 0.9375\n",
      "2018-05-23T13:17:23.138376: step 8219, loss 0.20903, acc 0.921875\n",
      "2018-05-23T13:17:23.464503: step 8220, loss 0.247349, acc 0.890625\n",
      "2018-05-23T13:17:23.793623: step 8221, loss 0.210718, acc 0.921875\n",
      "2018-05-23T13:17:24.126732: step 8222, loss 0.183733, acc 0.921875\n",
      "2018-05-23T13:17:24.453856: step 8223, loss 0.316436, acc 0.828125\n",
      "2018-05-23T13:17:24.784405: step 8224, loss 0.149448, acc 0.90625\n",
      "2018-05-23T13:17:25.111533: step 8225, loss 0.122945, acc 0.953125\n",
      "2018-05-23T13:17:25.448629: step 8226, loss 0.120595, acc 0.96875\n",
      "2018-05-23T13:17:25.785728: step 8227, loss 0.166042, acc 0.890625\n",
      "2018-05-23T13:17:26.119833: step 8228, loss 0.148846, acc 0.921875\n",
      "2018-05-23T13:17:26.541710: step 8229, loss 0.248808, acc 0.90625\n",
      "2018-05-23T13:17:26.940641: step 8230, loss 0.181859, acc 0.9375\n",
      "2018-05-23T13:17:27.384453: step 8231, loss 0.234657, acc 0.90625\n",
      "2018-05-23T13:17:27.832252: step 8232, loss 0.212427, acc 0.90625\n",
      "2018-05-23T13:17:28.277063: step 8233, loss 0.137351, acc 0.953125\n",
      "2018-05-23T13:17:28.691952: step 8234, loss 0.182799, acc 0.890625\n",
      "2018-05-23T13:17:29.121803: step 8235, loss 0.16498, acc 0.9375\n",
      "2018-05-23T13:17:29.542677: step 8236, loss 0.163207, acc 0.921875\n",
      "2018-05-23T13:17:29.953578: step 8237, loss 0.160703, acc 0.921875\n",
      "2018-05-23T13:17:30.429304: step 8238, loss 0.155751, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:17:30.779369: step 8239, loss 0.158069, acc 0.953125\n",
      "2018-05-23T13:17:31.128435: step 8240, loss 0.164826, acc 0.953125\n",
      "2018-05-23T13:17:31.595186: step 8241, loss 0.2609, acc 0.859375\n",
      "2018-05-23T13:17:32.072908: step 8242, loss 0.225547, acc 0.890625\n",
      "2018-05-23T13:17:32.450943: step 8243, loss 0.414742, acc 0.90625\n",
      "2018-05-23T13:17:32.839858: step 8244, loss 0.170131, acc 0.921875\n",
      "2018-05-23T13:17:33.230813: step 8245, loss 0.130175, acc 0.90625\n",
      "2018-05-23T13:17:33.654712: step 8246, loss 0.167093, acc 0.921875\n",
      "2018-05-23T13:17:34.058596: step 8247, loss 0.278167, acc 0.859375\n",
      "2018-05-23T13:17:34.510387: step 8248, loss 0.275146, acc 0.875\n",
      "2018-05-23T13:17:34.939241: step 8249, loss 0.159561, acc 0.921875\n",
      "2018-05-23T13:17:35.372082: step 8250, loss 0.0758464, acc 0.953125\n",
      "2018-05-23T13:17:35.803926: step 8251, loss 0.128978, acc 0.9375\n",
      "2018-05-23T13:17:36.201862: step 8252, loss 0.288713, acc 0.890625\n",
      "2018-05-23T13:17:36.549933: step 8253, loss 0.259058, acc 0.90625\n",
      "2018-05-23T13:17:36.890021: step 8254, loss 0.160039, acc 0.90625\n",
      "2018-05-23T13:17:37.239087: step 8255, loss 0.159378, acc 0.9375\n",
      "2018-05-23T13:17:37.590150: step 8256, loss 0.374021, acc 0.875\n",
      "2018-05-23T13:17:37.944253: step 8257, loss 0.220365, acc 0.90625\n",
      "2018-05-23T13:17:38.293320: step 8258, loss 0.283943, acc 0.890625\n",
      "2018-05-23T13:17:38.618450: step 8259, loss 0.119855, acc 0.96875\n",
      "2018-05-23T13:17:38.961534: step 8260, loss 0.198947, acc 0.90625\n",
      "2018-05-23T13:17:39.357475: step 8261, loss 0.196403, acc 0.875\n",
      "2018-05-23T13:17:39.737459: step 8262, loss 0.210453, acc 0.90625\n",
      "2018-05-23T13:17:40.082534: step 8263, loss 0.247304, acc 0.875\n",
      "2018-05-23T13:17:40.419631: step 8264, loss 0.189889, acc 0.921875\n",
      "2018-05-23T13:17:40.745759: step 8265, loss 0.190508, acc 0.90625\n",
      "2018-05-23T13:17:41.074879: step 8266, loss 0.156142, acc 0.90625\n",
      "2018-05-23T13:17:41.409985: step 8267, loss 0.167162, acc 0.9375\n",
      "2018-05-23T13:17:41.740101: step 8268, loss 0.244503, acc 0.90625\n",
      "2018-05-23T13:17:42.088170: step 8269, loss 0.209454, acc 0.90625\n",
      "2018-05-23T13:17:42.525000: step 8270, loss 0.271253, acc 0.890625\n",
      "2018-05-23T13:17:42.975795: step 8271, loss 0.24104, acc 0.90625\n",
      "2018-05-23T13:17:43.347798: step 8272, loss 0.226154, acc 0.921875\n",
      "2018-05-23T13:17:43.777648: step 8273, loss 0.212202, acc 0.921875\n",
      "2018-05-23T13:17:44.154640: step 8274, loss 0.132385, acc 0.9375\n",
      "2018-05-23T13:17:44.514676: step 8275, loss 0.302626, acc 0.890625\n",
      "2018-05-23T13:17:44.851777: step 8276, loss 0.152233, acc 0.875\n",
      "2018-05-23T13:17:45.199844: step 8277, loss 0.159311, acc 0.953125\n",
      "2018-05-23T13:17:45.543923: step 8278, loss 0.255045, acc 0.84375\n",
      "2018-05-23T13:17:45.993720: step 8279, loss 0.174425, acc 0.890625\n",
      "2018-05-23T13:17:46.348770: step 8280, loss 0.127484, acc 0.921875\n",
      "2018-05-23T13:17:46.705815: step 8281, loss 0.26911, acc 0.90625\n",
      "2018-05-23T13:17:47.074829: step 8282, loss 0.206569, acc 0.90625\n",
      "2018-05-23T13:17:47.528613: step 8283, loss 0.172385, acc 0.921875\n",
      "2018-05-23T13:17:47.992374: step 8284, loss 0.185846, acc 0.921875\n",
      "2018-05-23T13:17:48.408261: step 8285, loss 0.210206, acc 0.890625\n",
      "2018-05-23T13:17:48.852073: step 8286, loss 0.193973, acc 0.9375\n",
      "2018-05-23T13:17:49.266963: step 8287, loss 0.336823, acc 0.890625\n",
      "2018-05-23T13:17:49.677864: step 8288, loss 0.107278, acc 0.953125\n",
      "2018-05-23T13:17:50.094749: step 8289, loss 0.210027, acc 0.921875\n",
      "2018-05-23T13:17:50.501662: step 8290, loss 0.287245, acc 0.875\n",
      "2018-05-23T13:17:50.909569: step 8291, loss 0.155744, acc 0.9375\n",
      "2018-05-23T13:17:51.254646: step 8292, loss 0.168658, acc 0.921875\n",
      "2018-05-23T13:17:51.607702: step 8293, loss 0.247025, acc 0.921875\n",
      "2018-05-23T13:17:51.969735: step 8294, loss 0.276295, acc 0.84375\n",
      "2018-05-23T13:17:52.308828: step 8295, loss 0.139204, acc 0.953125\n",
      "2018-05-23T13:17:52.659888: step 8296, loss 0.321885, acc 0.859375\n",
      "2018-05-23T13:17:53.005963: step 8297, loss 0.162181, acc 0.921875\n",
      "2018-05-23T13:17:53.349045: step 8298, loss 0.220718, acc 0.9375\n",
      "2018-05-23T13:17:53.688136: step 8299, loss 0.256697, acc 0.875\n",
      "2018-05-23T13:17:54.035678: step 8300, loss 0.147073, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:17:59.113096: step 8300, loss 0.856342, acc 0.723961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8300\n",
      "\n",
      "2018-05-23T13:18:00.657964: step 8301, loss 0.250887, acc 0.875\n",
      "2018-05-23T13:18:01.229436: step 8302, loss 0.105613, acc 0.984375\n",
      "2018-05-23T13:18:01.640336: step 8303, loss 0.119232, acc 0.984375\n",
      "2018-05-23T13:18:02.032289: step 8304, loss 0.14082, acc 0.953125\n",
      "2018-05-23T13:18:02.462136: step 8305, loss 0.263155, acc 0.859375\n",
      "2018-05-23T13:18:02.855089: step 8306, loss 0.213222, acc 0.890625\n",
      "2018-05-23T13:18:03.256015: step 8307, loss 0.243902, acc 0.859375\n",
      "2018-05-23T13:18:03.631010: step 8308, loss 0.24838, acc 0.921875\n",
      "2018-05-23T13:18:04.077815: step 8309, loss 0.246824, acc 0.890625\n",
      "2018-05-23T13:18:04.471762: step 8310, loss 0.215447, acc 0.890625\n",
      "2018-05-23T13:18:04.974416: step 8311, loss 0.210181, acc 0.875\n",
      "2018-05-23T13:18:05.445157: step 8312, loss 0.209739, acc 0.890625\n",
      "2018-05-23T13:18:05.835112: step 8313, loss 0.146249, acc 0.9375\n",
      "2018-05-23T13:18:06.212107: step 8314, loss 0.164561, acc 0.921875\n",
      "2018-05-23T13:18:06.579124: step 8315, loss 0.238396, acc 0.859375\n",
      "2018-05-23T13:18:06.943151: step 8316, loss 0.17505, acc 0.921875\n",
      "2018-05-23T13:18:07.315154: step 8317, loss 0.168243, acc 0.9375\n",
      "2018-05-23T13:18:07.681174: step 8318, loss 0.339613, acc 0.84375\n",
      "2018-05-23T13:18:08.033234: step 8319, loss 0.170772, acc 0.953125\n",
      "2018-05-23T13:18:08.392274: step 8320, loss 0.216045, acc 0.890625\n",
      "2018-05-23T13:18:08.757295: step 8321, loss 0.139993, acc 0.9375\n",
      "2018-05-23T13:18:09.118331: step 8322, loss 0.206175, acc 0.90625\n",
      "2018-05-23T13:18:09.475374: step 8323, loss 0.0786495, acc 0.984375\n",
      "2018-05-23T13:18:09.839403: step 8324, loss 0.238902, acc 0.890625\n",
      "2018-05-23T13:18:10.207416: step 8325, loss 0.295617, acc 0.921875\n",
      "2018-05-23T13:18:10.566458: step 8326, loss 0.236658, acc 0.890625\n",
      "2018-05-23T13:18:10.931479: step 8327, loss 0.309138, acc 0.875\n",
      "2018-05-23T13:18:11.296505: step 8328, loss 0.275023, acc 0.875\n",
      "2018-05-23T13:18:11.662525: step 8329, loss 0.280519, acc 0.890625\n",
      "2018-05-23T13:18:12.035525: step 8330, loss 0.123319, acc 0.953125\n",
      "2018-05-23T13:18:12.423521: step 8331, loss 0.178897, acc 0.875\n",
      "2018-05-23T13:18:12.834391: step 8332, loss 0.224614, acc 0.890625\n",
      "2018-05-23T13:18:13.272218: step 8333, loss 0.259236, acc 0.90625\n",
      "2018-05-23T13:18:13.940429: step 8334, loss 0.121437, acc 0.953125\n",
      "2018-05-23T13:18:14.668482: step 8335, loss 0.144679, acc 0.9375\n",
      "2018-05-23T13:18:16.274186: step 8336, loss 0.167003, acc 0.9375\n",
      "2018-05-23T13:18:17.056094: step 8337, loss 0.24667, acc 0.90625\n",
      "2018-05-23T13:18:17.796116: step 8338, loss 0.193753, acc 0.921875\n",
      "2018-05-23T13:18:18.895176: step 8339, loss 0.125532, acc 0.953125\n",
      "2018-05-23T13:18:19.713985: step 8340, loss 0.267536, acc 0.875\n",
      "2018-05-23T13:18:20.373221: step 8341, loss 0.2992, acc 0.828125\n",
      "2018-05-23T13:18:21.367563: step 8342, loss 0.354606, acc 0.84375\n",
      "2018-05-23T13:18:22.268153: step 8343, loss 0.225952, acc 0.90625\n",
      "2018-05-23T13:18:23.047068: step 8344, loss 0.112364, acc 0.9375\n",
      "2018-05-23T13:18:23.750188: step 8345, loss 0.236342, acc 0.9375\n",
      "2018-05-23T13:18:24.381498: step 8346, loss 0.189485, acc 0.90625\n",
      "2018-05-23T13:18:24.944990: step 8347, loss 0.204256, acc 0.90625\n",
      "2018-05-23T13:18:25.871513: step 8348, loss 0.178555, acc 0.90625\n",
      "2018-05-23T13:18:27.031410: step 8349, loss 0.124173, acc 0.96875\n",
      "2018-05-23T13:18:27.768438: step 8350, loss 0.284518, acc 0.859375\n",
      "2018-05-23T13:18:28.534389: step 8351, loss 0.169003, acc 0.921875\n",
      "2018-05-23T13:18:29.314303: step 8352, loss 0.173559, acc 0.921875\n",
      "2018-05-23T13:18:29.923672: step 8353, loss 0.188979, acc 0.9375\n",
      "2018-05-23T13:18:30.719543: step 8354, loss 0.213342, acc 0.875\n",
      "2018-05-23T13:18:31.670998: step 8355, loss 0.229574, acc 0.921875\n",
      "2018-05-23T13:18:32.460884: step 8356, loss 0.182444, acc 0.9375\n",
      "2018-05-23T13:18:33.194921: step 8357, loss 0.214114, acc 0.921875\n",
      "2018-05-23T13:18:34.183276: step 8358, loss 0.199724, acc 0.9375\n",
      "2018-05-23T13:18:34.992113: step 8359, loss 0.140873, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:18:35.617440: step 8360, loss 0.153049, acc 0.921875\n",
      "2018-05-23T13:18:36.595823: step 8361, loss 0.200872, acc 0.90625\n",
      "2018-05-23T13:18:37.272013: step 8362, loss 0.278993, acc 0.890625\n",
      "2018-05-23T13:18:37.960173: step 8363, loss 0.158076, acc 0.9375\n",
      "2018-05-23T13:18:38.504716: step 8364, loss 0.122616, acc 0.953125\n",
      "2018-05-23T13:18:39.161958: step 8365, loss 0.215482, acc 0.9375\n",
      "2018-05-23T13:18:39.745396: step 8366, loss 0.209382, acc 0.921875\n",
      "2018-05-23T13:18:40.521321: step 8367, loss 0.270091, acc 0.90625\n",
      "2018-05-23T13:18:41.076834: step 8368, loss 0.0643801, acc 1\n",
      "2018-05-23T13:18:41.627363: step 8369, loss 0.0839875, acc 0.984375\n",
      "2018-05-23T13:18:42.198835: step 8370, loss 0.23221, acc 0.890625\n",
      "2018-05-23T13:18:42.741382: step 8371, loss 0.247167, acc 0.9375\n",
      "2018-05-23T13:18:43.330805: step 8372, loss 0.203782, acc 0.9375\n",
      "2018-05-23T13:18:43.801546: step 8373, loss 0.189439, acc 0.921875\n",
      "2018-05-23T13:18:44.205467: step 8374, loss 0.142245, acc 0.96875\n",
      "2018-05-23T13:18:44.581460: step 8375, loss 0.200166, acc 0.890625\n",
      "2018-05-23T13:18:44.936509: step 8376, loss 0.27801, acc 0.90625\n",
      "2018-05-23T13:18:45.299539: step 8377, loss 0.171115, acc 0.90625\n",
      "2018-05-23T13:18:45.672541: step 8378, loss 0.222367, acc 0.890625\n",
      "2018-05-23T13:18:46.018614: step 8379, loss 0.147405, acc 0.921875\n",
      "2018-05-23T13:18:46.384637: step 8380, loss 0.236519, acc 0.890625\n",
      "2018-05-23T13:18:46.734701: step 8381, loss 0.22611, acc 0.875\n",
      "2018-05-23T13:18:47.083765: step 8382, loss 0.474773, acc 0.8125\n",
      "2018-05-23T13:18:47.447791: step 8383, loss 0.206532, acc 0.921875\n",
      "2018-05-23T13:18:47.797855: step 8384, loss 0.169913, acc 0.9375\n",
      "2018-05-23T13:18:48.146922: step 8385, loss 0.228578, acc 0.890625\n",
      "2018-05-23T13:18:48.511946: step 8386, loss 0.339763, acc 0.875\n",
      "2018-05-23T13:18:48.895919: step 8387, loss 0.217517, acc 0.90625\n",
      "2018-05-23T13:18:49.252962: step 8388, loss 0.18229, acc 0.921875\n",
      "2018-05-23T13:18:49.887265: step 8389, loss 0.214165, acc 0.90625\n",
      "2018-05-23T13:18:50.420838: step 8390, loss 0.255856, acc 0.890625\n",
      "2018-05-23T13:18:51.019237: step 8391, loss 0.369843, acc 0.828125\n",
      "2018-05-23T13:18:51.600682: step 8392, loss 0.136831, acc 0.9375\n",
      "2018-05-23T13:18:52.214042: step 8393, loss 0.173758, acc 0.953125\n",
      "2018-05-23T13:18:52.773544: step 8394, loss 0.19255, acc 0.90625\n",
      "2018-05-23T13:18:53.314097: step 8395, loss 0.140812, acc 0.96875\n",
      "2018-05-23T13:18:53.819744: step 8396, loss 0.194882, acc 0.921875\n",
      "2018-05-23T13:18:54.298468: step 8397, loss 0.264318, acc 0.875\n",
      "2018-05-23T13:18:54.787157: step 8398, loss 0.244886, acc 0.828125\n",
      "2018-05-23T13:18:55.298789: step 8399, loss 0.153455, acc 0.921875\n",
      "2018-05-23T13:18:55.860288: step 8400, loss 0.130105, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:19:02.343941: step 8400, loss 0.87624, acc 0.723103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8400\n",
      "\n",
      "2018-05-23T13:19:04.660744: step 8401, loss 0.381893, acc 0.84375\n",
      "2018-05-23T13:19:05.395777: step 8402, loss 0.166379, acc 0.96875\n",
      "2018-05-23T13:19:05.895440: step 8403, loss 0.189307, acc 0.921875\n",
      "2018-05-23T13:19:06.494837: step 8404, loss 0.171685, acc 0.9375\n",
      "2018-05-23T13:19:07.144102: step 8405, loss 0.241602, acc 0.921875\n",
      "2018-05-23T13:19:07.613843: step 8406, loss 0.189752, acc 0.921875\n",
      "2018-05-23T13:19:08.108520: step 8407, loss 0.181205, acc 0.90625\n",
      "2018-05-23T13:19:08.630124: step 8408, loss 0.225315, acc 0.875\n",
      "2018-05-23T13:19:09.308318: step 8409, loss 0.108502, acc 0.953125\n",
      "2018-05-23T13:19:09.949595: step 8410, loss 0.0927874, acc 0.9375\n",
      "2018-05-23T13:19:10.415349: step 8411, loss 0.174852, acc 0.953125\n",
      "2018-05-23T13:19:10.851183: step 8412, loss 0.360314, acc 0.875\n",
      "2018-05-23T13:19:11.295993: step 8413, loss 0.119171, acc 0.984375\n",
      "2018-05-23T13:19:11.745793: step 8414, loss 0.280874, acc 0.828125\n",
      "2018-05-23T13:19:12.223512: step 8415, loss 0.211014, acc 0.890625\n",
      "2018-05-23T13:19:12.721179: step 8416, loss 0.220808, acc 0.875\n",
      "2018-05-23T13:19:13.220843: step 8417, loss 0.308448, acc 0.875\n",
      "2018-05-23T13:19:13.688592: step 8418, loss 0.248103, acc 0.90625\n",
      "2018-05-23T13:19:14.095505: step 8419, loss 0.235863, acc 0.875\n",
      "2018-05-23T13:19:14.522361: step 8420, loss 0.132619, acc 0.984375\n",
      "2018-05-23T13:19:14.964179: step 8421, loss 0.188423, acc 0.9375\n",
      "2018-05-23T13:19:15.378072: step 8422, loss 0.173082, acc 0.90625\n",
      "2018-05-23T13:19:15.796953: step 8423, loss 0.135193, acc 0.953125\n",
      "2018-05-23T13:19:16.254727: step 8424, loss 0.15426, acc 0.90625\n",
      "2018-05-23T13:19:16.676598: step 8425, loss 0.336608, acc 0.953125\n",
      "2018-05-23T13:19:17.068549: step 8426, loss 0.184783, acc 0.890625\n",
      "2018-05-23T13:19:17.485436: step 8427, loss 0.250616, acc 0.890625\n",
      "2018-05-23T13:19:17.949193: step 8428, loss 0.341221, acc 0.90625\n",
      "2018-05-23T13:19:18.400986: step 8429, loss 0.141982, acc 0.921875\n",
      "2018-05-23T13:19:18.914612: step 8430, loss 0.148318, acc 0.9375\n",
      "2018-05-23T13:19:19.345460: step 8431, loss 0.305753, acc 0.90625\n",
      "2018-05-23T13:19:19.843128: step 8432, loss 0.183735, acc 0.921875\n",
      "2018-05-23T13:19:20.291927: step 8433, loss 0.195126, acc 0.921875\n",
      "2018-05-23T13:19:20.723774: step 8434, loss 0.133287, acc 0.890625\n",
      "2018-05-23T13:19:21.294245: step 8435, loss 0.191431, acc 0.9375\n",
      "2018-05-23T13:19:21.816847: step 8436, loss 0.189107, acc 0.9375\n",
      "2018-05-23T13:19:22.353411: step 8437, loss 0.162562, acc 0.921875\n",
      "2018-05-23T13:19:22.866041: step 8438, loss 0.135471, acc 0.9375\n",
      "2018-05-23T13:19:23.410583: step 8439, loss 0.264915, acc 0.90625\n",
      "2018-05-23T13:19:23.902268: step 8440, loss 0.226852, acc 0.90625\n",
      "2018-05-23T13:19:24.429857: step 8441, loss 0.447255, acc 0.78125\n",
      "2018-05-23T13:19:24.850730: step 8442, loss 0.223829, acc 0.890625\n",
      "2018-05-23T13:19:25.284570: step 8443, loss 0.192669, acc 0.890625\n",
      "2018-05-23T13:19:25.783238: step 8444, loss 0.265299, acc 0.875\n",
      "2018-05-23T13:19:26.255972: step 8445, loss 0.212397, acc 0.875\n",
      "2018-05-23T13:19:26.679838: step 8446, loss 0.228934, acc 0.90625\n",
      "2018-05-23T13:19:27.210419: step 8447, loss 0.165113, acc 0.953125\n",
      "2018-05-23T13:19:27.786877: step 8448, loss 0.185498, acc 0.90625\n",
      "2018-05-23T13:19:28.564796: step 8449, loss 0.257823, acc 0.9375\n",
      "2018-05-23T13:19:29.167187: step 8450, loss 0.243343, acc 0.90625\n",
      "2018-05-23T13:19:29.649893: step 8451, loss 0.100873, acc 0.984375\n",
      "2018-05-23T13:19:30.120633: step 8452, loss 0.172221, acc 0.921875\n",
      "2018-05-23T13:19:30.604340: step 8453, loss 0.218497, acc 0.890625\n",
      "2018-05-23T13:19:31.352338: step 8454, loss 0.303235, acc 0.859375\n",
      "2018-05-23T13:19:31.818093: step 8455, loss 0.224865, acc 0.90625\n",
      "2018-05-23T13:19:32.226000: step 8456, loss 0.169452, acc 0.9375\n",
      "2018-05-23T13:19:32.712698: step 8457, loss 0.177146, acc 0.90625\n",
      "2018-05-23T13:19:33.137562: step 8458, loss 0.397133, acc 0.796875\n",
      "2018-05-23T13:19:33.577387: step 8459, loss 0.445665, acc 0.828125\n",
      "2018-05-23T13:19:34.087023: step 8460, loss 0.146275, acc 0.953125\n",
      "2018-05-23T13:19:34.557763: step 8461, loss 0.124315, acc 0.9375\n",
      "2018-05-23T13:19:34.954701: step 8462, loss 0.29086, acc 0.890625\n",
      "2018-05-23T13:19:35.583020: step 8463, loss 0.151662, acc 0.921875\n",
      "2018-05-23T13:19:36.171446: step 8464, loss 0.138011, acc 0.96875\n",
      "2018-05-23T13:19:36.612267: step 8465, loss 0.119467, acc 0.9375\n",
      "2018-05-23T13:19:37.128885: step 8466, loss 0.388092, acc 0.828125\n",
      "2018-05-23T13:19:37.639518: step 8467, loss 0.224896, acc 0.890625\n",
      "2018-05-23T13:19:38.188051: step 8468, loss 0.19487, acc 0.921875\n",
      "2018-05-23T13:19:38.833324: step 8469, loss 0.167928, acc 0.9375\n",
      "2018-05-23T13:19:39.545420: step 8470, loss 0.208945, acc 0.859375\n",
      "2018-05-23T13:19:40.098951: step 8471, loss 0.184538, acc 0.890625\n",
      "2018-05-23T13:19:40.603589: step 8472, loss 0.251854, acc 0.890625\n",
      "2018-05-23T13:19:41.065354: step 8473, loss 0.168508, acc 0.96875\n",
      "2018-05-23T13:19:41.557039: step 8474, loss 0.117365, acc 0.953125\n",
      "2018-05-23T13:19:42.141476: step 8475, loss 0.181186, acc 0.921875\n",
      "2018-05-23T13:19:42.709954: step 8476, loss 0.260622, acc 0.859375\n",
      "2018-05-23T13:19:43.221586: step 8477, loss 0.166582, acc 0.9375\n",
      "2018-05-23T13:19:43.723243: step 8478, loss 0.227047, acc 0.921875\n",
      "2018-05-23T13:19:44.261804: step 8479, loss 0.350682, acc 0.859375\n",
      "2018-05-23T13:19:44.898103: step 8480, loss 0.300539, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:19:45.427685: step 8481, loss 0.115182, acc 0.96875\n",
      "2018-05-23T13:19:45.967240: step 8482, loss 0.236563, acc 0.921875\n",
      "2018-05-23T13:19:46.419031: step 8483, loss 0.311957, acc 0.84375\n",
      "2018-05-23T13:19:46.875810: step 8484, loss 0.2386, acc 0.859375\n",
      "2018-05-23T13:19:47.326604: step 8485, loss 0.139043, acc 0.9375\n",
      "2018-05-23T13:19:47.818290: step 8486, loss 0.365864, acc 0.828125\n",
      "2018-05-23T13:19:48.279056: step 8487, loss 0.252348, acc 0.890625\n",
      "2018-05-23T13:19:49.076921: step 8488, loss 0.264985, acc 0.890625\n",
      "2018-05-23T13:19:49.826915: step 8489, loss 0.249979, acc 0.890625\n",
      "2018-05-23T13:19:50.436286: step 8490, loss 0.146677, acc 0.953125\n",
      "2018-05-23T13:19:51.357820: step 8491, loss 0.199736, acc 0.875\n",
      "2018-05-23T13:19:52.003094: step 8492, loss 0.219087, acc 0.875\n",
      "2018-05-23T13:19:52.512731: step 8493, loss 0.153788, acc 0.953125\n",
      "2018-05-23T13:19:53.080212: step 8494, loss 0.254209, acc 0.859375\n",
      "2018-05-23T13:19:53.656671: step 8495, loss 0.119854, acc 0.96875\n",
      "2018-05-23T13:19:54.174286: step 8496, loss 0.142779, acc 0.96875\n",
      "2018-05-23T13:19:54.746754: step 8497, loss 0.177977, acc 0.90625\n",
      "2018-05-23T13:19:55.482784: step 8498, loss 0.256365, acc 0.90625\n",
      "2018-05-23T13:19:56.276662: step 8499, loss 0.337272, acc 0.84375\n",
      "2018-05-23T13:19:56.978784: step 8500, loss 0.334095, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:20:06.410550: step 8500, loss 0.862429, acc 0.721246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8500\n",
      "\n",
      "2018-05-23T13:20:08.986660: step 8501, loss 0.2152, acc 0.890625\n",
      "2018-05-23T13:20:09.764578: step 8502, loss 0.188929, acc 0.90625\n",
      "2018-05-23T13:20:10.298151: step 8503, loss 0.26405, acc 0.875\n",
      "2018-05-23T13:20:11.064102: step 8504, loss 0.142938, acc 0.921875\n",
      "2018-05-23T13:20:11.600666: step 8505, loss 0.373209, acc 0.828125\n",
      "2018-05-23T13:20:12.134238: step 8506, loss 0.210697, acc 0.921875\n",
      "2018-05-23T13:20:12.620937: step 8507, loss 0.24338, acc 0.890625\n",
      "2018-05-23T13:20:13.107635: step 8508, loss 0.156011, acc 0.921875\n",
      "2018-05-23T13:20:13.608295: step 8509, loss 0.140478, acc 0.953125\n",
      "2018-05-23T13:20:14.162812: step 8510, loss 0.214398, acc 0.90625\n",
      "2018-05-23T13:20:14.645520: step 8511, loss 0.16518, acc 0.953125\n",
      "2018-05-23T13:20:15.159146: step 8512, loss 0.206087, acc 0.890625\n",
      "2018-05-23T13:20:15.655818: step 8513, loss 0.309351, acc 0.859375\n",
      "2018-05-23T13:20:16.171439: step 8514, loss 0.205339, acc 0.921875\n",
      "2018-05-23T13:20:16.667112: step 8515, loss 0.288323, acc 0.796875\n",
      "2018-05-23T13:20:17.160791: step 8516, loss 0.170655, acc 0.90625\n",
      "2018-05-23T13:20:17.657462: step 8517, loss 0.222839, acc 0.90625\n",
      "2018-05-23T13:20:18.155132: step 8518, loss 0.205683, acc 0.9375\n",
      "2018-05-23T13:20:18.657787: step 8519, loss 0.215698, acc 0.90625\n",
      "2018-05-23T13:20:19.154458: step 8520, loss 0.202106, acc 0.953125\n",
      "2018-05-23T13:20:19.656117: step 8521, loss 0.227552, acc 0.90625\n",
      "2018-05-23T13:20:20.134835: step 8522, loss 0.189876, acc 0.921875\n",
      "2018-05-23T13:20:20.634498: step 8523, loss 0.208272, acc 0.90625\n",
      "2018-05-23T13:20:21.197991: step 8524, loss 0.173499, acc 0.921875\n",
      "2018-05-23T13:20:21.697655: step 8525, loss 0.127953, acc 0.921875\n",
      "2018-05-23T13:20:22.224246: step 8526, loss 0.357055, acc 0.84375\n",
      "2018-05-23T13:20:22.730891: step 8527, loss 0.222391, acc 0.875\n",
      "2018-05-23T13:20:23.287403: step 8528, loss 0.191576, acc 0.90625\n",
      "2018-05-23T13:20:23.831946: step 8529, loss 0.33233, acc 0.875\n",
      "2018-05-23T13:20:24.411395: step 8530, loss 0.397794, acc 0.84375\n",
      "2018-05-23T13:20:24.981869: step 8531, loss 0.241418, acc 0.90625\n",
      "2018-05-23T13:20:25.536386: step 8532, loss 0.239981, acc 0.890625\n",
      "2018-05-23T13:20:26.071953: step 8533, loss 0.0918058, acc 0.9375\n",
      "2018-05-23T13:20:26.633450: step 8534, loss 0.165647, acc 0.921875\n",
      "2018-05-23T13:20:27.304655: step 8535, loss 0.249116, acc 0.890625\n",
      "2018-05-23T13:20:28.003785: step 8536, loss 0.160507, acc 0.9375\n",
      "2018-05-23T13:20:28.638088: step 8537, loss 0.250493, acc 0.90625\n",
      "2018-05-23T13:20:29.394067: step 8538, loss 0.160543, acc 0.90625\n",
      "2018-05-23T13:20:30.458221: step 8539, loss 0.25404, acc 0.875\n",
      "2018-05-23T13:20:31.029691: step 8540, loss 0.356172, acc 0.859375\n",
      "2018-05-23T13:20:31.654019: step 8541, loss 0.16153, acc 0.953125\n",
      "2018-05-23T13:20:32.241448: step 8542, loss 0.184341, acc 0.953125\n",
      "2018-05-23T13:20:33.099160: step 8543, loss 0.404654, acc 0.859375\n",
      "2018-05-23T13:20:34.171285: step 8544, loss 0.239622, acc 0.9375\n",
      "2018-05-23T13:20:34.905323: step 8545, loss 0.1584, acc 0.96875\n",
      "2018-05-23T13:20:35.651326: step 8546, loss 0.238053, acc 0.90625\n",
      "2018-05-23T13:20:36.343475: step 8547, loss 0.234169, acc 0.890625\n",
      "2018-05-23T13:20:36.876050: step 8548, loss 0.213447, acc 0.890625\n",
      "2018-05-23T13:20:37.587147: step 8549, loss 0.186988, acc 0.953125\n",
      "2018-05-23T13:20:38.209483: step 8550, loss 0.256753, acc 0.875\n",
      "2018-05-23T13:20:38.840795: step 8551, loss 0.207265, acc 0.890625\n",
      "2018-05-23T13:20:39.437197: step 8552, loss 0.231399, acc 0.90625\n",
      "2018-05-23T13:20:40.052560: step 8553, loss 0.227217, acc 0.890625\n",
      "2018-05-23T13:20:41.232396: step 8554, loss 0.214978, acc 0.9375\n",
      "2018-05-23T13:20:42.357388: step 8555, loss 0.268299, acc 0.90625\n",
      "2018-05-23T13:20:43.075466: step 8556, loss 0.23782, acc 0.875\n",
      "2018-05-23T13:20:43.891283: step 8557, loss 0.1163, acc 0.921875\n",
      "2018-05-23T13:20:44.703111: step 8558, loss 0.145268, acc 0.9375\n",
      "2018-05-23T13:20:45.464075: step 8559, loss 0.219374, acc 0.921875\n",
      "2018-05-23T13:20:46.368655: step 8560, loss 0.268569, acc 0.90625\n",
      "2018-05-23T13:20:47.150564: step 8561, loss 0.28033, acc 0.890625\n",
      "2018-05-23T13:20:47.995304: step 8562, loss 0.134987, acc 0.9375\n",
      "2018-05-23T13:20:48.612652: step 8563, loss 0.277924, acc 0.890625\n",
      "2018-05-23T13:20:49.221025: step 8564, loss 0.208764, acc 0.90625\n",
      "2018-05-23T13:20:49.790502: step 8565, loss 0.212135, acc 0.90625\n",
      "2018-05-23T13:20:50.366959: step 8566, loss 0.188543, acc 0.96875\n",
      "2018-05-23T13:20:51.333374: step 8567, loss 0.147041, acc 0.921875\n",
      "2018-05-23T13:20:52.183102: step 8568, loss 0.208489, acc 0.921875\n",
      "2018-05-23T13:20:53.038812: step 8569, loss 0.174852, acc 0.9375\n",
      "2018-05-23T13:20:53.827703: step 8570, loss 0.345297, acc 0.828125\n",
      "2018-05-23T13:20:54.423111: step 8571, loss 0.249559, acc 0.875\n",
      "2018-05-23T13:20:55.129220: step 8572, loss 0.0965403, acc 0.953125\n",
      "2018-05-23T13:20:55.754546: step 8573, loss 0.271809, acc 0.84375\n",
      "2018-05-23T13:20:56.454674: step 8574, loss 0.258687, acc 0.875\n",
      "2018-05-23T13:20:57.244560: step 8575, loss 0.288244, acc 0.84375\n",
      "2018-05-23T13:20:57.792096: step 8576, loss 0.176306, acc 0.921875\n",
      "2018-05-23T13:20:58.544100: step 8577, loss 0.278314, acc 0.9375\n",
      "2018-05-23T13:20:59.199331: step 8578, loss 0.19554, acc 0.921875\n",
      "2018-05-23T13:20:59.858568: step 8579, loss 0.325832, acc 0.84375\n",
      "2018-05-23T13:21:00.465943: step 8580, loss 0.28921, acc 0.859375\n",
      "2018-05-23T13:21:01.035418: step 8581, loss 0.160353, acc 0.921875\n",
      "2018-05-23T13:21:01.768458: step 8582, loss 0.326226, acc 0.84375\n",
      "2018-05-23T13:21:02.499502: step 8583, loss 0.179695, acc 0.953125\n",
      "2018-05-23T13:21:03.268445: step 8584, loss 0.264184, acc 0.875\n",
      "2018-05-23T13:21:04.067308: step 8585, loss 0.211811, acc 0.9375\n",
      "2018-05-23T13:21:04.765441: step 8586, loss 0.312243, acc 0.875\n",
      "2018-05-23T13:21:05.375807: step 8587, loss 0.15137, acc 0.96875\n",
      "2018-05-23T13:21:05.940298: step 8588, loss 0.148937, acc 0.953125\n",
      "2018-05-23T13:21:06.496811: step 8589, loss 0.123169, acc 0.96875\n",
      "2018-05-23T13:21:07.049331: step 8590, loss 0.310272, acc 0.84375\n",
      "2018-05-23T13:21:07.608834: step 8591, loss 0.198516, acc 0.890625\n",
      "2018-05-23T13:21:08.150385: step 8592, loss 0.237155, acc 0.875\n",
      "2018-05-23T13:21:08.714875: step 8593, loss 0.166873, acc 0.90625\n",
      "2018-05-23T13:21:09.219524: step 8594, loss 0.322434, acc 0.859375\n",
      "2018-05-23T13:21:09.759083: step 8595, loss 0.320462, acc 0.859375\n",
      "2018-05-23T13:21:10.382414: step 8596, loss 0.306094, acc 0.84375\n",
      "2018-05-23T13:21:10.998765: step 8597, loss 0.240739, acc 0.859375\n",
      "2018-05-23T13:21:11.600156: step 8598, loss 0.433447, acc 0.875\n",
      "2018-05-23T13:21:12.188582: step 8599, loss 0.101726, acc 0.984375\n",
      "2018-05-23T13:21:12.746090: step 8600, loss 0.278759, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:21:19.551883: step 8600, loss 0.848476, acc 0.722818\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8600\n",
      "\n",
      "2018-05-23T13:21:21.942489: step 8601, loss 0.256948, acc 0.90625\n",
      "2018-05-23T13:21:22.480050: step 8602, loss 0.291893, acc 0.859375\n",
      "2018-05-23T13:21:23.063489: step 8603, loss 0.207255, acc 0.921875\n",
      "2018-05-23T13:21:23.591079: step 8604, loss 0.296662, acc 0.90625\n",
      "2018-05-23T13:21:24.107696: step 8605, loss 0.391313, acc 0.84375\n",
      "2018-05-23T13:21:24.645257: step 8606, loss 0.304828, acc 0.84375\n",
      "2018-05-23T13:21:25.216730: step 8607, loss 0.207735, acc 0.859375\n",
      "2018-05-23T13:21:25.748308: step 8608, loss 0.208846, acc 0.875\n",
      "2018-05-23T13:21:26.267917: step 8609, loss 0.293291, acc 0.90625\n",
      "2018-05-23T13:21:26.897234: step 8610, loss 0.242599, acc 0.859375\n",
      "2018-05-23T13:21:27.523558: step 8611, loss 0.219092, acc 0.9375\n",
      "2018-05-23T13:21:28.064112: step 8612, loss 0.197576, acc 0.921875\n",
      "2018-05-23T13:21:28.652539: step 8613, loss 0.125083, acc 0.96875\n",
      "2018-05-23T13:21:29.445416: step 8614, loss 0.209586, acc 0.90625\n",
      "2018-05-23T13:21:30.015891: step 8615, loss 0.247725, acc 0.859375\n",
      "2018-05-23T13:21:30.586364: step 8616, loss 0.169431, acc 0.890625\n",
      "2018-05-23T13:21:31.147863: step 8617, loss 0.199716, acc 0.921875\n",
      "2018-05-23T13:21:31.703376: step 8618, loss 0.140528, acc 0.921875\n",
      "2018-05-23T13:21:32.224981: step 8619, loss 0.204491, acc 0.921875\n",
      "2018-05-23T13:21:32.869257: step 8620, loss 0.20911, acc 0.890625\n",
      "2018-05-23T13:21:33.555422: step 8621, loss 0.265558, acc 0.890625\n",
      "2018-05-23T13:21:34.395175: step 8622, loss 0.163813, acc 0.9375\n",
      "2018-05-23T13:21:35.013521: step 8623, loss 0.103136, acc 0.953125\n",
      "2018-05-23T13:21:35.630869: step 8624, loss 0.200752, acc 0.921875\n",
      "2018-05-23T13:21:36.733918: step 8625, loss 0.276339, acc 0.921875\n",
      "2018-05-23T13:21:37.544749: step 8626, loss 0.31742, acc 0.875\n",
      "2018-05-23T13:21:38.203986: step 8627, loss 0.273804, acc 0.859375\n",
      "2018-05-23T13:21:38.779449: step 8628, loss 0.195972, acc 0.890625\n",
      "2018-05-23T13:21:39.411754: step 8629, loss 0.178903, acc 0.96875\n",
      "2018-05-23T13:21:40.032095: step 8630, loss 0.192746, acc 0.9375\n",
      "2018-05-23T13:21:40.764136: step 8631, loss 0.138438, acc 0.953125\n",
      "2018-05-23T13:21:41.417389: step 8632, loss 0.289799, acc 0.90625\n",
      "2018-05-23T13:21:42.216253: step 8633, loss 0.202392, acc 0.9375\n",
      "2018-05-23T13:21:42.879478: step 8634, loss 0.149994, acc 0.921875\n",
      "2018-05-23T13:21:43.485856: step 8635, loss 0.254738, acc 0.875\n",
      "2018-05-23T13:21:44.026410: step 8636, loss 0.18572, acc 0.953125\n",
      "2018-05-23T13:21:44.581925: step 8637, loss 0.226843, acc 0.921875\n",
      "2018-05-23T13:21:45.235177: step 8638, loss 0.290393, acc 0.90625\n",
      "2018-05-23T13:21:45.812631: step 8639, loss 0.256098, acc 0.90625\n",
      "2018-05-23T13:21:46.472865: step 8640, loss 0.201274, acc 0.9375\n",
      "2018-05-23T13:21:47.071265: step 8641, loss 0.159801, acc 0.90625\n",
      "2018-05-23T13:21:47.669663: step 8642, loss 0.213007, acc 0.890625\n",
      "2018-05-23T13:21:48.317929: step 8643, loss 0.0783882, acc 0.96875\n",
      "2018-05-23T13:21:48.945251: step 8644, loss 0.223683, acc 0.890625\n",
      "2018-05-23T13:21:49.510738: step 8645, loss 0.181663, acc 0.921875\n",
      "2018-05-23T13:21:50.061267: step 8646, loss 0.270757, acc 0.84375\n",
      "2018-05-23T13:21:50.630742: step 8647, loss 0.362797, acc 0.859375\n",
      "2018-05-23T13:21:51.250085: step 8648, loss 0.103628, acc 0.953125\n",
      "2018-05-23T13:21:51.874416: step 8649, loss 0.198332, acc 0.890625\n",
      "2018-05-23T13:21:52.468825: step 8650, loss 0.19274, acc 0.890625\n",
      "2018-05-23T13:21:53.009378: step 8651, loss 0.341474, acc 0.859375\n",
      "2018-05-23T13:21:53.580849: step 8652, loss 0.287481, acc 0.84375\n",
      "2018-05-23T13:21:54.101456: step 8653, loss 0.210463, acc 0.90625\n",
      "2018-05-23T13:21:54.632039: step 8654, loss 0.178396, acc 0.890625\n",
      "2018-05-23T13:21:55.368069: step 8655, loss 0.288477, acc 0.859375\n",
      "2018-05-23T13:21:56.045257: step 8656, loss 0.151037, acc 0.984375\n",
      "2018-05-23T13:21:56.775304: step 8657, loss 0.207982, acc 0.890625\n",
      "2018-05-23T13:21:57.312866: step 8658, loss 0.525404, acc 0.796875\n",
      "2018-05-23T13:21:57.969110: step 8659, loss 0.322238, acc 0.890625\n",
      "2018-05-23T13:21:58.525622: step 8660, loss 0.206081, acc 0.9375\n",
      "2018-05-23T13:21:59.075152: step 8661, loss 0.219417, acc 0.890625\n",
      "2018-05-23T13:21:59.584787: step 8662, loss 0.153245, acc 0.921875\n",
      "2018-05-23T13:22:00.100409: step 8663, loss 0.239163, acc 0.890625\n",
      "2018-05-23T13:22:00.673875: step 8664, loss 0.240176, acc 0.921875\n",
      "2018-05-23T13:22:01.302194: step 8665, loss 0.197107, acc 0.921875\n",
      "2018-05-23T13:22:02.035233: step 8666, loss 0.115833, acc 0.9375\n",
      "2018-05-23T13:22:02.874987: step 8667, loss 0.273164, acc 0.875\n",
      "2018-05-23T13:22:03.706761: step 8668, loss 0.0976201, acc 0.984375\n",
      "2018-05-23T13:22:04.606354: step 8669, loss 0.268306, acc 0.859375\n",
      "2018-05-23T13:22:05.290525: step 8670, loss 0.317261, acc 0.875\n",
      "2018-05-23T13:22:05.860001: step 8671, loss 0.216698, acc 0.90625\n",
      "2018-05-23T13:22:06.447430: step 8672, loss 0.295038, acc 0.875\n",
      "2018-05-23T13:22:07.114645: step 8673, loss 0.187622, acc 0.9375\n",
      "2018-05-23T13:22:07.743961: step 8674, loss 0.151922, acc 0.953125\n",
      "2018-05-23T13:22:08.503928: step 8675, loss 0.267708, acc 0.953125\n",
      "2018-05-23T13:22:09.070412: step 8676, loss 0.172609, acc 0.9375\n",
      "2018-05-23T13:22:09.791483: step 8677, loss 0.22041, acc 0.890625\n",
      "2018-05-23T13:22:10.352982: step 8678, loss 0.187344, acc 0.921875\n",
      "2018-05-23T13:22:10.875583: step 8679, loss 0.210504, acc 0.953125\n",
      "2018-05-23T13:22:11.400180: step 8680, loss 0.300274, acc 0.890625\n",
      "2018-05-23T13:22:11.923779: step 8681, loss 0.236454, acc 0.921875\n",
      "2018-05-23T13:22:12.596978: step 8682, loss 0.26748, acc 0.859375\n",
      "2018-05-23T13:22:13.251228: step 8683, loss 0.180928, acc 0.921875\n",
      "2018-05-23T13:22:13.860597: step 8684, loss 0.254749, acc 0.875\n",
      "2018-05-23T13:22:14.649487: step 8685, loss 0.268818, acc 0.890625\n",
      "2018-05-23T13:22:15.498218: step 8686, loss 0.225274, acc 0.90625\n",
      "2018-05-23T13:22:16.470615: step 8687, loss 0.256403, acc 0.890625\n",
      "2018-05-23T13:22:17.260502: step 8688, loss 0.178203, acc 0.921875\n",
      "2018-05-23T13:22:18.033443: step 8689, loss 0.395428, acc 0.828125\n",
      "2018-05-23T13:22:19.085620: step 8690, loss 0.222424, acc 0.921875\n",
      "2018-05-23T13:22:19.959283: step 8691, loss 0.251691, acc 0.90625\n",
      "2018-05-23T13:22:20.717255: step 8692, loss 0.225999, acc 0.90625\n",
      "2018-05-23T13:22:21.743510: step 8693, loss 0.285584, acc 0.90625\n",
      "2018-05-23T13:22:22.898420: step 8694, loss 0.303337, acc 0.890625\n",
      "2018-05-23T13:22:23.692296: step 8695, loss 0.367792, acc 0.8125\n",
      "2018-05-23T13:22:24.308648: step 8696, loss 0.233781, acc 0.90625\n",
      "2018-05-23T13:22:25.049665: step 8697, loss 0.144715, acc 0.953125\n",
      "2018-05-23T13:22:25.711892: step 8698, loss 0.197285, acc 0.953125\n",
      "2018-05-23T13:22:26.329242: step 8699, loss 0.256362, acc 0.90625\n",
      "2018-05-23T13:22:27.076243: step 8700, loss 0.147651, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:22:33.435232: step 8700, loss 0.852126, acc 0.721103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8700\n",
      "\n",
      "2018-05-23T13:22:36.197841: step 8701, loss 0.271475, acc 0.859375\n",
      "2018-05-23T13:22:36.789259: step 8702, loss 0.222149, acc 0.953125\n",
      "2018-05-23T13:22:37.388655: step 8703, loss 0.224294, acc 0.890625\n",
      "2018-05-23T13:22:38.042905: step 8704, loss 0.159217, acc 0.90625\n",
      "2018-05-23T13:22:38.589443: step 8705, loss 0.326973, acc 0.875\n",
      "2018-05-23T13:22:39.133986: step 8706, loss 0.300434, acc 0.90625\n",
      "2018-05-23T13:22:39.684514: step 8707, loss 0.208669, acc 0.953125\n",
      "2018-05-23T13:22:40.234043: step 8708, loss 0.245694, acc 0.875\n",
      "2018-05-23T13:22:40.755647: step 8709, loss 0.259791, acc 0.90625\n",
      "2018-05-23T13:22:41.255311: step 8710, loss 0.165434, acc 0.9375\n",
      "2018-05-23T13:22:41.765946: step 8711, loss 0.288337, acc 0.859375\n",
      "2018-05-23T13:22:42.295529: step 8712, loss 0.13625, acc 0.953125\n",
      "2018-05-23T13:22:42.822120: step 8713, loss 0.167994, acc 0.953125\n",
      "2018-05-23T13:22:43.362673: step 8714, loss 0.113385, acc 0.953125\n",
      "2018-05-23T13:22:43.913203: step 8715, loss 0.350519, acc 0.84375\n",
      "2018-05-23T13:22:44.475696: step 8716, loss 0.207961, acc 0.953125\n",
      "2018-05-23T13:22:45.072101: step 8717, loss 0.173317, acc 0.9375\n",
      "2018-05-23T13:22:45.602681: step 8718, loss 0.339561, acc 0.859375\n",
      "2018-05-23T13:22:46.128276: step 8719, loss 0.315693, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:22:46.920156: step 8720, loss 0.334627, acc 0.890625\n",
      "2018-05-23T13:22:47.469687: step 8721, loss 0.290031, acc 0.8125\n",
      "2018-05-23T13:22:47.996278: step 8722, loss 0.254573, acc 0.890625\n",
      "2018-05-23T13:22:48.523866: step 8723, loss 0.229238, acc 0.90625\n",
      "2018-05-23T13:22:49.033503: step 8724, loss 0.160004, acc 0.9375\n",
      "2018-05-23T13:22:49.549124: step 8725, loss 0.215924, acc 0.890625\n",
      "2018-05-23T13:22:50.105636: step 8726, loss 0.203445, acc 0.921875\n",
      "2018-05-23T13:22:50.667133: step 8727, loss 0.276963, acc 0.921875\n",
      "2018-05-23T13:22:51.335345: step 8728, loss 0.313862, acc 0.84375\n",
      "2018-05-23T13:22:52.072375: step 8729, loss 0.175687, acc 0.921875\n",
      "2018-05-23T13:22:52.754549: step 8730, loss 0.395461, acc 0.84375\n",
      "2018-05-23T13:22:53.348958: step 8731, loss 0.292039, acc 0.890625\n",
      "2018-05-23T13:22:53.898489: step 8732, loss 0.328291, acc 0.90625\n",
      "2018-05-23T13:22:54.472952: step 8733, loss 0.174318, acc 0.96875\n",
      "2018-05-23T13:22:55.055394: step 8734, loss 0.148975, acc 0.921875\n",
      "2018-05-23T13:22:55.665761: step 8735, loss 0.303456, acc 0.828125\n",
      "2018-05-23T13:22:56.372869: step 8736, loss 0.299422, acc 0.890625\n",
      "2018-05-23T13:22:56.899461: step 8737, loss 0.164284, acc 0.953125\n",
      "2018-05-23T13:22:57.442009: step 8738, loss 0.243345, acc 0.890625\n",
      "2018-05-23T13:22:57.956632: step 8739, loss 0.252504, acc 0.890625\n",
      "2018-05-23T13:22:58.464274: step 8740, loss 0.210537, acc 0.921875\n",
      "2018-05-23T13:22:59.009815: step 8741, loss 0.261485, acc 0.875\n",
      "2018-05-23T13:22:59.769781: step 8742, loss 0.18921, acc 0.921875\n",
      "2018-05-23T13:23:00.516788: step 8743, loss 0.405187, acc 0.890625\n",
      "2018-05-23T13:23:01.133134: step 8744, loss 0.222087, acc 0.890625\n",
      "2018-05-23T13:23:01.835257: step 8745, loss 0.327411, acc 0.796875\n",
      "2018-05-23T13:23:02.397753: step 8746, loss 0.177934, acc 0.890625\n",
      "2018-05-23T13:23:03.053996: step 8747, loss 0.180881, acc 0.9375\n",
      "2018-05-23T13:23:03.664364: step 8748, loss 0.0820613, acc 0.96875\n",
      "2018-05-23T13:23:04.810298: step 8749, loss 0.0900357, acc 0.96875\n",
      "2018-05-23T13:23:05.374787: step 8750, loss 0.247111, acc 0.90625\n",
      "2018-05-23T13:23:05.927309: step 8751, loss 0.208406, acc 0.953125\n",
      "2018-05-23T13:23:06.535680: step 8752, loss 0.15376, acc 0.96875\n",
      "2018-05-23T13:23:07.016396: step 8753, loss 0.292037, acc 0.875\n",
      "2018-05-23T13:23:07.506085: step 8754, loss 0.176769, acc 0.890625\n",
      "2018-05-23T13:23:08.001759: step 8755, loss 0.193264, acc 0.9375\n",
      "2018-05-23T13:23:08.661993: step 8756, loss 0.120082, acc 0.9375\n",
      "2018-05-23T13:23:09.258398: step 8757, loss 0.290437, acc 0.890625\n",
      "2018-05-23T13:23:09.839842: step 8758, loss 0.121742, acc 0.96875\n",
      "2018-05-23T13:23:10.636779: step 8759, loss 0.225854, acc 0.90625\n",
      "2018-05-23T13:23:11.340890: step 8760, loss 0.236133, acc 0.890625\n",
      "2018-05-23T13:23:11.983186: step 8761, loss 0.147439, acc 0.921875\n",
      "2018-05-23T13:23:12.627447: step 8762, loss 0.488591, acc 0.828125\n",
      "2018-05-23T13:23:13.256765: step 8763, loss 0.244755, acc 0.9375\n",
      "2018-05-23T13:23:13.902039: step 8764, loss 0.229722, acc 0.890625\n",
      "2018-05-23T13:23:14.657020: step 8765, loss 0.136881, acc 0.96875\n",
      "2018-05-23T13:23:15.245452: step 8766, loss 0.1495, acc 0.96875\n",
      "2018-05-23T13:23:15.912659: step 8767, loss 0.123965, acc 0.96875\n",
      "2018-05-23T13:23:16.663650: step 8768, loss 0.321229, acc 0.84375\n",
      "2018-05-23T13:23:17.404668: step 8769, loss 0.159243, acc 0.9375\n",
      "2018-05-23T13:23:18.226470: step 8770, loss 0.134511, acc 0.984375\n",
      "2018-05-23T13:23:18.869748: step 8771, loss 0.258153, acc 0.875\n",
      "2018-05-23T13:23:19.356448: step 8772, loss 0.224172, acc 0.9375\n",
      "2018-05-23T13:23:20.161293: step 8773, loss 0.162725, acc 0.953125\n",
      "2018-05-23T13:23:20.700849: step 8774, loss 0.222068, acc 0.90625\n",
      "2018-05-23T13:23:21.192535: step 8775, loss 0.272018, acc 0.84375\n",
      "2018-05-23T13:23:21.756027: step 8776, loss 0.257087, acc 0.84375\n",
      "2018-05-23T13:23:22.345451: step 8777, loss 0.194949, acc 0.90625\n",
      "2018-05-23T13:23:22.811204: step 8778, loss 0.294134, acc 0.84375\n",
      "2018-05-23T13:23:23.335802: step 8779, loss 0.114151, acc 0.96875\n",
      "2018-05-23T13:23:23.723764: step 8780, loss 0.131493, acc 0.953125\n",
      "2018-05-23T13:23:24.113720: step 8781, loss 0.298035, acc 0.890625\n",
      "2018-05-23T13:23:24.480737: step 8782, loss 0.280965, acc 0.875\n",
      "2018-05-23T13:23:24.833795: step 8783, loss 0.283767, acc 0.875\n",
      "2018-05-23T13:23:25.194828: step 8784, loss 0.279004, acc 0.859375\n",
      "2018-05-23T13:23:25.739738: step 8785, loss 0.217001, acc 0.890625\n",
      "2018-05-23T13:23:26.174578: step 8786, loss 0.124967, acc 0.96875\n",
      "2018-05-23T13:23:26.606421: step 8787, loss 0.154063, acc 0.90625\n",
      "2018-05-23T13:23:26.997375: step 8788, loss 0.167895, acc 0.9375\n",
      "2018-05-23T13:23:27.450164: step 8789, loss 0.167754, acc 0.9375\n",
      "2018-05-23T13:23:28.017645: step 8790, loss 0.440361, acc 0.828125\n",
      "2018-05-23T13:23:28.476420: step 8791, loss 0.370123, acc 0.859375\n",
      "2018-05-23T13:23:28.918269: step 8792, loss 0.241883, acc 0.890625\n",
      "2018-05-23T13:23:29.348087: step 8793, loss 0.217801, acc 0.921875\n",
      "2018-05-23T13:23:29.820821: step 8794, loss 0.285834, acc 0.890625\n",
      "2018-05-23T13:23:30.206790: step 8795, loss 0.2898, acc 0.90625\n",
      "2018-05-23T13:23:30.605722: step 8796, loss 0.129235, acc 0.96875\n",
      "2018-05-23T13:23:31.064494: step 8797, loss 0.388206, acc 0.8125\n",
      "2018-05-23T13:23:31.513295: step 8798, loss 0.192145, acc 0.90625\n",
      "2018-05-23T13:23:31.898264: step 8799, loss 0.146898, acc 0.9375\n",
      "2018-05-23T13:23:32.282237: step 8800, loss 0.159305, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:23:38.442757: step 8800, loss 0.848982, acc 0.722103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8800\n",
      "\n",
      "2018-05-23T13:23:40.627911: step 8801, loss 0.197474, acc 0.921875\n",
      "2018-05-23T13:23:41.204370: step 8802, loss 0.23527, acc 0.921875\n",
      "2018-05-23T13:23:41.663141: step 8803, loss 0.26486, acc 0.875\n",
      "2018-05-23T13:23:42.427097: step 8804, loss 0.145264, acc 0.9375\n",
      "2018-05-23T13:23:43.173103: step 8805, loss 0.150997, acc 0.9375\n",
      "2018-05-23T13:23:43.810397: step 8806, loss 0.177275, acc 0.9375\n",
      "2018-05-23T13:23:44.591308: step 8807, loss 0.311263, acc 0.90625\n",
      "2018-05-23T13:23:45.105930: step 8808, loss 0.287763, acc 0.90625\n",
      "2018-05-23T13:23:45.509851: step 8809, loss 0.341551, acc 0.84375\n",
      "2018-05-23T13:23:45.876867: step 8810, loss 0.244074, acc 0.859375\n",
      "2018-05-23T13:23:46.241891: step 8811, loss 0.159625, acc 0.9375\n",
      "2018-05-23T13:23:46.629853: step 8812, loss 0.170198, acc 0.953125\n",
      "2018-05-23T13:23:46.992882: step 8813, loss 0.197078, acc 0.890625\n",
      "2018-05-23T13:23:47.350926: step 8814, loss 0.295434, acc 0.859375\n",
      "2018-05-23T13:23:47.729910: step 8815, loss 0.255058, acc 0.890625\n",
      "2018-05-23T13:23:48.073746: step 8816, loss 0.189419, acc 0.890625\n",
      "2018-05-23T13:23:48.424807: step 8817, loss 0.20604, acc 0.9375\n",
      "2018-05-23T13:23:48.804790: step 8818, loss 0.165834, acc 0.953125\n",
      "2018-05-23T13:23:49.193747: step 8819, loss 0.249147, acc 0.90625\n",
      "2018-05-23T13:23:49.595674: step 8820, loss 0.201634, acc 0.890625\n",
      "2018-05-23T13:23:50.063423: step 8821, loss 0.269136, acc 0.890625\n",
      "2018-05-23T13:23:50.429444: step 8822, loss 0.309085, acc 0.859375\n",
      "2018-05-23T13:23:50.786487: step 8823, loss 0.337014, acc 0.890625\n",
      "2018-05-23T13:23:51.141539: step 8824, loss 0.141906, acc 0.953125\n",
      "2018-05-23T13:23:51.543463: step 8825, loss 0.238589, acc 0.875\n",
      "2018-05-23T13:23:51.897517: step 8826, loss 0.359026, acc 0.890625\n",
      "2018-05-23T13:23:52.249575: step 8827, loss 0.287778, acc 0.90625\n",
      "2018-05-23T13:23:52.600635: step 8828, loss 0.263697, acc 0.859375\n",
      "2018-05-23T13:23:52.975632: step 8829, loss 0.188191, acc 0.921875\n",
      "2018-05-23T13:23:53.381545: step 8830, loss 0.245206, acc 0.890625\n",
      "2018-05-23T13:23:53.779481: step 8831, loss 0.188976, acc 0.921875\n",
      "2018-05-23T13:23:54.200354: step 8832, loss 0.186437, acc 0.90625\n",
      "2018-05-23T13:23:54.566377: step 8833, loss 0.286586, acc 0.84375\n",
      "2018-05-23T13:23:54.955335: step 8834, loss 0.192429, acc 0.90625\n",
      "2018-05-23T13:23:55.359255: step 8835, loss 0.311259, acc 0.875\n",
      "2018-05-23T13:23:55.731261: step 8836, loss 0.208301, acc 0.90625\n",
      "2018-05-23T13:23:56.088305: step 8837, loss 0.148373, acc 0.890625\n",
      "2018-05-23T13:23:56.440361: step 8838, loss 0.145613, acc 0.953125\n",
      "2018-05-23T13:23:56.797407: step 8839, loss 0.153112, acc 0.9375\n",
      "2018-05-23T13:23:57.164424: step 8840, loss 0.298535, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:23:57.606277: step 8841, loss 0.201936, acc 0.921875\n",
      "2018-05-23T13:23:58.001187: step 8842, loss 0.214003, acc 0.9375\n",
      "2018-05-23T13:23:58.395135: step 8843, loss 0.214691, acc 0.921875\n",
      "2018-05-23T13:23:58.789081: step 8844, loss 0.311467, acc 0.84375\n",
      "2018-05-23T13:23:59.176045: step 8845, loss 0.196931, acc 0.921875\n",
      "2018-05-23T13:23:59.553037: step 8846, loss 0.187404, acc 0.921875\n",
      "2018-05-23T13:23:59.950970: step 8847, loss 0.286209, acc 0.875\n",
      "2018-05-23T13:24:00.359878: step 8848, loss 0.325736, acc 0.875\n",
      "2018-05-23T13:24:00.721909: step 8849, loss 0.248104, acc 0.921875\n",
      "2018-05-23T13:24:01.072968: step 8850, loss 0.152599, acc 0.9375\n",
      "2018-05-23T13:24:01.504813: step 8851, loss 0.214807, acc 0.953125\n",
      "2018-05-23T13:24:01.919704: step 8852, loss 0.242694, acc 0.875\n",
      "2018-05-23T13:24:02.280739: step 8853, loss 0.170465, acc 0.953125\n",
      "2018-05-23T13:24:02.671694: step 8854, loss 0.297152, acc 0.890625\n",
      "2018-05-23T13:24:03.052673: step 8855, loss 0.281809, acc 0.890625\n",
      "2018-05-23T13:24:03.438643: step 8856, loss 0.371078, acc 0.859375\n",
      "2018-05-23T13:24:03.851536: step 8857, loss 0.238665, acc 0.890625\n",
      "2018-05-23T13:24:04.290363: step 8858, loss 0.282669, acc 0.859375\n",
      "2018-05-23T13:24:04.659376: step 8859, loss 0.171613, acc 0.96875\n",
      "2018-05-23T13:24:05.020411: step 8860, loss 0.286584, acc 0.828125\n",
      "2018-05-23T13:24:05.433305: step 8861, loss 0.195977, acc 0.90625\n",
      "2018-05-23T13:24:05.834231: step 8862, loss 0.155296, acc 0.9375\n",
      "2018-05-23T13:24:06.232167: step 8863, loss 0.199459, acc 0.921875\n",
      "2018-05-23T13:24:06.631102: step 8864, loss 0.269819, acc 0.921875\n",
      "2018-05-23T13:24:06.892402: step 8865, loss 0.107259, acc 1\n",
      "2018-05-23T13:24:07.314274: step 8866, loss 0.226927, acc 0.921875\n",
      "2018-05-23T13:24:07.706225: step 8867, loss 0.233396, acc 0.90625\n",
      "2018-05-23T13:24:08.067260: step 8868, loss 0.206989, acc 0.90625\n",
      "2018-05-23T13:24:08.431316: step 8869, loss 0.206334, acc 0.890625\n",
      "2018-05-23T13:24:08.808276: step 8870, loss 0.197437, acc 0.921875\n",
      "2018-05-23T13:24:09.205213: step 8871, loss 0.0887572, acc 1\n",
      "2018-05-23T13:24:09.584202: step 8872, loss 0.264166, acc 0.90625\n",
      "2018-05-23T13:24:10.225484: step 8873, loss 0.126102, acc 0.953125\n",
      "2018-05-23T13:24:10.610456: step 8874, loss 0.151563, acc 0.96875\n",
      "2018-05-23T13:24:10.982460: step 8875, loss 0.229836, acc 0.890625\n",
      "2018-05-23T13:24:11.366432: step 8876, loss 0.137579, acc 0.96875\n",
      "2018-05-23T13:24:11.869088: step 8877, loss 0.135384, acc 0.953125\n",
      "2018-05-23T13:24:12.242089: step 8878, loss 0.116166, acc 0.921875\n",
      "2018-05-23T13:24:12.600132: step 8879, loss 0.114056, acc 0.953125\n",
      "2018-05-23T13:24:13.003054: step 8880, loss 0.160219, acc 0.921875\n",
      "2018-05-23T13:24:13.411961: step 8881, loss 0.0958813, acc 1\n",
      "2018-05-23T13:24:13.809897: step 8882, loss 0.176534, acc 0.921875\n",
      "2018-05-23T13:24:14.161953: step 8883, loss 0.183125, acc 0.9375\n",
      "2018-05-23T13:24:14.507031: step 8884, loss 0.083124, acc 0.96875\n",
      "2018-05-23T13:24:14.864076: step 8885, loss 0.177704, acc 0.9375\n",
      "2018-05-23T13:24:15.229100: step 8886, loss 0.162291, acc 0.921875\n",
      "2018-05-23T13:24:15.592126: step 8887, loss 0.160994, acc 0.90625\n",
      "2018-05-23T13:24:15.973110: step 8888, loss 0.220107, acc 0.90625\n",
      "2018-05-23T13:24:16.315194: step 8889, loss 0.210575, acc 0.921875\n",
      "2018-05-23T13:24:16.661268: step 8890, loss 0.153659, acc 0.953125\n",
      "2018-05-23T13:24:17.025301: step 8891, loss 0.0957465, acc 0.96875\n",
      "2018-05-23T13:24:17.460130: step 8892, loss 0.256763, acc 0.875\n",
      "2018-05-23T13:24:17.886989: step 8893, loss 0.224809, acc 0.875\n",
      "2018-05-23T13:24:18.274951: step 8894, loss 0.141904, acc 0.953125\n",
      "2018-05-23T13:24:18.668898: step 8895, loss 0.234058, acc 0.890625\n",
      "2018-05-23T13:24:19.080795: step 8896, loss 0.0830189, acc 0.984375\n",
      "2018-05-23T13:24:19.446815: step 8897, loss 0.106149, acc 0.984375\n",
      "2018-05-23T13:24:19.799872: step 8898, loss 0.161437, acc 0.921875\n",
      "2018-05-23T13:24:20.166888: step 8899, loss 0.20806, acc 0.90625\n",
      "2018-05-23T13:24:20.515954: step 8900, loss 0.178198, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:24:27.189104: step 8900, loss 0.870391, acc 0.725961\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-8900\n",
      "\n",
      "2018-05-23T13:24:28.650251: step 8901, loss 0.17869, acc 0.953125\n",
      "2018-05-23T13:24:29.059158: step 8902, loss 0.121754, acc 0.921875\n",
      "2018-05-23T13:24:29.483021: step 8903, loss 0.159354, acc 0.953125\n",
      "2018-05-23T13:24:30.150238: step 8904, loss 0.116216, acc 0.96875\n",
      "2018-05-23T13:24:30.717721: step 8905, loss 0.209881, acc 0.921875\n",
      "2018-05-23T13:24:31.230349: step 8906, loss 0.167903, acc 0.9375\n",
      "2018-05-23T13:24:31.690120: step 8907, loss 0.241666, acc 0.84375\n",
      "2018-05-23T13:24:32.065114: step 8908, loss 0.101676, acc 0.984375\n",
      "2018-05-23T13:24:32.449088: step 8909, loss 0.140491, acc 0.921875\n",
      "2018-05-23T13:24:32.898885: step 8910, loss 0.193442, acc 0.90625\n",
      "2018-05-23T13:24:33.327736: step 8911, loss 0.159403, acc 0.9375\n",
      "2018-05-23T13:24:33.737640: step 8912, loss 0.100781, acc 0.96875\n",
      "2018-05-23T13:24:34.146545: step 8913, loss 0.197241, acc 0.953125\n",
      "2018-05-23T13:24:34.507581: step 8914, loss 0.166983, acc 0.921875\n",
      "2018-05-23T13:24:34.852658: step 8915, loss 0.14119, acc 0.953125\n",
      "2018-05-23T13:24:35.288491: step 8916, loss 0.221734, acc 0.90625\n",
      "2018-05-23T13:24:35.736293: step 8917, loss 0.230029, acc 0.890625\n",
      "2018-05-23T13:24:36.170133: step 8918, loss 0.158742, acc 0.953125\n",
      "2018-05-23T13:24:36.676778: step 8919, loss 0.165232, acc 0.921875\n",
      "2018-05-23T13:24:37.207358: step 8920, loss 0.196721, acc 0.921875\n",
      "2018-05-23T13:24:37.653165: step 8921, loss 0.137927, acc 0.9375\n",
      "2018-05-23T13:24:38.018195: step 8922, loss 0.112538, acc 0.96875\n",
      "2018-05-23T13:24:38.435073: step 8923, loss 0.188775, acc 0.953125\n",
      "2018-05-23T13:24:38.811069: step 8924, loss 0.200349, acc 0.90625\n",
      "2018-05-23T13:24:39.188058: step 8925, loss 0.240516, acc 0.90625\n",
      "2018-05-23T13:24:39.754544: step 8926, loss 0.0992842, acc 0.96875\n",
      "2018-05-23T13:24:40.186390: step 8927, loss 0.271918, acc 0.875\n",
      "2018-05-23T13:24:40.554403: step 8928, loss 0.216092, acc 0.921875\n",
      "2018-05-23T13:24:40.948349: step 8929, loss 0.182168, acc 0.90625\n",
      "2018-05-23T13:24:41.367228: step 8930, loss 0.209485, acc 0.890625\n",
      "2018-05-23T13:24:41.748211: step 8931, loss 0.263389, acc 0.90625\n",
      "2018-05-23T13:24:42.191026: step 8932, loss 0.0494288, acc 1\n",
      "2018-05-23T13:24:42.627856: step 8933, loss 0.0854443, acc 0.984375\n",
      "2018-05-23T13:24:43.089622: step 8934, loss 0.149774, acc 0.953125\n",
      "2018-05-23T13:24:43.523463: step 8935, loss 0.143069, acc 0.921875\n",
      "2018-05-23T13:24:43.952313: step 8936, loss 0.147597, acc 0.921875\n",
      "2018-05-23T13:24:44.370195: step 8937, loss 0.131364, acc 0.9375\n",
      "2018-05-23T13:24:44.964617: step 8938, loss 0.151504, acc 0.953125\n",
      "2018-05-23T13:24:45.464268: step 8939, loss 0.164407, acc 0.921875\n",
      "2018-05-23T13:24:45.922044: step 8940, loss 0.132847, acc 0.90625\n",
      "2018-05-23T13:24:46.338929: step 8941, loss 0.156531, acc 0.90625\n",
      "2018-05-23T13:24:46.816650: step 8942, loss 0.115734, acc 0.921875\n",
      "2018-05-23T13:24:47.276422: step 8943, loss 0.0845018, acc 0.96875\n",
      "2018-05-23T13:24:47.757136: step 8944, loss 0.147755, acc 0.921875\n",
      "2018-05-23T13:24:48.413380: step 8945, loss 0.153046, acc 0.9375\n",
      "2018-05-23T13:24:48.819293: step 8946, loss 0.148452, acc 0.9375\n",
      "2018-05-23T13:24:49.259149: step 8947, loss 0.13426, acc 0.9375\n",
      "2018-05-23T13:24:49.628043: step 8948, loss 0.0725491, acc 1\n",
      "2018-05-23T13:24:49.983093: step 8949, loss 0.218187, acc 0.890625\n",
      "2018-05-23T13:24:50.366070: step 8950, loss 0.224472, acc 0.890625\n",
      "2018-05-23T13:24:50.717131: step 8951, loss 0.190352, acc 0.890625\n",
      "2018-05-23T13:24:51.265663: step 8952, loss 0.175172, acc 0.921875\n",
      "2018-05-23T13:24:51.942851: step 8953, loss 0.203521, acc 0.90625\n",
      "2018-05-23T13:24:52.429551: step 8954, loss 0.34055, acc 0.78125\n",
      "2018-05-23T13:24:52.843443: step 8955, loss 0.213466, acc 0.90625\n",
      "2018-05-23T13:24:53.245366: step 8956, loss 0.151248, acc 0.953125\n",
      "2018-05-23T13:24:53.603408: step 8957, loss 0.271448, acc 0.921875\n",
      "2018-05-23T13:24:53.991371: step 8958, loss 0.160569, acc 0.9375\n",
      "2018-05-23T13:24:54.370356: step 8959, loss 0.197539, acc 0.90625\n",
      "2018-05-23T13:24:54.731426: step 8960, loss 0.0967267, acc 0.953125\n",
      "2018-05-23T13:24:55.093422: step 8961, loss 0.164831, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:24:55.455453: step 8962, loss 0.218201, acc 0.90625\n",
      "2018-05-23T13:24:55.800531: step 8963, loss 0.123149, acc 0.953125\n",
      "2018-05-23T13:24:56.441815: step 8964, loss 0.163588, acc 0.90625\n",
      "2018-05-23T13:24:57.138949: step 8965, loss 0.196677, acc 0.859375\n",
      "2018-05-23T13:24:57.566807: step 8966, loss 0.196657, acc 0.90625\n",
      "2018-05-23T13:24:58.027573: step 8967, loss 0.179523, acc 0.921875\n",
      "2018-05-23T13:24:58.466399: step 8968, loss 0.185887, acc 0.921875\n",
      "2018-05-23T13:24:58.927166: step 8969, loss 0.180613, acc 0.90625\n",
      "2018-05-23T13:24:59.490660: step 8970, loss 0.17443, acc 0.921875\n",
      "2018-05-23T13:24:59.970375: step 8971, loss 0.158056, acc 0.921875\n",
      "2018-05-23T13:25:00.432140: step 8972, loss 0.146168, acc 0.953125\n",
      "2018-05-23T13:25:00.847031: step 8973, loss 0.138784, acc 0.9375\n",
      "2018-05-23T13:25:01.303808: step 8974, loss 0.173747, acc 0.921875\n",
      "2018-05-23T13:25:01.760589: step 8975, loss 0.125188, acc 0.96875\n",
      "2018-05-23T13:25:02.269226: step 8976, loss 0.223446, acc 0.890625\n",
      "2018-05-23T13:25:02.756922: step 8977, loss 0.182794, acc 0.921875\n",
      "2018-05-23T13:25:03.280521: step 8978, loss 0.218399, acc 0.875\n",
      "2018-05-23T13:25:03.727324: step 8979, loss 0.155046, acc 0.9375\n",
      "2018-05-23T13:25:04.215021: step 8980, loss 0.121839, acc 0.9375\n",
      "2018-05-23T13:25:04.644870: step 8981, loss 0.15061, acc 0.9375\n",
      "2018-05-23T13:25:05.087685: step 8982, loss 0.204955, acc 0.90625\n",
      "2018-05-23T13:25:05.490612: step 8983, loss 0.103383, acc 0.96875\n",
      "2018-05-23T13:25:05.905498: step 8984, loss 0.0742095, acc 1\n",
      "2018-05-23T13:25:06.403167: step 8985, loss 0.137905, acc 0.9375\n",
      "2018-05-23T13:25:06.842992: step 8986, loss 0.126975, acc 0.921875\n",
      "2018-05-23T13:25:07.238932: step 8987, loss 0.234406, acc 0.90625\n",
      "2018-05-23T13:25:07.656812: step 8988, loss 0.237417, acc 0.90625\n",
      "2018-05-23T13:25:08.103617: step 8989, loss 0.17474, acc 0.953125\n",
      "2018-05-23T13:25:08.951350: step 8990, loss 0.228425, acc 0.9375\n",
      "2018-05-23T13:25:09.493899: step 8991, loss 0.196048, acc 0.921875\n",
      "2018-05-23T13:25:10.135183: step 8992, loss 0.1701, acc 0.953125\n",
      "2018-05-23T13:25:10.860243: step 8993, loss 0.153554, acc 0.9375\n",
      "2018-05-23T13:25:11.767819: step 8994, loss 0.139261, acc 0.9375\n",
      "2018-05-23T13:25:12.685361: step 8995, loss 0.117629, acc 0.953125\n",
      "2018-05-23T13:25:13.664741: step 8996, loss 0.220517, acc 0.90625\n",
      "2018-05-23T13:25:14.515464: step 8997, loss 0.118712, acc 0.953125\n",
      "2018-05-23T13:25:15.007149: step 8998, loss 0.140976, acc 0.9375\n",
      "2018-05-23T13:25:15.437998: step 8999, loss 0.150903, acc 0.953125\n",
      "2018-05-23T13:25:15.847900: step 9000, loss 0.19271, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:25:21.557627: step 9000, loss 0.912936, acc 0.721246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9000\n",
      "\n",
      "2018-05-23T13:25:23.679948: step 9001, loss 0.111546, acc 0.9375\n",
      "2018-05-23T13:25:24.137724: step 9002, loss 0.301667, acc 0.875\n",
      "2018-05-23T13:25:24.593504: step 9003, loss 0.100091, acc 0.953125\n",
      "2018-05-23T13:25:25.013380: step 9004, loss 0.160561, acc 0.953125\n",
      "2018-05-23T13:25:25.401343: step 9005, loss 0.238164, acc 0.890625\n",
      "2018-05-23T13:25:25.820223: step 9006, loss 0.116254, acc 0.953125\n",
      "2018-05-23T13:25:26.347812: step 9007, loss 0.127378, acc 0.9375\n",
      "2018-05-23T13:25:26.851463: step 9008, loss 0.169058, acc 0.9375\n",
      "2018-05-23T13:25:27.396006: step 9009, loss 0.330877, acc 0.90625\n",
      "2018-05-23T13:25:27.892678: step 9010, loss 0.104636, acc 0.96875\n",
      "2018-05-23T13:25:28.316544: step 9011, loss 0.149606, acc 0.90625\n",
      "2018-05-23T13:25:28.728442: step 9012, loss 0.116473, acc 0.96875\n",
      "2018-05-23T13:25:29.135353: step 9013, loss 0.258059, acc 0.90625\n",
      "2018-05-23T13:25:29.537278: step 9014, loss 0.14062, acc 0.953125\n",
      "2018-05-23T13:25:29.940202: step 9015, loss 0.16204, acc 0.9375\n",
      "2018-05-23T13:25:30.356088: step 9016, loss 0.123547, acc 0.953125\n",
      "2018-05-23T13:25:30.776962: step 9017, loss 0.148001, acc 0.921875\n",
      "2018-05-23T13:25:31.181878: step 9018, loss 0.237334, acc 0.921875\n",
      "2018-05-23T13:25:31.583805: step 9019, loss 0.186938, acc 0.890625\n",
      "2018-05-23T13:25:31.987723: step 9020, loss 0.261492, acc 0.859375\n",
      "2018-05-23T13:25:32.393638: step 9021, loss 0.181321, acc 0.9375\n",
      "2018-05-23T13:25:32.840445: step 9022, loss 0.14342, acc 0.9375\n",
      "2018-05-23T13:25:33.305199: step 9023, loss 0.219287, acc 0.890625\n",
      "2018-05-23T13:25:33.697150: step 9024, loss 0.207776, acc 0.921875\n",
      "2018-05-23T13:25:34.143955: step 9025, loss 0.117134, acc 0.953125\n",
      "2018-05-23T13:25:34.569815: step 9026, loss 0.20438, acc 0.921875\n",
      "2018-05-23T13:25:34.990689: step 9027, loss 0.21261, acc 0.90625\n",
      "2018-05-23T13:25:35.377653: step 9028, loss 0.270846, acc 0.921875\n",
      "2018-05-23T13:25:35.813488: step 9029, loss 0.166491, acc 0.953125\n",
      "2018-05-23T13:25:36.223391: step 9030, loss 0.220001, acc 0.953125\n",
      "2018-05-23T13:25:36.649253: step 9031, loss 0.228124, acc 0.875\n",
      "2018-05-23T13:25:37.062147: step 9032, loss 0.0919439, acc 0.984375\n",
      "2018-05-23T13:25:37.459085: step 9033, loss 0.14155, acc 0.9375\n",
      "2018-05-23T13:25:37.877965: step 9034, loss 0.250619, acc 0.9375\n",
      "2018-05-23T13:25:38.275901: step 9035, loss 0.126362, acc 0.953125\n",
      "2018-05-23T13:25:38.688796: step 9036, loss 0.131319, acc 0.953125\n",
      "2018-05-23T13:25:39.187463: step 9037, loss 0.164362, acc 0.9375\n",
      "2018-05-23T13:25:39.639253: step 9038, loss 0.133236, acc 0.9375\n",
      "2018-05-23T13:25:40.046165: step 9039, loss 0.169154, acc 0.921875\n",
      "2018-05-23T13:25:40.465044: step 9040, loss 0.098929, acc 0.9375\n",
      "2018-05-23T13:25:40.980665: step 9041, loss 0.0681102, acc 0.96875\n",
      "2018-05-23T13:25:41.423481: step 9042, loss 0.179946, acc 0.921875\n",
      "2018-05-23T13:25:41.972013: step 9043, loss 0.129417, acc 0.9375\n",
      "2018-05-23T13:25:42.501598: step 9044, loss 0.177515, acc 0.953125\n",
      "2018-05-23T13:25:43.108972: step 9045, loss 0.194824, acc 0.90625\n",
      "2018-05-23T13:25:43.672463: step 9046, loss 0.24363, acc 0.890625\n",
      "2018-05-23T13:25:44.515210: step 9047, loss 0.148085, acc 0.921875\n",
      "2018-05-23T13:25:45.152506: step 9048, loss 0.163508, acc 0.90625\n",
      "2018-05-23T13:25:45.722979: step 9049, loss 0.192146, acc 0.921875\n",
      "2018-05-23T13:25:46.230621: step 9050, loss 0.128525, acc 0.953125\n",
      "2018-05-23T13:25:46.733276: step 9051, loss 0.146174, acc 0.9375\n",
      "2018-05-23T13:25:47.284800: step 9052, loss 0.198183, acc 0.921875\n",
      "2018-05-23T13:25:47.770502: step 9053, loss 0.260678, acc 0.859375\n",
      "2018-05-23T13:25:48.262186: step 9054, loss 0.199232, acc 0.921875\n",
      "2018-05-23T13:25:48.744895: step 9055, loss 0.237606, acc 0.9375\n",
      "2018-05-23T13:25:49.271485: step 9056, loss 0.221946, acc 0.859375\n",
      "2018-05-23T13:25:49.794087: step 9057, loss 0.140171, acc 0.9375\n",
      "2018-05-23T13:25:50.310705: step 9058, loss 0.0910844, acc 0.96875\n",
      "2018-05-23T13:25:50.796407: step 9059, loss 0.12662, acc 0.953125\n",
      "2018-05-23T13:25:51.310032: step 9060, loss 0.272811, acc 0.875\n",
      "2018-05-23T13:25:51.830641: step 9061, loss 0.152976, acc 0.921875\n",
      "2018-05-23T13:25:52.339281: step 9062, loss 0.1582, acc 0.9375\n",
      "2018-05-23T13:25:52.832959: step 9063, loss 0.203174, acc 0.921875\n",
      "2018-05-23T13:25:53.314669: step 9064, loss 0.133984, acc 0.9375\n",
      "2018-05-23T13:25:53.865198: step 9065, loss 0.166764, acc 0.921875\n",
      "2018-05-23T13:25:54.375831: step 9066, loss 0.200848, acc 0.921875\n",
      "2018-05-23T13:25:54.862528: step 9067, loss 0.0879001, acc 0.96875\n",
      "2018-05-23T13:25:55.392111: step 9068, loss 0.116402, acc 0.984375\n",
      "2018-05-23T13:25:55.849887: step 9069, loss 0.109303, acc 0.953125\n",
      "2018-05-23T13:25:56.341572: step 9070, loss 0.202574, acc 0.90625\n",
      "2018-05-23T13:25:56.811314: step 9071, loss 0.163787, acc 0.90625\n",
      "2018-05-23T13:25:57.321951: step 9072, loss 0.209339, acc 0.875\n",
      "2018-05-23T13:25:57.824604: step 9073, loss 0.128405, acc 0.953125\n",
      "2018-05-23T13:25:58.309308: step 9074, loss 0.213635, acc 0.953125\n",
      "2018-05-23T13:25:58.752122: step 9075, loss 0.133127, acc 0.9375\n",
      "2018-05-23T13:25:59.206908: step 9076, loss 0.235057, acc 0.90625\n",
      "2018-05-23T13:25:59.651716: step 9077, loss 0.169529, acc 0.890625\n",
      "2018-05-23T13:26:00.119466: step 9078, loss 0.12063, acc 0.96875\n",
      "2018-05-23T13:26:00.577240: step 9079, loss 0.167103, acc 0.921875\n",
      "2018-05-23T13:26:01.028035: step 9080, loss 0.163418, acc 0.9375\n",
      "2018-05-23T13:26:01.514733: step 9081, loss 0.202973, acc 0.90625\n",
      "2018-05-23T13:26:01.998439: step 9082, loss 0.193183, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:26:02.493115: step 9083, loss 0.174902, acc 0.953125\n",
      "2018-05-23T13:26:02.975824: step 9084, loss 0.15907, acc 0.921875\n",
      "2018-05-23T13:26:03.478478: step 9085, loss 0.187713, acc 0.9375\n",
      "2018-05-23T13:26:04.027012: step 9086, loss 0.113129, acc 0.9375\n",
      "2018-05-23T13:26:04.537646: step 9087, loss 0.159693, acc 0.9375\n",
      "2018-05-23T13:26:05.069224: step 9088, loss 0.177054, acc 0.921875\n",
      "2018-05-23T13:26:05.533982: step 9089, loss 0.192329, acc 0.921875\n",
      "2018-05-23T13:26:06.063563: step 9090, loss 0.132892, acc 0.9375\n",
      "2018-05-23T13:26:06.653984: step 9091, loss 0.109536, acc 0.953125\n",
      "2018-05-23T13:26:07.169604: step 9092, loss 0.160504, acc 0.9375\n",
      "2018-05-23T13:26:07.695200: step 9093, loss 0.112126, acc 0.9375\n",
      "2018-05-23T13:26:08.171924: step 9094, loss 0.169137, acc 0.96875\n",
      "2018-05-23T13:26:08.660617: step 9095, loss 0.139367, acc 0.953125\n",
      "2018-05-23T13:26:09.174241: step 9096, loss 0.244334, acc 0.890625\n",
      "2018-05-23T13:26:09.650967: step 9097, loss 0.118693, acc 0.953125\n",
      "2018-05-23T13:26:10.134673: step 9098, loss 0.319422, acc 0.890625\n",
      "2018-05-23T13:26:10.594443: step 9099, loss 0.106214, acc 0.9375\n",
      "2018-05-23T13:26:11.066181: step 9100, loss 0.259762, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:26:16.411880: step 9100, loss 0.930171, acc 0.724532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9100\n",
      "\n",
      "2018-05-23T13:26:20.034190: step 9101, loss 0.173069, acc 0.9375\n",
      "2018-05-23T13:26:21.178130: step 9102, loss 0.186267, acc 0.9375\n",
      "2018-05-23T13:26:22.140555: step 9103, loss 0.133057, acc 0.96875\n",
      "2018-05-23T13:26:23.128911: step 9104, loss 0.204833, acc 0.890625\n",
      "2018-05-23T13:26:24.300777: step 9105, loss 0.307731, acc 0.828125\n",
      "2018-05-23T13:26:25.394363: step 9106, loss 0.145349, acc 0.921875\n",
      "2018-05-23T13:26:26.272014: step 9107, loss 0.188591, acc 0.921875\n",
      "2018-05-23T13:26:26.990093: step 9108, loss 0.0989037, acc 1\n",
      "2018-05-23T13:26:27.675260: step 9109, loss 0.217726, acc 0.890625\n",
      "2018-05-23T13:26:28.508032: step 9110, loss 0.180934, acc 0.90625\n",
      "2018-05-23T13:26:29.336820: step 9111, loss 0.152561, acc 0.9375\n",
      "2018-05-23T13:26:30.060878: step 9112, loss 0.129557, acc 0.96875\n",
      "2018-05-23T13:26:30.708146: step 9113, loss 0.122484, acc 0.953125\n",
      "2018-05-23T13:26:31.481079: step 9114, loss 0.111157, acc 0.9375\n",
      "2018-05-23T13:26:32.146299: step 9115, loss 0.120382, acc 0.953125\n",
      "2018-05-23T13:26:33.172554: step 9116, loss 0.117498, acc 0.96875\n",
      "2018-05-23T13:26:34.220750: step 9117, loss 0.137477, acc 0.9375\n",
      "2018-05-23T13:26:34.794215: step 9118, loss 0.245425, acc 0.921875\n",
      "2018-05-23T13:26:35.415553: step 9119, loss 0.15785, acc 0.953125\n",
      "2018-05-23T13:26:35.956107: step 9120, loss 0.0904098, acc 0.96875\n",
      "2018-05-23T13:26:36.617338: step 9121, loss 0.16341, acc 0.9375\n",
      "2018-05-23T13:26:37.254633: step 9122, loss 0.148251, acc 0.9375\n",
      "2018-05-23T13:26:37.779231: step 9123, loss 0.185903, acc 0.90625\n",
      "2018-05-23T13:26:38.276899: step 9124, loss 0.147877, acc 0.96875\n",
      "2018-05-23T13:26:38.919180: step 9125, loss 0.160247, acc 0.921875\n",
      "2018-05-23T13:26:39.759931: step 9126, loss 0.202292, acc 0.921875\n",
      "2018-05-23T13:26:40.393236: step 9127, loss 0.149571, acc 0.9375\n",
      "2018-05-23T13:26:40.910852: step 9128, loss 0.302353, acc 0.890625\n",
      "2018-05-23T13:26:41.497284: step 9129, loss 0.131465, acc 0.9375\n",
      "2018-05-23T13:26:42.086708: step 9130, loss 0.29143, acc 0.875\n",
      "2018-05-23T13:26:42.897538: step 9131, loss 0.312081, acc 0.875\n",
      "2018-05-23T13:26:43.623595: step 9132, loss 0.187695, acc 0.921875\n",
      "2018-05-23T13:26:44.229973: step 9133, loss 0.0888366, acc 0.96875\n",
      "2018-05-23T13:26:44.757562: step 9134, loss 0.16601, acc 0.921875\n",
      "2018-05-23T13:26:45.268195: step 9135, loss 0.249498, acc 0.921875\n",
      "2018-05-23T13:26:45.959346: step 9136, loss 0.137517, acc 0.9375\n",
      "2018-05-23T13:26:46.741255: step 9137, loss 0.172795, acc 0.90625\n",
      "2018-05-23T13:26:47.245904: step 9138, loss 0.142511, acc 0.921875\n",
      "2018-05-23T13:26:47.744571: step 9139, loss 0.199641, acc 0.859375\n",
      "2018-05-23T13:26:48.266175: step 9140, loss 0.0893531, acc 0.984375\n",
      "2018-05-23T13:26:48.775813: step 9141, loss 0.165193, acc 0.921875\n",
      "2018-05-23T13:26:49.261513: step 9142, loss 0.225481, acc 0.90625\n",
      "2018-05-23T13:26:49.918754: step 9143, loss 0.14169, acc 0.953125\n",
      "2018-05-23T13:26:50.523137: step 9144, loss 0.118896, acc 0.9375\n",
      "2018-05-23T13:26:51.083638: step 9145, loss 0.173311, acc 0.921875\n",
      "2018-05-23T13:26:51.637157: step 9146, loss 0.233117, acc 0.9375\n",
      "2018-05-23T13:26:52.243535: step 9147, loss 0.18813, acc 0.921875\n",
      "2018-05-23T13:26:52.935700: step 9148, loss 0.136351, acc 0.953125\n",
      "2018-05-23T13:26:53.518125: step 9149, loss 0.130553, acc 0.921875\n",
      "2018-05-23T13:26:54.051697: step 9150, loss 0.273269, acc 0.90625\n",
      "2018-05-23T13:26:54.546375: step 9151, loss 0.0726784, acc 0.953125\n",
      "2018-05-23T13:26:55.054017: step 9152, loss 0.116819, acc 0.96875\n",
      "2018-05-23T13:26:55.568639: step 9153, loss 0.148975, acc 0.9375\n",
      "2018-05-23T13:26:56.073289: step 9154, loss 0.0997488, acc 0.96875\n",
      "2018-05-23T13:26:56.640771: step 9155, loss 0.120085, acc 0.9375\n",
      "2018-05-23T13:26:57.203267: step 9156, loss 0.123899, acc 0.96875\n",
      "2018-05-23T13:26:57.774738: step 9157, loss 0.200312, acc 0.875\n",
      "2018-05-23T13:26:58.392087: step 9158, loss 0.107702, acc 0.96875\n",
      "2018-05-23T13:26:59.321601: step 9159, loss 0.0986692, acc 0.953125\n",
      "2018-05-23T13:27:00.139968: step 9160, loss 0.122168, acc 0.90625\n",
      "2018-05-23T13:27:00.847077: step 9161, loss 0.129683, acc 0.90625\n",
      "2018-05-23T13:27:01.558174: step 9162, loss 0.155585, acc 0.9375\n",
      "2018-05-23T13:27:02.325122: step 9163, loss 0.217566, acc 0.921875\n",
      "2018-05-23T13:27:03.209756: step 9164, loss 0.168248, acc 0.921875\n",
      "2018-05-23T13:27:03.848048: step 9165, loss 0.176596, acc 0.90625\n",
      "2018-05-23T13:27:04.543189: step 9166, loss 0.126361, acc 0.921875\n",
      "2018-05-23T13:27:05.121640: step 9167, loss 0.119745, acc 0.921875\n",
      "2018-05-23T13:27:05.694110: step 9168, loss 0.0939789, acc 0.953125\n",
      "2018-05-23T13:27:06.203745: step 9169, loss 0.212367, acc 0.875\n",
      "2018-05-23T13:27:06.740310: step 9170, loss 0.206951, acc 0.890625\n",
      "2018-05-23T13:27:07.264906: step 9171, loss 0.0905696, acc 0.984375\n",
      "2018-05-23T13:27:07.804463: step 9172, loss 0.225455, acc 0.90625\n",
      "2018-05-23T13:27:08.322080: step 9173, loss 0.144278, acc 0.953125\n",
      "2018-05-23T13:27:08.857645: step 9174, loss 0.167415, acc 0.921875\n",
      "2018-05-23T13:27:09.386232: step 9175, loss 0.209718, acc 0.921875\n",
      "2018-05-23T13:27:09.907837: step 9176, loss 0.154535, acc 0.90625\n",
      "2018-05-23T13:27:10.442406: step 9177, loss 0.143604, acc 0.921875\n",
      "2018-05-23T13:27:10.980965: step 9178, loss 0.148252, acc 0.9375\n",
      "2018-05-23T13:27:11.538474: step 9179, loss 0.171978, acc 0.90625\n",
      "2018-05-23T13:27:12.067060: step 9180, loss 0.125246, acc 0.96875\n",
      "2018-05-23T13:27:12.600633: step 9181, loss 0.153663, acc 0.9375\n",
      "2018-05-23T13:27:13.232941: step 9182, loss 0.19691, acc 0.921875\n",
      "2018-05-23T13:27:13.787458: step 9183, loss 0.135301, acc 0.953125\n",
      "2018-05-23T13:27:14.355937: step 9184, loss 0.100185, acc 0.96875\n",
      "2018-05-23T13:27:14.875547: step 9185, loss 0.290061, acc 0.90625\n",
      "2018-05-23T13:27:15.397152: step 9186, loss 0.12447, acc 0.96875\n",
      "2018-05-23T13:27:15.916761: step 9187, loss 0.154427, acc 0.90625\n",
      "2018-05-23T13:27:16.440361: step 9188, loss 0.184122, acc 0.9375\n",
      "2018-05-23T13:27:16.958973: step 9189, loss 0.22876, acc 0.921875\n",
      "2018-05-23T13:27:17.596268: step 9190, loss 0.123957, acc 0.921875\n",
      "2018-05-23T13:27:18.121862: step 9191, loss 0.161419, acc 0.921875\n",
      "2018-05-23T13:27:18.652443: step 9192, loss 0.26306, acc 0.859375\n",
      "2018-05-23T13:27:19.176043: step 9193, loss 0.139868, acc 0.953125\n",
      "2018-05-23T13:27:19.675706: step 9194, loss 0.219991, acc 0.953125\n",
      "2018-05-23T13:27:20.192323: step 9195, loss 0.230679, acc 0.890625\n",
      "2018-05-23T13:27:20.741853: step 9196, loss 0.129776, acc 0.921875\n",
      "2018-05-23T13:27:21.253485: step 9197, loss 0.282913, acc 0.890625\n",
      "2018-05-23T13:27:21.762125: step 9198, loss 0.0811886, acc 0.984375\n",
      "2018-05-23T13:27:22.283729: step 9199, loss 0.240026, acc 0.875\n",
      "2018-05-23T13:27:22.833259: step 9200, loss 0.216832, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:27:28.727490: step 9200, loss 0.954319, acc 0.724246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:27:30.762557: step 9201, loss 0.115974, acc 0.96875\n",
      "2018-05-23T13:27:31.284162: step 9202, loss 0.154463, acc 0.9375\n",
      "2018-05-23T13:27:31.831696: step 9203, loss 0.197564, acc 0.9375\n",
      "2018-05-23T13:27:32.430095: step 9204, loss 0.119381, acc 0.921875\n",
      "2018-05-23T13:27:33.015530: step 9205, loss 0.22527, acc 0.875\n",
      "2018-05-23T13:27:33.569049: step 9206, loss 0.207697, acc 0.875\n",
      "2018-05-23T13:27:34.104617: step 9207, loss 0.202504, acc 0.921875\n",
      "2018-05-23T13:27:34.691047: step 9208, loss 0.218504, acc 0.953125\n",
      "2018-05-23T13:27:35.223622: step 9209, loss 0.258728, acc 0.90625\n",
      "2018-05-23T13:27:35.824018: step 9210, loss 0.145054, acc 0.921875\n",
      "2018-05-23T13:27:36.454330: step 9211, loss 0.242378, acc 0.90625\n",
      "2018-05-23T13:27:37.220280: step 9212, loss 0.213186, acc 0.921875\n",
      "2018-05-23T13:27:37.910434: step 9213, loss 0.280983, acc 0.859375\n",
      "2018-05-23T13:27:38.505843: step 9214, loss 0.170749, acc 0.921875\n",
      "2018-05-23T13:27:39.159095: step 9215, loss 0.228194, acc 0.84375\n",
      "2018-05-23T13:27:39.772454: step 9216, loss 0.174245, acc 0.90625\n",
      "2018-05-23T13:27:40.327967: step 9217, loss 0.131735, acc 0.953125\n",
      "2018-05-23T13:27:40.840595: step 9218, loss 0.107079, acc 0.96875\n",
      "2018-05-23T13:27:41.325299: step 9219, loss 0.135969, acc 0.953125\n",
      "2018-05-23T13:27:41.839924: step 9220, loss 0.286282, acc 0.890625\n",
      "2018-05-23T13:27:42.571964: step 9221, loss 0.193868, acc 0.90625\n",
      "2018-05-23T13:27:43.091574: step 9222, loss 0.106595, acc 0.953125\n",
      "2018-05-23T13:27:43.635119: step 9223, loss 0.286882, acc 0.875\n",
      "2018-05-23T13:27:44.149744: step 9224, loss 0.118155, acc 0.953125\n",
      "2018-05-23T13:27:44.644420: step 9225, loss 0.103937, acc 0.96875\n",
      "2018-05-23T13:27:45.348537: step 9226, loss 0.33765, acc 0.8125\n",
      "2018-05-23T13:27:45.982839: step 9227, loss 0.212038, acc 0.890625\n",
      "2018-05-23T13:27:46.753779: step 9228, loss 0.194225, acc 0.90625\n",
      "2018-05-23T13:27:47.742164: step 9229, loss 0.240202, acc 0.90625\n",
      "2018-05-23T13:27:48.827854: step 9230, loss 0.282893, acc 0.875\n",
      "2018-05-23T13:27:49.507037: step 9231, loss 0.144242, acc 0.953125\n",
      "2018-05-23T13:27:50.145329: step 9232, loss 0.248158, acc 0.90625\n",
      "2018-05-23T13:27:50.879366: step 9233, loss 0.249945, acc 0.9375\n",
      "2018-05-23T13:27:51.814864: step 9234, loss 0.168412, acc 0.9375\n",
      "2018-05-23T13:27:52.532942: step 9235, loss 0.189355, acc 0.90625\n",
      "2018-05-23T13:27:53.276951: step 9236, loss 0.241813, acc 0.890625\n",
      "2018-05-23T13:27:54.007996: step 9237, loss 0.113646, acc 0.953125\n",
      "2018-05-23T13:27:54.501675: step 9238, loss 0.235963, acc 0.9375\n",
      "2018-05-23T13:27:54.990368: step 9239, loss 0.225483, acc 0.90625\n",
      "2018-05-23T13:27:55.491031: step 9240, loss 0.0872796, acc 0.953125\n",
      "2018-05-23T13:27:56.002660: step 9241, loss 0.163328, acc 0.90625\n",
      "2018-05-23T13:27:56.575128: step 9242, loss 0.130788, acc 0.953125\n",
      "2018-05-23T13:27:57.088754: step 9243, loss 0.0844252, acc 0.984375\n",
      "2018-05-23T13:27:57.589415: step 9244, loss 0.117105, acc 0.953125\n",
      "2018-05-23T13:27:58.076112: step 9245, loss 0.128888, acc 0.953125\n",
      "2018-05-23T13:27:58.908886: step 9246, loss 0.202052, acc 0.859375\n",
      "2018-05-23T13:27:59.690796: step 9247, loss 0.099078, acc 0.984375\n",
      "2018-05-23T13:28:00.433807: step 9248, loss 0.192627, acc 0.9375\n",
      "2018-05-23T13:28:01.120967: step 9249, loss 0.226015, acc 0.875\n",
      "2018-05-23T13:28:01.740310: step 9250, loss 0.221823, acc 0.921875\n",
      "2018-05-23T13:28:02.495291: step 9251, loss 0.169309, acc 0.9375\n",
      "2018-05-23T13:28:03.211375: step 9252, loss 0.128006, acc 0.921875\n",
      "2018-05-23T13:28:04.380250: step 9253, loss 0.158426, acc 0.9375\n",
      "2018-05-23T13:28:05.362620: step 9254, loss 0.152525, acc 0.953125\n",
      "2018-05-23T13:28:06.216336: step 9255, loss 0.181082, acc 0.9375\n",
      "2018-05-23T13:28:07.039135: step 9256, loss 0.119224, acc 0.96875\n",
      "2018-05-23T13:28:07.777162: step 9257, loss 0.17558, acc 0.9375\n",
      "2018-05-23T13:28:08.503218: step 9258, loss 0.0938871, acc 0.953125\n",
      "2018-05-23T13:28:09.042775: step 9259, loss 0.200532, acc 0.953125\n",
      "2018-05-23T13:28:09.796759: step 9260, loss 0.107928, acc 0.96875\n",
      "2018-05-23T13:28:10.592629: step 9261, loss 0.113956, acc 0.953125\n",
      "2018-05-23T13:28:11.206986: step 9262, loss 0.179578, acc 0.921875\n",
      "2018-05-23T13:28:11.745544: step 9263, loss 0.164102, acc 0.921875\n",
      "2018-05-23T13:28:12.297070: step 9264, loss 0.156771, acc 0.9375\n",
      "2018-05-23T13:28:12.879510: step 9265, loss 0.200542, acc 0.890625\n",
      "2018-05-23T13:28:13.402113: step 9266, loss 0.214307, acc 0.9375\n",
      "2018-05-23T13:28:13.910752: step 9267, loss 0.202797, acc 0.90625\n",
      "2018-05-23T13:28:14.489205: step 9268, loss 0.121558, acc 0.9375\n",
      "2018-05-23T13:28:15.009812: step 9269, loss 0.239804, acc 0.859375\n",
      "2018-05-23T13:28:15.563331: step 9270, loss 0.170809, acc 0.921875\n",
      "2018-05-23T13:28:16.240520: step 9271, loss 0.270186, acc 0.90625\n",
      "2018-05-23T13:28:16.764119: step 9272, loss 0.238026, acc 0.859375\n",
      "2018-05-23T13:28:17.401414: step 9273, loss 0.244593, acc 0.9375\n",
      "2018-05-23T13:28:18.077606: step 9274, loss 0.210742, acc 0.90625\n",
      "2018-05-23T13:28:18.724874: step 9275, loss 0.175901, acc 0.9375\n",
      "2018-05-23T13:28:20.275726: step 9276, loss 0.237987, acc 0.890625\n",
      "2018-05-23T13:28:20.875122: step 9277, loss 0.176379, acc 0.890625\n",
      "2018-05-23T13:28:21.450582: step 9278, loss 0.135582, acc 0.96875\n",
      "2018-05-23T13:28:22.098848: step 9279, loss 0.124496, acc 0.953125\n",
      "2018-05-23T13:28:22.733150: step 9280, loss 0.164431, acc 0.9375\n",
      "2018-05-23T13:28:23.494115: step 9281, loss 0.206669, acc 0.921875\n",
      "2018-05-23T13:28:24.051623: step 9282, loss 0.209376, acc 0.921875\n",
      "2018-05-23T13:28:24.642044: step 9283, loss 0.184284, acc 0.9375\n",
      "2018-05-23T13:28:25.312252: step 9284, loss 0.108584, acc 0.953125\n",
      "2018-05-23T13:28:25.875743: step 9285, loss 0.247094, acc 0.890625\n",
      "2018-05-23T13:28:26.432255: step 9286, loss 0.0477595, acc 0.984375\n",
      "2018-05-23T13:28:26.993752: step 9287, loss 0.365431, acc 0.90625\n",
      "2018-05-23T13:28:27.540291: step 9288, loss 0.153206, acc 0.9375\n",
      "2018-05-23T13:28:28.106775: step 9289, loss 0.223514, acc 0.875\n",
      "2018-05-23T13:28:28.642342: step 9290, loss 0.0814354, acc 0.96875\n",
      "2018-05-23T13:28:29.185888: step 9291, loss 0.236635, acc 0.90625\n",
      "2018-05-23T13:28:29.748385: step 9292, loss 0.0939393, acc 0.953125\n",
      "2018-05-23T13:28:30.360745: step 9293, loss 0.078803, acc 1\n",
      "2018-05-23T13:28:30.900301: step 9294, loss 0.193176, acc 0.890625\n",
      "2018-05-23T13:28:31.433874: step 9295, loss 0.191992, acc 0.921875\n",
      "2018-05-23T13:28:31.984402: step 9296, loss 0.191599, acc 0.890625\n",
      "2018-05-23T13:28:32.532934: step 9297, loss 0.154046, acc 0.953125\n",
      "2018-05-23T13:28:33.094432: step 9298, loss 0.20092, acc 0.890625\n",
      "2018-05-23T13:28:33.628005: step 9299, loss 0.104419, acc 0.9375\n",
      "2018-05-23T13:28:34.134649: step 9300, loss 0.11855, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:28:40.368973: step 9300, loss 0.955475, acc 0.72096\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9300\n",
      "\n",
      "2018-05-23T13:28:42.390563: step 9301, loss 0.340309, acc 0.84375\n",
      "2018-05-23T13:28:42.816424: step 9302, loss 0.317159, acc 0.875\n",
      "2018-05-23T13:28:43.262231: step 9303, loss 0.313817, acc 0.90625\n",
      "2018-05-23T13:28:43.678119: step 9304, loss 0.15806, acc 0.90625\n",
      "2018-05-23T13:28:44.062091: step 9305, loss 0.134471, acc 0.921875\n",
      "2018-05-23T13:28:44.522858: step 9306, loss 0.0924915, acc 0.96875\n",
      "2018-05-23T13:28:44.903838: step 9307, loss 0.136827, acc 0.96875\n",
      "2018-05-23T13:28:45.300779: step 9308, loss 0.0870389, acc 0.953125\n",
      "2018-05-23T13:28:45.687742: step 9309, loss 0.271292, acc 0.84375\n",
      "2018-05-23T13:28:46.069720: step 9310, loss 0.223517, acc 0.859375\n",
      "2018-05-23T13:28:46.457683: step 9311, loss 0.110891, acc 0.9375\n",
      "2018-05-23T13:28:46.843652: step 9312, loss 0.289764, acc 0.890625\n",
      "2018-05-23T13:28:47.311399: step 9313, loss 0.153351, acc 0.9375\n",
      "2018-05-23T13:28:47.704347: step 9314, loss 0.173354, acc 0.921875\n",
      "2018-05-23T13:28:48.078347: step 9315, loss 0.14215, acc 0.953125\n",
      "2018-05-23T13:28:48.483264: step 9316, loss 0.193397, acc 0.953125\n",
      "2018-05-23T13:28:48.913114: step 9317, loss 0.200324, acc 0.921875\n",
      "2018-05-23T13:28:49.310052: step 9318, loss 0.159214, acc 0.953125\n",
      "2018-05-23T13:28:49.746883: step 9319, loss 0.218006, acc 0.921875\n",
      "2018-05-23T13:28:50.132851: step 9320, loss 0.172434, acc 0.890625\n",
      "2018-05-23T13:28:50.564696: step 9321, loss 0.170186, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:28:51.010503: step 9322, loss 0.178903, acc 0.9375\n",
      "2018-05-23T13:28:51.437361: step 9323, loss 0.248566, acc 0.890625\n",
      "2018-05-23T13:28:51.889153: step 9324, loss 0.225776, acc 0.90625\n",
      "2018-05-23T13:28:52.272127: step 9325, loss 0.098441, acc 0.96875\n",
      "2018-05-23T13:28:52.668068: step 9326, loss 0.216079, acc 0.921875\n",
      "2018-05-23T13:28:53.079966: step 9327, loss 0.383546, acc 0.875\n",
      "2018-05-23T13:28:53.459950: step 9328, loss 0.165623, acc 0.921875\n",
      "2018-05-23T13:28:53.921714: step 9329, loss 0.373321, acc 0.890625\n",
      "2018-05-23T13:28:54.299705: step 9330, loss 0.235511, acc 0.953125\n",
      "2018-05-23T13:28:54.709607: step 9331, loss 0.219881, acc 0.890625\n",
      "2018-05-23T13:28:55.106547: step 9332, loss 0.125086, acc 0.953125\n",
      "2018-05-23T13:28:55.497499: step 9333, loss 0.191991, acc 0.921875\n",
      "2018-05-23T13:28:55.953279: step 9334, loss 0.158481, acc 0.921875\n",
      "2018-05-23T13:28:56.349222: step 9335, loss 0.211598, acc 0.875\n",
      "2018-05-23T13:28:56.817967: step 9336, loss 0.185636, acc 0.921875\n",
      "2018-05-23T13:28:57.229865: step 9337, loss 0.169619, acc 0.953125\n",
      "2018-05-23T13:28:57.603866: step 9338, loss 0.117426, acc 0.96875\n",
      "2018-05-23T13:28:58.058648: step 9339, loss 0.287787, acc 0.875\n",
      "2018-05-23T13:28:58.446609: step 9340, loss 0.136887, acc 0.96875\n",
      "2018-05-23T13:28:58.852524: step 9341, loss 0.175567, acc 0.921875\n",
      "2018-05-23T13:28:59.252454: step 9342, loss 0.129443, acc 0.921875\n",
      "2018-05-23T13:28:59.663354: step 9343, loss 0.234503, acc 0.90625\n",
      "2018-05-23T13:29:00.116143: step 9344, loss 0.205794, acc 0.921875\n",
      "2018-05-23T13:29:00.498124: step 9345, loss 0.282273, acc 0.90625\n",
      "2018-05-23T13:29:00.890074: step 9346, loss 0.242408, acc 0.90625\n",
      "2018-05-23T13:29:01.323914: step 9347, loss 0.158585, acc 0.921875\n",
      "2018-05-23T13:29:01.701902: step 9348, loss 0.283351, acc 0.890625\n",
      "2018-05-23T13:29:02.175634: step 9349, loss 0.19933, acc 0.90625\n",
      "2018-05-23T13:29:02.577559: step 9350, loss 0.260887, acc 0.859375\n",
      "2018-05-23T13:29:02.954553: step 9351, loss 0.122193, acc 0.921875\n",
      "2018-05-23T13:29:03.405344: step 9352, loss 0.146857, acc 0.9375\n",
      "2018-05-23T13:29:03.782335: step 9353, loss 0.181478, acc 0.9375\n",
      "2018-05-23T13:29:04.178278: step 9354, loss 0.193801, acc 0.90625\n",
      "2018-05-23T13:29:04.620095: step 9355, loss 0.160066, acc 0.953125\n",
      "2018-05-23T13:29:05.000078: step 9356, loss 0.138641, acc 0.9375\n",
      "2018-05-23T13:29:05.435913: step 9357, loss 0.263028, acc 0.921875\n",
      "2018-05-23T13:29:05.823875: step 9358, loss 0.265912, acc 0.859375\n",
      "2018-05-23T13:29:06.223805: step 9359, loss 0.179168, acc 0.9375\n",
      "2018-05-23T13:29:06.611766: step 9360, loss 0.236362, acc 0.921875\n",
      "2018-05-23T13:29:07.058572: step 9361, loss 0.184964, acc 0.9375\n",
      "2018-05-23T13:29:07.463487: step 9362, loss 0.161135, acc 0.953125\n",
      "2018-05-23T13:29:07.848459: step 9363, loss 0.35786, acc 0.875\n",
      "2018-05-23T13:29:08.265342: step 9364, loss 0.161259, acc 0.9375\n",
      "2018-05-23T13:29:08.793929: step 9365, loss 0.281741, acc 0.859375\n",
      "2018-05-23T13:29:09.219789: step 9366, loss 0.14704, acc 0.953125\n",
      "2018-05-23T13:29:09.618723: step 9367, loss 0.253898, acc 0.859375\n",
      "2018-05-23T13:29:09.998705: step 9368, loss 0.200747, acc 0.9375\n",
      "2018-05-23T13:29:10.388664: step 9369, loss 0.229601, acc 0.90625\n",
      "2018-05-23T13:29:10.764656: step 9370, loss 0.1268, acc 0.9375\n",
      "2018-05-23T13:29:11.151621: step 9371, loss 0.15198, acc 0.96875\n",
      "2018-05-23T13:29:11.545570: step 9372, loss 0.0867932, acc 0.96875\n",
      "2018-05-23T13:29:11.929540: step 9373, loss 0.177054, acc 0.953125\n",
      "2018-05-23T13:29:12.331464: step 9374, loss 0.182501, acc 0.90625\n",
      "2018-05-23T13:29:12.712446: step 9375, loss 0.225457, acc 0.90625\n",
      "2018-05-23T13:29:13.195154: step 9376, loss 0.133644, acc 0.9375\n",
      "2018-05-23T13:29:13.607053: step 9377, loss 0.198638, acc 0.890625\n",
      "2018-05-23T13:29:13.987036: step 9378, loss 0.286572, acc 0.890625\n",
      "2018-05-23T13:29:14.386966: step 9379, loss 0.359526, acc 0.890625\n",
      "2018-05-23T13:29:14.776923: step 9380, loss 0.26371, acc 0.8125\n",
      "2018-05-23T13:29:15.173861: step 9381, loss 0.219131, acc 0.921875\n",
      "2018-05-23T13:29:15.579777: step 9382, loss 0.151562, acc 0.9375\n",
      "2018-05-23T13:29:15.962750: step 9383, loss 0.19431, acc 0.921875\n",
      "2018-05-23T13:29:16.344728: step 9384, loss 0.156373, acc 0.90625\n",
      "2018-05-23T13:29:16.739672: step 9385, loss 0.193654, acc 0.9375\n",
      "2018-05-23T13:29:17.264269: step 9386, loss 0.269638, acc 0.890625\n",
      "2018-05-23T13:29:17.775902: step 9387, loss 0.228751, acc 0.890625\n",
      "2018-05-23T13:29:18.254620: step 9388, loss 0.134519, acc 0.953125\n",
      "2018-05-23T13:29:18.750293: step 9389, loss 0.251722, acc 0.890625\n",
      "2018-05-23T13:29:19.142245: step 9390, loss 0.165705, acc 0.9375\n",
      "2018-05-23T13:29:19.492308: step 9391, loss 0.27478, acc 0.890625\n",
      "2018-05-23T13:29:19.860326: step 9392, loss 0.186778, acc 0.90625\n",
      "2018-05-23T13:29:20.284190: step 9393, loss 0.26551, acc 0.84375\n",
      "2018-05-23T13:29:20.619293: step 9394, loss 0.332949, acc 0.859375\n",
      "2018-05-23T13:29:20.960380: step 9395, loss 0.242601, acc 0.90625\n",
      "2018-05-23T13:29:21.296482: step 9396, loss 0.308746, acc 0.859375\n",
      "2018-05-23T13:29:21.627598: step 9397, loss 0.193277, acc 0.890625\n",
      "2018-05-23T13:29:21.977659: step 9398, loss 0.174626, acc 0.953125\n",
      "2018-05-23T13:29:22.315758: step 9399, loss 0.14796, acc 0.953125\n",
      "2018-05-23T13:29:22.646871: step 9400, loss 0.118917, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:29:27.503876: step 9400, loss 0.931716, acc 0.720103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9400\n",
      "\n",
      "2018-05-23T13:29:29.066695: step 9401, loss 0.180408, acc 0.9375\n",
      "2018-05-23T13:29:29.440696: step 9402, loss 0.141301, acc 0.96875\n",
      "2018-05-23T13:29:29.789761: step 9403, loss 0.187139, acc 0.921875\n",
      "2018-05-23T13:29:30.140821: step 9404, loss 0.151838, acc 0.953125\n",
      "2018-05-23T13:29:30.507839: step 9405, loss 0.169983, acc 0.890625\n",
      "2018-05-23T13:29:30.878849: step 9406, loss 0.20096, acc 0.921875\n",
      "2018-05-23T13:29:31.216944: step 9407, loss 0.239454, acc 0.9375\n",
      "2018-05-23T13:29:31.559028: step 9408, loss 0.212471, acc 0.875\n",
      "2018-05-23T13:29:31.895130: step 9409, loss 0.184109, acc 0.9375\n",
      "2018-05-23T13:29:32.249183: step 9410, loss 0.266684, acc 0.875\n",
      "2018-05-23T13:29:32.604232: step 9411, loss 0.0977831, acc 0.953125\n",
      "2018-05-23T13:29:32.962273: step 9412, loss 0.0917223, acc 0.96875\n",
      "2018-05-23T13:29:33.307350: step 9413, loss 0.170062, acc 0.921875\n",
      "2018-05-23T13:29:33.647443: step 9414, loss 0.140517, acc 0.9375\n",
      "2018-05-23T13:29:33.985538: step 9415, loss 0.222555, acc 0.875\n",
      "2018-05-23T13:29:34.328619: step 9416, loss 0.201905, acc 0.890625\n",
      "2018-05-23T13:29:34.726554: step 9417, loss 0.194047, acc 0.953125\n",
      "2018-05-23T13:29:35.136459: step 9418, loss 0.157847, acc 0.890625\n",
      "2018-05-23T13:29:35.487519: step 9419, loss 0.099857, acc 0.96875\n",
      "2018-05-23T13:29:35.825615: step 9420, loss 0.18387, acc 0.90625\n",
      "2018-05-23T13:29:36.160719: step 9421, loss 0.170936, acc 0.921875\n",
      "2018-05-23T13:29:36.506791: step 9422, loss 0.152601, acc 0.9375\n",
      "2018-05-23T13:29:36.854861: step 9423, loss 0.203068, acc 0.890625\n",
      "2018-05-23T13:29:37.194953: step 9424, loss 0.205225, acc 0.90625\n",
      "2018-05-23T13:29:37.543020: step 9425, loss 0.264578, acc 0.890625\n",
      "2018-05-23T13:29:37.911035: step 9426, loss 0.203785, acc 0.90625\n",
      "2018-05-23T13:29:38.295008: step 9427, loss 0.18432, acc 0.9375\n",
      "2018-05-23T13:29:38.640086: step 9428, loss 0.167285, acc 0.921875\n",
      "2018-05-23T13:29:38.966214: step 9429, loss 0.220635, acc 0.9375\n",
      "2018-05-23T13:29:39.307299: step 9430, loss 0.186273, acc 0.9375\n",
      "2018-05-23T13:29:39.641406: step 9431, loss 0.107594, acc 0.96875\n",
      "2018-05-23T13:29:39.977506: step 9432, loss 0.151585, acc 0.953125\n",
      "2018-05-23T13:29:40.329567: step 9433, loss 0.268076, acc 0.875\n",
      "2018-05-23T13:29:40.684617: step 9434, loss 0.142321, acc 0.9375\n",
      "2018-05-23T13:29:41.024705: step 9435, loss 0.0636824, acc 0.96875\n",
      "2018-05-23T13:29:41.358813: step 9436, loss 0.102448, acc 0.984375\n",
      "2018-05-23T13:29:41.710691: step 9437, loss 0.140949, acc 0.953125\n",
      "2018-05-23T13:29:42.046792: step 9438, loss 0.158582, acc 0.9375\n",
      "2018-05-23T13:29:42.384890: step 9439, loss 0.155402, acc 0.921875\n",
      "2018-05-23T13:29:42.730963: step 9440, loss 0.197296, acc 0.875\n",
      "2018-05-23T13:29:43.067065: step 9441, loss 0.141755, acc 0.90625\n",
      "2018-05-23T13:29:43.403163: step 9442, loss 0.21588, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:29:43.742260: step 9443, loss 0.237476, acc 0.890625\n",
      "2018-05-23T13:29:44.083345: step 9444, loss 0.193103, acc 0.96875\n",
      "2018-05-23T13:29:44.411466: step 9445, loss 0.234914, acc 0.921875\n",
      "2018-05-23T13:29:44.742580: step 9446, loss 0.214582, acc 0.90625\n",
      "2018-05-23T13:29:45.086660: step 9447, loss 0.194523, acc 0.953125\n",
      "2018-05-23T13:29:45.420767: step 9448, loss 0.149446, acc 0.96875\n",
      "2018-05-23T13:29:45.755872: step 9449, loss 0.121968, acc 0.953125\n",
      "2018-05-23T13:29:46.168766: step 9450, loss 0.180585, acc 0.890625\n",
      "2018-05-23T13:29:46.519828: step 9451, loss 0.257057, acc 0.9375\n",
      "2018-05-23T13:29:46.852936: step 9452, loss 0.2328, acc 0.875\n",
      "2018-05-23T13:29:47.194023: step 9453, loss 0.289781, acc 0.90625\n",
      "2018-05-23T13:29:47.526136: step 9454, loss 0.103542, acc 0.953125\n",
      "2018-05-23T13:29:47.863269: step 9455, loss 0.0893873, acc 0.953125\n",
      "2018-05-23T13:29:48.205351: step 9456, loss 0.112514, acc 0.953125\n",
      "2018-05-23T13:29:48.548433: step 9457, loss 0.0824091, acc 0.96875\n",
      "2018-05-23T13:29:48.879910: step 9458, loss 0.0930931, acc 0.984375\n",
      "2018-05-23T13:29:49.217009: step 9459, loss 0.162916, acc 0.9375\n",
      "2018-05-23T13:29:49.597991: step 9460, loss 0.241694, acc 0.890625\n",
      "2018-05-23T13:29:49.937082: step 9461, loss 0.0792865, acc 0.96875\n",
      "2018-05-23T13:29:50.287148: step 9462, loss 0.139792, acc 0.921875\n",
      "2018-05-23T13:29:50.641199: step 9463, loss 0.274637, acc 0.859375\n",
      "2018-05-23T13:29:50.989269: step 9464, loss 0.190622, acc 0.9375\n",
      "2018-05-23T13:29:51.321379: step 9465, loss 0.219212, acc 0.90625\n",
      "2018-05-23T13:29:51.649503: step 9466, loss 0.0891102, acc 1\n",
      "2018-05-23T13:29:51.996572: step 9467, loss 0.253702, acc 0.875\n",
      "2018-05-23T13:29:52.326691: step 9468, loss 0.158164, acc 0.90625\n",
      "2018-05-23T13:29:52.662792: step 9469, loss 0.212733, acc 0.9375\n",
      "2018-05-23T13:29:52.998891: step 9470, loss 0.205669, acc 0.9375\n",
      "2018-05-23T13:29:53.330005: step 9471, loss 0.176625, acc 0.9375\n",
      "2018-05-23T13:29:53.663114: step 9472, loss 0.106716, acc 0.984375\n",
      "2018-05-23T13:29:54.002207: step 9473, loss 0.227647, acc 0.90625\n",
      "2018-05-23T13:29:54.336313: step 9474, loss 0.181841, acc 0.890625\n",
      "2018-05-23T13:29:54.671457: step 9475, loss 0.35533, acc 0.90625\n",
      "2018-05-23T13:29:55.006523: step 9476, loss 0.167001, acc 0.921875\n",
      "2018-05-23T13:29:55.340629: step 9477, loss 0.219698, acc 0.9375\n",
      "2018-05-23T13:29:55.671740: step 9478, loss 0.254193, acc 0.890625\n",
      "2018-05-23T13:29:56.022802: step 9479, loss 0.182925, acc 0.90625\n",
      "2018-05-23T13:29:56.425727: step 9480, loss 0.131038, acc 0.96875\n",
      "2018-05-23T13:29:56.772799: step 9481, loss 0.114583, acc 0.9375\n",
      "2018-05-23T13:29:57.111888: step 9482, loss 0.266435, acc 0.90625\n",
      "2018-05-23T13:29:57.454973: step 9483, loss 0.14221, acc 0.9375\n",
      "2018-05-23T13:29:57.788082: step 9484, loss 0.216216, acc 0.921875\n",
      "2018-05-23T13:29:58.135152: step 9485, loss 0.307169, acc 0.859375\n",
      "2018-05-23T13:29:58.477236: step 9486, loss 0.233605, acc 0.890625\n",
      "2018-05-23T13:29:58.870220: step 9487, loss 0.129225, acc 0.96875\n",
      "2018-05-23T13:29:59.222244: step 9488, loss 0.198658, acc 0.921875\n",
      "2018-05-23T13:29:59.559341: step 9489, loss 0.201233, acc 0.921875\n",
      "2018-05-23T13:29:59.891454: step 9490, loss 0.197213, acc 0.890625\n",
      "2018-05-23T13:30:00.243511: step 9491, loss 0.174238, acc 0.90625\n",
      "2018-05-23T13:30:00.579613: step 9492, loss 0.305122, acc 0.859375\n",
      "2018-05-23T13:30:00.913717: step 9493, loss 0.154505, acc 0.953125\n",
      "2018-05-23T13:30:01.270768: step 9494, loss 0.230199, acc 0.890625\n",
      "2018-05-23T13:30:01.650746: step 9495, loss 0.231959, acc 0.90625\n",
      "2018-05-23T13:30:01.992831: step 9496, loss 0.30754, acc 0.859375\n",
      "2018-05-23T13:30:02.349878: step 9497, loss 0.213134, acc 0.875\n",
      "2018-05-23T13:30:02.720884: step 9498, loss 0.372005, acc 0.84375\n",
      "2018-05-23T13:30:03.078927: step 9499, loss 0.0995689, acc 0.984375\n",
      "2018-05-23T13:30:03.418018: step 9500, loss 0.161888, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:30:08.171303: step 9500, loss 0.955531, acc 0.719246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9500\n",
      "\n",
      "2018-05-23T13:30:09.270363: step 9501, loss 0.10177, acc 0.9375\n",
      "2018-05-23T13:30:09.732127: step 9502, loss 0.146052, acc 0.9375\n",
      "2018-05-23T13:30:10.096153: step 9503, loss 0.192176, acc 0.890625\n",
      "2018-05-23T13:30:10.486110: step 9504, loss 0.150214, acc 0.921875\n",
      "2018-05-23T13:30:10.826200: step 9505, loss 0.199607, acc 0.9375\n",
      "2018-05-23T13:30:11.164301: step 9506, loss 0.127898, acc 0.953125\n",
      "2018-05-23T13:30:11.496407: step 9507, loss 0.172637, acc 0.921875\n",
      "2018-05-23T13:30:11.863425: step 9508, loss 0.272934, acc 0.921875\n",
      "2018-05-23T13:30:12.272334: step 9509, loss 0.160525, acc 0.9375\n",
      "2018-05-23T13:30:12.608432: step 9510, loss 0.137976, acc 0.9375\n",
      "2018-05-23T13:30:12.970463: step 9511, loss 0.210894, acc 0.921875\n",
      "2018-05-23T13:30:13.393333: step 9512, loss 0.291623, acc 0.90625\n",
      "2018-05-23T13:30:13.781297: step 9513, loss 0.26793, acc 0.859375\n",
      "2018-05-23T13:30:14.120389: step 9514, loss 0.117867, acc 0.984375\n",
      "2018-05-23T13:30:14.458483: step 9515, loss 0.145892, acc 0.921875\n",
      "2018-05-23T13:30:14.782615: step 9516, loss 0.215223, acc 0.875\n",
      "2018-05-23T13:30:15.120711: step 9517, loss 0.0738932, acc 0.984375\n",
      "2018-05-23T13:30:15.442314: step 9518, loss 0.180926, acc 0.953125\n",
      "2018-05-23T13:30:15.774423: step 9519, loss 0.352286, acc 0.875\n",
      "2018-05-23T13:30:16.117507: step 9520, loss 0.282739, acc 0.90625\n",
      "2018-05-23T13:30:16.450615: step 9521, loss 0.265909, acc 0.890625\n",
      "2018-05-23T13:30:16.787713: step 9522, loss 0.13678, acc 0.90625\n",
      "2018-05-23T13:30:17.136781: step 9523, loss 0.227475, acc 0.875\n",
      "2018-05-23T13:30:17.471884: step 9524, loss 0.277514, acc 0.875\n",
      "2018-05-23T13:30:17.802996: step 9525, loss 0.25114, acc 0.875\n",
      "2018-05-23T13:30:18.138100: step 9526, loss 0.18585, acc 0.921875\n",
      "2018-05-23T13:30:18.466223: step 9527, loss 0.153262, acc 0.953125\n",
      "2018-05-23T13:30:18.806260: step 9528, loss 0.160345, acc 0.96875\n",
      "2018-05-23T13:30:19.211175: step 9529, loss 0.105988, acc 0.96875\n",
      "2018-05-23T13:30:19.597150: step 9530, loss 0.116699, acc 0.984375\n",
      "2018-05-23T13:30:19.960172: step 9531, loss 0.170525, acc 0.890625\n",
      "2018-05-23T13:30:20.292284: step 9532, loss 0.283144, acc 0.875\n",
      "2018-05-23T13:30:20.627388: step 9533, loss 0.16798, acc 0.9375\n",
      "2018-05-23T13:30:20.962491: step 9534, loss 0.20555, acc 0.875\n",
      "2018-05-23T13:30:21.306571: step 9535, loss 0.137402, acc 0.96875\n",
      "2018-05-23T13:30:21.695531: step 9536, loss 0.294685, acc 0.890625\n",
      "2018-05-23T13:30:22.111417: step 9537, loss 0.209361, acc 0.875\n",
      "2018-05-23T13:30:22.460155: step 9538, loss 0.199567, acc 0.90625\n",
      "2018-05-23T13:30:22.793261: step 9539, loss 0.19756, acc 0.90625\n",
      "2018-05-23T13:30:23.129364: step 9540, loss 0.299698, acc 0.90625\n",
      "2018-05-23T13:30:23.460478: step 9541, loss 0.234454, acc 0.921875\n",
      "2018-05-23T13:30:23.793586: step 9542, loss 0.145262, acc 0.9375\n",
      "2018-05-23T13:30:24.132679: step 9543, loss 0.188155, acc 0.953125\n",
      "2018-05-23T13:30:24.463793: step 9544, loss 0.18403, acc 0.921875\n",
      "2018-05-23T13:30:24.934536: step 9545, loss 0.174952, acc 0.921875\n",
      "2018-05-23T13:30:25.326485: step 9546, loss 0.222341, acc 0.90625\n",
      "2018-05-23T13:30:25.657601: step 9547, loss 0.101539, acc 0.984375\n",
      "2018-05-23T13:30:25.988713: step 9548, loss 0.145585, acc 0.953125\n",
      "2018-05-23T13:30:26.330799: step 9549, loss 0.16562, acc 0.90625\n",
      "2018-05-23T13:30:26.663906: step 9550, loss 0.230446, acc 0.921875\n",
      "2018-05-23T13:30:26.991031: step 9551, loss 0.108539, acc 0.9375\n",
      "2018-05-23T13:30:27.343091: step 9552, loss 0.28992, acc 0.9375\n",
      "2018-05-23T13:30:27.687170: step 9553, loss 0.109926, acc 0.9375\n",
      "2018-05-23T13:30:28.019281: step 9554, loss 0.410454, acc 0.875\n",
      "2018-05-23T13:30:28.356379: step 9555, loss 0.138943, acc 0.9375\n",
      "2018-05-23T13:30:28.692480: step 9556, loss 0.10061, acc 0.984375\n",
      "2018-05-23T13:30:29.021602: step 9557, loss 0.167834, acc 0.96875\n",
      "2018-05-23T13:30:29.360694: step 9558, loss 0.211305, acc 0.90625\n",
      "2018-05-23T13:30:29.693801: step 9559, loss 0.146651, acc 0.890625\n",
      "2018-05-23T13:30:30.074782: step 9560, loss 0.224135, acc 0.890625\n",
      "2018-05-23T13:30:30.445792: step 9561, loss 0.136816, acc 0.921875\n",
      "2018-05-23T13:30:30.780893: step 9562, loss 0.247368, acc 0.890625\n",
      "2018-05-23T13:30:31.116994: step 9563, loss 0.183506, acc 0.953125\n",
      "2018-05-23T13:30:31.506951: step 9564, loss 0.226228, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:30:31.831084: step 9565, loss 0.149858, acc 0.9375\n",
      "2018-05-23T13:30:32.165190: step 9566, loss 0.130583, acc 0.921875\n",
      "2018-05-23T13:30:32.507274: step 9567, loss 0.0982764, acc 0.953125\n",
      "2018-05-23T13:30:32.838392: step 9568, loss 0.212623, acc 0.90625\n",
      "2018-05-23T13:30:33.202415: step 9569, loss 0.077574, acc 0.984375\n",
      "2018-05-23T13:30:33.574421: step 9570, loss 0.127508, acc 0.953125\n",
      "2018-05-23T13:30:33.904539: step 9571, loss 0.202605, acc 0.9375\n",
      "2018-05-23T13:30:34.235653: step 9572, loss 0.182306, acc 0.921875\n",
      "2018-05-23T13:30:34.573749: step 9573, loss 0.242359, acc 0.875\n",
      "2018-05-23T13:30:34.904861: step 9574, loss 0.206947, acc 0.90625\n",
      "2018-05-23T13:30:35.289832: step 9575, loss 0.180244, acc 0.921875\n",
      "2018-05-23T13:30:35.645880: step 9576, loss 0.171266, acc 0.90625\n",
      "2018-05-23T13:30:35.984970: step 9577, loss 0.280926, acc 0.859375\n",
      "2018-05-23T13:30:36.311098: step 9578, loss 0.278386, acc 0.921875\n",
      "2018-05-23T13:30:36.651189: step 9579, loss 0.118193, acc 0.953125\n",
      "2018-05-23T13:30:36.995271: step 9580, loss 0.306524, acc 0.859375\n",
      "2018-05-23T13:30:37.336358: step 9581, loss 0.272567, acc 0.875\n",
      "2018-05-23T13:30:37.674452: step 9582, loss 0.229338, acc 0.921875\n",
      "2018-05-23T13:30:38.003571: step 9583, loss 0.184808, acc 0.921875\n",
      "2018-05-23T13:30:38.338677: step 9584, loss 0.16676, acc 0.953125\n",
      "2018-05-23T13:30:38.677770: step 9585, loss 0.213724, acc 0.921875\n",
      "2018-05-23T13:30:39.010881: step 9586, loss 0.135103, acc 0.9375\n",
      "2018-05-23T13:30:39.337003: step 9587, loss 0.337272, acc 0.859375\n",
      "2018-05-23T13:30:39.666125: step 9588, loss 0.356407, acc 0.875\n",
      "2018-05-23T13:30:39.999234: step 9589, loss 0.152155, acc 0.96875\n",
      "2018-05-23T13:30:40.328354: step 9590, loss 0.315672, acc 0.890625\n",
      "2018-05-23T13:30:40.659468: step 9591, loss 0.254396, acc 0.90625\n",
      "2018-05-23T13:30:41.037456: step 9592, loss 0.167551, acc 0.9375\n",
      "2018-05-23T13:30:41.395496: step 9593, loss 0.136138, acc 0.921875\n",
      "2018-05-23T13:30:41.727610: step 9594, loss 0.0728994, acc 0.96875\n",
      "2018-05-23T13:30:42.054735: step 9595, loss 0.164565, acc 0.921875\n",
      "2018-05-23T13:30:42.386846: step 9596, loss 0.0647459, acc 0.96875\n",
      "2018-05-23T13:30:42.724941: step 9597, loss 0.289438, acc 0.921875\n",
      "2018-05-23T13:30:43.091958: step 9598, loss 0.0966404, acc 0.984375\n",
      "2018-05-23T13:30:43.440030: step 9599, loss 0.133966, acc 0.953125\n",
      "2018-05-23T13:30:43.787100: step 9600, loss 0.306544, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:30:48.282075: step 9600, loss 0.956652, acc 0.722532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9600\n",
      "\n",
      "2018-05-23T13:30:49.863257: step 9601, loss 0.263483, acc 0.9375\n",
      "2018-05-23T13:30:50.308066: step 9602, loss 0.155632, acc 0.921875\n",
      "2018-05-23T13:30:50.668103: step 9603, loss 0.247221, acc 0.875\n",
      "2018-05-23T13:30:51.126877: step 9604, loss 0.128299, acc 0.96875\n",
      "2018-05-23T13:30:51.617564: step 9605, loss 0.129512, acc 0.9375\n",
      "2018-05-23T13:30:52.453327: step 9606, loss 0.251584, acc 0.875\n",
      "2018-05-23T13:30:52.817353: step 9607, loss 0.242507, acc 0.875\n",
      "2018-05-23T13:30:53.391818: step 9608, loss 0.231341, acc 0.921875\n",
      "2018-05-23T13:30:54.122860: step 9609, loss 0.249976, acc 0.890625\n",
      "2018-05-23T13:30:54.504074: step 9610, loss 0.285558, acc 0.84375\n",
      "2018-05-23T13:30:54.847154: step 9611, loss 0.18112, acc 0.875\n",
      "2018-05-23T13:30:55.260049: step 9612, loss 0.204013, acc 0.90625\n",
      "2018-05-23T13:30:55.757719: step 9613, loss 0.185867, acc 0.9375\n",
      "2018-05-23T13:30:56.209511: step 9614, loss 0.195515, acc 0.90625\n",
      "2018-05-23T13:30:56.715159: step 9615, loss 0.188217, acc 0.90625\n",
      "2018-05-23T13:30:57.086165: step 9616, loss 0.207657, acc 0.890625\n",
      "2018-05-23T13:30:57.439219: step 9617, loss 0.206766, acc 0.890625\n",
      "2018-05-23T13:30:57.841144: step 9618, loss 0.128367, acc 0.953125\n",
      "2018-05-23T13:30:58.257032: step 9619, loss 0.290345, acc 0.875\n",
      "2018-05-23T13:30:58.940205: step 9620, loss 0.38606, acc 0.9375\n",
      "2018-05-23T13:30:59.359083: step 9621, loss 0.25071, acc 0.875\n",
      "2018-05-23T13:30:59.725105: step 9622, loss 0.144386, acc 0.921875\n",
      "2018-05-23T13:31:00.096111: step 9623, loss 0.388895, acc 0.875\n",
      "2018-05-23T13:31:00.511999: step 9624, loss 0.209726, acc 0.90625\n",
      "2018-05-23T13:31:01.117382: step 9625, loss 0.179604, acc 0.90625\n",
      "2018-05-23T13:31:01.704809: step 9626, loss 0.24896, acc 0.890625\n",
      "2018-05-23T13:31:02.297225: step 9627, loss 0.19707, acc 0.90625\n",
      "2018-05-23T13:31:02.995357: step 9628, loss 0.160082, acc 0.96875\n",
      "2018-05-23T13:31:03.565837: step 9629, loss 0.205014, acc 0.9375\n",
      "2018-05-23T13:31:04.121344: step 9630, loss 0.273773, acc 0.921875\n",
      "2018-05-23T13:31:04.709770: step 9631, loss 0.405737, acc 0.828125\n",
      "2018-05-23T13:31:05.313156: step 9632, loss 0.1798, acc 0.9375\n",
      "2018-05-23T13:31:05.828776: step 9633, loss 0.085001, acc 0.984375\n",
      "2018-05-23T13:31:06.316472: step 9634, loss 0.158826, acc 0.90625\n",
      "2018-05-23T13:31:06.790205: step 9635, loss 0.201338, acc 0.890625\n",
      "2018-05-23T13:31:07.205094: step 9636, loss 0.155337, acc 0.921875\n",
      "2018-05-23T13:31:07.615995: step 9637, loss 0.184269, acc 0.90625\n",
      "2018-05-23T13:31:08.042854: step 9638, loss 0.183065, acc 0.90625\n",
      "2018-05-23T13:31:08.572439: step 9639, loss 0.26619, acc 0.875\n",
      "2018-05-23T13:31:09.067114: step 9640, loss 0.19779, acc 0.90625\n",
      "2018-05-23T13:31:09.532867: step 9641, loss 0.25047, acc 0.890625\n",
      "2018-05-23T13:31:10.026549: step 9642, loss 0.0893395, acc 0.96875\n",
      "2018-05-23T13:31:10.718695: step 9643, loss 0.180021, acc 0.921875\n",
      "2018-05-23T13:31:11.241296: step 9644, loss 0.143224, acc 0.921875\n",
      "2018-05-23T13:31:11.751932: step 9645, loss 0.127244, acc 0.90625\n",
      "2018-05-23T13:31:12.312431: step 9646, loss 0.220381, acc 0.890625\n",
      "2018-05-23T13:31:12.785167: step 9647, loss 0.175481, acc 0.9375\n",
      "2018-05-23T13:31:13.264882: step 9648, loss 0.203874, acc 0.90625\n",
      "2018-05-23T13:31:13.785493: step 9649, loss 0.278226, acc 0.890625\n",
      "2018-05-23T13:31:14.224317: step 9650, loss 0.171422, acc 0.9375\n",
      "2018-05-23T13:31:14.584354: step 9651, loss 0.278832, acc 0.84375\n",
      "2018-05-23T13:31:14.936413: step 9652, loss 0.143189, acc 0.953125\n",
      "2018-05-23T13:31:15.317393: step 9653, loss 0.177529, acc 0.9375\n",
      "2018-05-23T13:31:15.918784: step 9654, loss 0.307801, acc 0.875\n",
      "2018-05-23T13:31:16.448368: step 9655, loss 0.133591, acc 0.953125\n",
      "2018-05-23T13:31:16.956008: step 9656, loss 0.301631, acc 0.890625\n",
      "2018-05-23T13:31:17.639183: step 9657, loss 0.151704, acc 0.921875\n",
      "2018-05-23T13:31:18.300412: step 9658, loss 0.177006, acc 0.953125\n",
      "2018-05-23T13:31:18.718294: step 9659, loss 0.264755, acc 0.890625\n",
      "2018-05-23T13:31:19.134182: step 9660, loss 0.166813, acc 0.90625\n",
      "2018-05-23T13:31:19.498208: step 9661, loss 0.247044, acc 0.890625\n",
      "2018-05-23T13:31:19.843285: step 9662, loss 0.102808, acc 0.96875\n",
      "2018-05-23T13:31:20.186368: step 9663, loss 0.364334, acc 0.828125\n",
      "2018-05-23T13:31:20.531443: step 9664, loss 0.185881, acc 0.96875\n",
      "2018-05-23T13:31:20.865551: step 9665, loss 0.154738, acc 0.9375\n",
      "2018-05-23T13:31:21.401118: step 9666, loss 0.419147, acc 0.875\n",
      "2018-05-23T13:31:21.862882: step 9667, loss 0.176235, acc 0.90625\n",
      "2018-05-23T13:31:22.217934: step 9668, loss 0.237508, acc 0.875\n",
      "2018-05-23T13:31:22.561015: step 9669, loss 0.172143, acc 0.90625\n",
      "2018-05-23T13:31:22.949973: step 9670, loss 0.124276, acc 0.9375\n",
      "2018-05-23T13:31:23.341928: step 9671, loss 0.139407, acc 0.9375\n",
      "2018-05-23T13:31:23.680021: step 9672, loss 0.162107, acc 0.9375\n",
      "2018-05-23T13:31:24.014126: step 9673, loss 0.29161, acc 0.921875\n",
      "2018-05-23T13:31:24.360200: step 9674, loss 0.13821, acc 0.9375\n",
      "2018-05-23T13:31:24.694309: step 9675, loss 0.189124, acc 0.921875\n",
      "2018-05-23T13:31:25.042376: step 9676, loss 0.112742, acc 0.953125\n",
      "2018-05-23T13:31:25.385458: step 9677, loss 0.204151, acc 0.890625\n",
      "2018-05-23T13:31:25.727545: step 9678, loss 0.135935, acc 0.9375\n",
      "2018-05-23T13:31:26.067635: step 9679, loss 0.111417, acc 0.953125\n",
      "2018-05-23T13:31:26.410402: step 9680, loss 0.256431, acc 0.875\n",
      "2018-05-23T13:31:26.750492: step 9681, loss 0.215451, acc 0.953125\n",
      "2018-05-23T13:31:27.090585: step 9682, loss 0.354334, acc 0.890625\n",
      "2018-05-23T13:31:27.459596: step 9683, loss 0.135529, acc 0.953125\n",
      "2018-05-23T13:31:27.801990: step 9684, loss 0.199721, acc 0.890625\n",
      "2018-05-23T13:31:28.147068: step 9685, loss 0.20888, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:31:28.496134: step 9686, loss 0.139708, acc 0.921875\n",
      "2018-05-23T13:31:28.833231: step 9687, loss 0.279432, acc 0.859375\n",
      "2018-05-23T13:31:29.178309: step 9688, loss 0.146971, acc 0.921875\n",
      "2018-05-23T13:31:29.518398: step 9689, loss 0.190627, acc 0.921875\n",
      "2018-05-23T13:31:29.861482: step 9690, loss 0.274126, acc 0.828125\n",
      "2018-05-23T13:31:30.205560: step 9691, loss 0.282402, acc 0.890625\n",
      "2018-05-23T13:31:30.548642: step 9692, loss 0.238022, acc 0.875\n",
      "2018-05-23T13:31:30.891724: step 9693, loss 0.250555, acc 0.875\n",
      "2018-05-23T13:31:31.237800: step 9694, loss 0.12712, acc 0.921875\n",
      "2018-05-23T13:31:31.584871: step 9695, loss 0.243535, acc 0.921875\n",
      "2018-05-23T13:31:31.926957: step 9696, loss 0.308169, acc 0.875\n",
      "2018-05-23T13:31:32.263055: step 9697, loss 0.186493, acc 0.9375\n",
      "2018-05-23T13:31:32.612121: step 9698, loss 0.15562, acc 0.953125\n",
      "2018-05-23T13:31:32.950218: step 9699, loss 0.098718, acc 0.984375\n",
      "2018-05-23T13:31:33.349151: step 9700, loss 0.263979, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:31:38.273977: step 9700, loss 0.948856, acc 0.723103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9700\n",
      "\n",
      "2018-05-23T13:31:39.684353: step 9701, loss 0.147653, acc 0.921875\n",
      "2018-05-23T13:31:40.067329: step 9702, loss 0.151438, acc 0.984375\n",
      "2018-05-23T13:31:40.512139: step 9703, loss 0.197202, acc 0.9375\n",
      "2018-05-23T13:31:40.892122: step 9704, loss 0.221719, acc 0.90625\n",
      "2018-05-23T13:31:41.247174: step 9705, loss 0.0898354, acc 0.96875\n",
      "2018-05-23T13:31:41.620174: step 9706, loss 0.16052, acc 0.9375\n",
      "2018-05-23T13:31:42.007140: step 9707, loss 0.258652, acc 0.890625\n",
      "2018-05-23T13:31:42.525751: step 9708, loss 0.229663, acc 0.90625\n",
      "2018-05-23T13:31:42.926687: step 9709, loss 0.146695, acc 0.90625\n",
      "2018-05-23T13:31:43.460257: step 9710, loss 0.366045, acc 0.90625\n",
      "2018-05-23T13:31:43.873147: step 9711, loss 0.193044, acc 0.921875\n",
      "2018-05-23T13:31:44.255126: step 9712, loss 0.330013, acc 0.859375\n",
      "2018-05-23T13:31:44.653060: step 9713, loss 0.301009, acc 0.859375\n",
      "2018-05-23T13:31:45.259438: step 9714, loss 0.158879, acc 0.9375\n",
      "2018-05-23T13:31:45.663359: step 9715, loss 0.150768, acc 0.9375\n",
      "2018-05-23T13:31:46.040349: step 9716, loss 0.231278, acc 0.859375\n",
      "2018-05-23T13:31:46.486158: step 9717, loss 0.283881, acc 0.890625\n",
      "2018-05-23T13:31:46.862151: step 9718, loss 0.126704, acc 0.953125\n",
      "2018-05-23T13:31:47.224184: step 9719, loss 0.147445, acc 0.9375\n",
      "2018-05-23T13:31:47.768727: step 9720, loss 0.250697, acc 0.875\n",
      "2018-05-23T13:31:48.331222: step 9721, loss 0.303492, acc 0.859375\n",
      "2018-05-23T13:31:48.783014: step 9722, loss 0.279175, acc 0.875\n",
      "2018-05-23T13:31:49.149036: step 9723, loss 0.144358, acc 0.90625\n",
      "2018-05-23T13:31:49.538990: step 9724, loss 0.292447, acc 0.859375\n",
      "2018-05-23T13:31:49.927951: step 9725, loss 0.27475, acc 0.875\n",
      "2018-05-23T13:31:50.309929: step 9726, loss 0.187838, acc 0.875\n",
      "2018-05-23T13:31:50.683965: step 9727, loss 0.173701, acc 0.921875\n",
      "2018-05-23T13:31:51.053938: step 9728, loss 0.0884384, acc 0.984375\n",
      "2018-05-23T13:31:51.393033: step 9729, loss 0.128626, acc 0.9375\n",
      "2018-05-23T13:31:51.732125: step 9730, loss 0.120966, acc 0.984375\n",
      "2018-05-23T13:31:52.135045: step 9731, loss 0.102472, acc 0.96875\n",
      "2018-05-23T13:31:52.491093: step 9732, loss 0.176143, acc 0.96875\n",
      "2018-05-23T13:31:52.880052: step 9733, loss 0.218728, acc 0.953125\n",
      "2018-05-23T13:31:53.258043: step 9734, loss 0.317807, acc 0.875\n",
      "2018-05-23T13:31:53.602121: step 9735, loss 0.231778, acc 0.875\n",
      "2018-05-23T13:31:53.941213: step 9736, loss 0.160242, acc 0.921875\n",
      "2018-05-23T13:31:54.287289: step 9737, loss 0.329514, acc 0.8125\n",
      "2018-05-23T13:31:54.630371: step 9738, loss 0.151265, acc 0.953125\n",
      "2018-05-23T13:31:54.984423: step 9739, loss 0.154673, acc 0.9375\n",
      "2018-05-23T13:31:55.630694: step 9740, loss 0.103391, acc 0.96875\n",
      "2018-05-23T13:31:56.119387: step 9741, loss 0.14234, acc 0.921875\n",
      "2018-05-23T13:31:56.688864: step 9742, loss 0.244011, acc 0.875\n",
      "2018-05-23T13:31:57.406943: step 9743, loss 0.14532, acc 0.90625\n",
      "2018-05-23T13:31:57.897630: step 9744, loss 0.183688, acc 0.921875\n",
      "2018-05-23T13:31:58.855069: step 9745, loss 0.123187, acc 0.953125\n",
      "2018-05-23T13:31:59.454464: step 9746, loss 0.251478, acc 0.921875\n",
      "2018-05-23T13:32:00.099738: step 9747, loss 0.246164, acc 0.875\n",
      "2018-05-23T13:32:00.975395: step 9748, loss 0.0738883, acc 0.984375\n",
      "2018-05-23T13:32:01.726388: step 9749, loss 0.191996, acc 0.9375\n",
      "2018-05-23T13:32:02.428508: step 9750, loss 0.297177, acc 0.875\n",
      "2018-05-23T13:32:03.256294: step 9751, loss 0.197788, acc 0.9375\n",
      "2018-05-23T13:32:03.988337: step 9752, loss 0.177653, acc 0.953125\n",
      "2018-05-23T13:32:04.850030: step 9753, loss 0.176147, acc 0.921875\n",
      "2018-05-23T13:32:05.639918: step 9754, loss 0.235595, acc 0.875\n",
      "2018-05-23T13:32:06.417837: step 9755, loss 0.167541, acc 0.9375\n",
      "2018-05-23T13:32:07.328401: step 9756, loss 0.195553, acc 0.953125\n",
      "2018-05-23T13:32:07.817092: step 9757, loss 0.290762, acc 0.859375\n",
      "2018-05-23T13:32:08.349668: step 9758, loss 0.121966, acc 0.9375\n",
      "2018-05-23T13:32:08.838362: step 9759, loss 0.279489, acc 0.828125\n",
      "2018-05-23T13:32:09.350990: step 9760, loss 0.152005, acc 0.9375\n",
      "2018-05-23T13:32:09.791810: step 9761, loss 0.132755, acc 0.90625\n",
      "2018-05-23T13:32:10.153843: step 9762, loss 0.301149, acc 0.875\n",
      "2018-05-23T13:32:10.657493: step 9763, loss 0.283217, acc 0.859375\n",
      "2018-05-23T13:32:11.191066: step 9764, loss 0.235824, acc 0.859375\n",
      "2018-05-23T13:32:11.588006: step 9765, loss 0.106231, acc 0.96875\n",
      "2018-05-23T13:32:11.963999: step 9766, loss 0.300614, acc 0.890625\n",
      "2018-05-23T13:32:12.330021: step 9767, loss 0.192067, acc 0.9375\n",
      "2018-05-23T13:32:12.975294: step 9768, loss 0.306435, acc 0.84375\n",
      "2018-05-23T13:32:13.457006: step 9769, loss 0.0968224, acc 0.9375\n",
      "2018-05-23T13:32:13.826017: step 9770, loss 0.232394, acc 0.921875\n",
      "2018-05-23T13:32:14.177079: step 9771, loss 0.229868, acc 0.875\n",
      "2018-05-23T13:32:14.537116: step 9772, loss 0.207831, acc 0.875\n",
      "2018-05-23T13:32:14.964971: step 9773, loss 0.19551, acc 0.875\n",
      "2018-05-23T13:32:15.647145: step 9774, loss 0.292965, acc 0.859375\n",
      "2018-05-23T13:32:16.159775: step 9775, loss 0.313463, acc 0.84375\n",
      "2018-05-23T13:32:16.677391: step 9776, loss 0.241998, acc 0.890625\n",
      "2018-05-23T13:32:17.282771: step 9777, loss 0.259186, acc 0.875\n",
      "2018-05-23T13:32:17.874187: step 9778, loss 0.186928, acc 0.90625\n",
      "2018-05-23T13:32:18.326976: step 9779, loss 0.157459, acc 0.921875\n",
      "2018-05-23T13:32:18.806694: step 9780, loss 0.24899, acc 0.890625\n",
      "2018-05-23T13:32:19.463936: step 9781, loss 0.166934, acc 0.96875\n",
      "2018-05-23T13:32:20.287751: step 9782, loss 0.238127, acc 0.90625\n",
      "2018-05-23T13:32:21.172366: step 9783, loss 0.206439, acc 0.90625\n",
      "2018-05-23T13:32:21.893438: step 9784, loss 0.168979, acc 0.921875\n",
      "2018-05-23T13:32:22.499815: step 9785, loss 0.154856, acc 0.9375\n",
      "2018-05-23T13:32:22.997482: step 9786, loss 0.282491, acc 0.859375\n",
      "2018-05-23T13:32:23.683648: step 9787, loss 0.164105, acc 0.9375\n",
      "2018-05-23T13:32:24.306980: step 9788, loss 0.110981, acc 0.9375\n",
      "2018-05-23T13:32:24.829582: step 9789, loss 0.149713, acc 0.9375\n",
      "2018-05-23T13:32:25.404045: step 9790, loss 0.129756, acc 0.9375\n",
      "2018-05-23T13:32:26.149051: step 9791, loss 0.211854, acc 0.90625\n",
      "2018-05-23T13:32:26.779366: step 9792, loss 0.137279, acc 0.96875\n",
      "2018-05-23T13:32:27.231156: step 9793, loss 0.159055, acc 0.90625\n",
      "2018-05-23T13:32:27.734810: step 9794, loss 0.221248, acc 0.90625\n",
      "2018-05-23T13:32:28.257412: step 9795, loss 0.167783, acc 0.90625\n",
      "2018-05-23T13:32:28.870770: step 9796, loss 0.20281, acc 0.90625\n",
      "2018-05-23T13:32:29.464183: step 9797, loss 0.200169, acc 0.890625\n",
      "2018-05-23T13:32:30.136386: step 9798, loss 0.152814, acc 0.921875\n",
      "2018-05-23T13:32:30.907324: step 9799, loss 0.282305, acc 0.859375\n",
      "2018-05-23T13:32:31.764031: step 9800, loss 0.127379, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:32:39.740691: step 9800, loss 0.954289, acc 0.718531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9800\n",
      "\n",
      "2018-05-23T13:32:41.686486: step 9801, loss 0.154947, acc 0.953125\n",
      "2018-05-23T13:32:42.127306: step 9802, loss 0.156867, acc 0.953125\n",
      "2018-05-23T13:32:42.568127: step 9803, loss 0.163209, acc 0.9375\n",
      "2018-05-23T13:32:43.717054: step 9804, loss 0.188023, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:32:44.568879: step 9805, loss 0.104328, acc 0.96875\n",
      "2018-05-23T13:32:45.569204: step 9806, loss 0.296511, acc 0.890625\n",
      "2018-05-23T13:32:46.258354: step 9807, loss 0.232107, acc 0.890625\n",
      "2018-05-23T13:32:46.929559: step 9808, loss 0.160326, acc 0.9375\n",
      "2018-05-23T13:32:47.593782: step 9809, loss 0.111648, acc 0.9375\n",
      "2018-05-23T13:32:48.021638: step 9810, loss 0.152867, acc 0.953125\n",
      "2018-05-23T13:32:48.439519: step 9811, loss 0.18415, acc 0.953125\n",
      "2018-05-23T13:32:48.880339: step 9812, loss 0.25575, acc 0.875\n",
      "2018-05-23T13:32:49.315176: step 9813, loss 0.248524, acc 0.9375\n",
      "2018-05-23T13:32:49.872684: step 9814, loss 0.17534, acc 0.921875\n",
      "2018-05-23T13:32:50.356391: step 9815, loss 0.11594, acc 0.953125\n",
      "2018-05-23T13:32:50.756321: step 9816, loss 0.0808215, acc 0.984375\n",
      "2018-05-23T13:32:51.180186: step 9817, loss 0.356598, acc 0.859375\n",
      "2018-05-23T13:32:51.578122: step 9818, loss 0.264687, acc 0.90625\n",
      "2018-05-23T13:32:51.961100: step 9819, loss 0.182887, acc 0.9375\n",
      "2018-05-23T13:32:52.360030: step 9820, loss 0.18645, acc 0.921875\n",
      "2018-05-23T13:32:52.739016: step 9821, loss 0.186361, acc 0.90625\n",
      "2018-05-23T13:32:53.142936: step 9822, loss 0.235467, acc 0.90625\n",
      "2018-05-23T13:32:53.534891: step 9823, loss 0.242536, acc 0.921875\n",
      "2018-05-23T13:32:53.924846: step 9824, loss 0.227931, acc 0.890625\n",
      "2018-05-23T13:32:54.312808: step 9825, loss 0.14151, acc 0.953125\n",
      "2018-05-23T13:32:54.710742: step 9826, loss 0.206538, acc 0.875\n",
      "2018-05-23T13:32:55.102693: step 9827, loss 0.126629, acc 0.953125\n",
      "2018-05-23T13:32:55.496640: step 9828, loss 0.0862907, acc 1\n",
      "2018-05-23T13:32:56.084069: step 9829, loss 0.171763, acc 0.9375\n",
      "2018-05-23T13:32:56.760260: step 9830, loss 0.110571, acc 0.96875\n",
      "2018-05-23T13:32:57.639906: step 9831, loss 0.208017, acc 0.921875\n",
      "2018-05-23T13:32:58.335047: step 9832, loss 0.21176, acc 0.921875\n",
      "2018-05-23T13:32:59.174801: step 9833, loss 0.173008, acc 0.9375\n",
      "2018-05-23T13:33:00.150191: step 9834, loss 0.249729, acc 0.859375\n",
      "2018-05-23T13:33:01.121595: step 9835, loss 0.421313, acc 0.84375\n",
      "2018-05-23T13:33:01.991266: step 9836, loss 0.106338, acc 0.96875\n",
      "2018-05-23T13:33:02.689398: step 9837, loss 0.109104, acc 0.984375\n",
      "2018-05-23T13:33:03.605947: step 9838, loss 0.234265, acc 0.890625\n",
      "2018-05-23T13:33:04.317044: step 9839, loss 0.34692, acc 0.796875\n",
      "2018-05-23T13:33:05.003209: step 9840, loss 0.138648, acc 0.96875\n",
      "2018-05-23T13:33:05.767166: step 9841, loss 0.53105, acc 0.84375\n",
      "2018-05-23T13:33:06.568022: step 9842, loss 0.14133, acc 0.953125\n",
      "2018-05-23T13:33:07.261168: step 9843, loss 0.194237, acc 0.90625\n",
      "2018-05-23T13:33:07.987226: step 9844, loss 0.293399, acc 0.890625\n",
      "2018-05-23T13:33:08.684360: step 9845, loss 0.295365, acc 0.84375\n",
      "2018-05-23T13:33:09.497186: step 9846, loss 0.139552, acc 0.921875\n",
      "2018-05-23T13:33:10.288070: step 9847, loss 0.153916, acc 0.9375\n",
      "2018-05-23T13:33:10.923370: step 9848, loss 0.188437, acc 0.921875\n",
      "2018-05-23T13:33:11.548697: step 9849, loss 0.246868, acc 0.875\n",
      "2018-05-23T13:33:12.103214: step 9850, loss 0.132652, acc 0.941176\n",
      "2018-05-23T13:33:12.931000: step 9851, loss 0.111684, acc 0.953125\n",
      "2018-05-23T13:33:14.151735: step 9852, loss 0.099713, acc 0.984375\n",
      "2018-05-23T13:33:15.133296: step 9853, loss 0.108307, acc 0.953125\n",
      "2018-05-23T13:33:15.827723: step 9854, loss 0.15032, acc 0.953125\n",
      "2018-05-23T13:33:16.635562: step 9855, loss 0.117918, acc 0.9375\n",
      "2018-05-23T13:33:17.329705: step 9856, loss 0.10433, acc 0.953125\n",
      "2018-05-23T13:33:18.195406: step 9857, loss 0.109636, acc 0.984375\n",
      "2018-05-23T13:33:18.854684: step 9858, loss 0.193551, acc 0.921875\n",
      "2018-05-23T13:33:19.503949: step 9859, loss 0.10245, acc 0.953125\n",
      "2018-05-23T13:33:20.231252: step 9860, loss 0.145759, acc 0.921875\n",
      "2018-05-23T13:33:20.881117: step 9861, loss 0.0986037, acc 0.953125\n",
      "2018-05-23T13:33:21.715212: step 9862, loss 0.218948, acc 0.875\n",
      "2018-05-23T13:33:22.490425: step 9863, loss 0.158117, acc 0.921875\n",
      "2018-05-23T13:33:23.138384: step 9864, loss 0.153562, acc 0.953125\n",
      "2018-05-23T13:33:23.800154: step 9865, loss 0.166294, acc 0.90625\n",
      "2018-05-23T13:33:24.562651: step 9866, loss 0.118109, acc 0.953125\n",
      "2018-05-23T13:33:25.438854: step 9867, loss 0.0456115, acc 0.984375\n",
      "2018-05-23T13:33:26.068170: step 9868, loss 0.124494, acc 0.953125\n",
      "2018-05-23T13:33:26.680530: step 9869, loss 0.168998, acc 0.9375\n",
      "2018-05-23T13:33:27.473615: step 9870, loss 0.202811, acc 0.921875\n",
      "2018-05-23T13:33:28.054591: step 9871, loss 0.109722, acc 0.953125\n",
      "2018-05-23T13:33:28.695935: step 9872, loss 0.111723, acc 0.9375\n",
      "2018-05-23T13:33:29.232644: step 9873, loss 0.106449, acc 0.96875\n",
      "2018-05-23T13:33:29.797360: step 9874, loss 0.0523513, acc 0.984375\n",
      "2018-05-23T13:33:30.406846: step 9875, loss 0.175732, acc 0.96875\n",
      "2018-05-23T13:33:30.981863: step 9876, loss 0.175478, acc 0.9375\n",
      "2018-05-23T13:33:31.540408: step 9877, loss 0.197483, acc 0.96875\n",
      "2018-05-23T13:33:32.139127: step 9878, loss 0.113113, acc 0.9375\n",
      "2018-05-23T13:33:32.733350: step 9879, loss 0.174392, acc 0.9375\n",
      "2018-05-23T13:33:33.278608: step 9880, loss 0.110738, acc 0.9375\n",
      "2018-05-23T13:33:33.897271: step 9881, loss 0.108383, acc 0.9375\n",
      "2018-05-23T13:33:34.531486: step 9882, loss 0.164202, acc 0.921875\n",
      "2018-05-23T13:33:35.192235: step 9883, loss 0.131336, acc 0.921875\n",
      "2018-05-23T13:33:35.751616: step 9884, loss 0.119623, acc 0.953125\n",
      "2018-05-23T13:33:36.465568: step 9885, loss 0.100492, acc 0.96875\n",
      "2018-05-23T13:33:37.243165: step 9886, loss 0.0773678, acc 0.96875\n",
      "2018-05-23T13:33:38.278406: step 9887, loss 0.12615, acc 0.953125\n",
      "2018-05-23T13:33:38.889046: step 9888, loss 0.121494, acc 0.953125\n",
      "2018-05-23T13:33:39.483377: step 9889, loss 0.0643886, acc 1\n",
      "2018-05-23T13:33:40.068192: step 9890, loss 0.146829, acc 0.9375\n",
      "2018-05-23T13:33:40.747896: step 9891, loss 0.132815, acc 0.9375\n",
      "2018-05-23T13:33:41.331197: step 9892, loss 0.152277, acc 0.921875\n",
      "2018-05-23T13:33:41.993567: step 9893, loss 0.159813, acc 0.921875\n",
      "2018-05-23T13:33:42.602439: step 9894, loss 0.112271, acc 0.953125\n",
      "2018-05-23T13:33:43.264314: step 9895, loss 0.260649, acc 0.890625\n",
      "2018-05-23T13:33:44.080189: step 9896, loss 0.267611, acc 0.90625\n",
      "2018-05-23T13:33:45.193097: step 9897, loss 0.265289, acc 0.921875\n",
      "2018-05-23T13:33:45.784552: step 9898, loss 0.20055, acc 0.90625\n",
      "2018-05-23T13:33:46.412384: step 9899, loss 0.171675, acc 0.9375\n",
      "2018-05-23T13:33:47.028292: step 9900, loss 0.300151, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:33:54.046125: step 9900, loss 0.986474, acc 0.720246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-9900\n",
      "\n",
      "2018-05-23T13:33:56.491797: step 9901, loss 0.107185, acc 0.96875\n",
      "2018-05-23T13:33:57.062926: step 9902, loss 0.101171, acc 0.953125\n",
      "2018-05-23T13:33:57.729574: step 9903, loss 0.2581, acc 0.875\n",
      "2018-05-23T13:33:58.277733: step 9904, loss 0.226676, acc 0.890625\n",
      "2018-05-23T13:33:58.851796: step 9905, loss 0.101244, acc 0.984375\n",
      "2018-05-23T13:33:59.463309: step 9906, loss 0.067788, acc 0.984375\n",
      "2018-05-23T13:34:00.098668: step 9907, loss 0.12563, acc 0.953125\n",
      "2018-05-23T13:34:00.657858: step 9908, loss 0.120663, acc 0.9375\n",
      "2018-05-23T13:34:01.276890: step 9909, loss 0.158282, acc 0.90625\n",
      "2018-05-23T13:34:01.891755: step 9910, loss 0.2092, acc 0.921875\n",
      "2018-05-23T13:34:02.549056: step 9911, loss 0.139343, acc 0.921875\n",
      "2018-05-23T13:34:03.122068: step 9912, loss 0.113911, acc 0.953125\n",
      "2018-05-23T13:34:03.693507: step 9913, loss 0.146861, acc 0.953125\n",
      "2018-05-23T13:34:04.272471: step 9914, loss 0.200701, acc 0.90625\n",
      "2018-05-23T13:34:04.884398: step 9915, loss 0.114275, acc 0.9375\n",
      "2018-05-23T13:34:05.516707: step 9916, loss 0.252152, acc 0.875\n",
      "2018-05-23T13:34:06.147122: step 9917, loss 0.152957, acc 0.953125\n",
      "2018-05-23T13:34:06.706178: step 9918, loss 0.178927, acc 0.890625\n",
      "2018-05-23T13:34:07.358239: step 9919, loss 0.0775456, acc 0.96875\n",
      "2018-05-23T13:34:07.975701: step 9920, loss 0.170245, acc 0.9375\n",
      "2018-05-23T13:34:08.527225: step 9921, loss 0.225693, acc 0.875\n",
      "2018-05-23T13:34:09.116648: step 9922, loss 0.202641, acc 0.90625\n",
      "2018-05-23T13:34:09.665181: step 9923, loss 0.146222, acc 0.953125\n",
      "2018-05-23T13:34:10.232690: step 9924, loss 0.235231, acc 0.9375\n",
      "2018-05-23T13:34:10.821176: step 9925, loss 0.18361, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:34:11.387176: step 9926, loss 0.0821531, acc 0.96875\n",
      "2018-05-23T13:34:12.007077: step 9927, loss 0.113319, acc 1\n",
      "2018-05-23T13:34:12.655390: step 9928, loss 0.217285, acc 0.875\n",
      "2018-05-23T13:34:13.189471: step 9929, loss 0.119517, acc 0.953125\n",
      "2018-05-23T13:34:13.928029: step 9930, loss 0.205087, acc 0.921875\n",
      "2018-05-23T13:34:14.573810: step 9931, loss 0.105465, acc 0.953125\n",
      "2018-05-23T13:34:15.346293: step 9932, loss 0.129449, acc 0.9375\n",
      "2018-05-23T13:34:16.139327: step 9933, loss 0.201704, acc 0.953125\n",
      "2018-05-23T13:34:16.915586: step 9934, loss 0.163776, acc 0.90625\n",
      "2018-05-23T13:34:17.542860: step 9935, loss 0.239178, acc 0.90625\n",
      "2018-05-23T13:34:18.141331: step 9936, loss 0.101088, acc 0.953125\n",
      "2018-05-23T13:34:18.739215: step 9937, loss 0.159386, acc 0.9375\n",
      "2018-05-23T13:34:19.339407: step 9938, loss 0.192536, acc 0.859375\n",
      "2018-05-23T13:34:20.020984: step 9939, loss 0.128161, acc 0.921875\n",
      "2018-05-23T13:34:20.568393: step 9940, loss 0.0575972, acc 0.96875\n",
      "2018-05-23T13:34:21.123155: step 9941, loss 0.0907751, acc 0.9375\n",
      "2018-05-23T13:34:21.588699: step 9942, loss 0.143053, acc 0.9375\n",
      "2018-05-23T13:34:22.235138: step 9943, loss 0.15091, acc 0.921875\n",
      "2018-05-23T13:34:22.805200: step 9944, loss 0.235482, acc 0.875\n",
      "2018-05-23T13:34:23.407428: step 9945, loss 0.15085, acc 0.921875\n",
      "2018-05-23T13:34:23.909885: step 9946, loss 0.109662, acc 0.96875\n",
      "2018-05-23T13:34:24.399440: step 9947, loss 0.110757, acc 0.953125\n",
      "2018-05-23T13:34:24.855369: step 9948, loss 0.125981, acc 0.96875\n",
      "2018-05-23T13:34:25.324833: step 9949, loss 0.104352, acc 0.9375\n",
      "2018-05-23T13:34:25.803052: step 9950, loss 0.183544, acc 0.90625\n",
      "2018-05-23T13:34:26.313840: step 9951, loss 0.231319, acc 0.90625\n",
      "2018-05-23T13:34:26.757018: step 9952, loss 0.168399, acc 0.890625\n",
      "2018-05-23T13:34:27.210292: step 9953, loss 0.0920013, acc 0.96875\n",
      "2018-05-23T13:34:27.661072: step 9954, loss 0.0985237, acc 0.96875\n",
      "2018-05-23T13:34:28.126373: step 9955, loss 0.0932959, acc 0.96875\n",
      "2018-05-23T13:34:28.631542: step 9956, loss 0.127491, acc 0.96875\n",
      "2018-05-23T13:34:29.091465: step 9957, loss 0.230282, acc 0.90625\n",
      "2018-05-23T13:34:29.593341: step 9958, loss 0.191359, acc 0.875\n",
      "2018-05-23T13:34:30.092282: step 9959, loss 0.218911, acc 0.90625\n",
      "2018-05-23T13:34:30.630270: step 9960, loss 0.154473, acc 0.9375\n",
      "2018-05-23T13:34:31.121827: step 9961, loss 0.11823, acc 0.953125\n",
      "2018-05-23T13:34:31.601162: step 9962, loss 0.122421, acc 0.96875\n",
      "2018-05-23T13:34:32.073076: step 9963, loss 0.0647536, acc 0.96875\n",
      "2018-05-23T13:34:32.604178: step 9964, loss 0.218743, acc 0.890625\n",
      "2018-05-23T13:34:33.066823: step 9965, loss 0.0625305, acc 0.984375\n",
      "2018-05-23T13:34:33.511189: step 9966, loss 0.133319, acc 0.96875\n",
      "2018-05-23T13:34:33.959346: step 9967, loss 0.138575, acc 0.953125\n",
      "2018-05-23T13:34:34.414328: step 9968, loss 0.121643, acc 0.921875\n",
      "2018-05-23T13:34:34.877758: step 9969, loss 0.148838, acc 0.921875\n",
      "2018-05-23T13:34:35.337596: step 9970, loss 0.247194, acc 0.890625\n",
      "2018-05-23T13:34:35.770513: step 9971, loss 0.120053, acc 0.953125\n",
      "2018-05-23T13:34:36.226301: step 9972, loss 0.145102, acc 0.9375\n",
      "2018-05-23T13:34:36.754295: step 9973, loss 0.244865, acc 0.90625\n",
      "2018-05-23T13:34:37.253512: step 9974, loss 0.155539, acc 0.921875\n",
      "2018-05-23T13:34:37.705370: step 9975, loss 0.219736, acc 0.890625\n",
      "2018-05-23T13:34:38.261404: step 9976, loss 0.145932, acc 0.9375\n",
      "2018-05-23T13:34:38.807324: step 9977, loss 0.140094, acc 0.9375\n",
      "2018-05-23T13:34:39.306240: step 9978, loss 0.197641, acc 0.90625\n",
      "2018-05-23T13:34:39.775824: step 9979, loss 0.106424, acc 0.953125\n",
      "2018-05-23T13:34:40.314834: step 9980, loss 0.155708, acc 0.96875\n",
      "2018-05-23T13:34:40.812180: step 9981, loss 0.228139, acc 0.890625\n",
      "2018-05-23T13:34:41.270746: step 9982, loss 0.182352, acc 0.921875\n",
      "2018-05-23T13:34:41.749543: step 9983, loss 0.143022, acc 0.890625\n",
      "2018-05-23T13:34:42.284379: step 9984, loss 0.120006, acc 0.953125\n",
      "2018-05-23T13:34:42.888224: step 9985, loss 0.0846745, acc 0.984375\n",
      "2018-05-23T13:34:43.445222: step 9986, loss 0.13928, acc 0.96875\n",
      "2018-05-23T13:34:43.999668: step 9987, loss 0.170847, acc 0.953125\n",
      "2018-05-23T13:34:44.521481: step 9988, loss 0.176694, acc 0.921875\n",
      "2018-05-23T13:34:45.018964: step 9989, loss 0.103448, acc 0.96875\n",
      "2018-05-23T13:34:45.539162: step 9990, loss 0.324256, acc 0.890625\n",
      "2018-05-23T13:34:46.016700: step 9991, loss 0.141316, acc 0.90625\n",
      "2018-05-23T13:34:46.519996: step 9992, loss 0.19535, acc 0.90625\n",
      "2018-05-23T13:34:46.977083: step 9993, loss 0.10343, acc 0.96875\n",
      "2018-05-23T13:34:47.457954: step 9994, loss 0.182527, acc 0.921875\n",
      "2018-05-23T13:34:47.976540: step 9995, loss 0.185775, acc 0.9375\n",
      "2018-05-23T13:34:48.455426: step 9996, loss 0.176692, acc 0.953125\n",
      "2018-05-23T13:34:48.912420: step 9997, loss 0.146836, acc 0.953125\n",
      "2018-05-23T13:34:49.374739: step 9998, loss 0.221136, acc 0.90625\n",
      "2018-05-23T13:34:49.841544: step 9999, loss 0.216695, acc 0.890625\n",
      "2018-05-23T13:34:50.395232: step 10000, loss 0.165148, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:34:56.369208: step 10000, loss 1.01533, acc 0.720532\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10000\n",
      "\n",
      "2018-05-23T13:34:58.612233: step 10001, loss 0.139896, acc 0.921875\n",
      "2018-05-23T13:34:59.262064: step 10002, loss 0.157666, acc 0.90625\n",
      "2018-05-23T13:34:59.758414: step 10003, loss 0.0840662, acc 0.984375\n",
      "2018-05-23T13:35:00.444940: step 10004, loss 0.116425, acc 0.953125\n",
      "2018-05-23T13:35:01.067098: step 10005, loss 0.16031, acc 0.890625\n",
      "2018-05-23T13:35:01.669487: step 10006, loss 0.0892018, acc 0.96875\n",
      "2018-05-23T13:35:02.286716: step 10007, loss 0.185719, acc 0.875\n",
      "2018-05-23T13:35:02.839515: step 10008, loss 0.15404, acc 0.9375\n",
      "2018-05-23T13:35:03.365380: step 10009, loss 0.0693217, acc 0.96875\n",
      "2018-05-23T13:35:03.832073: step 10010, loss 0.254283, acc 0.9375\n",
      "2018-05-23T13:35:04.346726: step 10011, loss 0.0882541, acc 0.953125\n",
      "2018-05-23T13:35:04.880101: step 10012, loss 0.155769, acc 0.90625\n",
      "2018-05-23T13:35:05.385013: step 10013, loss 0.22695, acc 0.890625\n",
      "2018-05-23T13:35:05.869799: step 10014, loss 0.160732, acc 0.890625\n",
      "2018-05-23T13:35:06.346223: step 10015, loss 0.157825, acc 0.9375\n",
      "2018-05-23T13:35:06.838964: step 10016, loss 0.206818, acc 0.921875\n",
      "2018-05-23T13:35:07.342530: step 10017, loss 0.11344, acc 0.9375\n",
      "2018-05-23T13:35:07.803183: step 10018, loss 0.18621, acc 0.953125\n",
      "2018-05-23T13:35:08.374444: step 10019, loss 0.144028, acc 0.953125\n",
      "2018-05-23T13:35:08.916254: step 10020, loss 0.234496, acc 0.890625\n",
      "2018-05-23T13:35:09.389277: step 10021, loss 0.247221, acc 0.90625\n",
      "2018-05-23T13:35:09.791666: step 10022, loss 0.180025, acc 0.953125\n",
      "2018-05-23T13:35:10.215927: step 10023, loss 0.211061, acc 0.90625\n",
      "2018-05-23T13:35:10.605170: step 10024, loss 0.240379, acc 0.890625\n",
      "2018-05-23T13:35:11.042228: step 10025, loss 0.126948, acc 0.953125\n",
      "2018-05-23T13:35:11.514258: step 10026, loss 0.223874, acc 0.890625\n",
      "2018-05-23T13:35:11.974552: step 10027, loss 0.222341, acc 0.921875\n",
      "2018-05-23T13:35:12.392532: step 10028, loss 0.128299, acc 0.9375\n",
      "2018-05-23T13:35:12.849790: step 10029, loss 0.142641, acc 0.953125\n",
      "2018-05-23T13:35:13.250314: step 10030, loss 0.132348, acc 0.96875\n",
      "2018-05-23T13:35:13.672283: step 10031, loss 0.119255, acc 0.953125\n",
      "2018-05-23T13:35:14.072311: step 10032, loss 0.0801886, acc 0.96875\n",
      "2018-05-23T13:35:14.454375: step 10033, loss 0.115556, acc 0.96875\n",
      "2018-05-23T13:35:14.845385: step 10034, loss 0.127989, acc 0.9375\n",
      "2018-05-23T13:35:15.242069: step 10035, loss 0.161104, acc 0.921875\n",
      "2018-05-23T13:35:15.621173: step 10036, loss 0.156391, acc 0.921875\n",
      "2018-05-23T13:35:16.031634: step 10037, loss 0.236434, acc 0.890625\n",
      "2018-05-23T13:35:16.421253: step 10038, loss 0.156665, acc 0.921875\n",
      "2018-05-23T13:35:16.812868: step 10039, loss 0.103936, acc 0.953125\n",
      "2018-05-23T13:35:17.208398: step 10040, loss 0.2043, acc 0.953125\n",
      "2018-05-23T13:35:17.663476: step 10041, loss 0.220387, acc 0.90625\n",
      "2018-05-23T13:35:18.127140: step 10042, loss 0.208408, acc 0.890625\n",
      "2018-05-23T13:35:18.527417: step 10043, loss 0.152463, acc 0.921875\n",
      "2018-05-23T13:35:18.907766: step 10044, loss 0.0933611, acc 0.96875\n",
      "2018-05-23T13:35:19.304475: step 10045, loss 0.14359, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:35:19.685688: step 10046, loss 0.116919, acc 0.96875\n",
      "2018-05-23T13:35:20.221339: step 10047, loss 0.199061, acc 0.90625\n",
      "2018-05-23T13:35:20.606367: step 10048, loss 0.259657, acc 0.890625\n",
      "2018-05-23T13:35:21.035260: step 10049, loss 0.160597, acc 0.9375\n",
      "2018-05-23T13:35:21.444735: step 10050, loss 0.120545, acc 0.953125\n",
      "2018-05-23T13:35:21.873141: step 10051, loss 0.177688, acc 0.953125\n",
      "2018-05-23T13:35:22.424259: step 10052, loss 0.122955, acc 0.921875\n",
      "2018-05-23T13:35:22.841198: step 10053, loss 0.123773, acc 0.953125\n",
      "2018-05-23T13:35:23.273301: step 10054, loss 0.172192, acc 0.953125\n",
      "2018-05-23T13:35:23.661950: step 10055, loss 0.180956, acc 0.921875\n",
      "2018-05-23T13:35:24.077043: step 10056, loss 0.141862, acc 0.9375\n",
      "2018-05-23T13:35:24.498410: step 10057, loss 0.193939, acc 0.9375\n",
      "2018-05-23T13:35:24.942188: step 10058, loss 0.14878, acc 0.9375\n",
      "2018-05-23T13:35:25.336467: step 10059, loss 0.13241, acc 0.984375\n",
      "2018-05-23T13:35:25.723714: step 10060, loss 0.184094, acc 0.90625\n",
      "2018-05-23T13:35:26.116142: step 10061, loss 0.0748478, acc 0.984375\n",
      "2018-05-23T13:35:26.511775: step 10062, loss 0.223593, acc 0.890625\n",
      "2018-05-23T13:35:26.895083: step 10063, loss 0.175949, acc 0.890625\n",
      "2018-05-23T13:35:27.299982: step 10064, loss 0.227019, acc 0.90625\n",
      "2018-05-23T13:35:27.728628: step 10065, loss 0.139447, acc 0.953125\n",
      "2018-05-23T13:35:28.137208: step 10066, loss 0.134205, acc 0.921875\n",
      "2018-05-23T13:35:28.531560: step 10067, loss 0.224552, acc 0.90625\n",
      "2018-05-23T13:35:28.910537: step 10068, loss 0.20533, acc 0.921875\n",
      "2018-05-23T13:35:29.300057: step 10069, loss 0.17386, acc 0.921875\n",
      "2018-05-23T13:35:29.693682: step 10070, loss 0.106581, acc 0.96875\n",
      "2018-05-23T13:35:30.075239: step 10071, loss 0.180268, acc 0.9375\n",
      "2018-05-23T13:35:30.468492: step 10072, loss 0.179395, acc 0.9375\n",
      "2018-05-23T13:35:30.855973: step 10073, loss 0.318004, acc 0.859375\n",
      "2018-05-23T13:35:31.253157: step 10074, loss 0.0878772, acc 0.96875\n",
      "2018-05-23T13:35:31.650361: step 10075, loss 0.132943, acc 0.9375\n",
      "2018-05-23T13:35:32.039372: step 10076, loss 0.16044, acc 0.953125\n",
      "2018-05-23T13:35:32.495988: step 10077, loss 0.28481, acc 0.890625\n",
      "2018-05-23T13:35:32.884130: step 10078, loss 0.130008, acc 0.9375\n",
      "2018-05-23T13:35:33.282876: step 10079, loss 0.0958665, acc 0.96875\n",
      "2018-05-23T13:35:33.677480: step 10080, loss 0.119793, acc 0.953125\n",
      "2018-05-23T13:35:34.080027: step 10081, loss 0.18326, acc 0.921875\n",
      "2018-05-23T13:35:34.470625: step 10082, loss 0.162532, acc 0.9375\n",
      "2018-05-23T13:35:34.937962: step 10083, loss 0.0935544, acc 0.953125\n",
      "2018-05-23T13:35:35.371990: step 10084, loss 0.320097, acc 0.953125\n",
      "2018-05-23T13:35:35.803936: step 10085, loss 0.143616, acc 0.9375\n",
      "2018-05-23T13:35:36.213543: step 10086, loss 0.295507, acc 0.875\n",
      "2018-05-23T13:35:36.660746: step 10087, loss 0.119413, acc 0.96875\n",
      "2018-05-23T13:35:37.074625: step 10088, loss 0.130441, acc 0.953125\n",
      "2018-05-23T13:35:37.465410: step 10089, loss 0.257859, acc 0.90625\n",
      "2018-05-23T13:35:37.896764: step 10090, loss 0.10142, acc 0.953125\n",
      "2018-05-23T13:35:38.312219: step 10091, loss 0.223159, acc 0.875\n",
      "2018-05-23T13:35:38.762323: step 10092, loss 0.223089, acc 0.890625\n",
      "2018-05-23T13:35:39.192325: step 10093, loss 0.281967, acc 0.90625\n",
      "2018-05-23T13:35:39.630357: step 10094, loss 0.21191, acc 0.875\n",
      "2018-05-23T13:35:40.071568: step 10095, loss 0.199011, acc 0.90625\n",
      "2018-05-23T13:35:40.470032: step 10096, loss 0.17801, acc 0.90625\n",
      "2018-05-23T13:35:40.878076: step 10097, loss 0.141682, acc 0.921875\n",
      "2018-05-23T13:35:41.269869: step 10098, loss 0.0994239, acc 0.953125\n",
      "2018-05-23T13:35:41.658315: step 10099, loss 0.151243, acc 0.921875\n",
      "2018-05-23T13:35:42.048940: step 10100, loss 0.107635, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:35:47.643779: step 10100, loss 1.03211, acc 0.715531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10100\n",
      "\n",
      "2018-05-23T13:35:49.386979: step 10101, loss 0.152886, acc 0.890625\n",
      "2018-05-23T13:35:49.795287: step 10102, loss 0.1189, acc 0.953125\n",
      "2018-05-23T13:35:50.191163: step 10103, loss 0.21183, acc 0.90625\n",
      "2018-05-23T13:35:50.589543: step 10104, loss 0.134064, acc 0.9375\n",
      "2018-05-23T13:35:50.992299: step 10105, loss 0.203186, acc 0.859375\n",
      "2018-05-23T13:35:51.380261: step 10106, loss 0.0795834, acc 0.984375\n",
      "2018-05-23T13:35:51.815105: step 10107, loss 0.0651146, acc 0.984375\n",
      "2018-05-23T13:35:52.215169: step 10108, loss 0.089158, acc 0.984375\n",
      "2018-05-23T13:35:52.675412: step 10109, loss 0.205515, acc 0.9375\n",
      "2018-05-23T13:35:53.071112: step 10110, loss 0.150907, acc 0.953125\n",
      "2018-05-23T13:35:53.456283: step 10111, loss 0.106523, acc 0.953125\n",
      "2018-05-23T13:35:53.865160: step 10112, loss 0.11713, acc 0.96875\n",
      "2018-05-23T13:35:54.292182: step 10113, loss 0.0853654, acc 0.96875\n",
      "2018-05-23T13:35:54.722757: step 10114, loss 0.296396, acc 0.9375\n",
      "2018-05-23T13:35:55.175774: step 10115, loss 0.198055, acc 0.90625\n",
      "2018-05-23T13:35:55.607381: step 10116, loss 0.149905, acc 0.9375\n",
      "2018-05-23T13:35:55.990100: step 10117, loss 0.218883, acc 0.90625\n",
      "2018-05-23T13:35:56.435572: step 10118, loss 0.114704, acc 0.953125\n",
      "2018-05-23T13:35:56.822445: step 10119, loss 0.145816, acc 0.953125\n",
      "2018-05-23T13:35:57.269061: step 10120, loss 0.0675511, acc 0.984375\n",
      "2018-05-23T13:35:57.671253: step 10121, loss 0.148008, acc 0.921875\n",
      "2018-05-23T13:35:58.059600: step 10122, loss 0.0913724, acc 0.953125\n",
      "2018-05-23T13:35:58.444520: step 10123, loss 0.129406, acc 0.9375\n",
      "2018-05-23T13:35:58.836033: step 10124, loss 0.134218, acc 0.9375\n",
      "2018-05-23T13:35:59.236029: step 10125, loss 0.195744, acc 0.9375\n",
      "2018-05-23T13:35:59.613085: step 10126, loss 0.228067, acc 0.90625\n",
      "2018-05-23T13:36:00.019925: step 10127, loss 0.289878, acc 0.890625\n",
      "2018-05-23T13:36:00.416893: step 10128, loss 0.188686, acc 0.921875\n",
      "2018-05-23T13:36:00.892898: step 10129, loss 0.137616, acc 0.921875\n",
      "2018-05-23T13:36:01.492488: step 10130, loss 0.179191, acc 0.90625\n",
      "2018-05-23T13:36:01.936311: step 10131, loss 0.181326, acc 0.921875\n",
      "2018-05-23T13:36:02.361234: step 10132, loss 0.255117, acc 0.90625\n",
      "2018-05-23T13:36:02.816031: step 10133, loss 0.0807917, acc 0.96875\n",
      "2018-05-23T13:36:03.221962: step 10134, loss 0.0800691, acc 0.984375\n",
      "2018-05-23T13:36:03.615069: step 10135, loss 0.160611, acc 0.9375\n",
      "2018-05-23T13:36:04.058305: step 10136, loss 0.219809, acc 0.921875\n",
      "2018-05-23T13:36:04.465219: step 10137, loss 0.0826493, acc 0.953125\n",
      "2018-05-23T13:36:04.939896: step 10138, loss 0.136343, acc 0.921875\n",
      "2018-05-23T13:36:05.340780: step 10139, loss 0.252649, acc 0.890625\n",
      "2018-05-23T13:36:05.735182: step 10140, loss 0.134127, acc 0.96875\n",
      "2018-05-23T13:36:06.133119: step 10141, loss 0.13959, acc 0.9375\n",
      "2018-05-23T13:36:06.525811: step 10142, loss 0.167065, acc 0.9375\n",
      "2018-05-23T13:36:06.927334: step 10143, loss 0.178903, acc 0.921875\n",
      "2018-05-23T13:36:07.346222: step 10144, loss 0.185171, acc 0.921875\n",
      "2018-05-23T13:36:07.735195: step 10145, loss 0.220288, acc 0.890625\n",
      "2018-05-23T13:36:08.123197: step 10146, loss 0.229563, acc 0.921875\n",
      "2018-05-23T13:36:08.516299: step 10147, loss 0.237039, acc 0.890625\n",
      "2018-05-23T13:36:08.899338: step 10148, loss 0.207386, acc 0.890625\n",
      "2018-05-23T13:36:09.308878: step 10149, loss 0.204106, acc 0.953125\n",
      "2018-05-23T13:36:09.692362: step 10150, loss 0.0843869, acc 0.953125\n",
      "2018-05-23T13:36:10.073859: step 10151, loss 0.145095, acc 0.9375\n",
      "2018-05-23T13:36:10.478284: step 10152, loss 0.125285, acc 0.953125\n",
      "2018-05-23T13:36:10.863273: step 10153, loss 0.128677, acc 0.921875\n",
      "2018-05-23T13:36:11.265219: step 10154, loss 0.135973, acc 0.9375\n",
      "2018-05-23T13:36:11.650886: step 10155, loss 0.0894836, acc 0.984375\n",
      "2018-05-23T13:36:12.043841: step 10156, loss 0.238123, acc 0.9375\n",
      "2018-05-23T13:36:12.437788: step 10157, loss 0.15378, acc 0.921875\n",
      "2018-05-23T13:36:12.822201: step 10158, loss 0.131808, acc 0.953125\n",
      "2018-05-23T13:36:13.209706: step 10159, loss 0.408906, acc 0.828125\n",
      "2018-05-23T13:36:13.657016: step 10160, loss 0.255665, acc 0.890625\n",
      "2018-05-23T13:36:14.058950: step 10161, loss 0.160501, acc 0.890625\n",
      "2018-05-23T13:36:14.448929: step 10162, loss 0.296382, acc 0.84375\n",
      "2018-05-23T13:36:14.842880: step 10163, loss 0.141229, acc 0.9375\n",
      "2018-05-23T13:36:15.241841: step 10164, loss 0.20968, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:36:15.638041: step 10165, loss 0.118323, acc 0.96875\n",
      "2018-05-23T13:36:16.029027: step 10166, loss 0.22623, acc 0.875\n",
      "2018-05-23T13:36:16.435757: step 10167, loss 0.164531, acc 0.90625\n",
      "2018-05-23T13:36:16.836690: step 10168, loss 0.178561, acc 0.9375\n",
      "2018-05-23T13:36:17.229131: step 10169, loss 0.176403, acc 0.890625\n",
      "2018-05-23T13:36:17.647913: step 10170, loss 0.232543, acc 0.90625\n",
      "2018-05-23T13:36:18.087735: step 10171, loss 0.128586, acc 0.9375\n",
      "2018-05-23T13:36:18.548020: step 10172, loss 0.152055, acc 0.921875\n",
      "2018-05-23T13:36:18.985893: step 10173, loss 0.0950202, acc 1\n",
      "2018-05-23T13:36:19.431176: step 10174, loss 0.0944777, acc 0.953125\n",
      "2018-05-23T13:36:19.865051: step 10175, loss 0.0935387, acc 0.96875\n",
      "2018-05-23T13:36:20.254088: step 10176, loss 0.191137, acc 0.9375\n",
      "2018-05-23T13:36:20.685896: step 10177, loss 0.229662, acc 0.90625\n",
      "2018-05-23T13:36:21.115276: step 10178, loss 0.154063, acc 0.96875\n",
      "2018-05-23T13:36:21.579501: step 10179, loss 0.236337, acc 0.9375\n",
      "2018-05-23T13:36:22.023394: step 10180, loss 0.111126, acc 0.96875\n",
      "2018-05-23T13:36:22.436838: step 10181, loss 0.175533, acc 0.9375\n",
      "2018-05-23T13:36:22.886743: step 10182, loss 0.109136, acc 0.953125\n",
      "2018-05-23T13:36:23.281063: step 10183, loss 0.188041, acc 0.921875\n",
      "2018-05-23T13:36:23.702159: step 10184, loss 0.142164, acc 0.953125\n",
      "2018-05-23T13:36:24.130742: step 10185, loss 0.175898, acc 0.921875\n",
      "2018-05-23T13:36:24.564232: step 10186, loss 0.12987, acc 0.953125\n",
      "2018-05-23T13:36:24.969718: step 10187, loss 0.24667, acc 0.859375\n",
      "2018-05-23T13:36:25.380157: step 10188, loss 0.0558834, acc 0.96875\n",
      "2018-05-23T13:36:25.775182: step 10189, loss 0.139052, acc 0.953125\n",
      "2018-05-23T13:36:26.228902: step 10190, loss 0.203073, acc 0.921875\n",
      "2018-05-23T13:36:26.619887: step 10191, loss 0.377733, acc 0.90625\n",
      "2018-05-23T13:36:27.014991: step 10192, loss 0.0736197, acc 0.984375\n",
      "2018-05-23T13:36:27.408861: step 10193, loss 0.0836413, acc 0.953125\n",
      "2018-05-23T13:36:27.801944: step 10194, loss 0.113108, acc 0.953125\n",
      "2018-05-23T13:36:28.204478: step 10195, loss 0.157804, acc 0.9375\n",
      "2018-05-23T13:36:28.673477: step 10196, loss 0.0802019, acc 0.984375\n",
      "2018-05-23T13:36:29.083404: step 10197, loss 0.0839558, acc 0.96875\n",
      "2018-05-23T13:36:29.481519: step 10198, loss 0.123018, acc 0.9375\n",
      "2018-05-23T13:36:29.879939: step 10199, loss 0.38199, acc 0.875\n",
      "2018-05-23T13:36:30.284907: step 10200, loss 0.077758, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:36:36.347662: step 10200, loss 1.05483, acc 0.717531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10200\n",
      "\n",
      "2018-05-23T13:36:38.410854: step 10201, loss 0.156581, acc 0.9375\n",
      "2018-05-23T13:36:38.851060: step 10202, loss 0.161133, acc 0.96875\n",
      "2018-05-23T13:36:39.323760: step 10203, loss 0.235448, acc 0.890625\n",
      "2018-05-23T13:36:39.766154: step 10204, loss 0.125863, acc 0.953125\n",
      "2018-05-23T13:36:40.212330: step 10205, loss 0.256597, acc 0.890625\n",
      "2018-05-23T13:36:40.685862: step 10206, loss 0.213164, acc 0.875\n",
      "2018-05-23T13:36:41.118138: step 10207, loss 0.135569, acc 0.953125\n",
      "2018-05-23T13:36:41.585718: step 10208, loss 0.236571, acc 0.90625\n",
      "2018-05-23T13:36:42.006498: step 10209, loss 0.152808, acc 0.90625\n",
      "2018-05-23T13:36:42.418008: step 10210, loss 0.127146, acc 0.921875\n",
      "2018-05-23T13:36:42.872359: step 10211, loss 0.0994426, acc 0.96875\n",
      "2018-05-23T13:36:43.260361: step 10212, loss 0.164552, acc 0.90625\n",
      "2018-05-23T13:36:43.672026: step 10213, loss 0.134491, acc 0.953125\n",
      "2018-05-23T13:36:44.086088: step 10214, loss 0.137805, acc 0.9375\n",
      "2018-05-23T13:36:44.484889: step 10215, loss 0.200155, acc 0.90625\n",
      "2018-05-23T13:36:44.895856: step 10216, loss 0.0764664, acc 0.984375\n",
      "2018-05-23T13:36:45.299193: step 10217, loss 0.225821, acc 0.9375\n",
      "2018-05-23T13:36:45.693844: step 10218, loss 0.10422, acc 0.953125\n",
      "2018-05-23T13:36:46.094787: step 10219, loss 0.179389, acc 0.90625\n",
      "2018-05-23T13:36:46.489275: step 10220, loss 0.191482, acc 0.9375\n",
      "2018-05-23T13:36:46.887736: step 10221, loss 0.208509, acc 0.890625\n",
      "2018-05-23T13:36:47.274672: step 10222, loss 0.209626, acc 0.875\n",
      "2018-05-23T13:36:47.661729: step 10223, loss 0.269033, acc 0.859375\n",
      "2018-05-23T13:36:48.093157: step 10224, loss 0.0846068, acc 0.984375\n",
      "2018-05-23T13:36:48.492011: step 10225, loss 0.28126, acc 0.921875\n",
      "2018-05-23T13:36:48.880190: step 10226, loss 0.227451, acc 0.921875\n",
      "2018-05-23T13:36:49.284188: step 10227, loss 0.105874, acc 0.96875\n",
      "2018-05-23T13:36:49.670665: step 10228, loss 0.0981306, acc 0.953125\n",
      "2018-05-23T13:36:50.090639: step 10229, loss 0.125645, acc 0.9375\n",
      "2018-05-23T13:36:50.484772: step 10230, loss 0.17239, acc 0.90625\n",
      "2018-05-23T13:36:50.872804: step 10231, loss 0.152155, acc 0.921875\n",
      "2018-05-23T13:36:51.269979: step 10232, loss 0.15991, acc 0.921875\n",
      "2018-05-23T13:36:51.659067: step 10233, loss 0.227572, acc 0.9375\n",
      "2018-05-23T13:36:52.046635: step 10234, loss 0.101626, acc 0.96875\n",
      "2018-05-23T13:36:52.523885: step 10235, loss 0.157951, acc 0.921875\n",
      "2018-05-23T13:36:52.912500: step 10236, loss 0.200371, acc 0.921875\n",
      "2018-05-23T13:36:53.308528: step 10237, loss 0.154641, acc 0.9375\n",
      "2018-05-23T13:36:53.701311: step 10238, loss 0.172409, acc 0.875\n",
      "2018-05-23T13:36:54.102470: step 10239, loss 0.153415, acc 0.953125\n",
      "2018-05-23T13:36:54.510446: step 10240, loss 0.130705, acc 0.921875\n",
      "2018-05-23T13:36:54.962197: step 10241, loss 0.0595263, acc 0.984375\n",
      "2018-05-23T13:36:55.368760: step 10242, loss 0.171819, acc 0.9375\n",
      "2018-05-23T13:36:55.775225: step 10243, loss 0.0797155, acc 0.96875\n",
      "2018-05-23T13:36:56.169793: step 10244, loss 0.102035, acc 0.984375\n",
      "2018-05-23T13:36:56.572435: step 10245, loss 0.112619, acc 0.953125\n",
      "2018-05-23T13:36:57.145431: step 10246, loss 0.166809, acc 0.90625\n",
      "2018-05-23T13:36:57.646777: step 10247, loss 0.194412, acc 0.90625\n",
      "2018-05-23T13:36:58.133165: step 10248, loss 0.159174, acc 0.9375\n",
      "2018-05-23T13:36:58.647674: step 10249, loss 0.088573, acc 0.96875\n",
      "2018-05-23T13:36:59.075530: step 10250, loss 0.252781, acc 0.921875\n",
      "2018-05-23T13:36:59.602519: step 10251, loss 0.20864, acc 0.921875\n",
      "2018-05-23T13:37:00.076781: step 10252, loss 0.121197, acc 0.953125\n",
      "2018-05-23T13:37:00.588412: step 10253, loss 0.132292, acc 0.9375\n",
      "2018-05-23T13:37:00.991793: step 10254, loss 0.0472991, acc 1\n",
      "2018-05-23T13:37:01.407210: step 10255, loss 0.100944, acc 0.96875\n",
      "2018-05-23T13:37:01.824153: step 10256, loss 0.148591, acc 0.921875\n",
      "2018-05-23T13:37:02.233026: step 10257, loss 0.17405, acc 0.96875\n",
      "2018-05-23T13:37:02.756642: step 10258, loss 0.175798, acc 0.90625\n",
      "2018-05-23T13:37:03.156585: step 10259, loss 0.115019, acc 0.953125\n",
      "2018-05-23T13:37:03.552890: step 10260, loss 0.21343, acc 0.90625\n",
      "2018-05-23T13:37:03.942864: step 10261, loss 0.203007, acc 0.921875\n",
      "2018-05-23T13:37:04.367723: step 10262, loss 0.169316, acc 0.90625\n",
      "2018-05-23T13:37:04.765176: step 10263, loss 0.204958, acc 0.90625\n",
      "2018-05-23T13:37:05.197915: step 10264, loss 0.178883, acc 0.921875\n",
      "2018-05-23T13:37:05.584880: step 10265, loss 0.173776, acc 0.9375\n",
      "2018-05-23T13:37:05.976588: step 10266, loss 0.152447, acc 0.96875\n",
      "2018-05-23T13:37:06.371552: step 10267, loss 0.172895, acc 0.90625\n",
      "2018-05-23T13:37:06.907060: step 10268, loss 0.116694, acc 0.953125\n",
      "2018-05-23T13:37:07.457124: step 10269, loss 0.207321, acc 0.859375\n",
      "2018-05-23T13:37:07.864036: step 10270, loss 0.109842, acc 0.96875\n",
      "2018-05-23T13:37:08.275008: step 10271, loss 0.151847, acc 0.9375\n",
      "2018-05-23T13:37:08.678004: step 10272, loss 0.130364, acc 0.953125\n",
      "2018-05-23T13:37:09.189688: step 10273, loss 0.112435, acc 0.953125\n",
      "2018-05-23T13:37:09.633071: step 10274, loss 0.1602, acc 0.953125\n",
      "2018-05-23T13:37:10.073029: step 10275, loss 0.275782, acc 0.890625\n",
      "2018-05-23T13:37:10.640022: step 10276, loss 0.0987671, acc 0.9375\n",
      "2018-05-23T13:37:11.150656: step 10277, loss 0.247587, acc 0.90625\n",
      "2018-05-23T13:37:11.566101: step 10278, loss 0.174945, acc 0.9375\n",
      "2018-05-23T13:37:11.974523: step 10279, loss 0.243783, acc 0.859375\n",
      "2018-05-23T13:37:12.391430: step 10280, loss 0.227501, acc 0.90625\n",
      "2018-05-23T13:37:12.813666: step 10281, loss 0.166448, acc 0.921875\n",
      "2018-05-23T13:37:13.290100: step 10282, loss 0.182726, acc 0.953125\n",
      "2018-05-23T13:37:13.740931: step 10283, loss 0.138317, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:37:14.138534: step 10284, loss 0.144947, acc 0.9375\n",
      "2018-05-23T13:37:14.528558: step 10285, loss 0.0997631, acc 0.9375\n",
      "2018-05-23T13:37:14.974442: step 10286, loss 0.131035, acc 0.953125\n",
      "2018-05-23T13:37:15.369438: step 10287, loss 0.157626, acc 0.953125\n",
      "2018-05-23T13:37:15.760518: step 10288, loss 0.193667, acc 0.890625\n",
      "2018-05-23T13:37:16.164766: step 10289, loss 0.123532, acc 0.953125\n",
      "2018-05-23T13:37:16.610622: step 10290, loss 0.121194, acc 0.96875\n",
      "2018-05-23T13:37:17.017489: step 10291, loss 0.098778, acc 0.96875\n",
      "2018-05-23T13:37:17.429329: step 10292, loss 0.0523598, acc 0.984375\n",
      "2018-05-23T13:37:17.934045: step 10293, loss 0.103191, acc 0.96875\n",
      "2018-05-23T13:37:18.430308: step 10294, loss 0.117224, acc 0.953125\n",
      "2018-05-23T13:37:18.851202: step 10295, loss 0.210617, acc 0.890625\n",
      "2018-05-23T13:37:19.262688: step 10296, loss 0.127905, acc 0.953125\n",
      "2018-05-23T13:37:19.668630: step 10297, loss 0.137175, acc 0.9375\n",
      "2018-05-23T13:37:20.130910: step 10298, loss 0.177944, acc 0.96875\n",
      "2018-05-23T13:37:20.536301: step 10299, loss 0.133464, acc 0.96875\n",
      "2018-05-23T13:37:20.994601: step 10300, loss 0.127455, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:37:26.548762: step 10300, loss 1.05572, acc 0.71996\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10300\n",
      "\n",
      "2018-05-23T13:37:28.229775: step 10301, loss 0.0690762, acc 0.96875\n",
      "2018-05-23T13:37:28.713873: step 10302, loss 0.1372, acc 0.953125\n",
      "2018-05-23T13:37:29.189702: step 10303, loss 0.312639, acc 0.875\n",
      "2018-05-23T13:37:29.698344: step 10304, loss 0.101064, acc 0.984375\n",
      "2018-05-23T13:37:30.114231: step 10305, loss 0.272377, acc 0.859375\n",
      "2018-05-23T13:37:30.532146: step 10306, loss 0.148325, acc 0.953125\n",
      "2018-05-23T13:37:30.943109: step 10307, loss 0.165067, acc 0.953125\n",
      "2018-05-23T13:37:31.334665: step 10308, loss 0.271329, acc 0.875\n",
      "2018-05-23T13:37:31.857808: step 10309, loss 0.139991, acc 0.921875\n",
      "2018-05-23T13:37:32.272734: step 10310, loss 0.147055, acc 0.953125\n",
      "2018-05-23T13:37:32.662697: step 10311, loss 0.294484, acc 0.921875\n",
      "2018-05-23T13:37:33.083674: step 10312, loss 0.204335, acc 0.890625\n",
      "2018-05-23T13:37:33.478079: step 10313, loss 0.147352, acc 0.9375\n",
      "2018-05-23T13:37:33.920094: step 10314, loss 0.262505, acc 0.9375\n",
      "2018-05-23T13:37:34.327535: step 10315, loss 0.163005, acc 0.90625\n",
      "2018-05-23T13:37:34.715058: step 10316, loss 0.245863, acc 0.90625\n",
      "2018-05-23T13:37:35.139559: step 10317, loss 0.130039, acc 0.953125\n",
      "2018-05-23T13:37:35.532459: step 10318, loss 0.0586756, acc 0.984375\n",
      "2018-05-23T13:37:35.922083: step 10319, loss 0.166249, acc 0.921875\n",
      "2018-05-23T13:37:36.322126: step 10320, loss 0.144547, acc 0.953125\n",
      "2018-05-23T13:37:36.706748: step 10321, loss 0.255212, acc 0.890625\n",
      "2018-05-23T13:37:37.180820: step 10322, loss 0.112997, acc 0.96875\n",
      "2018-05-23T13:37:37.621472: step 10323, loss 0.210772, acc 0.9375\n",
      "2018-05-23T13:37:38.031908: step 10324, loss 0.202173, acc 0.890625\n",
      "2018-05-23T13:37:38.437160: step 10325, loss 0.177445, acc 0.921875\n",
      "2018-05-23T13:37:38.835990: step 10326, loss 0.124894, acc 0.96875\n",
      "2018-05-23T13:37:39.248572: step 10327, loss 0.118605, acc 0.96875\n",
      "2018-05-23T13:37:39.641887: step 10328, loss 0.105398, acc 0.9375\n",
      "2018-05-23T13:37:40.064887: step 10329, loss 0.133132, acc 0.96875\n",
      "2018-05-23T13:37:40.530972: step 10330, loss 0.115152, acc 0.9375\n",
      "2018-05-23T13:37:41.145948: step 10331, loss 0.287551, acc 0.84375\n",
      "2018-05-23T13:37:41.841028: step 10332, loss 0.0992277, acc 1\n",
      "2018-05-23T13:37:42.684603: step 10333, loss 0.165041, acc 0.9375\n",
      "2018-05-23T13:37:43.276088: step 10334, loss 0.101991, acc 0.953125\n",
      "2018-05-23T13:37:43.834103: step 10335, loss 0.151988, acc 0.921875\n",
      "2018-05-23T13:37:44.319337: step 10336, loss 0.0961485, acc 0.96875\n",
      "2018-05-23T13:37:44.766663: step 10337, loss 0.120283, acc 0.953125\n",
      "2018-05-23T13:37:45.210492: step 10338, loss 0.0966179, acc 0.96875\n",
      "2018-05-23T13:37:45.628654: step 10339, loss 0.171817, acc 0.953125\n",
      "2018-05-23T13:37:46.047088: step 10340, loss 0.126327, acc 0.96875\n",
      "2018-05-23T13:37:46.556032: step 10341, loss 0.173136, acc 0.90625\n",
      "2018-05-23T13:37:46.969949: step 10342, loss 0.160368, acc 0.9375\n",
      "2018-05-23T13:37:47.435614: step 10343, loss 0.184583, acc 0.921875\n",
      "2018-05-23T13:37:47.839589: step 10344, loss 0.0882336, acc 0.96875\n",
      "2018-05-23T13:37:48.233534: step 10345, loss 0.114998, acc 0.953125\n",
      "2018-05-23T13:37:48.649032: step 10346, loss 0.0978408, acc 0.96875\n",
      "2018-05-23T13:37:49.114808: step 10347, loss 0.163189, acc 0.921875\n",
      "2018-05-23T13:37:49.560745: step 10348, loss 0.185263, acc 0.921875\n",
      "2018-05-23T13:37:50.002924: step 10349, loss 0.162269, acc 0.9375\n",
      "2018-05-23T13:37:50.399381: step 10350, loss 0.230596, acc 0.90625\n",
      "2018-05-23T13:37:50.829238: step 10351, loss 0.147046, acc 0.9375\n",
      "2018-05-23T13:37:51.229192: step 10352, loss 0.270132, acc 0.921875\n",
      "2018-05-23T13:37:51.636622: step 10353, loss 0.239439, acc 0.859375\n",
      "2018-05-23T13:37:52.085933: step 10354, loss 0.215663, acc 0.859375\n",
      "2018-05-23T13:37:52.610552: step 10355, loss 0.223074, acc 0.921875\n",
      "2018-05-23T13:37:53.105798: step 10356, loss 0.159318, acc 0.90625\n",
      "2018-05-23T13:37:53.514120: step 10357, loss 0.243026, acc 0.890625\n",
      "2018-05-23T13:37:54.010327: step 10358, loss 0.0847114, acc 0.953125\n",
      "2018-05-23T13:37:54.494981: step 10359, loss 0.196756, acc 0.9375\n",
      "2018-05-23T13:37:55.065455: step 10360, loss 0.265527, acc 0.875\n",
      "2018-05-23T13:37:55.471371: step 10361, loss 0.215834, acc 0.90625\n",
      "2018-05-23T13:37:55.890288: step 10362, loss 0.15657, acc 0.90625\n",
      "2018-05-23T13:37:56.301199: step 10363, loss 0.174073, acc 0.90625\n",
      "2018-05-23T13:37:56.703913: step 10364, loss 0.24107, acc 0.9375\n",
      "2018-05-23T13:37:57.114839: step 10365, loss 0.0722036, acc 0.96875\n",
      "2018-05-23T13:37:57.569644: step 10366, loss 0.105702, acc 0.953125\n",
      "2018-05-23T13:37:57.981054: step 10367, loss 0.157861, acc 0.96875\n",
      "2018-05-23T13:37:58.384491: step 10368, loss 0.177821, acc 0.90625\n",
      "2018-05-23T13:37:58.786926: step 10369, loss 0.112866, acc 0.96875\n",
      "2018-05-23T13:37:59.217771: step 10370, loss 0.197651, acc 0.90625\n",
      "2018-05-23T13:37:59.611727: step 10371, loss 0.149148, acc 0.953125\n",
      "2018-05-23T13:38:00.152299: step 10372, loss 0.209399, acc 0.890625\n",
      "2018-05-23T13:38:00.680736: step 10373, loss 0.0654913, acc 0.984375\n",
      "2018-05-23T13:38:01.190903: step 10374, loss 0.196336, acc 0.921875\n",
      "2018-05-23T13:38:01.667627: step 10375, loss 0.193253, acc 0.9375\n",
      "2018-05-23T13:38:02.090173: step 10376, loss 0.156493, acc 0.921875\n",
      "2018-05-23T13:38:02.498591: step 10377, loss 0.135034, acc 0.953125\n",
      "2018-05-23T13:38:02.913508: step 10378, loss 0.109283, acc 0.96875\n",
      "2018-05-23T13:38:03.326402: step 10379, loss 0.242206, acc 0.90625\n",
      "2018-05-23T13:38:03.722852: step 10380, loss 0.123109, acc 0.984375\n",
      "2018-05-23T13:38:04.113828: step 10381, loss 0.20213, acc 0.875\n",
      "2018-05-23T13:38:04.508024: step 10382, loss 0.114872, acc 0.96875\n",
      "2018-05-23T13:38:05.000236: step 10383, loss 0.135851, acc 0.9375\n",
      "2018-05-23T13:38:05.398032: step 10384, loss 0.231706, acc 0.90625\n",
      "2018-05-23T13:38:05.795612: step 10385, loss 0.332588, acc 0.859375\n",
      "2018-05-23T13:38:06.196836: step 10386, loss 0.196799, acc 0.921875\n",
      "2018-05-23T13:38:06.597279: step 10387, loss 0.170821, acc 0.90625\n",
      "2018-05-23T13:38:06.997214: step 10388, loss 0.168022, acc 0.96875\n",
      "2018-05-23T13:38:07.516376: step 10389, loss 0.141553, acc 0.921875\n",
      "2018-05-23T13:38:07.925280: step 10390, loss 0.121989, acc 0.96875\n",
      "2018-05-23T13:38:08.326230: step 10391, loss 0.271645, acc 0.890625\n",
      "2018-05-23T13:38:08.721689: step 10392, loss 0.133042, acc 0.984375\n",
      "2018-05-23T13:38:09.146552: step 10393, loss 0.0911578, acc 0.984375\n",
      "2018-05-23T13:38:09.581399: step 10394, loss 0.179295, acc 0.953125\n",
      "2018-05-23T13:38:09.985337: step 10395, loss 0.203144, acc 0.9375\n",
      "2018-05-23T13:38:10.387279: step 10396, loss 0.139694, acc 0.9375\n",
      "2018-05-23T13:38:11.072049: step 10397, loss 0.24486, acc 0.921875\n",
      "2018-05-23T13:38:11.549141: step 10398, loss 0.108302, acc 0.96875\n",
      "2018-05-23T13:38:11.961131: step 10399, loss 0.178712, acc 0.90625\n",
      "2018-05-23T13:38:12.392847: step 10400, loss 0.152881, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:38:18.359559: step 10400, loss 1.06352, acc 0.718817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10400\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:38:20.431706: step 10401, loss 0.241426, acc 0.921875\n",
      "2018-05-23T13:38:20.863941: step 10402, loss 0.14385, acc 0.9375\n",
      "2018-05-23T13:38:21.472099: step 10403, loss 0.109673, acc 0.9375\n",
      "2018-05-23T13:38:21.894996: step 10404, loss 0.135175, acc 0.953125\n",
      "2018-05-23T13:38:22.347531: step 10405, loss 0.159258, acc 0.953125\n",
      "2018-05-23T13:38:22.863877: step 10406, loss 0.17065, acc 0.953125\n",
      "2018-05-23T13:38:23.280397: step 10407, loss 0.265453, acc 0.90625\n",
      "2018-05-23T13:38:23.689424: step 10408, loss 0.119287, acc 0.921875\n",
      "2018-05-23T13:38:24.093985: step 10409, loss 0.215011, acc 0.9375\n",
      "2018-05-23T13:38:24.490962: step 10410, loss 0.223351, acc 0.96875\n",
      "2018-05-23T13:38:24.896501: step 10411, loss 0.111945, acc 0.9375\n",
      "2018-05-23T13:38:25.337955: step 10412, loss 0.118784, acc 0.921875\n",
      "2018-05-23T13:38:25.861821: step 10413, loss 0.159165, acc 0.921875\n",
      "2018-05-23T13:38:26.259940: step 10414, loss 0.19521, acc 0.890625\n",
      "2018-05-23T13:38:26.664942: step 10415, loss 0.174659, acc 0.90625\n",
      "2018-05-23T13:38:27.080754: step 10416, loss 0.0978726, acc 0.953125\n",
      "2018-05-23T13:38:27.545383: step 10417, loss 0.16597, acc 0.921875\n",
      "2018-05-23T13:38:27.946377: step 10418, loss 0.164849, acc 0.9375\n",
      "2018-05-23T13:38:28.421157: step 10419, loss 0.143118, acc 0.9375\n",
      "2018-05-23T13:38:28.815592: step 10420, loss 0.269487, acc 0.875\n",
      "2018-05-23T13:38:29.246973: step 10421, loss 0.172051, acc 0.921875\n",
      "2018-05-23T13:38:29.647924: step 10422, loss 0.113866, acc 0.953125\n",
      "2018-05-23T13:38:30.063608: step 10423, loss 0.210917, acc 0.890625\n",
      "2018-05-23T13:38:30.548043: step 10424, loss 0.17746, acc 0.921875\n",
      "2018-05-23T13:38:31.003941: step 10425, loss 0.131658, acc 0.921875\n",
      "2018-05-23T13:38:31.502717: step 10426, loss 0.0870783, acc 0.96875\n",
      "2018-05-23T13:38:31.980776: step 10427, loss 0.177952, acc 0.859375\n",
      "2018-05-23T13:38:32.395759: step 10428, loss 0.17651, acc 0.90625\n",
      "2018-05-23T13:38:32.920565: step 10429, loss 0.284077, acc 0.875\n",
      "2018-05-23T13:38:33.398874: step 10430, loss 0.0813898, acc 0.953125\n",
      "2018-05-23T13:38:33.852601: step 10431, loss 0.210368, acc 0.90625\n",
      "2018-05-23T13:38:34.286637: step 10432, loss 0.113636, acc 0.921875\n",
      "2018-05-23T13:38:34.695624: step 10433, loss 0.128931, acc 0.9375\n",
      "2018-05-23T13:38:35.163911: step 10434, loss 0.229221, acc 0.875\n",
      "2018-05-23T13:38:35.606308: step 10435, loss 0.223346, acc 0.9375\n",
      "2018-05-23T13:38:36.115123: step 10436, loss 0.341245, acc 0.890625\n",
      "2018-05-23T13:38:36.518206: step 10437, loss 0.0738089, acc 0.96875\n",
      "2018-05-23T13:38:36.910232: step 10438, loss 0.143119, acc 0.953125\n",
      "2018-05-23T13:38:37.372565: step 10439, loss 0.155971, acc 0.921875\n",
      "2018-05-23T13:38:37.822498: step 10440, loss 0.103439, acc 0.9375\n",
      "2018-05-23T13:38:38.217969: step 10441, loss 0.206958, acc 0.859375\n",
      "2018-05-23T13:38:38.720784: step 10442, loss 0.164743, acc 0.921875\n",
      "2018-05-23T13:38:39.127300: step 10443, loss 0.170157, acc 0.890625\n",
      "2018-05-23T13:38:39.526375: step 10444, loss 0.115329, acc 0.953125\n",
      "2018-05-23T13:38:39.979406: step 10445, loss 0.142389, acc 0.9375\n",
      "2018-05-23T13:38:40.396596: step 10446, loss 0.218883, acc 0.921875\n",
      "2018-05-23T13:38:41.039063: step 10447, loss 0.219917, acc 0.9375\n",
      "2018-05-23T13:38:41.653420: step 10448, loss 0.109711, acc 0.9375\n",
      "2018-05-23T13:38:42.300689: step 10449, loss 0.267435, acc 0.875\n",
      "2018-05-23T13:38:42.980963: step 10450, loss 0.222729, acc 0.90625\n",
      "2018-05-23T13:38:43.601332: step 10451, loss 0.122982, acc 0.953125\n",
      "2018-05-23T13:38:44.147889: step 10452, loss 0.275175, acc 0.859375\n",
      "2018-05-23T13:38:44.584726: step 10453, loss 0.231247, acc 0.890625\n",
      "2018-05-23T13:38:44.999616: step 10454, loss 0.177685, acc 0.9375\n",
      "2018-05-23T13:38:45.395577: step 10455, loss 0.123438, acc 0.921875\n",
      "2018-05-23T13:38:45.794021: step 10456, loss 0.221928, acc 0.890625\n",
      "2018-05-23T13:38:46.200953: step 10457, loss 0.138703, acc 0.9375\n",
      "2018-05-23T13:38:46.659926: step 10458, loss 0.134517, acc 0.9375\n",
      "2018-05-23T13:38:47.084288: step 10459, loss 0.221068, acc 0.875\n",
      "2018-05-23T13:38:47.523135: step 10460, loss 0.15898, acc 0.921875\n",
      "2018-05-23T13:38:47.923172: step 10461, loss 0.124843, acc 0.9375\n",
      "2018-05-23T13:38:48.321129: step 10462, loss 0.219421, acc 0.921875\n",
      "2018-05-23T13:38:48.784890: step 10463, loss 0.0909613, acc 0.953125\n",
      "2018-05-23T13:38:49.226730: step 10464, loss 0.298764, acc 0.875\n",
      "2018-05-23T13:38:49.635160: step 10465, loss 0.244545, acc 0.921875\n",
      "2018-05-23T13:38:50.039590: step 10466, loss 0.144255, acc 0.9375\n",
      "2018-05-23T13:38:50.436530: step 10467, loss 0.0734963, acc 0.96875\n",
      "2018-05-23T13:38:50.849957: step 10468, loss 0.172598, acc 0.921875\n",
      "2018-05-23T13:38:51.244934: step 10469, loss 0.14851, acc 0.921875\n",
      "2018-05-23T13:38:51.633894: step 10470, loss 0.152222, acc 0.953125\n",
      "2018-05-23T13:38:52.069258: step 10471, loss 0.139035, acc 0.921875\n",
      "2018-05-23T13:38:52.537515: step 10472, loss 0.158641, acc 0.921875\n",
      "2018-05-23T13:38:52.937910: step 10473, loss 0.134394, acc 0.9375\n",
      "2018-05-23T13:38:53.367759: step 10474, loss 0.242072, acc 0.84375\n",
      "2018-05-23T13:38:53.800949: step 10475, loss 0.125336, acc 0.9375\n",
      "2018-05-23T13:38:54.230801: step 10476, loss 0.143196, acc 0.953125\n",
      "2018-05-23T13:38:54.744049: step 10477, loss 0.102199, acc 0.953125\n",
      "2018-05-23T13:38:55.173408: step 10478, loss 0.109961, acc 0.96875\n",
      "2018-05-23T13:38:55.570894: step 10479, loss 0.142719, acc 0.9375\n",
      "2018-05-23T13:38:55.979799: step 10480, loss 0.192412, acc 0.921875\n",
      "2018-05-23T13:38:56.383721: step 10481, loss 0.0683215, acc 1\n",
      "2018-05-23T13:38:56.883398: step 10482, loss 0.148401, acc 0.953125\n",
      "2018-05-23T13:38:57.291310: step 10483, loss 0.116846, acc 0.953125\n",
      "2018-05-23T13:38:57.693160: step 10484, loss 0.169789, acc 0.921875\n",
      "2018-05-23T13:38:58.088122: step 10485, loss 0.146297, acc 0.921875\n",
      "2018-05-23T13:38:58.480095: step 10486, loss 0.137317, acc 0.921875\n",
      "2018-05-23T13:38:58.880048: step 10487, loss 0.208761, acc 0.921875\n",
      "2018-05-23T13:38:59.336880: step 10488, loss 0.200734, acc 0.921875\n",
      "2018-05-23T13:38:59.732334: step 10489, loss 0.300681, acc 0.890625\n",
      "2018-05-23T13:39:00.144753: step 10490, loss 0.103992, acc 0.953125\n",
      "2018-05-23T13:39:00.532717: step 10491, loss 0.225799, acc 0.90625\n",
      "2018-05-23T13:39:00.936635: step 10492, loss 0.117267, acc 0.984375\n",
      "2018-05-23T13:39:01.373685: step 10493, loss 0.149558, acc 0.9375\n",
      "2018-05-23T13:39:01.774613: step 10494, loss 0.110451, acc 0.953125\n",
      "2018-05-23T13:39:02.178559: step 10495, loss 0.0750778, acc 0.953125\n",
      "2018-05-23T13:39:02.577662: step 10496, loss 0.0788028, acc 0.984375\n",
      "2018-05-23T13:39:02.962634: step 10497, loss 0.400895, acc 0.828125\n",
      "2018-05-23T13:39:03.366708: step 10498, loss 0.192891, acc 0.921875\n",
      "2018-05-23T13:39:03.766656: step 10499, loss 0.145011, acc 0.96875\n",
      "2018-05-23T13:39:04.156618: step 10500, loss 0.163861, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:39:09.736328: step 10500, loss 1.06752, acc 0.71996\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10500\n",
      "\n",
      "2018-05-23T13:39:11.309532: step 10501, loss 0.134412, acc 0.9375\n",
      "2018-05-23T13:39:11.796815: step 10502, loss 0.20228, acc 0.875\n",
      "2018-05-23T13:39:12.204741: step 10503, loss 0.247947, acc 0.875\n",
      "2018-05-23T13:39:12.621624: step 10504, loss 0.10109, acc 0.9375\n",
      "2018-05-23T13:39:13.018847: step 10505, loss 0.132264, acc 0.921875\n",
      "2018-05-23T13:39:13.418689: step 10506, loss 0.0948582, acc 0.984375\n",
      "2018-05-23T13:39:13.829422: step 10507, loss 0.17327, acc 0.9375\n",
      "2018-05-23T13:39:14.227851: step 10508, loss 0.117298, acc 0.9375\n",
      "2018-05-23T13:39:14.698667: step 10509, loss 0.148823, acc 0.96875\n",
      "2018-05-23T13:39:15.154959: step 10510, loss 0.128419, acc 0.953125\n",
      "2018-05-23T13:39:15.717044: step 10511, loss 0.233472, acc 0.90625\n",
      "2018-05-23T13:39:16.264107: step 10512, loss 0.284335, acc 0.890625\n",
      "2018-05-23T13:39:16.693134: step 10513, loss 0.186824, acc 0.921875\n",
      "2018-05-23T13:39:17.163389: step 10514, loss 0.0933827, acc 0.953125\n",
      "2018-05-23T13:39:17.751977: step 10515, loss 0.146475, acc 0.9375\n",
      "2018-05-23T13:39:18.297536: step 10516, loss 0.106162, acc 0.9375\n",
      "2018-05-23T13:39:18.850568: step 10517, loss 0.0784825, acc 0.96875\n",
      "2018-05-23T13:39:19.335272: step 10518, loss 0.211234, acc 0.9375\n",
      "2018-05-23T13:39:19.800032: step 10519, loss 0.22079, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:39:20.362073: step 10520, loss 0.141152, acc 0.921875\n",
      "2018-05-23T13:39:20.804400: step 10521, loss 0.211964, acc 0.921875\n",
      "2018-05-23T13:39:21.206327: step 10522, loss 0.15259, acc 0.90625\n",
      "2018-05-23T13:39:21.609886: step 10523, loss 0.184138, acc 0.90625\n",
      "2018-05-23T13:39:22.009836: step 10524, loss 0.139755, acc 0.953125\n",
      "2018-05-23T13:39:22.410783: step 10525, loss 0.227042, acc 0.90625\n",
      "2018-05-23T13:39:22.854305: step 10526, loss 0.145879, acc 0.921875\n",
      "2018-05-23T13:39:23.248755: step 10527, loss 0.139147, acc 0.9375\n",
      "2018-05-23T13:39:23.635740: step 10528, loss 0.17353, acc 0.921875\n",
      "2018-05-23T13:39:24.038685: step 10529, loss 0.154323, acc 0.921875\n",
      "2018-05-23T13:39:24.432631: step 10530, loss 0.168537, acc 0.9375\n",
      "2018-05-23T13:39:24.824165: step 10531, loss 0.201136, acc 0.890625\n",
      "2018-05-23T13:39:25.226916: step 10532, loss 0.206971, acc 0.9375\n",
      "2018-05-23T13:39:25.613898: step 10533, loss 0.169527, acc 0.921875\n",
      "2018-05-23T13:39:26.015842: step 10534, loss 0.19044, acc 0.875\n",
      "2018-05-23T13:39:26.404801: step 10535, loss 0.112838, acc 0.953125\n",
      "2018-05-23T13:39:26.824683: step 10536, loss 0.169191, acc 0.9375\n",
      "2018-05-23T13:39:27.230614: step 10537, loss 0.109016, acc 0.953125\n",
      "2018-05-23T13:39:27.614587: step 10538, loss 0.156217, acc 0.9375\n",
      "2018-05-23T13:39:28.002072: step 10539, loss 0.254702, acc 0.890625\n",
      "2018-05-23T13:39:28.465536: step 10540, loss 0.106209, acc 0.953125\n",
      "2018-05-23T13:39:29.046353: step 10541, loss 0.129307, acc 0.984375\n",
      "2018-05-23T13:39:29.751191: step 10542, loss 0.192577, acc 0.921875\n",
      "2018-05-23T13:39:30.238795: step 10543, loss 0.231567, acc 0.890625\n",
      "2018-05-23T13:39:30.686622: step 10544, loss 0.192583, acc 0.953125\n",
      "2018-05-23T13:39:31.148320: step 10545, loss 0.336224, acc 0.828125\n",
      "2018-05-23T13:39:31.568437: step 10546, loss 0.18355, acc 0.9375\n",
      "2018-05-23T13:39:31.969303: step 10547, loss 0.180098, acc 0.921875\n",
      "2018-05-23T13:39:32.551512: step 10548, loss 0.204503, acc 0.921875\n",
      "2018-05-23T13:39:33.023144: step 10549, loss 0.130385, acc 0.953125\n",
      "2018-05-23T13:39:33.470701: step 10550, loss 0.275006, acc 0.828125\n",
      "2018-05-23T13:39:33.894830: step 10551, loss 0.164641, acc 0.9375\n",
      "2018-05-23T13:39:34.317899: step 10552, loss 0.140131, acc 0.890625\n",
      "2018-05-23T13:39:34.753959: step 10553, loss 0.0964568, acc 0.9375\n",
      "2018-05-23T13:39:35.226877: step 10554, loss 0.186967, acc 0.90625\n",
      "2018-05-23T13:39:35.688383: step 10555, loss 0.25991, acc 0.921875\n",
      "2018-05-23T13:39:36.094738: step 10556, loss 0.118816, acc 0.953125\n",
      "2018-05-23T13:39:36.558352: step 10557, loss 0.14714, acc 0.953125\n",
      "2018-05-23T13:39:36.940937: step 10558, loss 0.151034, acc 0.953125\n",
      "2018-05-23T13:39:37.527896: step 10559, loss 0.141149, acc 0.984375\n",
      "2018-05-23T13:39:37.926873: step 10560, loss 0.134688, acc 0.96875\n",
      "2018-05-23T13:39:38.364802: step 10561, loss 0.277922, acc 0.859375\n",
      "2018-05-23T13:39:38.758913: step 10562, loss 0.129414, acc 0.953125\n",
      "2018-05-23T13:39:39.147884: step 10563, loss 0.136209, acc 0.953125\n",
      "2018-05-23T13:39:39.570251: step 10564, loss 0.16353, acc 0.921875\n",
      "2018-05-23T13:39:40.027870: step 10565, loss 0.233508, acc 0.921875\n",
      "2018-05-23T13:39:40.432883: step 10566, loss 0.10415, acc 0.96875\n",
      "2018-05-23T13:39:40.827299: step 10567, loss 0.16182, acc 0.953125\n",
      "2018-05-23T13:39:41.220290: step 10568, loss 0.220732, acc 0.921875\n",
      "2018-05-23T13:39:41.617825: step 10569, loss 0.174231, acc 0.921875\n",
      "2018-05-23T13:39:42.164670: step 10570, loss 0.171434, acc 0.921875\n",
      "2018-05-23T13:39:42.648615: step 10571, loss 0.247598, acc 0.859375\n",
      "2018-05-23T13:39:43.155287: step 10572, loss 0.160285, acc 0.921875\n",
      "2018-05-23T13:39:43.677404: step 10573, loss 0.134955, acc 0.953125\n",
      "2018-05-23T13:39:44.237787: step 10574, loss 0.253684, acc 0.90625\n",
      "2018-05-23T13:39:44.742009: step 10575, loss 0.100954, acc 0.96875\n",
      "2018-05-23T13:39:45.165765: step 10576, loss 0.139159, acc 0.953125\n",
      "2018-05-23T13:39:45.570790: step 10577, loss 0.19366, acc 0.953125\n",
      "2018-05-23T13:39:45.969815: step 10578, loss 0.29608, acc 0.890625\n",
      "2018-05-23T13:39:46.375458: step 10579, loss 0.122055, acc 0.96875\n",
      "2018-05-23T13:39:46.831691: step 10580, loss 0.195118, acc 0.890625\n",
      "2018-05-23T13:39:47.319140: step 10581, loss 0.149663, acc 0.953125\n",
      "2018-05-23T13:39:47.733313: step 10582, loss 0.0861953, acc 0.953125\n",
      "2018-05-23T13:39:48.155196: step 10583, loss 0.148734, acc 0.9375\n",
      "2018-05-23T13:39:48.570833: step 10584, loss 0.244621, acc 0.875\n",
      "2018-05-23T13:39:49.011691: step 10585, loss 0.190927, acc 0.921875\n",
      "2018-05-23T13:39:49.404819: step 10586, loss 0.103339, acc 0.9375\n",
      "2018-05-23T13:39:49.806790: step 10587, loss 0.182357, acc 0.9375\n",
      "2018-05-23T13:39:50.269102: step 10588, loss 0.157136, acc 0.953125\n",
      "2018-05-23T13:39:50.672022: step 10589, loss 0.122705, acc 0.9375\n",
      "2018-05-23T13:39:51.073830: step 10590, loss 0.128084, acc 0.96875\n",
      "2018-05-23T13:39:51.464764: step 10591, loss 0.148802, acc 0.9375\n",
      "2018-05-23T13:39:51.862711: step 10592, loss 0.139097, acc 0.953125\n",
      "2018-05-23T13:39:52.260174: step 10593, loss 0.19214, acc 0.921875\n",
      "2018-05-23T13:39:52.704046: step 10594, loss 0.0767249, acc 0.96875\n",
      "2018-05-23T13:39:53.101869: step 10595, loss 0.20757, acc 0.9375\n",
      "2018-05-23T13:39:53.542541: step 10596, loss 0.14595, acc 0.9375\n",
      "2018-05-23T13:39:54.132510: step 10597, loss 0.0601751, acc 1\n",
      "2018-05-23T13:39:54.605349: step 10598, loss 0.201304, acc 0.921875\n",
      "2018-05-23T13:39:55.237605: step 10599, loss 0.226572, acc 0.859375\n",
      "2018-05-23T13:39:55.919178: step 10600, loss 0.140953, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:40:04.292081: step 10600, loss 1.07191, acc 0.721103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10600\n",
      "\n",
      "2018-05-23T13:40:06.271072: step 10601, loss 0.264674, acc 0.90625\n",
      "2018-05-23T13:40:06.783508: step 10602, loss 0.121845, acc 0.96875\n",
      "2018-05-23T13:40:07.290204: step 10603, loss 0.18025, acc 0.921875\n",
      "2018-05-23T13:40:07.698118: step 10604, loss 0.19198, acc 0.890625\n",
      "2018-05-23T13:40:08.101576: step 10605, loss 0.113882, acc 0.921875\n",
      "2018-05-23T13:40:08.512689: step 10606, loss 0.230166, acc 0.875\n",
      "2018-05-23T13:40:08.930539: step 10607, loss 0.151132, acc 0.90625\n",
      "2018-05-23T13:40:09.387429: step 10608, loss 0.329786, acc 0.859375\n",
      "2018-05-23T13:40:09.782824: step 10609, loss 0.0982889, acc 0.96875\n",
      "2018-05-23T13:40:10.177825: step 10610, loss 0.0863352, acc 0.953125\n",
      "2018-05-23T13:40:10.565150: step 10611, loss 0.364528, acc 0.875\n",
      "2018-05-23T13:40:10.963260: step 10612, loss 0.250585, acc 0.9375\n",
      "2018-05-23T13:40:11.360926: step 10613, loss 0.25395, acc 0.921875\n",
      "2018-05-23T13:40:11.824900: step 10614, loss 0.165357, acc 0.953125\n",
      "2018-05-23T13:40:12.228063: step 10615, loss 0.239369, acc 0.84375\n",
      "2018-05-23T13:40:12.618699: step 10616, loss 0.136027, acc 0.953125\n",
      "2018-05-23T13:40:13.142881: step 10617, loss 0.108707, acc 0.921875\n",
      "2018-05-23T13:40:13.641752: step 10618, loss 0.122877, acc 0.921875\n",
      "2018-05-23T13:40:14.063550: step 10619, loss 0.160627, acc 0.9375\n",
      "2018-05-23T13:40:14.467502: step 10620, loss 0.117481, acc 0.9375\n",
      "2018-05-23T13:40:14.868574: step 10621, loss 0.151338, acc 0.90625\n",
      "2018-05-23T13:40:15.323651: step 10622, loss 0.144136, acc 0.9375\n",
      "2018-05-23T13:40:15.727540: step 10623, loss 0.153006, acc 0.9375\n",
      "2018-05-23T13:40:16.252852: step 10624, loss 0.159069, acc 0.890625\n",
      "2018-05-23T13:40:16.676322: step 10625, loss 0.078096, acc 1\n",
      "2018-05-23T13:40:17.187351: step 10626, loss 0.203683, acc 0.890625\n",
      "2018-05-23T13:40:17.613757: step 10627, loss 0.126649, acc 0.9375\n",
      "2018-05-23T13:40:18.020871: step 10628, loss 0.182148, acc 0.953125\n",
      "2018-05-23T13:40:18.466100: step 10629, loss 0.227683, acc 0.90625\n",
      "2018-05-23T13:40:18.861146: step 10630, loss 0.126113, acc 0.9375\n",
      "2018-05-23T13:40:19.310742: step 10631, loss 0.202874, acc 0.90625\n",
      "2018-05-23T13:40:19.707731: step 10632, loss 0.107155, acc 0.953125\n",
      "2018-05-23T13:40:20.179128: step 10633, loss 0.10514, acc 0.984375\n",
      "2018-05-23T13:40:20.592719: step 10634, loss 0.149636, acc 0.9375\n",
      "2018-05-23T13:40:20.981843: step 10635, loss 0.121144, acc 0.953125\n",
      "2018-05-23T13:40:21.384792: step 10636, loss 0.188985, acc 0.921875\n",
      "2018-05-23T13:40:21.772825: step 10637, loss 0.0639482, acc 1\n",
      "2018-05-23T13:40:22.172928: step 10638, loss 0.126959, acc 0.96875\n",
      "2018-05-23T13:40:22.635367: step 10639, loss 0.196787, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:40:23.072058: step 10640, loss 0.093127, acc 0.984375\n",
      "2018-05-23T13:40:23.520362: step 10641, loss 0.368153, acc 0.875\n",
      "2018-05-23T13:40:23.951546: step 10642, loss 0.262959, acc 0.890625\n",
      "2018-05-23T13:40:24.370903: step 10643, loss 0.120349, acc 0.9375\n",
      "2018-05-23T13:40:24.815513: step 10644, loss 0.108907, acc 0.96875\n",
      "2018-05-23T13:40:25.217309: step 10645, loss 0.239138, acc 0.921875\n",
      "2018-05-23T13:40:25.672085: step 10646, loss 0.31467, acc 0.890625\n",
      "2018-05-23T13:40:26.066264: step 10647, loss 0.194008, acc 0.875\n",
      "2018-05-23T13:40:26.546885: step 10648, loss 0.130399, acc 0.953125\n",
      "2018-05-23T13:40:26.947405: step 10649, loss 0.117129, acc 0.96875\n",
      "2018-05-23T13:40:27.345027: step 10650, loss 0.184305, acc 0.90625\n",
      "2018-05-23T13:40:27.745853: step 10651, loss 0.200047, acc 0.90625\n",
      "2018-05-23T13:40:28.135813: step 10652, loss 0.245226, acc 0.921875\n",
      "2018-05-23T13:40:28.525838: step 10653, loss 0.215239, acc 0.890625\n",
      "2018-05-23T13:40:28.967722: step 10654, loss 0.217887, acc 0.921875\n",
      "2018-05-23T13:40:29.571460: step 10655, loss 0.128022, acc 0.9375\n",
      "2018-05-23T13:40:30.120748: step 10656, loss 0.186784, acc 0.890625\n",
      "2018-05-23T13:40:30.710193: step 10657, loss 0.290199, acc 0.875\n",
      "2018-05-23T13:40:31.491579: step 10658, loss 0.221227, acc 0.90625\n",
      "2018-05-23T13:40:32.006148: step 10659, loss 0.176952, acc 0.921875\n",
      "2018-05-23T13:40:32.572230: step 10660, loss 0.180729, acc 0.921875\n",
      "2018-05-23T13:40:33.098850: step 10661, loss 0.0881043, acc 0.953125\n",
      "2018-05-23T13:40:33.518888: step 10662, loss 0.19147, acc 0.890625\n",
      "2018-05-23T13:40:33.955425: step 10663, loss 0.134073, acc 0.953125\n",
      "2018-05-23T13:40:34.362850: step 10664, loss 0.147177, acc 0.921875\n",
      "2018-05-23T13:40:34.769283: step 10665, loss 0.251129, acc 0.875\n",
      "2018-05-23T13:40:35.268527: step 10666, loss 0.168519, acc 0.9375\n",
      "2018-05-23T13:40:35.693965: step 10667, loss 0.178106, acc 0.921875\n",
      "2018-05-23T13:40:36.103923: step 10668, loss 0.160084, acc 0.890625\n",
      "2018-05-23T13:40:36.588465: step 10669, loss 0.213478, acc 0.921875\n",
      "2018-05-23T13:40:36.984978: step 10670, loss 0.217456, acc 0.921875\n",
      "2018-05-23T13:40:37.444821: step 10671, loss 0.316739, acc 0.890625\n",
      "2018-05-23T13:40:37.836359: step 10672, loss 0.195757, acc 0.921875\n",
      "2018-05-23T13:40:38.239687: step 10673, loss 0.155318, acc 0.90625\n",
      "2018-05-23T13:40:38.710538: step 10674, loss 0.178651, acc 0.9375\n",
      "2018-05-23T13:40:39.113777: step 10675, loss 0.193309, acc 0.90625\n",
      "2018-05-23T13:40:39.511104: step 10676, loss 0.248542, acc 0.875\n",
      "2018-05-23T13:40:39.898476: step 10677, loss 0.125073, acc 0.953125\n",
      "2018-05-23T13:40:40.308026: step 10678, loss 0.123192, acc 0.96875\n",
      "2018-05-23T13:40:40.699332: step 10679, loss 0.125068, acc 0.9375\n",
      "2018-05-23T13:40:41.185056: step 10680, loss 0.110159, acc 0.953125\n",
      "2018-05-23T13:40:41.581745: step 10681, loss 0.0958079, acc 0.96875\n",
      "2018-05-23T13:40:41.974315: step 10682, loss 0.112521, acc 0.953125\n",
      "2018-05-23T13:40:42.385053: step 10683, loss 0.198071, acc 0.921875\n",
      "2018-05-23T13:40:42.780447: step 10684, loss 0.172335, acc 0.921875\n",
      "2018-05-23T13:40:43.186355: step 10685, loss 0.098015, acc 0.96875\n",
      "2018-05-23T13:40:43.660907: step 10686, loss 0.134624, acc 0.9375\n",
      "2018-05-23T13:40:44.109923: step 10687, loss 0.147633, acc 0.953125\n",
      "2018-05-23T13:40:44.594178: step 10688, loss 0.187673, acc 0.890625\n",
      "2018-05-23T13:40:45.011352: step 10689, loss 0.200413, acc 0.9375\n",
      "2018-05-23T13:40:45.413088: step 10690, loss 0.185147, acc 0.921875\n",
      "2018-05-23T13:40:45.893043: step 10691, loss 0.196369, acc 0.90625\n",
      "2018-05-23T13:40:46.291772: step 10692, loss 0.110804, acc 0.96875\n",
      "2018-05-23T13:40:46.686321: step 10693, loss 0.176032, acc 0.9375\n",
      "2018-05-23T13:40:47.084836: step 10694, loss 0.181987, acc 0.953125\n",
      "2018-05-23T13:40:47.561948: step 10695, loss 0.318929, acc 0.921875\n",
      "2018-05-23T13:40:48.018210: step 10696, loss 0.139865, acc 0.921875\n",
      "2018-05-23T13:40:48.421786: step 10697, loss 0.184979, acc 0.9375\n",
      "2018-05-23T13:40:48.821858: step 10698, loss 0.210581, acc 0.875\n",
      "2018-05-23T13:40:49.222662: step 10699, loss 0.153062, acc 0.953125\n",
      "2018-05-23T13:40:49.666617: step 10700, loss 0.155886, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:40:55.583746: step 10700, loss 1.06753, acc 0.720103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10700\n",
      "\n",
      "2018-05-23T13:40:57.200600: step 10701, loss 0.195675, acc 0.921875\n",
      "2018-05-23T13:40:57.634171: step 10702, loss 0.107498, acc 0.953125\n",
      "2018-05-23T13:40:58.099594: step 10703, loss 0.0984056, acc 0.9375\n",
      "2018-05-23T13:40:58.507928: step 10704, loss 0.162292, acc 0.90625\n",
      "2018-05-23T13:40:58.903453: step 10705, loss 0.111745, acc 0.96875\n",
      "2018-05-23T13:40:59.388067: step 10706, loss 0.241397, acc 0.921875\n",
      "2018-05-23T13:40:59.846913: step 10707, loss 0.211934, acc 0.9375\n",
      "2018-05-23T13:41:00.273735: step 10708, loss 0.128358, acc 0.96875\n",
      "2018-05-23T13:41:00.661837: step 10709, loss 0.169115, acc 0.921875\n",
      "2018-05-23T13:41:01.052825: step 10710, loss 0.0794465, acc 0.984375\n",
      "2018-05-23T13:41:01.447833: step 10711, loss 0.149976, acc 0.953125\n",
      "2018-05-23T13:41:01.862789: step 10712, loss 0.165941, acc 0.90625\n",
      "2018-05-23T13:41:02.367506: step 10713, loss 0.167579, acc 0.953125\n",
      "2018-05-23T13:41:02.763500: step 10714, loss 0.232138, acc 0.890625\n",
      "2018-05-23T13:41:03.165910: step 10715, loss 0.262572, acc 0.921875\n",
      "2018-05-23T13:41:03.561406: step 10716, loss 0.152633, acc 0.9375\n",
      "2018-05-23T13:41:03.950438: step 10717, loss 0.136285, acc 0.90625\n",
      "2018-05-23T13:41:04.502598: step 10718, loss 0.100302, acc 0.953125\n",
      "2018-05-23T13:41:04.945174: step 10719, loss 0.149307, acc 0.9375\n",
      "2018-05-23T13:41:05.359131: step 10720, loss 0.177153, acc 0.9375\n",
      "2018-05-23T13:41:05.771206: step 10721, loss 0.130179, acc 0.984375\n",
      "2018-05-23T13:41:06.179309: step 10722, loss 0.133748, acc 0.90625\n",
      "2018-05-23T13:41:06.609376: step 10723, loss 0.150688, acc 0.921875\n",
      "2018-05-23T13:41:07.018270: step 10724, loss 0.171071, acc 0.921875\n",
      "2018-05-23T13:41:07.547978: step 10725, loss 0.236626, acc 0.859375\n",
      "2018-05-23T13:41:07.974861: step 10726, loss 0.156418, acc 0.921875\n",
      "2018-05-23T13:41:08.426137: step 10727, loss 0.0616788, acc 0.96875\n",
      "2018-05-23T13:41:08.893003: step 10728, loss 0.152474, acc 0.953125\n",
      "2018-05-23T13:41:09.339974: step 10729, loss 0.0989236, acc 0.96875\n",
      "2018-05-23T13:41:09.746745: step 10730, loss 0.167732, acc 0.921875\n",
      "2018-05-23T13:41:10.198579: step 10731, loss 0.184469, acc 0.890625\n",
      "2018-05-23T13:41:10.654581: step 10732, loss 0.0906354, acc 0.96875\n",
      "2018-05-23T13:41:11.088129: step 10733, loss 0.15868, acc 0.90625\n",
      "2018-05-23T13:41:11.503420: step 10734, loss 0.136836, acc 0.9375\n",
      "2018-05-23T13:41:11.950317: step 10735, loss 0.308386, acc 0.90625\n",
      "2018-05-23T13:41:12.357001: step 10736, loss 0.145277, acc 0.9375\n",
      "2018-05-23T13:41:12.770100: step 10737, loss 0.277717, acc 0.875\n",
      "2018-05-23T13:41:13.169088: step 10738, loss 0.156155, acc 0.9375\n",
      "2018-05-23T13:41:13.595769: step 10739, loss 0.10473, acc 0.953125\n",
      "2018-05-23T13:41:14.016455: step 10740, loss 0.272697, acc 0.921875\n",
      "2018-05-23T13:41:14.512836: step 10741, loss 0.118496, acc 0.921875\n",
      "2018-05-23T13:41:15.070665: step 10742, loss 0.152519, acc 0.90625\n",
      "2018-05-23T13:41:15.522219: step 10743, loss 0.17117, acc 0.953125\n",
      "2018-05-23T13:41:15.984114: step 10744, loss 0.247226, acc 0.890625\n",
      "2018-05-23T13:41:16.397630: step 10745, loss 0.157409, acc 0.90625\n",
      "2018-05-23T13:41:16.863739: step 10746, loss 0.229687, acc 0.9375\n",
      "2018-05-23T13:41:17.260673: step 10747, loss 0.177069, acc 0.90625\n",
      "2018-05-23T13:41:17.735663: step 10748, loss 0.231104, acc 0.890625\n",
      "2018-05-23T13:41:18.226636: step 10749, loss 0.281257, acc 0.890625\n",
      "2018-05-23T13:41:18.684627: step 10750, loss 0.199377, acc 0.90625\n",
      "2018-05-23T13:41:19.165677: step 10751, loss 0.0979737, acc 0.96875\n",
      "2018-05-23T13:41:19.572837: step 10752, loss 0.167149, acc 0.9375\n",
      "2018-05-23T13:41:20.003982: step 10753, loss 0.140773, acc 0.953125\n",
      "2018-05-23T13:41:20.441230: step 10754, loss 0.128126, acc 0.9375\n",
      "2018-05-23T13:41:20.881947: step 10755, loss 0.241784, acc 0.875\n",
      "2018-05-23T13:41:21.318862: step 10756, loss 0.130322, acc 0.921875\n",
      "2018-05-23T13:41:21.722718: step 10757, loss 0.333949, acc 0.890625\n",
      "2018-05-23T13:41:22.132161: step 10758, loss 0.293624, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:41:22.558592: step 10759, loss 0.165609, acc 0.921875\n",
      "2018-05-23T13:41:22.946563: step 10760, loss 0.174959, acc 0.9375\n",
      "2018-05-23T13:41:23.340539: step 10761, loss 0.281388, acc 0.90625\n",
      "2018-05-23T13:41:23.825996: step 10762, loss 0.227893, acc 0.9375\n",
      "2018-05-23T13:41:24.360642: step 10763, loss 0.0959268, acc 0.96875\n",
      "2018-05-23T13:41:24.886786: step 10764, loss 0.167497, acc 0.953125\n",
      "2018-05-23T13:41:25.515262: step 10765, loss 0.100594, acc 0.9375\n",
      "2018-05-23T13:41:26.039577: step 10766, loss 0.146305, acc 0.953125\n",
      "2018-05-23T13:41:26.473132: step 10767, loss 0.14188, acc 0.96875\n",
      "2018-05-23T13:41:26.892591: step 10768, loss 0.254481, acc 0.921875\n",
      "2018-05-23T13:41:27.371932: step 10769, loss 0.177596, acc 0.953125\n",
      "2018-05-23T13:41:27.770605: step 10770, loss 0.147161, acc 0.9375\n",
      "2018-05-23T13:41:28.187752: step 10771, loss 0.12402, acc 0.9375\n",
      "2018-05-23T13:41:28.589304: step 10772, loss 0.132158, acc 0.9375\n",
      "2018-05-23T13:41:29.065742: step 10773, loss 0.294598, acc 0.890625\n",
      "2018-05-23T13:41:29.528228: step 10774, loss 0.211873, acc 0.90625\n",
      "2018-05-23T13:41:29.921207: step 10775, loss 0.0992672, acc 0.953125\n",
      "2018-05-23T13:41:30.396559: step 10776, loss 0.142161, acc 0.921875\n",
      "2018-05-23T13:41:30.796552: step 10777, loss 0.146687, acc 0.9375\n",
      "2018-05-23T13:41:31.200120: step 10778, loss 0.134107, acc 0.953125\n",
      "2018-05-23T13:41:31.632371: step 10779, loss 0.214322, acc 0.921875\n",
      "2018-05-23T13:41:32.024901: step 10780, loss 0.104291, acc 0.953125\n",
      "2018-05-23T13:41:32.467773: step 10781, loss 0.199962, acc 0.90625\n",
      "2018-05-23T13:41:32.895732: step 10782, loss 0.147679, acc 0.921875\n",
      "2018-05-23T13:41:33.297474: step 10783, loss 0.157886, acc 0.953125\n",
      "2018-05-23T13:41:33.739650: step 10784, loss 0.137827, acc 0.921875\n",
      "2018-05-23T13:41:34.134182: step 10785, loss 0.195653, acc 0.953125\n",
      "2018-05-23T13:41:34.538760: step 10786, loss 0.121317, acc 0.96875\n",
      "2018-05-23T13:41:34.978500: step 10787, loss 0.216518, acc 0.890625\n",
      "2018-05-23T13:41:35.434016: step 10788, loss 0.183124, acc 0.921875\n",
      "2018-05-23T13:41:35.821522: step 10789, loss 0.226488, acc 0.90625\n",
      "2018-05-23T13:41:36.224247: step 10790, loss 0.276436, acc 0.90625\n",
      "2018-05-23T13:41:36.677107: step 10791, loss 0.184908, acc 0.9375\n",
      "2018-05-23T13:41:37.086189: step 10792, loss 0.196324, acc 0.921875\n",
      "2018-05-23T13:41:37.537531: step 10793, loss 0.148101, acc 0.953125\n",
      "2018-05-23T13:41:38.010627: step 10794, loss 0.122678, acc 0.96875\n",
      "2018-05-23T13:41:38.422773: step 10795, loss 0.264399, acc 0.875\n",
      "2018-05-23T13:41:38.821741: step 10796, loss 0.176723, acc 0.921875\n",
      "2018-05-23T13:41:39.218814: step 10797, loss 0.201751, acc 0.921875\n",
      "2018-05-23T13:41:39.611574: step 10798, loss 0.253741, acc 0.84375\n",
      "2018-05-23T13:41:40.155580: step 10799, loss 0.235899, acc 0.90625\n",
      "2018-05-23T13:41:40.639565: step 10800, loss 0.111913, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:41:47.252596: step 10800, loss 1.0676, acc 0.721817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10800\n",
      "\n",
      "2018-05-23T13:41:49.085754: step 10801, loss 0.0779325, acc 0.984375\n",
      "2018-05-23T13:41:49.546313: step 10802, loss 0.098863, acc 0.953125\n",
      "2018-05-23T13:41:49.969785: step 10803, loss 0.109983, acc 0.96875\n",
      "2018-05-23T13:41:50.380760: step 10804, loss 0.171577, acc 0.953125\n",
      "2018-05-23T13:41:50.783574: step 10805, loss 0.10212, acc 0.9375\n",
      "2018-05-23T13:41:51.183930: step 10806, loss 0.0583937, acc 0.96875\n",
      "2018-05-23T13:41:51.604461: step 10807, loss 0.104091, acc 0.953125\n",
      "2018-05-23T13:41:52.033087: step 10808, loss 0.136944, acc 0.9375\n",
      "2018-05-23T13:41:52.481409: step 10809, loss 0.194495, acc 0.90625\n",
      "2018-05-23T13:41:52.867437: step 10810, loss 0.234769, acc 0.921875\n",
      "2018-05-23T13:41:53.263549: step 10811, loss 0.14118, acc 0.953125\n",
      "2018-05-23T13:41:53.653560: step 10812, loss 0.114533, acc 0.96875\n",
      "2018-05-23T13:41:54.043585: step 10813, loss 0.117206, acc 0.96875\n",
      "2018-05-23T13:41:54.544386: step 10814, loss 0.16572, acc 0.953125\n",
      "2018-05-23T13:41:54.957680: step 10815, loss 0.191184, acc 0.90625\n",
      "2018-05-23T13:41:55.387091: step 10816, loss 0.158671, acc 0.9375\n",
      "2018-05-23T13:41:55.949499: step 10817, loss 0.0973219, acc 0.96875\n",
      "2018-05-23T13:41:56.416641: step 10818, loss 0.222855, acc 0.875\n",
      "2018-05-23T13:41:56.854718: step 10819, loss 0.126097, acc 0.953125\n",
      "2018-05-23T13:41:57.257587: step 10820, loss 0.218638, acc 0.859375\n",
      "2018-05-23T13:41:57.715968: step 10821, loss 0.110326, acc 0.96875\n",
      "2018-05-23T13:41:58.119908: step 10822, loss 0.163701, acc 0.90625\n",
      "2018-05-23T13:41:58.567709: step 10823, loss 0.165089, acc 0.921875\n",
      "2018-05-23T13:41:58.965646: step 10824, loss 0.139559, acc 0.9375\n",
      "2018-05-23T13:41:59.367487: step 10825, loss 0.146914, acc 0.953125\n",
      "2018-05-23T13:41:59.767414: step 10826, loss 0.257419, acc 0.921875\n",
      "2018-05-23T13:42:00.192772: step 10827, loss 0.161068, acc 0.90625\n",
      "2018-05-23T13:42:00.630602: step 10828, loss 0.116855, acc 0.9375\n",
      "2018-05-23T13:42:01.076433: step 10829, loss 0.164431, acc 0.9375\n",
      "2018-05-23T13:42:01.517803: step 10830, loss 0.244078, acc 0.921875\n",
      "2018-05-23T13:42:01.938683: step 10831, loss 0.108155, acc 0.96875\n",
      "2018-05-23T13:42:02.525338: step 10832, loss 0.123284, acc 0.96875\n",
      "2018-05-23T13:42:03.009344: step 10833, loss 0.351567, acc 0.875\n",
      "2018-05-23T13:42:03.481524: step 10834, loss 0.146297, acc 0.9375\n",
      "2018-05-23T13:42:03.843698: step 10835, loss 0.117859, acc 1\n",
      "2018-05-23T13:42:04.299465: step 10836, loss 0.0751576, acc 0.984375\n",
      "2018-05-23T13:42:04.715546: step 10837, loss 0.138886, acc 0.9375\n",
      "2018-05-23T13:42:05.135982: step 10838, loss 0.0780097, acc 0.96875\n",
      "2018-05-23T13:42:05.531788: step 10839, loss 0.206705, acc 0.90625\n",
      "2018-05-23T13:42:05.958367: step 10840, loss 0.151182, acc 0.921875\n",
      "2018-05-23T13:42:06.456321: step 10841, loss 0.113306, acc 0.96875\n",
      "2018-05-23T13:42:06.892154: step 10842, loss 0.153941, acc 0.9375\n",
      "2018-05-23T13:42:07.301071: step 10843, loss 0.0908795, acc 0.984375\n",
      "2018-05-23T13:42:07.759847: step 10844, loss 0.103592, acc 0.953125\n",
      "2018-05-23T13:42:08.171765: step 10845, loss 0.159431, acc 0.96875\n",
      "2018-05-23T13:42:08.596629: step 10846, loss 0.118874, acc 0.953125\n",
      "2018-05-23T13:42:09.007528: step 10847, loss 0.206468, acc 0.921875\n",
      "2018-05-23T13:42:09.482270: step 10848, loss 0.215651, acc 0.90625\n",
      "2018-05-23T13:42:09.881556: step 10849, loss 0.165584, acc 0.9375\n",
      "2018-05-23T13:42:10.282022: step 10850, loss 0.121065, acc 0.9375\n",
      "2018-05-23T13:42:10.673973: step 10851, loss 0.0916616, acc 0.984375\n",
      "2018-05-23T13:42:11.075897: step 10852, loss 0.123817, acc 0.921875\n",
      "2018-05-23T13:42:11.466365: step 10853, loss 0.20099, acc 0.9375\n",
      "2018-05-23T13:42:11.860311: step 10854, loss 0.0540286, acc 0.96875\n",
      "2018-05-23T13:42:12.258255: step 10855, loss 0.178585, acc 0.96875\n",
      "2018-05-23T13:42:12.661688: step 10856, loss 0.132019, acc 0.953125\n",
      "2018-05-23T13:42:13.081963: step 10857, loss 0.164453, acc 0.890625\n",
      "2018-05-23T13:42:13.528972: step 10858, loss 0.116462, acc 0.953125\n",
      "2018-05-23T13:42:13.961786: step 10859, loss 0.163596, acc 0.921875\n",
      "2018-05-23T13:42:14.372208: step 10860, loss 0.111746, acc 0.953125\n",
      "2018-05-23T13:42:14.765636: step 10861, loss 0.165945, acc 0.890625\n",
      "2018-05-23T13:42:15.282803: step 10862, loss 0.176597, acc 0.9375\n",
      "2018-05-23T13:42:16.002955: step 10863, loss 0.134973, acc 0.984375\n",
      "2018-05-23T13:42:16.507000: step 10864, loss 0.137488, acc 0.9375\n",
      "2018-05-23T13:42:16.918973: step 10865, loss 0.16712, acc 0.9375\n",
      "2018-05-23T13:42:17.344622: step 10866, loss 0.0942601, acc 0.96875\n",
      "2018-05-23T13:42:17.818936: step 10867, loss 0.101897, acc 0.953125\n",
      "2018-05-23T13:42:18.290707: step 10868, loss 0.114645, acc 0.96875\n",
      "2018-05-23T13:42:18.882671: step 10869, loss 0.119553, acc 0.953125\n",
      "2018-05-23T13:42:19.292710: step 10870, loss 0.0905355, acc 0.96875\n",
      "2018-05-23T13:42:19.728538: step 10871, loss 0.111444, acc 0.9375\n",
      "2018-05-23T13:42:20.155907: step 10872, loss 0.0611842, acc 0.984375\n",
      "2018-05-23T13:42:20.582103: step 10873, loss 0.113686, acc 0.9375\n",
      "2018-05-23T13:42:21.031961: step 10874, loss 0.138995, acc 0.9375\n",
      "2018-05-23T13:42:21.440411: step 10875, loss 0.149921, acc 0.953125\n",
      "2018-05-23T13:42:21.833394: step 10876, loss 0.135637, acc 0.9375\n",
      "2018-05-23T13:42:22.223000: step 10877, loss 0.0669546, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:42:22.667123: step 10878, loss 0.227849, acc 0.875\n",
      "2018-05-23T13:42:23.078042: step 10879, loss 0.150165, acc 0.921875\n",
      "2018-05-23T13:42:23.540686: step 10880, loss 0.229437, acc 0.859375\n",
      "2018-05-23T13:42:23.931160: step 10881, loss 0.211578, acc 0.953125\n",
      "2018-05-23T13:42:24.324618: step 10882, loss 0.224257, acc 0.921875\n",
      "2018-05-23T13:42:24.719617: step 10883, loss 0.0709339, acc 0.96875\n",
      "2018-05-23T13:42:25.140032: step 10884, loss 0.209821, acc 0.890625\n",
      "2018-05-23T13:42:25.605358: step 10885, loss 0.308337, acc 0.84375\n",
      "2018-05-23T13:42:25.997705: step 10886, loss 0.182137, acc 0.890625\n",
      "2018-05-23T13:42:26.393649: step 10887, loss 0.141368, acc 0.96875\n",
      "2018-05-23T13:42:26.794591: step 10888, loss 0.119673, acc 0.9375\n",
      "2018-05-23T13:42:27.187560: step 10889, loss 0.113389, acc 0.96875\n",
      "2018-05-23T13:42:27.611938: step 10890, loss 0.152206, acc 0.90625\n",
      "2018-05-23T13:42:28.045778: step 10891, loss 0.184763, acc 0.9375\n",
      "2018-05-23T13:42:28.443497: step 10892, loss 0.162436, acc 0.9375\n",
      "2018-05-23T13:42:28.844424: step 10893, loss 0.132288, acc 0.953125\n",
      "2018-05-23T13:42:29.233076: step 10894, loss 0.250622, acc 0.953125\n",
      "2018-05-23T13:42:29.637512: step 10895, loss 0.117515, acc 0.9375\n",
      "2018-05-23T13:42:30.039468: step 10896, loss 0.0939481, acc 0.953125\n",
      "2018-05-23T13:42:30.505731: step 10897, loss 0.0499579, acc 0.984375\n",
      "2018-05-23T13:42:30.899153: step 10898, loss 0.0868616, acc 0.984375\n",
      "2018-05-23T13:42:31.297105: step 10899, loss 0.0847327, acc 0.953125\n",
      "2018-05-23T13:42:31.690564: step 10900, loss 0.124852, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:42:37.426855: step 10900, loss 1.12935, acc 0.721389\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-10900\n",
      "\n",
      "2018-05-23T13:42:39.185324: step 10901, loss 0.0991503, acc 0.9375\n",
      "2018-05-23T13:42:39.798681: step 10902, loss 0.13015, acc 0.9375\n",
      "2018-05-23T13:42:40.235584: step 10903, loss 0.0922019, acc 0.96875\n",
      "2018-05-23T13:42:40.652440: step 10904, loss 0.143529, acc 0.9375\n",
      "2018-05-23T13:42:41.057121: step 10905, loss 0.073943, acc 0.984375\n",
      "2018-05-23T13:42:41.475096: step 10906, loss 0.0765916, acc 0.96875\n",
      "2018-05-23T13:42:42.175045: step 10907, loss 0.122158, acc 0.96875\n",
      "2018-05-23T13:42:43.082501: step 10908, loss 0.164608, acc 0.921875\n",
      "2018-05-23T13:42:43.724730: step 10909, loss 0.0803425, acc 0.953125\n",
      "2018-05-23T13:42:44.358044: step 10910, loss 0.0786907, acc 0.984375\n",
      "2018-05-23T13:42:44.895125: step 10911, loss 0.108076, acc 0.9375\n",
      "2018-05-23T13:42:45.355893: step 10912, loss 0.185294, acc 0.921875\n",
      "2018-05-23T13:42:45.883481: step 10913, loss 0.113401, acc 0.9375\n",
      "2018-05-23T13:42:46.346295: step 10914, loss 0.103872, acc 0.96875\n",
      "2018-05-23T13:42:46.793004: step 10915, loss 0.114316, acc 0.96875\n",
      "2018-05-23T13:42:47.351021: step 10916, loss 0.0943045, acc 0.984375\n",
      "2018-05-23T13:42:47.848203: step 10917, loss 0.0883896, acc 0.984375\n",
      "2018-05-23T13:42:48.243656: step 10918, loss 0.0893872, acc 0.984375\n",
      "2018-05-23T13:42:48.753316: step 10919, loss 0.13607, acc 0.9375\n",
      "2018-05-23T13:42:49.141277: step 10920, loss 0.123912, acc 0.953125\n",
      "2018-05-23T13:42:49.549193: step 10921, loss 0.340055, acc 0.875\n",
      "2018-05-23T13:42:50.061829: step 10922, loss 0.14782, acc 0.96875\n",
      "2018-05-23T13:42:50.455473: step 10923, loss 0.126132, acc 0.953125\n",
      "2018-05-23T13:42:51.003284: step 10924, loss 0.176654, acc 0.90625\n",
      "2018-05-23T13:42:51.389251: step 10925, loss 0.114328, acc 0.953125\n",
      "2018-05-23T13:42:51.764167: step 10926, loss 0.132382, acc 0.96875\n",
      "2018-05-23T13:42:52.139163: step 10927, loss 0.0886878, acc 0.984375\n",
      "2018-05-23T13:42:52.513161: step 10928, loss 0.156183, acc 0.9375\n",
      "2018-05-23T13:42:52.941021: step 10929, loss 0.108102, acc 0.984375\n",
      "2018-05-23T13:42:53.582306: step 10930, loss 0.0692189, acc 0.984375\n",
      "2018-05-23T13:42:54.056039: step 10931, loss 0.126536, acc 0.953125\n",
      "2018-05-23T13:42:54.465097: step 10932, loss 0.116839, acc 0.9375\n",
      "2018-05-23T13:42:54.999778: step 10933, loss 0.0722457, acc 1\n",
      "2018-05-23T13:42:55.604232: step 10934, loss 0.263522, acc 0.890625\n",
      "2018-05-23T13:42:56.075060: step 10935, loss 0.202605, acc 0.9375\n",
      "2018-05-23T13:42:56.636072: step 10936, loss 0.139429, acc 0.9375\n",
      "2018-05-23T13:42:57.196269: step 10937, loss 0.230826, acc 0.921875\n",
      "2018-05-23T13:42:57.749813: step 10938, loss 0.0735413, acc 0.984375\n",
      "2018-05-23T13:42:58.224071: step 10939, loss 0.16134, acc 0.9375\n",
      "2018-05-23T13:42:59.158641: step 10940, loss 0.127349, acc 0.921875\n",
      "2018-05-23T13:42:59.802917: step 10941, loss 0.199588, acc 0.890625\n",
      "2018-05-23T13:43:00.444601: step 10942, loss 0.159242, acc 0.9375\n",
      "2018-05-23T13:43:00.954601: step 10943, loss 0.153595, acc 0.9375\n",
      "2018-05-23T13:43:01.420111: step 10944, loss 0.0597442, acc 0.96875\n",
      "2018-05-23T13:43:01.827612: step 10945, loss 0.0829946, acc 0.984375\n",
      "2018-05-23T13:43:02.351411: step 10946, loss 0.108194, acc 0.96875\n",
      "2018-05-23T13:43:02.780898: step 10947, loss 0.147899, acc 0.9375\n",
      "2018-05-23T13:43:03.221431: step 10948, loss 0.0987808, acc 0.96875\n",
      "2018-05-23T13:43:03.771042: step 10949, loss 0.0400436, acc 0.984375\n",
      "2018-05-23T13:43:04.224902: step 10950, loss 0.102854, acc 0.953125\n",
      "2018-05-23T13:43:04.630078: step 10951, loss 0.176977, acc 0.90625\n",
      "2018-05-23T13:43:05.048476: step 10952, loss 0.0999694, acc 0.9375\n",
      "2018-05-23T13:43:05.456192: step 10953, loss 0.227904, acc 0.921875\n",
      "2018-05-23T13:43:05.855945: step 10954, loss 0.122695, acc 0.921875\n",
      "2018-05-23T13:43:06.277820: step 10955, loss 0.106824, acc 0.9375\n",
      "2018-05-23T13:43:06.732367: step 10956, loss 0.156242, acc 0.96875\n",
      "2018-05-23T13:43:07.130342: step 10957, loss 0.13868, acc 0.96875\n",
      "2018-05-23T13:43:07.625532: step 10958, loss 0.260603, acc 0.890625\n",
      "2018-05-23T13:43:08.051381: step 10959, loss 0.159516, acc 0.890625\n",
      "2018-05-23T13:43:08.459813: step 10960, loss 0.10525, acc 0.953125\n",
      "2018-05-23T13:43:08.892658: step 10961, loss 0.10083, acc 0.96875\n",
      "2018-05-23T13:43:09.301089: step 10962, loss 0.108989, acc 0.953125\n",
      "2018-05-23T13:43:09.776850: step 10963, loss 0.123715, acc 0.9375\n",
      "2018-05-23T13:43:10.221036: step 10964, loss 0.0910838, acc 0.96875\n",
      "2018-05-23T13:43:10.637477: step 10965, loss 0.147637, acc 0.953125\n",
      "2018-05-23T13:43:11.084791: step 10966, loss 0.115922, acc 0.953125\n",
      "2018-05-23T13:43:11.549848: step 10967, loss 0.116481, acc 0.96875\n",
      "2018-05-23T13:43:12.039067: step 10968, loss 0.155704, acc 0.90625\n",
      "2018-05-23T13:43:12.455974: step 10969, loss 0.129127, acc 0.953125\n",
      "2018-05-23T13:43:12.865790: step 10970, loss 0.0938141, acc 0.953125\n",
      "2018-05-23T13:43:13.271526: step 10971, loss 0.046284, acc 1\n",
      "2018-05-23T13:43:13.720840: step 10972, loss 0.238428, acc 0.890625\n",
      "2018-05-23T13:43:14.167666: step 10973, loss 0.0902183, acc 0.9375\n",
      "2018-05-23T13:43:14.573615: step 10974, loss 0.202129, acc 0.9375\n",
      "2018-05-23T13:43:15.002495: step 10975, loss 0.134997, acc 0.9375\n",
      "2018-05-23T13:43:15.432394: step 10976, loss 0.159549, acc 0.921875\n",
      "2018-05-23T13:43:15.867253: step 10977, loss 0.138374, acc 0.9375\n",
      "2018-05-23T13:43:16.304163: step 10978, loss 0.10987, acc 0.96875\n",
      "2018-05-23T13:43:16.704818: step 10979, loss 0.0918569, acc 0.96875\n",
      "2018-05-23T13:43:17.137660: step 10980, loss 0.137046, acc 0.9375\n",
      "2018-05-23T13:43:17.625080: step 10981, loss 0.218394, acc 0.921875\n",
      "2018-05-23T13:43:18.022421: step 10982, loss 0.0705189, acc 0.984375\n",
      "2018-05-23T13:43:18.564718: step 10983, loss 0.0678837, acc 0.984375\n",
      "2018-05-23T13:43:18.961048: step 10984, loss 0.116493, acc 0.9375\n",
      "2018-05-23T13:43:19.370987: step 10985, loss 0.118835, acc 0.953125\n",
      "2018-05-23T13:43:19.922730: step 10986, loss 0.119338, acc 0.9375\n",
      "2018-05-23T13:43:20.374264: step 10987, loss 0.184957, acc 0.953125\n",
      "2018-05-23T13:43:20.926318: step 10988, loss 0.138094, acc 0.921875\n",
      "2018-05-23T13:43:21.474944: step 10989, loss 0.148504, acc 0.90625\n",
      "2018-05-23T13:43:21.948051: step 10990, loss 0.0937239, acc 0.953125\n",
      "2018-05-23T13:43:22.462795: step 10991, loss 0.141, acc 0.9375\n",
      "2018-05-23T13:43:22.886852: step 10992, loss 0.153511, acc 0.9375\n",
      "2018-05-23T13:43:23.691279: step 10993, loss 0.137128, acc 0.90625\n",
      "2018-05-23T13:43:24.172578: step 10994, loss 0.0728921, acc 0.96875\n",
      "2018-05-23T13:43:24.668710: step 10995, loss 0.0896942, acc 0.96875\n",
      "2018-05-23T13:43:25.074571: step 10996, loss 0.137603, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:43:25.560471: step 10997, loss 0.219507, acc 0.875\n",
      "2018-05-23T13:43:26.281018: step 10998, loss 0.066425, acc 0.96875\n",
      "2018-05-23T13:43:26.877632: step 10999, loss 0.133013, acc 0.9375\n",
      "2018-05-23T13:43:27.421941: step 11000, loss 0.151475, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:43:34.507608: step 11000, loss 1.12729, acc 0.723818\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11000\n",
      "\n",
      "2018-05-23T13:43:36.666655: step 11001, loss 0.212414, acc 0.90625\n",
      "2018-05-23T13:43:37.105460: step 11002, loss 0.160817, acc 0.921875\n",
      "2018-05-23T13:43:37.548488: step 11003, loss 0.0999077, acc 0.96875\n",
      "2018-05-23T13:43:37.984817: step 11004, loss 0.101739, acc 0.96875\n",
      "2018-05-23T13:43:38.553454: step 11005, loss 0.157211, acc 0.90625\n",
      "2018-05-23T13:43:38.994452: step 11006, loss 0.101915, acc 0.9375\n",
      "2018-05-23T13:43:39.407463: step 11007, loss 0.101406, acc 0.96875\n",
      "2018-05-23T13:43:39.839444: step 11008, loss 0.142486, acc 0.9375\n",
      "2018-05-23T13:43:40.260454: step 11009, loss 0.14043, acc 0.953125\n",
      "2018-05-23T13:43:40.704932: step 11010, loss 0.220126, acc 0.9375\n",
      "2018-05-23T13:43:41.132522: step 11011, loss 0.154139, acc 0.9375\n",
      "2018-05-23T13:43:41.548527: step 11012, loss 0.162261, acc 0.9375\n",
      "2018-05-23T13:43:41.933890: step 11013, loss 0.183442, acc 0.9375\n",
      "2018-05-23T13:43:42.419578: step 11014, loss 0.0914657, acc 0.953125\n",
      "2018-05-23T13:43:42.982743: step 11015, loss 0.137527, acc 0.9375\n",
      "2018-05-23T13:43:43.524358: step 11016, loss 0.192194, acc 0.90625\n",
      "2018-05-23T13:43:44.044529: step 11017, loss 0.124815, acc 0.953125\n",
      "2018-05-23T13:43:44.652453: step 11018, loss 0.14241, acc 0.953125\n",
      "2018-05-23T13:43:45.079828: step 11019, loss 0.154409, acc 0.96875\n",
      "2018-05-23T13:43:45.543615: step 11020, loss 0.10643, acc 0.96875\n",
      "2018-05-23T13:43:45.952536: step 11021, loss 0.0846547, acc 0.96875\n",
      "2018-05-23T13:43:46.534042: step 11022, loss 0.151911, acc 0.90625\n",
      "2018-05-23T13:43:47.076121: step 11023, loss 0.135546, acc 0.96875\n",
      "2018-05-23T13:43:47.585481: step 11024, loss 0.111848, acc 0.984375\n",
      "2018-05-23T13:43:48.026942: step 11025, loss 0.120836, acc 0.9375\n",
      "2018-05-23T13:43:48.444909: step 11026, loss 0.0719725, acc 0.984375\n",
      "2018-05-23T13:43:48.860597: step 11027, loss 0.137219, acc 0.953125\n",
      "2018-05-23T13:43:49.272036: step 11028, loss 0.121048, acc 0.953125\n",
      "2018-05-23T13:43:49.757545: step 11029, loss 0.0763495, acc 0.953125\n",
      "2018-05-23T13:43:50.197438: step 11030, loss 0.159009, acc 0.9375\n",
      "2018-05-23T13:43:50.680891: step 11031, loss 0.0894553, acc 0.96875\n",
      "2018-05-23T13:43:51.091975: step 11032, loss 0.0829201, acc 0.96875\n",
      "2018-05-23T13:43:51.487286: step 11033, loss 0.118093, acc 0.953125\n",
      "2018-05-23T13:43:51.885342: step 11034, loss 0.157172, acc 0.921875\n",
      "2018-05-23T13:43:52.287879: step 11035, loss 0.101894, acc 0.96875\n",
      "2018-05-23T13:43:52.768800: step 11036, loss 0.127009, acc 0.9375\n",
      "2018-05-23T13:43:53.172525: step 11037, loss 0.122984, acc 0.90625\n",
      "2018-05-23T13:43:53.592579: step 11038, loss 0.144329, acc 0.921875\n",
      "2018-05-23T13:43:54.022470: step 11039, loss 0.139455, acc 0.921875\n",
      "2018-05-23T13:43:54.459538: step 11040, loss 0.136807, acc 0.921875\n",
      "2018-05-23T13:43:54.903371: step 11041, loss 0.123459, acc 0.921875\n",
      "2018-05-23T13:43:55.308430: step 11042, loss 0.233973, acc 0.9375\n",
      "2018-05-23T13:43:55.703320: step 11043, loss 0.130379, acc 0.9375\n",
      "2018-05-23T13:43:56.162449: step 11044, loss 0.135062, acc 0.953125\n",
      "2018-05-23T13:43:56.552314: step 11045, loss 0.118291, acc 0.9375\n",
      "2018-05-23T13:43:57.014873: step 11046, loss 0.13296, acc 0.921875\n",
      "2018-05-23T13:43:57.451302: step 11047, loss 0.211736, acc 0.921875\n",
      "2018-05-23T13:43:57.921540: step 11048, loss 0.14616, acc 0.921875\n",
      "2018-05-23T13:43:58.327681: step 11049, loss 0.241103, acc 0.90625\n",
      "2018-05-23T13:43:58.727642: step 11050, loss 0.100612, acc 0.953125\n",
      "2018-05-23T13:43:59.172592: step 11051, loss 0.0911748, acc 0.984375\n",
      "2018-05-23T13:43:59.591712: step 11052, loss 0.121699, acc 0.953125\n",
      "2018-05-23T13:44:00.085437: step 11053, loss 0.0746125, acc 0.984375\n",
      "2018-05-23T13:44:00.670908: step 11054, loss 0.299178, acc 0.90625\n",
      "2018-05-23T13:44:01.133732: step 11055, loss 0.156068, acc 0.90625\n",
      "2018-05-23T13:44:01.596209: step 11056, loss 0.149138, acc 0.9375\n",
      "2018-05-23T13:44:02.022167: step 11057, loss 0.207803, acc 0.921875\n",
      "2018-05-23T13:44:02.503916: step 11058, loss 0.139868, acc 0.9375\n",
      "2018-05-23T13:44:02.927575: step 11059, loss 0.154417, acc 0.90625\n",
      "2018-05-23T13:44:03.426062: step 11060, loss 0.0694879, acc 0.984375\n",
      "2018-05-23T13:44:03.833890: step 11061, loss 0.108231, acc 0.953125\n",
      "2018-05-23T13:44:04.312365: step 11062, loss 0.144261, acc 0.953125\n",
      "2018-05-23T13:44:04.704270: step 11063, loss 0.183192, acc 0.9375\n",
      "2018-05-23T13:44:05.104840: step 11064, loss 0.13898, acc 0.96875\n",
      "2018-05-23T13:44:05.505310: step 11065, loss 0.153236, acc 0.921875\n",
      "2018-05-23T13:44:05.912820: step 11066, loss 0.182193, acc 0.921875\n",
      "2018-05-23T13:44:06.314639: step 11067, loss 0.183689, acc 0.921875\n",
      "2018-05-23T13:44:06.708872: step 11068, loss 0.206087, acc 0.9375\n",
      "2018-05-23T13:44:07.139816: step 11069, loss 0.0523193, acc 0.984375\n",
      "2018-05-23T13:44:07.550190: step 11070, loss 0.28527, acc 0.828125\n",
      "2018-05-23T13:44:08.140688: step 11071, loss 0.12599, acc 0.953125\n",
      "2018-05-23T13:44:08.590962: step 11072, loss 0.184229, acc 0.9375\n",
      "2018-05-23T13:44:08.993829: step 11073, loss 0.269011, acc 0.875\n",
      "2018-05-23T13:44:09.466534: step 11074, loss 0.173195, acc 0.921875\n",
      "2018-05-23T13:44:09.877309: step 11075, loss 0.0666144, acc 0.984375\n",
      "2018-05-23T13:44:10.313663: step 11076, loss 0.143664, acc 0.921875\n",
      "2018-05-23T13:44:10.748012: step 11077, loss 0.129481, acc 0.9375\n",
      "2018-05-23T13:44:11.213282: step 11078, loss 0.203782, acc 0.921875\n",
      "2018-05-23T13:44:11.727951: step 11079, loss 0.179317, acc 0.9375\n",
      "2018-05-23T13:44:12.186238: step 11080, loss 0.156511, acc 0.9375\n",
      "2018-05-23T13:44:12.701563: step 11081, loss 0.222353, acc 0.921875\n",
      "2018-05-23T13:44:13.131500: step 11082, loss 0.15328, acc 0.9375\n",
      "2018-05-23T13:44:13.680418: step 11083, loss 0.252031, acc 0.90625\n",
      "2018-05-23T13:44:14.107278: step 11084, loss 0.0720629, acc 0.96875\n",
      "2018-05-23T13:44:14.601558: step 11085, loss 0.0832755, acc 0.96875\n",
      "2018-05-23T13:44:15.079546: step 11086, loss 0.140054, acc 0.921875\n",
      "2018-05-23T13:44:15.504815: step 11087, loss 0.134047, acc 0.9375\n",
      "2018-05-23T13:44:15.951487: step 11088, loss 0.119561, acc 0.921875\n",
      "2018-05-23T13:44:16.416789: step 11089, loss 0.113777, acc 0.9375\n",
      "2018-05-23T13:44:16.815753: step 11090, loss 0.234107, acc 0.890625\n",
      "2018-05-23T13:44:17.272127: step 11091, loss 0.0518869, acc 1\n",
      "2018-05-23T13:44:17.902941: step 11092, loss 0.120606, acc 0.953125\n",
      "2018-05-23T13:44:18.475724: step 11093, loss 0.111528, acc 0.9375\n",
      "2018-05-23T13:44:19.186489: step 11094, loss 0.048549, acc 1\n",
      "2018-05-23T13:44:19.650279: step 11095, loss 0.144455, acc 0.921875\n",
      "2018-05-23T13:44:20.166459: step 11096, loss 0.159582, acc 0.9375\n",
      "2018-05-23T13:44:20.581171: step 11097, loss 0.14659, acc 0.9375\n",
      "2018-05-23T13:44:21.041652: step 11098, loss 0.175142, acc 0.921875\n",
      "2018-05-23T13:44:21.564462: step 11099, loss 0.0985057, acc 0.96875\n",
      "2018-05-23T13:44:22.052299: step 11100, loss 0.157365, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:44:28.766489: step 11100, loss 1.14078, acc 0.721246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11100\n",
      "\n",
      "2018-05-23T13:44:30.591403: step 11101, loss 0.183075, acc 0.921875\n",
      "2018-05-23T13:44:31.076248: step 11102, loss 0.0911923, acc 0.96875\n",
      "2018-05-23T13:44:31.500056: step 11103, loss 0.13516, acc 0.921875\n",
      "2018-05-23T13:44:31.908999: step 11104, loss 0.0990782, acc 0.953125\n",
      "2018-05-23T13:44:32.304393: step 11105, loss 0.144085, acc 0.9375\n",
      "2018-05-23T13:44:32.711303: step 11106, loss 0.104895, acc 0.96875\n",
      "2018-05-23T13:44:33.136199: step 11107, loss 0.214046, acc 0.84375\n",
      "2018-05-23T13:44:33.634188: step 11108, loss 0.213941, acc 0.890625\n",
      "2018-05-23T13:44:34.046699: step 11109, loss 0.0680793, acc 0.96875\n",
      "2018-05-23T13:44:34.442242: step 11110, loss 0.217908, acc 0.9375\n",
      "2018-05-23T13:44:34.849739: step 11111, loss 0.163566, acc 0.921875\n",
      "2018-05-23T13:44:35.245724: step 11112, loss 0.221044, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:44:35.639880: step 11113, loss 0.142766, acc 0.921875\n",
      "2018-05-23T13:44:36.081731: step 11114, loss 0.0876895, acc 1\n",
      "2018-05-23T13:44:36.573075: step 11115, loss 0.245385, acc 0.9375\n",
      "2018-05-23T13:44:37.019317: step 11116, loss 0.221441, acc 0.921875\n",
      "2018-05-23T13:44:37.420782: step 11117, loss 0.139341, acc 0.9375\n",
      "2018-05-23T13:44:37.904423: step 11118, loss 0.223868, acc 0.875\n",
      "2018-05-23T13:44:38.298447: step 11119, loss 0.192869, acc 0.953125\n",
      "2018-05-23T13:44:38.701410: step 11120, loss 0.155658, acc 0.9375\n",
      "2018-05-23T13:44:39.169718: step 11121, loss 0.0878003, acc 0.96875\n",
      "2018-05-23T13:44:39.568982: step 11122, loss 0.265333, acc 0.84375\n",
      "2018-05-23T13:44:40.021445: step 11123, loss 0.131098, acc 0.90625\n",
      "2018-05-23T13:44:40.419384: step 11124, loss 0.0851878, acc 0.96875\n",
      "2018-05-23T13:44:40.823497: step 11125, loss 0.249448, acc 0.921875\n",
      "2018-05-23T13:44:41.269953: step 11126, loss 0.124925, acc 0.9375\n",
      "2018-05-23T13:44:41.694768: step 11127, loss 0.141067, acc 0.90625\n",
      "2018-05-23T13:44:42.130743: step 11128, loss 0.150486, acc 0.921875\n",
      "2018-05-23T13:44:42.555428: step 11129, loss 0.230986, acc 0.9375\n",
      "2018-05-23T13:44:42.991316: step 11130, loss 0.151102, acc 0.953125\n",
      "2018-05-23T13:44:43.442373: step 11131, loss 0.206766, acc 0.9375\n",
      "2018-05-23T13:44:43.871389: step 11132, loss 0.0833729, acc 0.96875\n",
      "2018-05-23T13:44:44.326432: step 11133, loss 0.109548, acc 0.953125\n",
      "2018-05-23T13:44:44.719380: step 11134, loss 0.106139, acc 0.953125\n",
      "2018-05-23T13:44:45.129368: step 11135, loss 0.114634, acc 0.9375\n",
      "2018-05-23T13:44:45.578488: step 11136, loss 0.153928, acc 0.953125\n",
      "2018-05-23T13:44:45.980482: step 11137, loss 0.162592, acc 0.90625\n",
      "2018-05-23T13:44:46.564476: step 11138, loss 0.163017, acc 0.921875\n",
      "2018-05-23T13:44:46.997346: step 11139, loss 0.167454, acc 0.9375\n",
      "2018-05-23T13:44:47.415205: step 11140, loss 0.170723, acc 0.90625\n",
      "2018-05-23T13:44:47.911562: step 11141, loss 0.110318, acc 0.96875\n",
      "2018-05-23T13:44:48.391137: step 11142, loss 0.0939967, acc 0.96875\n",
      "2018-05-23T13:44:48.800632: step 11143, loss 0.220797, acc 0.921875\n",
      "2018-05-23T13:44:49.228922: step 11144, loss 0.0552769, acc 1\n",
      "2018-05-23T13:44:49.658840: step 11145, loss 0.188414, acc 0.921875\n",
      "2018-05-23T13:44:50.115684: step 11146, loss 0.0915998, acc 0.984375\n",
      "2018-05-23T13:44:50.602576: step 11147, loss 0.109592, acc 0.953125\n",
      "2018-05-23T13:44:51.050662: step 11148, loss 0.135395, acc 0.9375\n",
      "2018-05-23T13:44:51.465520: step 11149, loss 0.185709, acc 0.90625\n",
      "2018-05-23T13:44:51.861676: step 11150, loss 0.0751715, acc 0.984375\n",
      "2018-05-23T13:44:52.270890: step 11151, loss 0.080826, acc 0.96875\n",
      "2018-05-23T13:44:52.769109: step 11152, loss 0.268997, acc 0.859375\n",
      "2018-05-23T13:44:53.175023: step 11153, loss 0.0924884, acc 0.984375\n",
      "2018-05-23T13:44:53.626046: step 11154, loss 0.0740255, acc 0.96875\n",
      "2018-05-23T13:44:54.019068: step 11155, loss 0.12977, acc 0.953125\n",
      "2018-05-23T13:44:54.432201: step 11156, loss 0.215312, acc 0.890625\n",
      "2018-05-23T13:44:54.826181: step 11157, loss 0.137415, acc 0.921875\n",
      "2018-05-23T13:44:55.231127: step 11158, loss 0.247079, acc 0.859375\n",
      "2018-05-23T13:44:55.640059: step 11159, loss 0.175245, acc 0.96875\n",
      "2018-05-23T13:44:56.037519: step 11160, loss 0.10796, acc 0.953125\n",
      "2018-05-23T13:44:56.438974: step 11161, loss 0.170039, acc 0.921875\n",
      "2018-05-23T13:44:56.829426: step 11162, loss 0.109048, acc 0.96875\n",
      "2018-05-23T13:44:57.225765: step 11163, loss 0.048025, acc 1\n",
      "2018-05-23T13:44:57.634410: step 11164, loss 0.149493, acc 0.953125\n",
      "2018-05-23T13:44:58.039346: step 11165, loss 0.130226, acc 0.9375\n",
      "2018-05-23T13:44:58.429312: step 11166, loss 0.0799845, acc 0.984375\n",
      "2018-05-23T13:44:58.828509: step 11167, loss 0.101499, acc 0.96875\n",
      "2018-05-23T13:44:59.231963: step 11168, loss 0.255061, acc 0.90625\n",
      "2018-05-23T13:44:59.668112: step 11169, loss 0.135379, acc 0.953125\n",
      "2018-05-23T13:45:00.135158: step 11170, loss 0.230829, acc 0.90625\n",
      "2018-05-23T13:45:00.541327: step 11171, loss 0.239315, acc 0.90625\n",
      "2018-05-23T13:45:00.941257: step 11172, loss 0.197021, acc 0.921875\n",
      "2018-05-23T13:45:01.342297: step 11173, loss 0.0983336, acc 0.96875\n",
      "2018-05-23T13:45:01.750214: step 11174, loss 0.080214, acc 0.984375\n",
      "2018-05-23T13:45:02.229484: step 11175, loss 0.133538, acc 0.9375\n",
      "2018-05-23T13:45:02.630427: step 11176, loss 0.161863, acc 0.921875\n",
      "2018-05-23T13:45:03.092650: step 11177, loss 0.143707, acc 0.9375\n",
      "2018-05-23T13:45:03.676123: step 11178, loss 0.103622, acc 0.953125\n",
      "2018-05-23T13:45:04.146929: step 11179, loss 0.126758, acc 0.9375\n",
      "2018-05-23T13:45:04.563265: step 11180, loss 0.0466465, acc 0.96875\n",
      "2018-05-23T13:45:05.004095: step 11181, loss 0.084941, acc 0.96875\n",
      "2018-05-23T13:45:05.410524: step 11182, loss 0.116979, acc 0.9375\n",
      "2018-05-23T13:45:05.818434: step 11183, loss 0.133817, acc 0.953125\n",
      "2018-05-23T13:45:06.221581: step 11184, loss 0.189213, acc 0.921875\n",
      "2018-05-23T13:45:06.697410: step 11185, loss 0.154413, acc 0.9375\n",
      "2018-05-23T13:45:07.278855: step 11186, loss 0.177153, acc 0.921875\n",
      "2018-05-23T13:45:07.771050: step 11187, loss 0.120412, acc 0.953125\n",
      "2018-05-23T13:45:08.259742: step 11188, loss 0.163674, acc 0.9375\n",
      "2018-05-23T13:45:08.683613: step 11189, loss 0.0830343, acc 0.984375\n",
      "2018-05-23T13:45:09.100007: step 11190, loss 0.275999, acc 0.890625\n",
      "2018-05-23T13:45:09.516425: step 11191, loss 0.129347, acc 0.9375\n",
      "2018-05-23T13:45:09.958620: step 11192, loss 0.17879, acc 0.921875\n",
      "2018-05-23T13:45:10.479475: step 11193, loss 0.118527, acc 0.953125\n",
      "2018-05-23T13:45:10.976178: step 11194, loss 0.0758155, acc 0.953125\n",
      "2018-05-23T13:45:11.423036: step 11195, loss 0.0938796, acc 0.96875\n",
      "2018-05-23T13:45:11.815999: step 11196, loss 0.140517, acc 0.921875\n",
      "2018-05-23T13:45:12.243389: step 11197, loss 0.186123, acc 0.859375\n",
      "2018-05-23T13:45:12.641439: step 11198, loss 0.205852, acc 0.90625\n",
      "2018-05-23T13:45:13.092384: step 11199, loss 0.0694634, acc 0.984375\n",
      "2018-05-23T13:45:13.566679: step 11200, loss 0.271453, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:45:19.542590: step 11200, loss 1.18004, acc 0.713245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11200\n",
      "\n",
      "2018-05-23T13:45:21.225782: step 11201, loss 0.186299, acc 0.921875\n",
      "2018-05-23T13:45:21.687446: step 11202, loss 0.118819, acc 0.921875\n",
      "2018-05-23T13:45:22.177303: step 11203, loss 0.120628, acc 0.921875\n",
      "2018-05-23T13:45:22.614095: step 11204, loss 0.104519, acc 0.953125\n",
      "2018-05-23T13:45:23.049103: step 11205, loss 0.189539, acc 0.921875\n",
      "2018-05-23T13:45:23.448045: step 11206, loss 0.0784379, acc 0.984375\n",
      "2018-05-23T13:45:23.892346: step 11207, loss 0.0862916, acc 0.96875\n",
      "2018-05-23T13:45:24.301870: step 11208, loss 0.109406, acc 0.921875\n",
      "2018-05-23T13:45:24.716831: step 11209, loss 0.120229, acc 0.984375\n",
      "2018-05-23T13:45:25.203884: step 11210, loss 0.0676932, acc 0.953125\n",
      "2018-05-23T13:45:25.591990: step 11211, loss 0.14769, acc 0.9375\n",
      "2018-05-23T13:45:25.997008: step 11212, loss 0.147651, acc 0.9375\n",
      "2018-05-23T13:45:26.394972: step 11213, loss 0.114569, acc 0.9375\n",
      "2018-05-23T13:45:26.819697: step 11214, loss 0.145784, acc 0.953125\n",
      "2018-05-23T13:45:27.222610: step 11215, loss 0.125468, acc 0.96875\n",
      "2018-05-23T13:45:27.621703: step 11216, loss 0.086951, acc 0.984375\n",
      "2018-05-23T13:45:28.094765: step 11217, loss 0.0885197, acc 0.953125\n",
      "2018-05-23T13:45:28.518324: step 11218, loss 0.0922294, acc 0.96875\n",
      "2018-05-23T13:45:28.912183: step 11219, loss 0.0996133, acc 0.953125\n",
      "2018-05-23T13:45:29.334634: step 11220, loss 0.141933, acc 0.921875\n",
      "2018-05-23T13:45:29.744549: step 11221, loss 0.0788472, acc 0.953125\n",
      "2018-05-23T13:45:30.257158: step 11222, loss 0.132036, acc 0.9375\n",
      "2018-05-23T13:45:30.694143: step 11223, loss 0.124293, acc 0.9375\n",
      "2018-05-23T13:45:31.104176: step 11224, loss 0.203876, acc 0.90625\n",
      "2018-05-23T13:45:31.510821: step 11225, loss 0.178813, acc 0.921875\n",
      "2018-05-23T13:45:32.135348: step 11226, loss 0.09604, acc 0.984375\n",
      "2018-05-23T13:45:32.606756: step 11227, loss 0.154834, acc 0.90625\n",
      "2018-05-23T13:45:33.071680: step 11228, loss 0.139039, acc 0.9375\n",
      "2018-05-23T13:45:33.530709: step 11229, loss 0.115581, acc 0.953125\n",
      "2018-05-23T13:45:34.004282: step 11230, loss 0.129081, acc 0.96875\n",
      "2018-05-23T13:45:34.554987: step 11231, loss 0.0875311, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:45:35.007251: step 11232, loss 0.282524, acc 0.890625\n",
      "2018-05-23T13:45:35.428652: step 11233, loss 0.156993, acc 0.921875\n",
      "2018-05-23T13:45:35.837084: step 11234, loss 0.085584, acc 0.96875\n",
      "2018-05-23T13:45:36.265354: step 11235, loss 0.0709202, acc 1\n",
      "2018-05-23T13:45:36.677489: step 11236, loss 0.164932, acc 0.953125\n",
      "2018-05-23T13:45:37.094537: step 11237, loss 0.124951, acc 0.953125\n",
      "2018-05-23T13:45:37.607002: step 11238, loss 0.137841, acc 0.953125\n",
      "2018-05-23T13:45:38.020971: step 11239, loss 0.156485, acc 0.9375\n",
      "2018-05-23T13:45:38.433058: step 11240, loss 0.0906489, acc 0.9375\n",
      "2018-05-23T13:45:38.839017: step 11241, loss 0.0731368, acc 0.96875\n",
      "2018-05-23T13:45:39.244659: step 11242, loss 0.0656578, acc 1\n",
      "2018-05-23T13:45:39.748790: step 11243, loss 0.14418, acc 0.9375\n",
      "2018-05-23T13:45:40.164254: step 11244, loss 0.110475, acc 0.96875\n",
      "2018-05-23T13:45:40.569219: step 11245, loss 0.0660199, acc 0.953125\n",
      "2018-05-23T13:45:40.974342: step 11246, loss 0.18636, acc 0.921875\n",
      "2018-05-23T13:45:41.386258: step 11247, loss 0.121352, acc 0.96875\n",
      "2018-05-23T13:45:41.833653: step 11248, loss 0.176229, acc 0.921875\n",
      "2018-05-23T13:45:42.261534: step 11249, loss 0.12265, acc 0.921875\n",
      "2018-05-23T13:45:42.663910: step 11250, loss 0.148587, acc 0.9375\n",
      "2018-05-23T13:45:43.068364: step 11251, loss 0.100042, acc 0.953125\n",
      "2018-05-23T13:45:43.504168: step 11252, loss 0.148885, acc 0.96875\n",
      "2018-05-23T13:45:44.172939: step 11253, loss 0.0921412, acc 0.96875\n",
      "2018-05-23T13:45:44.695586: step 11254, loss 0.101522, acc 0.9375\n",
      "2018-05-23T13:45:45.174308: step 11255, loss 0.143962, acc 0.921875\n",
      "2018-05-23T13:45:45.595347: step 11256, loss 0.131866, acc 0.96875\n",
      "2018-05-23T13:45:46.062350: step 11257, loss 0.187447, acc 0.921875\n",
      "2018-05-23T13:45:46.484045: step 11258, loss 0.0958025, acc 0.96875\n",
      "2018-05-23T13:45:46.944339: step 11259, loss 0.274508, acc 0.90625\n",
      "2018-05-23T13:45:47.411303: step 11260, loss 0.0533656, acc 0.96875\n",
      "2018-05-23T13:45:47.804392: step 11261, loss 0.215378, acc 0.875\n",
      "2018-05-23T13:45:48.272886: step 11262, loss 0.130668, acc 0.96875\n",
      "2018-05-23T13:45:48.670834: step 11263, loss 0.0764922, acc 0.953125\n",
      "2018-05-23T13:45:49.108251: step 11264, loss 0.195789, acc 0.921875\n",
      "2018-05-23T13:45:49.511748: step 11265, loss 0.0877558, acc 1\n",
      "2018-05-23T13:45:50.034245: step 11266, loss 0.189443, acc 0.90625\n",
      "2018-05-23T13:45:50.433836: step 11267, loss 0.108677, acc 0.9375\n",
      "2018-05-23T13:45:50.862916: step 11268, loss 0.183149, acc 0.921875\n",
      "2018-05-23T13:45:51.443410: step 11269, loss 0.138881, acc 0.953125\n",
      "2018-05-23T13:45:52.017006: step 11270, loss 0.191759, acc 0.9375\n",
      "2018-05-23T13:45:53.011024: step 11271, loss 0.196944, acc 0.953125\n",
      "2018-05-23T13:45:53.615083: step 11272, loss 0.17526, acc 0.921875\n",
      "2018-05-23T13:45:54.142292: step 11273, loss 0.107932, acc 0.96875\n",
      "2018-05-23T13:45:54.620453: step 11274, loss 0.21324, acc 0.90625\n",
      "2018-05-23T13:45:55.027629: step 11275, loss 0.411504, acc 0.84375\n",
      "2018-05-23T13:45:55.473170: step 11276, loss 0.0674487, acc 0.953125\n",
      "2018-05-23T13:45:55.883430: step 11277, loss 0.243315, acc 0.921875\n",
      "2018-05-23T13:45:56.305319: step 11278, loss 0.11248, acc 0.9375\n",
      "2018-05-23T13:45:56.779089: step 11279, loss 0.0963019, acc 0.96875\n",
      "2018-05-23T13:45:57.176947: step 11280, loss 0.11237, acc 0.953125\n",
      "2018-05-23T13:45:57.588887: step 11281, loss 0.0539875, acc 0.96875\n",
      "2018-05-23T13:45:58.012301: step 11282, loss 0.100526, acc 0.953125\n",
      "2018-05-23T13:45:58.521359: step 11283, loss 0.17826, acc 0.90625\n",
      "2018-05-23T13:45:58.971665: step 11284, loss 0.0713258, acc 0.984375\n",
      "2018-05-23T13:45:59.377098: step 11285, loss 0.119733, acc 0.96875\n",
      "2018-05-23T13:45:59.796334: step 11286, loss 0.238003, acc 0.890625\n",
      "2018-05-23T13:46:00.202294: step 11287, loss 0.135867, acc 0.96875\n",
      "2018-05-23T13:46:00.654103: step 11288, loss 0.175169, acc 0.953125\n",
      "2018-05-23T13:46:01.131833: step 11289, loss 0.307642, acc 0.890625\n",
      "2018-05-23T13:46:01.606144: step 11290, loss 0.201003, acc 0.921875\n",
      "2018-05-23T13:46:02.015445: step 11291, loss 0.261772, acc 0.921875\n",
      "2018-05-23T13:46:02.419819: step 11292, loss 0.248935, acc 0.890625\n",
      "2018-05-23T13:46:02.910128: step 11293, loss 0.141218, acc 0.9375\n",
      "2018-05-23T13:46:03.382928: step 11294, loss 0.0646759, acc 0.984375\n",
      "2018-05-23T13:46:03.789350: step 11295, loss 0.276227, acc 0.921875\n",
      "2018-05-23T13:46:04.183322: step 11296, loss 0.132898, acc 0.953125\n",
      "2018-05-23T13:46:04.599259: step 11297, loss 0.112873, acc 0.96875\n",
      "2018-05-23T13:46:05.009687: step 11298, loss 0.0463304, acc 0.984375\n",
      "2018-05-23T13:46:05.407307: step 11299, loss 0.119997, acc 0.96875\n",
      "2018-05-23T13:46:05.881680: step 11300, loss 0.168218, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:46:11.675255: step 11300, loss 1.15868, acc 0.719103\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11300\n",
      "\n",
      "2018-05-23T13:46:13.428370: step 11301, loss 0.0806543, acc 0.96875\n",
      "2018-05-23T13:46:13.869211: step 11302, loss 0.155167, acc 0.9375\n",
      "2018-05-23T13:46:14.399170: step 11303, loss 0.170815, acc 0.90625\n",
      "2018-05-23T13:46:14.864342: step 11304, loss 0.201018, acc 0.90625\n",
      "2018-05-23T13:46:15.324661: step 11305, loss 0.0849442, acc 0.96875\n",
      "2018-05-23T13:46:15.728550: step 11306, loss 0.142597, acc 0.953125\n",
      "2018-05-23T13:46:16.207146: step 11307, loss 0.0769876, acc 0.984375\n",
      "2018-05-23T13:46:16.609103: step 11308, loss 0.178892, acc 0.953125\n",
      "2018-05-23T13:46:17.158584: step 11309, loss 0.204002, acc 0.921875\n",
      "2018-05-23T13:46:17.664316: step 11310, loss 0.184222, acc 0.90625\n",
      "2018-05-23T13:46:18.070186: step 11311, loss 0.328358, acc 0.875\n",
      "2018-05-23T13:46:18.507257: step 11312, loss 0.102296, acc 0.953125\n",
      "2018-05-23T13:46:18.908640: step 11313, loss 0.182991, acc 0.90625\n",
      "2018-05-23T13:46:19.336006: step 11314, loss 0.0923118, acc 0.953125\n",
      "2018-05-23T13:46:19.788796: step 11315, loss 0.119872, acc 0.953125\n",
      "2018-05-23T13:46:20.311401: step 11316, loss 0.157979, acc 0.9375\n",
      "2018-05-23T13:46:20.779329: step 11317, loss 0.135285, acc 0.9375\n",
      "2018-05-23T13:46:21.177554: step 11318, loss 0.0538859, acc 1\n",
      "2018-05-23T13:46:21.601930: step 11319, loss 0.16021, acc 0.90625\n",
      "2018-05-23T13:46:22.001372: step 11320, loss 0.0502501, acc 0.96875\n",
      "2018-05-23T13:46:22.412295: step 11321, loss 0.14601, acc 0.921875\n",
      "2018-05-23T13:46:22.832194: step 11322, loss 0.289809, acc 0.9375\n",
      "2018-05-23T13:46:23.241119: step 11323, loss 0.102729, acc 0.9375\n",
      "2018-05-23T13:46:23.725157: step 11324, loss 0.0758365, acc 0.984375\n",
      "2018-05-23T13:46:24.125009: step 11325, loss 0.0928542, acc 0.96875\n",
      "2018-05-23T13:46:24.526533: step 11326, loss 0.101563, acc 0.953125\n",
      "2018-05-23T13:46:24.930459: step 11327, loss 0.27041, acc 0.890625\n",
      "2018-05-23T13:46:25.343352: step 11328, loss 0.513252, acc 0.890625\n",
      "2018-05-23T13:46:25.805137: step 11329, loss 0.107217, acc 0.953125\n",
      "2018-05-23T13:46:26.190392: step 11330, loss 0.148914, acc 0.9375\n",
      "2018-05-23T13:46:26.579366: step 11331, loss 0.195007, acc 0.890625\n",
      "2018-05-23T13:46:26.990239: step 11332, loss 0.145623, acc 0.90625\n",
      "2018-05-23T13:46:27.380060: step 11333, loss 0.146218, acc 0.953125\n",
      "2018-05-23T13:46:27.782981: step 11334, loss 0.101526, acc 0.953125\n",
      "2018-05-23T13:46:28.175128: step 11335, loss 0.142588, acc 0.9375\n",
      "2018-05-23T13:46:28.579068: step 11336, loss 0.112558, acc 0.9375\n",
      "2018-05-23T13:46:28.979025: step 11337, loss 0.151501, acc 0.921875\n",
      "2018-05-23T13:46:29.381010: step 11338, loss 0.12609, acc 0.9375\n",
      "2018-05-23T13:46:29.787253: step 11339, loss 0.135031, acc 0.9375\n",
      "2018-05-23T13:46:30.222125: step 11340, loss 0.142189, acc 0.96875\n",
      "2018-05-23T13:46:30.664118: step 11341, loss 0.166974, acc 0.921875\n",
      "2018-05-23T13:46:31.148538: step 11342, loss 0.117826, acc 0.953125\n",
      "2018-05-23T13:46:31.575903: step 11343, loss 0.241838, acc 0.875\n",
      "2018-05-23T13:46:31.996117: step 11344, loss 0.142273, acc 0.90625\n",
      "2018-05-23T13:46:32.405505: step 11345, loss 0.114896, acc 0.9375\n",
      "2018-05-23T13:46:32.903110: step 11346, loss 0.0766775, acc 0.953125\n",
      "2018-05-23T13:46:33.339170: step 11347, loss 0.123981, acc 0.953125\n",
      "2018-05-23T13:46:33.742983: step 11348, loss 0.27892, acc 0.875\n",
      "2018-05-23T13:46:34.206079: step 11349, loss 0.189089, acc 0.953125\n",
      "2018-05-23T13:46:34.625990: step 11350, loss 0.322204, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:46:35.040866: step 11351, loss 0.200233, acc 0.9375\n",
      "2018-05-23T13:46:35.521603: step 11352, loss 0.136437, acc 0.953125\n",
      "2018-05-23T13:46:35.917900: step 11353, loss 0.0482079, acc 0.984375\n",
      "2018-05-23T13:46:36.339213: step 11354, loss 0.189811, acc 0.9375\n",
      "2018-05-23T13:46:36.738309: step 11355, loss 0.111004, acc 0.953125\n",
      "2018-05-23T13:46:37.155240: step 11356, loss 0.169373, acc 0.953125\n",
      "2018-05-23T13:46:37.573635: step 11357, loss 0.155831, acc 0.921875\n",
      "2018-05-23T13:46:37.996575: step 11358, loss 0.118263, acc 0.953125\n",
      "2018-05-23T13:46:38.570007: step 11359, loss 0.0933643, acc 0.953125\n",
      "2018-05-23T13:46:39.022764: step 11360, loss 0.276166, acc 0.890625\n",
      "2018-05-23T13:46:39.578789: step 11361, loss 0.0726735, acc 0.984375\n",
      "2018-05-23T13:46:40.089262: step 11362, loss 0.0767784, acc 0.953125\n",
      "2018-05-23T13:46:40.551553: step 11363, loss 0.143532, acc 0.953125\n",
      "2018-05-23T13:46:40.965451: step 11364, loss 0.141902, acc 0.9375\n",
      "2018-05-23T13:46:41.377867: step 11365, loss 0.129177, acc 0.96875\n",
      "2018-05-23T13:46:41.799295: step 11366, loss 0.148029, acc 0.921875\n",
      "2018-05-23T13:46:42.225678: step 11367, loss 0.18299, acc 0.921875\n",
      "2018-05-23T13:46:42.746307: step 11368, loss 0.080662, acc 0.96875\n",
      "2018-05-23T13:46:43.151230: step 11369, loss 0.206004, acc 0.90625\n",
      "2018-05-23T13:46:43.659299: step 11370, loss 0.152505, acc 0.921875\n",
      "2018-05-23T13:46:44.063105: step 11371, loss 0.145748, acc 0.921875\n",
      "2018-05-23T13:46:44.459473: step 11372, loss 0.121706, acc 0.953125\n",
      "2018-05-23T13:46:44.885427: step 11373, loss 0.132564, acc 0.921875\n",
      "2018-05-23T13:46:45.282297: step 11374, loss 0.157753, acc 0.953125\n",
      "2018-05-23T13:46:45.729371: step 11375, loss 0.140335, acc 0.921875\n",
      "2018-05-23T13:46:46.130448: step 11376, loss 0.155422, acc 0.90625\n",
      "2018-05-23T13:46:46.548352: step 11377, loss 0.227583, acc 0.953125\n",
      "2018-05-23T13:46:47.017614: step 11378, loss 0.117091, acc 0.953125\n",
      "2018-05-23T13:46:47.417581: step 11379, loss 0.224802, acc 0.9375\n",
      "2018-05-23T13:46:47.813522: step 11380, loss 0.152496, acc 0.953125\n",
      "2018-05-23T13:46:48.214221: step 11381, loss 0.214522, acc 0.890625\n",
      "2018-05-23T13:46:48.619157: step 11382, loss 0.204654, acc 0.875\n",
      "2018-05-23T13:46:49.012151: step 11383, loss 0.0807078, acc 0.984375\n",
      "2018-05-23T13:46:49.432743: step 11384, loss 0.228499, acc 0.859375\n",
      "2018-05-23T13:46:49.893028: step 11385, loss 0.130259, acc 0.953125\n",
      "2018-05-23T13:46:50.319409: step 11386, loss 0.250486, acc 0.953125\n",
      "2018-05-23T13:46:50.710427: step 11387, loss 0.177988, acc 0.953125\n",
      "2018-05-23T13:46:51.179172: step 11388, loss 0.244819, acc 0.875\n",
      "2018-05-23T13:46:51.619259: step 11389, loss 0.248958, acc 0.890625\n",
      "2018-05-23T13:46:52.077076: step 11390, loss 0.161993, acc 0.953125\n",
      "2018-05-23T13:46:52.477984: step 11391, loss 0.0953362, acc 0.96875\n",
      "2018-05-23T13:46:52.898905: step 11392, loss 0.110468, acc 0.984375\n",
      "2018-05-23T13:46:53.377345: step 11393, loss 0.195044, acc 0.890625\n",
      "2018-05-23T13:46:53.763884: step 11394, loss 0.101415, acc 0.953125\n",
      "2018-05-23T13:46:54.159336: step 11395, loss 0.123293, acc 0.9375\n",
      "2018-05-23T13:46:54.558341: step 11396, loss 0.11271, acc 0.953125\n",
      "2018-05-23T13:46:54.947350: step 11397, loss 0.124217, acc 0.921875\n",
      "2018-05-23T13:46:55.344327: step 11398, loss 0.0920901, acc 0.96875\n",
      "2018-05-23T13:46:55.740610: step 11399, loss 0.0976188, acc 0.96875\n",
      "2018-05-23T13:46:56.208061: step 11400, loss 0.209789, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:47:02.054739: step 11400, loss 1.17481, acc 0.719388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11400\n",
      "\n",
      "2018-05-23T13:47:03.735303: step 11401, loss 0.2053, acc 0.9375\n",
      "2018-05-23T13:47:04.207144: step 11402, loss 0.116523, acc 0.921875\n",
      "2018-05-23T13:47:04.790880: step 11403, loss 0.1755, acc 0.90625\n",
      "2018-05-23T13:47:05.462668: step 11404, loss 0.292359, acc 0.90625\n",
      "2018-05-23T13:47:05.897868: step 11405, loss 0.214212, acc 0.875\n",
      "2018-05-23T13:47:06.342745: step 11406, loss 0.139767, acc 0.953125\n",
      "2018-05-23T13:47:06.791663: step 11407, loss 0.166211, acc 0.9375\n",
      "2018-05-23T13:47:07.212704: step 11408, loss 0.0894049, acc 0.96875\n",
      "2018-05-23T13:47:07.695243: step 11409, loss 0.0938174, acc 0.9375\n",
      "2018-05-23T13:47:08.157543: step 11410, loss 0.127636, acc 0.953125\n",
      "2018-05-23T13:47:08.610408: step 11411, loss 0.230394, acc 0.953125\n",
      "2018-05-23T13:47:09.007986: step 11412, loss 0.214911, acc 0.9375\n",
      "2018-05-23T13:47:09.407467: step 11413, loss 0.0740606, acc 0.984375\n",
      "2018-05-23T13:47:09.857544: step 11414, loss 0.140279, acc 0.90625\n",
      "2018-05-23T13:47:10.266990: step 11415, loss 0.133822, acc 0.9375\n",
      "2018-05-23T13:47:10.664479: step 11416, loss 0.14401, acc 0.953125\n",
      "2018-05-23T13:47:11.102170: step 11417, loss 0.141413, acc 0.921875\n",
      "2018-05-23T13:47:11.504055: step 11418, loss 0.272806, acc 0.859375\n",
      "2018-05-23T13:47:11.916215: step 11419, loss 0.288947, acc 0.921875\n",
      "2018-05-23T13:47:12.324866: step 11420, loss 0.149346, acc 0.953125\n",
      "2018-05-23T13:47:12.739025: step 11421, loss 0.123683, acc 0.96875\n",
      "2018-05-23T13:47:13.166907: step 11422, loss 0.0879999, acc 0.953125\n",
      "2018-05-23T13:47:13.571351: step 11423, loss 0.153782, acc 0.953125\n",
      "2018-05-23T13:47:13.970261: step 11424, loss 0.101751, acc 0.96875\n",
      "2018-05-23T13:47:14.375421: step 11425, loss 0.182787, acc 0.921875\n",
      "2018-05-23T13:47:14.769333: step 11426, loss 0.118343, acc 0.96875\n",
      "2018-05-23T13:47:15.167041: step 11427, loss 0.120727, acc 0.90625\n",
      "2018-05-23T13:47:15.559281: step 11428, loss 0.0825468, acc 0.96875\n",
      "2018-05-23T13:47:16.021246: step 11429, loss 0.190042, acc 0.890625\n",
      "2018-05-23T13:47:16.425117: step 11430, loss 0.186881, acc 0.90625\n",
      "2018-05-23T13:47:16.858218: step 11431, loss 0.103533, acc 0.9375\n",
      "2018-05-23T13:47:17.258150: step 11432, loss 0.122069, acc 0.9375\n",
      "2018-05-23T13:47:17.698701: step 11433, loss 0.132505, acc 0.953125\n",
      "2018-05-23T13:47:18.234343: step 11434, loss 0.0902548, acc 0.953125\n",
      "2018-05-23T13:47:18.835373: step 11435, loss 0.188093, acc 0.90625\n",
      "2018-05-23T13:47:19.361187: step 11436, loss 0.126946, acc 0.953125\n",
      "2018-05-23T13:47:19.793183: step 11437, loss 0.125527, acc 0.9375\n",
      "2018-05-23T13:47:20.262289: step 11438, loss 0.268351, acc 0.859375\n",
      "2018-05-23T13:47:20.672247: step 11439, loss 0.175916, acc 0.890625\n",
      "2018-05-23T13:47:21.080082: step 11440, loss 0.165347, acc 0.953125\n",
      "2018-05-23T13:47:21.540090: step 11441, loss 0.134977, acc 0.96875\n",
      "2018-05-23T13:47:21.939375: step 11442, loss 0.164335, acc 0.90625\n",
      "2018-05-23T13:47:22.402266: step 11443, loss 0.166241, acc 0.9375\n",
      "2018-05-23T13:47:22.907489: step 11444, loss 0.16888, acc 0.9375\n",
      "2018-05-23T13:47:23.334494: step 11445, loss 0.176995, acc 0.921875\n",
      "2018-05-23T13:47:23.771334: step 11446, loss 0.109753, acc 0.953125\n",
      "2018-05-23T13:47:24.166333: step 11447, loss 0.296198, acc 0.890625\n",
      "2018-05-23T13:47:24.582665: step 11448, loss 0.179474, acc 0.9375\n",
      "2018-05-23T13:47:24.979322: step 11449, loss 0.14457, acc 0.9375\n",
      "2018-05-23T13:47:25.432353: step 11450, loss 0.212851, acc 0.890625\n",
      "2018-05-23T13:47:25.826303: step 11451, loss 0.169883, acc 0.921875\n",
      "2018-05-23T13:47:26.226334: step 11452, loss 0.141438, acc 0.921875\n",
      "2018-05-23T13:47:26.626307: step 11453, loss 0.0807765, acc 0.953125\n",
      "2018-05-23T13:47:27.018359: step 11454, loss 0.106727, acc 0.953125\n",
      "2018-05-23T13:47:27.484364: step 11455, loss 0.115645, acc 0.953125\n",
      "2018-05-23T13:47:27.878980: step 11456, loss 0.209257, acc 0.96875\n",
      "2018-05-23T13:47:28.280970: step 11457, loss 0.0596403, acc 1\n",
      "2018-05-23T13:47:28.716113: step 11458, loss 0.156298, acc 0.953125\n",
      "2018-05-23T13:47:29.109860: step 11459, loss 0.119584, acc 0.953125\n",
      "2018-05-23T13:47:29.508292: step 11460, loss 0.0544724, acc 0.984375\n",
      "2018-05-23T13:47:30.034023: step 11461, loss 0.161243, acc 0.953125\n",
      "2018-05-23T13:47:30.748970: step 11462, loss 0.147294, acc 0.953125\n",
      "2018-05-23T13:47:31.491371: step 11463, loss 0.144643, acc 0.921875\n",
      "2018-05-23T13:47:32.065914: step 11464, loss 0.0934448, acc 0.953125\n",
      "2018-05-23T13:47:32.532559: step 11465, loss 0.168106, acc 0.9375\n",
      "2018-05-23T13:47:32.983245: step 11466, loss 0.139615, acc 0.921875\n",
      "2018-05-23T13:47:33.413102: step 11467, loss 0.205784, acc 0.90625\n",
      "2018-05-23T13:47:33.821045: step 11468, loss 0.16692, acc 0.921875\n",
      "2018-05-23T13:47:34.225981: step 11469, loss 0.0492506, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:47:34.709207: step 11470, loss 0.167188, acc 0.96875\n",
      "2018-05-23T13:47:35.255891: step 11471, loss 0.241553, acc 0.921875\n",
      "2018-05-23T13:47:35.650836: step 11472, loss 0.157713, acc 0.9375\n",
      "2018-05-23T13:47:36.123447: step 11473, loss 0.0982036, acc 0.953125\n",
      "2018-05-23T13:47:36.519411: step 11474, loss 0.213137, acc 0.921875\n",
      "2018-05-23T13:47:36.934323: step 11475, loss 0.034013, acc 1\n",
      "2018-05-23T13:47:37.362259: step 11476, loss 0.175334, acc 0.953125\n",
      "2018-05-23T13:47:37.799197: step 11477, loss 0.148937, acc 0.953125\n",
      "2018-05-23T13:47:38.213096: step 11478, loss 0.125941, acc 0.9375\n",
      "2018-05-23T13:47:38.621012: step 11479, loss 0.220574, acc 0.875\n",
      "2018-05-23T13:47:39.019068: step 11480, loss 0.0986211, acc 0.953125\n",
      "2018-05-23T13:47:39.417531: step 11481, loss 0.207523, acc 0.90625\n",
      "2018-05-23T13:47:39.809487: step 11482, loss 0.124034, acc 0.9375\n",
      "2018-05-23T13:47:40.238845: step 11483, loss 0.134285, acc 0.9375\n",
      "2018-05-23T13:47:40.668289: step 11484, loss 0.192401, acc 0.90625\n",
      "2018-05-23T13:47:41.073757: step 11485, loss 0.245551, acc 0.90625\n",
      "2018-05-23T13:47:41.514596: step 11486, loss 0.133509, acc 0.9375\n",
      "2018-05-23T13:47:41.943123: step 11487, loss 0.170312, acc 0.90625\n",
      "2018-05-23T13:47:42.412956: step 11488, loss 0.160965, acc 0.921875\n",
      "2018-05-23T13:47:42.898562: step 11489, loss 0.095475, acc 0.96875\n",
      "2018-05-23T13:47:43.353454: step 11490, loss 0.0843474, acc 0.953125\n",
      "2018-05-23T13:47:43.796782: step 11491, loss 0.149264, acc 0.953125\n",
      "2018-05-23T13:47:44.198710: step 11492, loss 0.0901486, acc 0.953125\n",
      "2018-05-23T13:47:44.594956: step 11493, loss 0.100833, acc 0.984375\n",
      "2018-05-23T13:47:45.012866: step 11494, loss 0.163676, acc 0.9375\n",
      "2018-05-23T13:47:45.419350: step 11495, loss 0.146882, acc 0.9375\n",
      "2018-05-23T13:47:45.855717: step 11496, loss 0.191294, acc 0.921875\n",
      "2018-05-23T13:47:46.295631: step 11497, loss 0.142062, acc 0.90625\n",
      "2018-05-23T13:47:46.830272: step 11498, loss 0.23898, acc 0.890625\n",
      "2018-05-23T13:47:47.357023: step 11499, loss 0.192996, acc 0.90625\n",
      "2018-05-23T13:47:47.829276: step 11500, loss 0.20255, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:47:53.891815: step 11500, loss 1.18592, acc 0.715674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11500\n",
      "\n",
      "2018-05-23T13:47:55.703124: step 11501, loss 0.0917051, acc 0.9375\n",
      "2018-05-23T13:47:56.211427: step 11502, loss 0.15886, acc 0.90625\n",
      "2018-05-23T13:47:56.710667: step 11503, loss 0.190025, acc 0.921875\n",
      "2018-05-23T13:47:57.152566: step 11504, loss 0.204902, acc 0.890625\n",
      "2018-05-23T13:47:57.582459: step 11505, loss 0.231374, acc 0.90625\n",
      "2018-05-23T13:47:58.017605: step 11506, loss 0.0925064, acc 0.96875\n",
      "2018-05-23T13:47:58.446541: step 11507, loss 0.119718, acc 0.9375\n",
      "2018-05-23T13:47:58.852457: step 11508, loss 0.223037, acc 0.90625\n",
      "2018-05-23T13:47:59.292862: step 11509, loss 0.113956, acc 0.953125\n",
      "2018-05-23T13:47:59.686989: step 11510, loss 0.0804133, acc 0.953125\n",
      "2018-05-23T13:48:00.219204: step 11511, loss 0.157414, acc 0.921875\n",
      "2018-05-23T13:48:00.626078: step 11512, loss 0.129476, acc 0.953125\n",
      "2018-05-23T13:48:01.034988: step 11513, loss 0.11034, acc 0.96875\n",
      "2018-05-23T13:48:01.528966: step 11514, loss 0.212607, acc 0.90625\n",
      "2018-05-23T13:48:01.980719: step 11515, loss 0.107112, acc 0.9375\n",
      "2018-05-23T13:48:02.444110: step 11516, loss 0.0745362, acc 0.96875\n",
      "2018-05-23T13:48:02.944796: step 11517, loss 0.139338, acc 0.953125\n",
      "2018-05-23T13:48:03.407330: step 11518, loss 0.119777, acc 0.96875\n",
      "2018-05-23T13:48:03.810243: step 11519, loss 0.191611, acc 0.890625\n",
      "2018-05-23T13:48:04.213711: step 11520, loss 0.234149, acc 0.921875\n",
      "2018-05-23T13:48:04.606429: step 11521, loss 0.280288, acc 0.9375\n",
      "2018-05-23T13:48:05.001384: step 11522, loss 0.143337, acc 0.9375\n",
      "2018-05-23T13:48:05.453355: step 11523, loss 0.100784, acc 0.96875\n",
      "2018-05-23T13:48:05.928159: step 11524, loss 0.216055, acc 0.859375\n",
      "2018-05-23T13:48:06.449509: step 11525, loss 0.157096, acc 0.953125\n",
      "2018-05-23T13:48:06.888518: step 11526, loss 0.153866, acc 0.9375\n",
      "2018-05-23T13:48:07.314228: step 11527, loss 0.153018, acc 0.90625\n",
      "2018-05-23T13:48:07.751849: step 11528, loss 0.0915445, acc 0.984375\n",
      "2018-05-23T13:48:08.252621: step 11529, loss 0.274335, acc 0.90625\n",
      "2018-05-23T13:48:08.664737: step 11530, loss 0.15361, acc 0.953125\n",
      "2018-05-23T13:48:09.064700: step 11531, loss 0.21321, acc 0.90625\n",
      "2018-05-23T13:48:09.479611: step 11532, loss 0.100949, acc 0.953125\n",
      "2018-05-23T13:48:09.888224: step 11533, loss 0.131385, acc 0.9375\n",
      "2018-05-23T13:48:10.346125: step 11534, loss 0.143809, acc 0.953125\n",
      "2018-05-23T13:48:10.744238: step 11535, loss 0.128037, acc 0.953125\n",
      "2018-05-23T13:48:11.136296: step 11536, loss 0.201233, acc 0.9375\n",
      "2018-05-23T13:48:11.550763: step 11537, loss 0.0962113, acc 0.984375\n",
      "2018-05-23T13:48:11.946165: step 11538, loss 0.208738, acc 0.921875\n",
      "2018-05-23T13:48:12.410362: step 11539, loss 0.14355, acc 0.9375\n",
      "2018-05-23T13:48:12.816805: step 11540, loss 0.145057, acc 0.921875\n",
      "2018-05-23T13:48:13.215073: step 11541, loss 0.169571, acc 0.90625\n",
      "2018-05-23T13:48:13.725374: step 11542, loss 0.0998432, acc 0.953125\n",
      "2018-05-23T13:48:14.124827: step 11543, loss 0.0839836, acc 0.953125\n",
      "2018-05-23T13:48:14.522445: step 11544, loss 0.107151, acc 0.953125\n",
      "2018-05-23T13:48:14.976458: step 11545, loss 0.178724, acc 0.90625\n",
      "2018-05-23T13:48:15.424260: step 11546, loss 0.0979777, acc 0.96875\n",
      "2018-05-23T13:48:16.004852: step 11547, loss 0.260061, acc 0.875\n",
      "2018-05-23T13:48:16.418288: step 11548, loss 0.14888, acc 0.953125\n",
      "2018-05-23T13:48:16.829571: step 11549, loss 0.0967116, acc 0.953125\n",
      "2018-05-23T13:48:17.226742: step 11550, loss 0.114804, acc 0.96875\n",
      "2018-05-23T13:48:17.639980: step 11551, loss 0.23873, acc 0.90625\n",
      "2018-05-23T13:48:18.169756: step 11552, loss 0.108091, acc 0.9375\n",
      "2018-05-23T13:48:18.615548: step 11553, loss 0.301612, acc 0.890625\n",
      "2018-05-23T13:48:19.066672: step 11554, loss 0.131071, acc 0.96875\n",
      "2018-05-23T13:48:19.473874: step 11555, loss 0.0637348, acc 0.953125\n",
      "2018-05-23T13:48:19.914397: step 11556, loss 0.11182, acc 0.953125\n",
      "2018-05-23T13:48:20.406330: step 11557, loss 0.191675, acc 0.890625\n",
      "2018-05-23T13:48:20.851149: step 11558, loss 0.0963487, acc 0.984375\n",
      "2018-05-23T13:48:21.266293: step 11559, loss 0.213927, acc 0.9375\n",
      "2018-05-23T13:48:21.668231: step 11560, loss 0.145224, acc 0.9375\n",
      "2018-05-23T13:48:22.074145: step 11561, loss 0.138036, acc 0.9375\n",
      "2018-05-23T13:48:22.472783: step 11562, loss 0.14687, acc 0.921875\n",
      "2018-05-23T13:48:23.110884: step 11563, loss 0.166819, acc 0.9375\n",
      "2018-05-23T13:48:23.572165: step 11564, loss 0.2184, acc 0.9375\n",
      "2018-05-23T13:48:23.984065: step 11565, loss 0.137609, acc 0.953125\n",
      "2018-05-23T13:48:24.405967: step 11566, loss 0.10788, acc 0.984375\n",
      "2018-05-23T13:48:24.816861: step 11567, loss 0.28346, acc 0.921875\n",
      "2018-05-23T13:48:25.233753: step 11568, loss 0.168953, acc 0.90625\n",
      "2018-05-23T13:48:25.643068: step 11569, loss 0.104661, acc 0.953125\n",
      "2018-05-23T13:48:26.113809: step 11570, loss 0.153191, acc 0.90625\n",
      "2018-05-23T13:48:26.516746: step 11571, loss 0.149142, acc 0.9375\n",
      "2018-05-23T13:48:26.921188: step 11572, loss 0.116009, acc 0.953125\n",
      "2018-05-23T13:48:27.426347: step 11573, loss 0.119684, acc 0.9375\n",
      "2018-05-23T13:48:27.838252: step 11574, loss 0.0769949, acc 0.96875\n",
      "2018-05-23T13:48:28.247669: step 11575, loss 0.191589, acc 0.9375\n",
      "2018-05-23T13:48:28.652106: step 11576, loss 0.169052, acc 0.890625\n",
      "2018-05-23T13:48:29.083060: step 11577, loss 0.211735, acc 0.890625\n",
      "2018-05-23T13:48:29.564284: step 11578, loss 0.09364, acc 0.953125\n",
      "2018-05-23T13:48:30.014079: step 11579, loss 0.151143, acc 0.9375\n",
      "2018-05-23T13:48:30.494314: step 11580, loss 0.111678, acc 0.96875\n",
      "2018-05-23T13:48:30.911196: step 11581, loss 0.103781, acc 0.96875\n",
      "2018-05-23T13:48:31.328588: step 11582, loss 0.0877312, acc 0.984375\n",
      "2018-05-23T13:48:31.760433: step 11583, loss 0.160306, acc 0.953125\n",
      "2018-05-23T13:48:32.150900: step 11584, loss 0.289253, acc 0.875\n",
      "2018-05-23T13:48:32.611669: step 11585, loss 0.167105, acc 0.90625\n",
      "2018-05-23T13:48:33.262477: step 11586, loss 0.165104, acc 0.9375\n",
      "2018-05-23T13:48:33.725281: step 11587, loss 0.0765253, acc 0.953125\n",
      "2018-05-23T13:48:34.149655: step 11588, loss 0.176211, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:48:34.555083: step 11589, loss 0.273142, acc 0.9375\n",
      "2018-05-23T13:48:35.030341: step 11590, loss 0.148119, acc 0.953125\n",
      "2018-05-23T13:48:35.492630: step 11591, loss 0.195005, acc 0.890625\n",
      "2018-05-23T13:48:35.987818: step 11592, loss 0.299529, acc 0.875\n",
      "2018-05-23T13:48:36.447587: step 11593, loss 0.228734, acc 0.90625\n",
      "2018-05-23T13:48:36.905362: step 11594, loss 0.12225, acc 0.9375\n",
      "2018-05-23T13:48:37.315266: step 11595, loss 0.116109, acc 0.96875\n",
      "2018-05-23T13:48:37.796511: step 11596, loss 0.122431, acc 0.96875\n",
      "2018-05-23T13:48:38.227014: step 11597, loss 0.107245, acc 0.96875\n",
      "2018-05-23T13:48:38.636938: step 11598, loss 0.111213, acc 0.96875\n",
      "2018-05-23T13:48:39.038862: step 11599, loss 0.0620974, acc 0.984375\n",
      "2018-05-23T13:48:39.478252: step 11600, loss 0.10622, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:48:45.293328: step 11600, loss 1.1874, acc 0.709816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11600\n",
      "\n",
      "2018-05-23T13:48:47.230188: step 11601, loss 0.196023, acc 0.921875\n",
      "2018-05-23T13:48:47.693970: step 11602, loss 0.0996276, acc 0.96875\n",
      "2018-05-23T13:48:48.156744: step 11603, loss 0.123945, acc 0.953125\n",
      "2018-05-23T13:48:48.714270: step 11604, loss 0.153858, acc 0.9375\n",
      "2018-05-23T13:48:49.158103: step 11605, loss 0.173692, acc 0.9375\n",
      "2018-05-23T13:48:49.596956: step 11606, loss 0.136541, acc 0.9375\n",
      "2018-05-23T13:48:49.994900: step 11607, loss 0.236522, acc 0.875\n",
      "2018-05-23T13:48:50.417084: step 11608, loss 0.127762, acc 0.953125\n",
      "2018-05-23T13:48:50.845984: step 11609, loss 0.123061, acc 0.9375\n",
      "2018-05-23T13:48:51.278128: step 11610, loss 0.220549, acc 0.875\n",
      "2018-05-23T13:48:51.753878: step 11611, loss 0.204554, acc 0.90625\n",
      "2018-05-23T13:48:52.146018: step 11612, loss 0.19607, acc 0.90625\n",
      "2018-05-23T13:48:52.549956: step 11613, loss 0.141806, acc 0.953125\n",
      "2018-05-23T13:48:52.965954: step 11614, loss 0.199152, acc 0.9375\n",
      "2018-05-23T13:48:53.416749: step 11615, loss 0.171405, acc 0.921875\n",
      "2018-05-23T13:48:53.827711: step 11616, loss 0.116703, acc 0.9375\n",
      "2018-05-23T13:48:54.227265: step 11617, loss 0.11373, acc 0.96875\n",
      "2018-05-23T13:48:54.668595: step 11618, loss 0.107042, acc 0.953125\n",
      "2018-05-23T13:48:55.085485: step 11619, loss 0.126848, acc 0.9375\n",
      "2018-05-23T13:48:55.481787: step 11620, loss 0.160592, acc 0.9375\n",
      "2018-05-23T13:48:55.962549: step 11621, loss 0.178148, acc 0.90625\n",
      "2018-05-23T13:48:56.372462: step 11622, loss 0.127519, acc 0.9375\n",
      "2018-05-23T13:48:56.798837: step 11623, loss 0.0460008, acc 1\n",
      "2018-05-23T13:48:57.195859: step 11624, loss 0.19984, acc 0.875\n",
      "2018-05-23T13:48:57.597294: step 11625, loss 0.0931554, acc 0.96875\n",
      "2018-05-23T13:48:58.009192: step 11626, loss 0.109263, acc 0.96875\n",
      "2018-05-23T13:48:58.472957: step 11627, loss 0.18974, acc 0.90625\n",
      "2018-05-23T13:48:58.918108: step 11628, loss 0.20731, acc 0.921875\n",
      "2018-05-23T13:48:59.315070: step 11629, loss 0.183653, acc 0.921875\n",
      "2018-05-23T13:48:59.736941: step 11630, loss 0.234917, acc 0.890625\n",
      "2018-05-23T13:49:00.163689: step 11631, loss 0.258221, acc 0.953125\n",
      "2018-05-23T13:49:00.718736: step 11632, loss 0.16092, acc 0.921875\n",
      "2018-05-23T13:49:01.171922: step 11633, loss 0.0694977, acc 0.96875\n",
      "2018-05-23T13:49:01.578834: step 11634, loss 0.125946, acc 0.9375\n",
      "2018-05-23T13:49:01.996308: step 11635, loss 0.142328, acc 0.953125\n",
      "2018-05-23T13:49:02.392274: step 11636, loss 0.152509, acc 0.921875\n",
      "2018-05-23T13:49:02.913410: step 11637, loss 0.126883, acc 0.921875\n",
      "2018-05-23T13:49:03.344771: step 11638, loss 0.326317, acc 0.890625\n",
      "2018-05-23T13:49:03.788582: step 11639, loss 0.0819428, acc 0.96875\n",
      "2018-05-23T13:49:04.212974: step 11640, loss 0.163349, acc 0.9375\n",
      "2018-05-23T13:49:04.608918: step 11641, loss 0.224948, acc 0.921875\n",
      "2018-05-23T13:49:05.040300: step 11642, loss 0.0871694, acc 0.96875\n",
      "2018-05-23T13:49:05.455695: step 11643, loss 0.144974, acc 0.90625\n",
      "2018-05-23T13:49:05.940587: step 11644, loss 0.136738, acc 0.9375\n",
      "2018-05-23T13:49:06.397165: step 11645, loss 0.283164, acc 0.90625\n",
      "2018-05-23T13:49:06.787123: step 11646, loss 0.18988, acc 0.953125\n",
      "2018-05-23T13:49:07.212006: step 11647, loss 0.102101, acc 0.96875\n",
      "2018-05-23T13:49:07.602960: step 11648, loss 0.202848, acc 0.890625\n",
      "2018-05-23T13:49:08.107802: step 11649, loss 0.151454, acc 0.953125\n",
      "2018-05-23T13:49:08.580586: step 11650, loss 0.092172, acc 0.96875\n",
      "2018-05-23T13:49:08.983509: step 11651, loss 0.342096, acc 0.921875\n",
      "2018-05-23T13:49:09.393927: step 11652, loss 0.152584, acc 0.9375\n",
      "2018-05-23T13:49:09.786876: step 11653, loss 0.120034, acc 0.9375\n",
      "2018-05-23T13:49:10.271120: step 11654, loss 0.104024, acc 0.96875\n",
      "2018-05-23T13:49:10.703991: step 11655, loss 0.158191, acc 0.921875\n",
      "2018-05-23T13:49:11.106914: step 11656, loss 0.236946, acc 0.875\n",
      "2018-05-23T13:49:11.518813: step 11657, loss 0.0881004, acc 0.984375\n",
      "2018-05-23T13:49:11.925263: step 11658, loss 0.155522, acc 0.953125\n",
      "2018-05-23T13:49:12.413005: step 11659, loss 0.134511, acc 0.953125\n",
      "2018-05-23T13:49:12.843085: step 11660, loss 0.155373, acc 0.921875\n",
      "2018-05-23T13:49:13.375214: step 11661, loss 0.142952, acc 0.953125\n",
      "2018-05-23T13:49:13.858919: step 11662, loss 0.170593, acc 0.9375\n",
      "2018-05-23T13:49:14.358667: step 11663, loss 0.170822, acc 0.953125\n",
      "2018-05-23T13:49:14.763601: step 11664, loss 0.206487, acc 0.921875\n",
      "2018-05-23T13:49:15.164046: step 11665, loss 0.195126, acc 0.90625\n",
      "2018-05-23T13:49:15.570475: step 11666, loss 0.104833, acc 0.953125\n",
      "2018-05-23T13:49:16.035715: step 11667, loss 0.180664, acc 0.90625\n",
      "2018-05-23T13:49:16.435646: step 11668, loss 0.241847, acc 0.921875\n",
      "2018-05-23T13:49:16.870057: step 11669, loss 0.0880976, acc 0.96875\n",
      "2018-05-23T13:49:17.275458: step 11670, loss 0.078759, acc 0.96875\n",
      "2018-05-23T13:49:17.708829: step 11671, loss 0.100928, acc 0.9375\n",
      "2018-05-23T13:49:18.189584: step 11672, loss 0.293957, acc 0.875\n",
      "2018-05-23T13:49:18.602241: step 11673, loss 0.232541, acc 0.90625\n",
      "2018-05-23T13:49:19.091866: step 11674, loss 0.169119, acc 0.921875\n",
      "2018-05-23T13:49:19.528697: step 11675, loss 0.172555, acc 0.953125\n",
      "2018-05-23T13:49:19.968000: step 11676, loss 0.117609, acc 0.921875\n",
      "2018-05-23T13:49:20.510590: step 11677, loss 0.132834, acc 0.9375\n",
      "2018-05-23T13:49:20.945933: step 11678, loss 0.220436, acc 0.921875\n",
      "2018-05-23T13:49:21.355073: step 11679, loss 0.0987441, acc 0.953125\n",
      "2018-05-23T13:49:21.757842: step 11680, loss 0.166316, acc 0.9375\n",
      "2018-05-23T13:49:22.204671: step 11681, loss 0.188073, acc 0.90625\n",
      "2018-05-23T13:49:22.645491: step 11682, loss 0.319183, acc 0.875\n",
      "2018-05-23T13:49:23.274324: step 11683, loss 0.153296, acc 0.9375\n",
      "2018-05-23T13:49:23.856766: step 11684, loss 0.139094, acc 0.9375\n",
      "2018-05-23T13:49:24.470652: step 11685, loss 0.149018, acc 0.921875\n",
      "2018-05-23T13:49:25.050612: step 11686, loss 0.184593, acc 0.890625\n",
      "2018-05-23T13:49:25.589171: step 11687, loss 0.397113, acc 0.828125\n",
      "2018-05-23T13:49:26.073875: step 11688, loss 0.279908, acc 0.890625\n",
      "2018-05-23T13:49:26.495746: step 11689, loss 0.0511552, acc 1\n",
      "2018-05-23T13:49:26.903679: step 11690, loss 0.0959523, acc 0.96875\n",
      "2018-05-23T13:49:27.296084: step 11691, loss 0.104121, acc 0.96875\n",
      "2018-05-23T13:49:27.684982: step 11692, loss 0.118558, acc 0.9375\n",
      "2018-05-23T13:49:28.126822: step 11693, loss 0.153645, acc 0.921875\n",
      "2018-05-23T13:49:28.615052: step 11694, loss 0.140086, acc 0.9375\n",
      "2018-05-23T13:49:29.071828: step 11695, loss 0.101272, acc 0.96875\n",
      "2018-05-23T13:49:29.486567: step 11696, loss 0.147109, acc 0.953125\n",
      "2018-05-23T13:49:29.921914: step 11697, loss 0.264094, acc 0.84375\n",
      "2018-05-23T13:49:30.365265: step 11698, loss 0.196007, acc 0.90625\n",
      "2018-05-23T13:49:30.762176: step 11699, loss 0.239075, acc 0.90625\n",
      "2018-05-23T13:49:31.178097: step 11700, loss 0.0979002, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:49:36.868156: step 11700, loss 1.17156, acc 0.718245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11700\n",
      "\n",
      "2018-05-23T13:49:38.409150: step 11701, loss 0.235911, acc 0.90625\n",
      "2018-05-23T13:49:38.925515: step 11702, loss 0.240597, acc 0.921875\n",
      "2018-05-23T13:49:39.354861: step 11703, loss 0.182427, acc 0.890625\n",
      "2018-05-23T13:49:39.806461: step 11704, loss 0.292322, acc 0.890625\n",
      "2018-05-23T13:49:40.218693: step 11705, loss 0.175304, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:49:40.623186: step 11706, loss 0.126061, acc 0.9375\n",
      "2018-05-23T13:49:41.053847: step 11707, loss 0.105727, acc 0.953125\n",
      "2018-05-23T13:49:41.458374: step 11708, loss 0.117419, acc 0.984375\n",
      "2018-05-23T13:49:41.887953: step 11709, loss 0.206134, acc 0.9375\n",
      "2018-05-23T13:49:42.282533: step 11710, loss 0.192503, acc 0.921875\n",
      "2018-05-23T13:49:42.690302: step 11711, loss 0.0894799, acc 0.96875\n",
      "2018-05-23T13:49:43.090364: step 11712, loss 0.131568, acc 0.96875\n",
      "2018-05-23T13:49:43.496914: step 11713, loss 0.141099, acc 0.90625\n",
      "2018-05-23T13:49:44.053487: step 11714, loss 0.140651, acc 0.9375\n",
      "2018-05-23T13:49:44.604571: step 11715, loss 0.188943, acc 0.90625\n",
      "2018-05-23T13:49:45.132768: step 11716, loss 0.238548, acc 0.921875\n",
      "2018-05-23T13:49:45.579599: step 11717, loss 0.0884827, acc 0.953125\n",
      "2018-05-23T13:49:46.048357: step 11718, loss 0.123374, acc 0.953125\n",
      "2018-05-23T13:49:46.483706: step 11719, loss 0.306303, acc 0.875\n",
      "2018-05-23T13:49:46.888147: step 11720, loss 0.146769, acc 0.921875\n",
      "2018-05-23T13:49:47.308675: step 11721, loss 0.174579, acc 0.921875\n",
      "2018-05-23T13:49:47.750494: step 11722, loss 0.12484, acc 0.921875\n",
      "2018-05-23T13:49:48.197345: step 11723, loss 0.142456, acc 0.921875\n",
      "2018-05-23T13:49:48.611090: step 11724, loss 0.173452, acc 0.921875\n",
      "2018-05-23T13:49:49.010673: step 11725, loss 0.10267, acc 0.984375\n",
      "2018-05-23T13:49:49.477868: step 11726, loss 0.13689, acc 0.90625\n",
      "2018-05-23T13:49:49.998187: step 11727, loss 0.133455, acc 0.921875\n",
      "2018-05-23T13:49:50.573722: step 11728, loss 0.274286, acc 0.90625\n",
      "2018-05-23T13:49:51.077460: step 11729, loss 0.128342, acc 0.96875\n",
      "2018-05-23T13:49:51.571834: step 11730, loss 0.0979149, acc 0.96875\n",
      "2018-05-23T13:49:52.041138: step 11731, loss 0.206618, acc 0.90625\n",
      "2018-05-23T13:49:52.467855: step 11732, loss 0.184114, acc 0.875\n",
      "2018-05-23T13:49:52.928208: step 11733, loss 0.161687, acc 0.9375\n",
      "2018-05-23T13:49:53.401024: step 11734, loss 0.157387, acc 0.9375\n",
      "2018-05-23T13:49:53.890009: step 11735, loss 0.123681, acc 0.953125\n",
      "2018-05-23T13:49:54.313878: step 11736, loss 0.191522, acc 0.875\n",
      "2018-05-23T13:49:54.767853: step 11737, loss 0.182718, acc 0.921875\n",
      "2018-05-23T13:49:55.168787: step 11738, loss 0.148251, acc 0.9375\n",
      "2018-05-23T13:49:55.617779: step 11739, loss 0.170438, acc 0.921875\n",
      "2018-05-23T13:49:56.028192: step 11740, loss 0.164124, acc 0.921875\n",
      "2018-05-23T13:49:56.445514: step 11741, loss 0.200429, acc 0.90625\n",
      "2018-05-23T13:49:56.841697: step 11742, loss 0.195984, acc 0.9375\n",
      "2018-05-23T13:49:57.240977: step 11743, loss 0.181472, acc 0.890625\n",
      "2018-05-23T13:49:57.828548: step 11744, loss 0.0938201, acc 0.953125\n",
      "2018-05-23T13:49:58.324352: step 11745, loss 0.254471, acc 0.9375\n",
      "2018-05-23T13:49:58.738248: step 11746, loss 0.194447, acc 0.953125\n",
      "2018-05-23T13:49:59.154647: step 11747, loss 0.123714, acc 0.953125\n",
      "2018-05-23T13:49:59.553639: step 11748, loss 0.124849, acc 0.953125\n",
      "2018-05-23T13:50:00.145586: step 11749, loss 0.266304, acc 0.921875\n",
      "2018-05-23T13:50:00.706093: step 11750, loss 0.174236, acc 0.96875\n",
      "2018-05-23T13:50:01.212257: step 11751, loss 0.0583884, acc 0.984375\n",
      "2018-05-23T13:50:01.781505: step 11752, loss 0.223932, acc 0.9375\n",
      "2018-05-23T13:50:02.222846: step 11753, loss 0.233434, acc 0.9375\n",
      "2018-05-23T13:50:02.662822: step 11754, loss 0.256855, acc 0.84375\n",
      "2018-05-23T13:50:03.128912: step 11755, loss 0.0782771, acc 0.96875\n",
      "2018-05-23T13:50:03.530586: step 11756, loss 0.0395932, acc 1\n",
      "2018-05-23T13:50:03.969409: step 11757, loss 0.140454, acc 0.953125\n",
      "2018-05-23T13:50:04.421755: step 11758, loss 0.139741, acc 0.9375\n",
      "2018-05-23T13:50:04.856871: step 11759, loss 0.120898, acc 0.953125\n",
      "2018-05-23T13:50:05.288748: step 11760, loss 0.102587, acc 0.96875\n",
      "2018-05-23T13:50:05.683246: step 11761, loss 0.219746, acc 0.90625\n",
      "2018-05-23T13:50:06.176003: step 11762, loss 0.108635, acc 0.953125\n",
      "2018-05-23T13:50:06.568990: step 11763, loss 0.176929, acc 0.90625\n",
      "2018-05-23T13:50:07.010845: step 11764, loss 0.156742, acc 0.9375\n",
      "2018-05-23T13:50:07.443742: step 11765, loss 0.186561, acc 0.90625\n",
      "2018-05-23T13:50:07.948476: step 11766, loss 0.133399, acc 0.9375\n",
      "2018-05-23T13:50:08.390822: step 11767, loss 0.157734, acc 0.953125\n",
      "2018-05-23T13:50:08.781782: step 11768, loss 0.153252, acc 0.921875\n",
      "2018-05-23T13:50:09.188205: step 11769, loss 0.135711, acc 0.96875\n",
      "2018-05-23T13:50:09.592640: step 11770, loss 0.124317, acc 0.90625\n",
      "2018-05-23T13:50:09.990590: step 11771, loss 0.128565, acc 0.96875\n",
      "2018-05-23T13:50:10.455854: step 11772, loss 0.211662, acc 0.921875\n",
      "2018-05-23T13:50:10.851308: step 11773, loss 0.17745, acc 0.90625\n",
      "2018-05-23T13:50:11.266040: step 11774, loss 0.165948, acc 0.9375\n",
      "2018-05-23T13:50:11.672980: step 11775, loss 0.0913493, acc 0.984375\n",
      "2018-05-23T13:50:12.081891: step 11776, loss 0.150841, acc 0.953125\n",
      "2018-05-23T13:50:12.620892: step 11777, loss 0.165673, acc 0.9375\n",
      "2018-05-23T13:50:13.388452: step 11778, loss 0.191648, acc 0.921875\n",
      "2018-05-23T13:50:14.006562: step 11779, loss 0.0803559, acc 0.953125\n",
      "2018-05-23T13:50:14.597524: step 11780, loss 0.157571, acc 0.953125\n",
      "2018-05-23T13:50:15.104728: step 11781, loss 0.156276, acc 0.953125\n",
      "2018-05-23T13:50:15.810840: step 11782, loss 0.252281, acc 0.921875\n",
      "2018-05-23T13:50:16.750379: step 11783, loss 0.193586, acc 0.890625\n",
      "2018-05-23T13:50:17.558734: step 11784, loss 0.141454, acc 0.953125\n",
      "2018-05-23T13:50:18.315209: step 11785, loss 0.134606, acc 0.921875\n",
      "2018-05-23T13:50:18.941732: step 11786, loss 0.119342, acc 0.96875\n",
      "2018-05-23T13:50:19.442791: step 11787, loss 0.0872639, acc 0.953125\n",
      "2018-05-23T13:50:19.900083: step 11788, loss 0.13371, acc 0.953125\n",
      "2018-05-23T13:50:20.365735: step 11789, loss 0.155797, acc 0.9375\n",
      "2018-05-23T13:50:20.840926: step 11790, loss 0.189513, acc 0.9375\n",
      "2018-05-23T13:50:21.359607: step 11791, loss 0.196295, acc 0.875\n",
      "2018-05-23T13:50:21.833880: step 11792, loss 0.191105, acc 0.9375\n",
      "2018-05-23T13:50:22.320111: step 11793, loss 0.183543, acc 0.890625\n",
      "2018-05-23T13:50:22.828770: step 11794, loss 0.141719, acc 0.9375\n",
      "2018-05-23T13:50:23.292085: step 11795, loss 0.21066, acc 0.90625\n",
      "2018-05-23T13:50:23.749374: step 11796, loss 0.134096, acc 0.96875\n",
      "2018-05-23T13:50:24.204155: step 11797, loss 0.29068, acc 0.84375\n",
      "2018-05-23T13:50:24.603087: step 11798, loss 0.110527, acc 0.953125\n",
      "2018-05-23T13:50:25.146794: step 11799, loss 0.229771, acc 0.859375\n",
      "2018-05-23T13:50:25.601221: step 11800, loss 0.13656, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:50:31.924914: step 11800, loss 1.17572, acc 0.71153\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11800\n",
      "\n",
      "2018-05-23T13:50:33.678977: step 11801, loss 0.242083, acc 0.875\n",
      "2018-05-23T13:50:34.091982: step 11802, loss 0.127514, acc 0.921875\n",
      "2018-05-23T13:50:34.585997: step 11803, loss 0.141955, acc 0.9375\n",
      "2018-05-23T13:50:35.127927: step 11804, loss 0.256867, acc 0.9375\n",
      "2018-05-23T13:50:35.781528: step 11805, loss 0.194115, acc 0.921875\n",
      "2018-05-23T13:50:36.309236: step 11806, loss 0.34562, acc 0.875\n",
      "2018-05-23T13:50:36.722669: step 11807, loss 0.0948517, acc 0.96875\n",
      "2018-05-23T13:50:37.140578: step 11808, loss 0.311145, acc 0.9375\n",
      "2018-05-23T13:50:37.547386: step 11809, loss 0.320138, acc 0.890625\n",
      "2018-05-23T13:50:38.068548: step 11810, loss 0.0852576, acc 0.96875\n",
      "2018-05-23T13:50:38.608616: step 11811, loss 0.118335, acc 0.921875\n",
      "2018-05-23T13:50:39.379176: step 11812, loss 0.161936, acc 0.9375\n",
      "2018-05-23T13:50:40.021480: step 11813, loss 0.0646365, acc 1\n",
      "2018-05-23T13:50:40.542122: step 11814, loss 0.14406, acc 0.921875\n",
      "2018-05-23T13:50:40.980967: step 11815, loss 0.129204, acc 0.9375\n",
      "2018-05-23T13:50:41.764477: step 11816, loss 0.176817, acc 0.921875\n",
      "2018-05-23T13:50:42.576970: step 11817, loss 0.07976, acc 0.96875\n",
      "2018-05-23T13:50:43.131492: step 11818, loss 0.168981, acc 0.9375\n",
      "2018-05-23T13:50:43.577830: step 11819, loss 0.197215, acc 0.921875\n",
      "2018-05-23T13:50:44.003800: step 11820, loss 0.162697, acc 0.882353\n",
      "2018-05-23T13:50:44.475543: step 11821, loss 0.169782, acc 0.90625\n",
      "2018-05-23T13:50:44.999782: step 11822, loss 0.0951704, acc 0.96875\n",
      "2018-05-23T13:50:45.481789: step 11823, loss 0.0688926, acc 0.96875\n",
      "2018-05-23T13:50:45.964809: step 11824, loss 0.135663, acc 0.90625\n",
      "2018-05-23T13:50:46.488928: step 11825, loss 0.092524, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:50:46.999952: step 11826, loss 0.0699521, acc 0.984375\n",
      "2018-05-23T13:50:47.457867: step 11827, loss 0.108189, acc 0.953125\n",
      "2018-05-23T13:50:47.975959: step 11828, loss 0.101468, acc 0.96875\n",
      "2018-05-23T13:50:48.408890: step 11829, loss 0.0777493, acc 0.96875\n",
      "2018-05-23T13:50:48.821792: step 11830, loss 0.0660467, acc 0.984375\n",
      "2018-05-23T13:50:49.248668: step 11831, loss 0.122314, acc 0.953125\n",
      "2018-05-23T13:50:49.657569: step 11832, loss 0.0772758, acc 0.953125\n",
      "2018-05-23T13:50:50.117344: step 11833, loss 0.0862421, acc 0.953125\n",
      "2018-05-23T13:50:50.603079: step 11834, loss 0.15268, acc 0.9375\n",
      "2018-05-23T13:50:51.011759: step 11835, loss 0.101482, acc 0.96875\n",
      "2018-05-23T13:50:51.421639: step 11836, loss 0.0966075, acc 0.96875\n",
      "2018-05-23T13:50:51.863509: step 11837, loss 0.319923, acc 0.859375\n",
      "2018-05-23T13:50:52.266150: step 11838, loss 0.0935624, acc 0.984375\n",
      "2018-05-23T13:50:52.694053: step 11839, loss 0.0835525, acc 0.96875\n",
      "2018-05-23T13:50:53.114931: step 11840, loss 0.0970573, acc 0.9375\n",
      "2018-05-23T13:50:53.536447: step 11841, loss 0.0645358, acc 0.984375\n",
      "2018-05-23T13:50:54.017952: step 11842, loss 0.146666, acc 0.90625\n",
      "2018-05-23T13:50:54.462782: step 11843, loss 0.0963866, acc 0.9375\n",
      "2018-05-23T13:50:54.903178: step 11844, loss 0.0912983, acc 0.953125\n",
      "2018-05-23T13:50:55.400772: step 11845, loss 0.164824, acc 0.921875\n",
      "2018-05-23T13:50:55.983401: step 11846, loss 0.093366, acc 0.953125\n",
      "2018-05-23T13:50:56.606671: step 11847, loss 0.187817, acc 0.9375\n",
      "2018-05-23T13:50:57.339990: step 11848, loss 0.0695947, acc 0.953125\n",
      "2018-05-23T13:50:57.837199: step 11849, loss 0.0715542, acc 0.984375\n",
      "2018-05-23T13:50:58.340106: step 11850, loss 0.238729, acc 0.890625\n",
      "2018-05-23T13:50:58.746698: step 11851, loss 0.116723, acc 0.9375\n",
      "2018-05-23T13:50:59.161635: step 11852, loss 0.155779, acc 0.921875\n",
      "2018-05-23T13:50:59.562572: step 11853, loss 0.144987, acc 0.96875\n",
      "2018-05-23T13:50:59.968019: step 11854, loss 0.171263, acc 0.9375\n",
      "2018-05-23T13:51:00.395476: step 11855, loss 0.106837, acc 0.96875\n",
      "2018-05-23T13:51:00.857308: step 11856, loss 0.155277, acc 0.921875\n",
      "2018-05-23T13:51:01.261228: step 11857, loss 0.10275, acc 0.96875\n",
      "2018-05-23T13:51:01.691931: step 11858, loss 0.0486386, acc 0.984375\n",
      "2018-05-23T13:51:02.107616: step 11859, loss 0.119972, acc 0.953125\n",
      "2018-05-23T13:51:02.508681: step 11860, loss 0.163445, acc 0.90625\n",
      "2018-05-23T13:51:02.907653: step 11861, loss 0.0701532, acc 1\n",
      "2018-05-23T13:51:03.319566: step 11862, loss 0.15593, acc 0.890625\n",
      "2018-05-23T13:51:03.832353: step 11863, loss 0.143062, acc 0.953125\n",
      "2018-05-23T13:51:04.463846: step 11864, loss 0.119287, acc 0.9375\n",
      "2018-05-23T13:51:05.041750: step 11865, loss 0.109453, acc 0.953125\n",
      "2018-05-23T13:51:05.494779: step 11866, loss 0.237506, acc 0.90625\n",
      "2018-05-23T13:51:06.009817: step 11867, loss 0.096105, acc 0.96875\n",
      "2018-05-23T13:51:06.477277: step 11868, loss 0.184222, acc 0.953125\n",
      "2018-05-23T13:51:07.160980: step 11869, loss 0.210592, acc 0.875\n",
      "2018-05-23T13:51:07.658658: step 11870, loss 0.15057, acc 0.96875\n",
      "2018-05-23T13:51:08.243336: step 11871, loss 0.183881, acc 0.9375\n",
      "2018-05-23T13:51:08.671288: step 11872, loss 0.0637306, acc 0.96875\n",
      "2018-05-23T13:51:09.107231: step 11873, loss 0.0949537, acc 0.953125\n",
      "2018-05-23T13:51:09.565191: step 11874, loss 0.277659, acc 0.875\n",
      "2018-05-23T13:51:10.148645: step 11875, loss 0.231462, acc 0.9375\n",
      "2018-05-23T13:51:10.596908: step 11876, loss 0.190257, acc 0.90625\n",
      "2018-05-23T13:51:11.077831: step 11877, loss 0.0475147, acc 0.984375\n",
      "2018-05-23T13:51:11.537856: step 11878, loss 0.203125, acc 0.90625\n",
      "2018-05-23T13:51:11.939897: step 11879, loss 0.0655831, acc 0.953125\n",
      "2018-05-23T13:51:12.364654: step 11880, loss 0.0807788, acc 0.984375\n",
      "2018-05-23T13:51:12.776483: step 11881, loss 0.0588901, acc 0.984375\n",
      "2018-05-23T13:51:13.204849: step 11882, loss 0.0814534, acc 0.96875\n",
      "2018-05-23T13:51:13.651224: step 11883, loss 0.154208, acc 0.953125\n",
      "2018-05-23T13:51:14.089448: step 11884, loss 0.164276, acc 0.953125\n",
      "2018-05-23T13:51:14.631567: step 11885, loss 0.129342, acc 0.9375\n",
      "2018-05-23T13:51:15.075949: step 11886, loss 0.137664, acc 0.90625\n",
      "2018-05-23T13:51:15.529912: step 11887, loss 0.23798, acc 0.921875\n",
      "2018-05-23T13:51:15.929388: step 11888, loss 0.0994983, acc 0.953125\n",
      "2018-05-23T13:51:16.357777: step 11889, loss 0.100795, acc 0.96875\n",
      "2018-05-23T13:51:16.794959: step 11890, loss 0.108472, acc 0.9375\n",
      "2018-05-23T13:51:17.273827: step 11891, loss 0.140664, acc 0.9375\n",
      "2018-05-23T13:51:17.794443: step 11892, loss 0.222555, acc 0.890625\n",
      "2018-05-23T13:51:18.266265: step 11893, loss 0.21816, acc 0.9375\n",
      "2018-05-23T13:51:18.816365: step 11894, loss 0.171716, acc 0.921875\n",
      "2018-05-23T13:51:19.228126: step 11895, loss 0.0803289, acc 0.984375\n",
      "2018-05-23T13:51:19.645778: step 11896, loss 0.164267, acc 0.953125\n",
      "2018-05-23T13:51:20.049876: step 11897, loss 0.156368, acc 0.921875\n",
      "2018-05-23T13:51:20.638587: step 11898, loss 0.135912, acc 0.953125\n",
      "2018-05-23T13:51:21.091166: step 11899, loss 0.104581, acc 0.953125\n",
      "2018-05-23T13:51:21.503129: step 11900, loss 0.224864, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:51:27.078168: step 11900, loss 1.19812, acc 0.715102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-11900\n",
      "\n",
      "2018-05-23T13:51:28.661814: step 11901, loss 0.118904, acc 0.9375\n",
      "2018-05-23T13:51:29.147555: step 11902, loss 0.074616, acc 0.96875\n",
      "2018-05-23T13:51:29.584684: step 11903, loss 0.126168, acc 0.9375\n",
      "2018-05-23T13:51:30.006973: step 11904, loss 0.184306, acc 0.90625\n",
      "2018-05-23T13:51:30.434598: step 11905, loss 0.0856815, acc 0.984375\n",
      "2018-05-23T13:51:30.847873: step 11906, loss 0.0671877, acc 0.96875\n",
      "2018-05-23T13:51:31.268821: step 11907, loss 0.137719, acc 0.953125\n",
      "2018-05-23T13:51:31.699328: step 11908, loss 0.0972442, acc 0.953125\n",
      "2018-05-23T13:51:32.159626: step 11909, loss 0.112459, acc 0.9375\n",
      "2018-05-23T13:51:32.649347: step 11910, loss 0.0903413, acc 0.96875\n",
      "2018-05-23T13:51:33.259756: step 11911, loss 0.104734, acc 0.953125\n",
      "2018-05-23T13:51:33.844259: step 11912, loss 0.0906835, acc 0.96875\n",
      "2018-05-23T13:51:34.322373: step 11913, loss 0.140681, acc 0.90625\n",
      "2018-05-23T13:51:34.838021: step 11914, loss 0.0763039, acc 0.96875\n",
      "2018-05-23T13:51:35.325737: step 11915, loss 0.10301, acc 0.9375\n",
      "2018-05-23T13:51:35.832913: step 11916, loss 0.129154, acc 0.953125\n",
      "2018-05-23T13:51:36.351548: step 11917, loss 0.0904612, acc 0.96875\n",
      "2018-05-23T13:51:36.935466: step 11918, loss 0.13077, acc 0.96875\n",
      "2018-05-23T13:51:37.442110: step 11919, loss 0.144295, acc 0.9375\n",
      "2018-05-23T13:51:38.093105: step 11920, loss 0.0961368, acc 0.953125\n",
      "2018-05-23T13:51:38.589782: step 11921, loss 0.117554, acc 0.9375\n",
      "2018-05-23T13:51:39.052338: step 11922, loss 0.277216, acc 0.875\n",
      "2018-05-23T13:51:39.554500: step 11923, loss 0.0730829, acc 0.984375\n",
      "2018-05-23T13:51:40.039707: step 11924, loss 0.209892, acc 0.9375\n",
      "2018-05-23T13:51:40.508996: step 11925, loss 0.237588, acc 0.90625\n",
      "2018-05-23T13:51:41.038199: step 11926, loss 0.107341, acc 0.9375\n",
      "2018-05-23T13:51:41.519768: step 11927, loss 0.0838684, acc 0.96875\n",
      "2018-05-23T13:51:42.046893: step 11928, loss 0.133243, acc 0.9375\n",
      "2018-05-23T13:51:42.495973: step 11929, loss 0.155317, acc 0.984375\n",
      "2018-05-23T13:51:42.987784: step 11930, loss 0.0897673, acc 0.96875\n",
      "2018-05-23T13:51:43.436062: step 11931, loss 0.204368, acc 0.90625\n",
      "2018-05-23T13:51:44.138388: step 11932, loss 0.0964217, acc 0.9375\n",
      "2018-05-23T13:51:44.604841: step 11933, loss 0.0600835, acc 0.984375\n",
      "2018-05-23T13:51:45.105536: step 11934, loss 0.0846984, acc 0.96875\n",
      "2018-05-23T13:51:45.532700: step 11935, loss 0.0995028, acc 0.9375\n",
      "2018-05-23T13:51:46.018334: step 11936, loss 0.10038, acc 0.953125\n",
      "2018-05-23T13:51:46.558641: step 11937, loss 0.070086, acc 0.96875\n",
      "2018-05-23T13:51:47.277537: step 11938, loss 0.0558082, acc 0.984375\n",
      "2018-05-23T13:51:47.847162: step 11939, loss 0.133179, acc 0.953125\n",
      "2018-05-23T13:51:48.395485: step 11940, loss 0.260625, acc 0.921875\n",
      "2018-05-23T13:51:48.985365: step 11941, loss 0.103827, acc 0.96875\n",
      "2018-05-23T13:51:49.616238: step 11942, loss 0.307477, acc 0.890625\n",
      "2018-05-23T13:51:50.088085: step 11943, loss 0.103395, acc 0.953125\n",
      "2018-05-23T13:51:50.572753: step 11944, loss 0.136047, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:51:50.982680: step 11945, loss 0.132694, acc 0.921875\n",
      "2018-05-23T13:51:51.421578: step 11946, loss 0.0752278, acc 0.96875\n",
      "2018-05-23T13:51:51.833544: step 11947, loss 0.065306, acc 0.984375\n",
      "2018-05-23T13:51:52.326345: step 11948, loss 0.0990319, acc 0.96875\n",
      "2018-05-23T13:51:52.724304: step 11949, loss 0.260433, acc 0.921875\n",
      "2018-05-23T13:51:53.143333: step 11950, loss 0.121794, acc 0.953125\n",
      "2018-05-23T13:51:53.541964: step 11951, loss 0.0381267, acc 0.984375\n",
      "2018-05-23T13:51:53.940489: step 11952, loss 0.120059, acc 0.953125\n",
      "2018-05-23T13:51:54.413207: step 11953, loss 0.0702823, acc 0.984375\n",
      "2018-05-23T13:51:54.846354: step 11954, loss 0.119003, acc 0.953125\n",
      "2018-05-23T13:51:55.255290: step 11955, loss 0.25375, acc 0.9375\n",
      "2018-05-23T13:51:55.647092: step 11956, loss 0.102608, acc 0.953125\n",
      "2018-05-23T13:51:56.091696: step 11957, loss 0.116314, acc 0.96875\n",
      "2018-05-23T13:51:56.542718: step 11958, loss 0.122983, acc 0.9375\n",
      "2018-05-23T13:51:57.037131: step 11959, loss 0.0938562, acc 0.953125\n",
      "2018-05-23T13:51:57.498039: step 11960, loss 0.11656, acc 0.921875\n",
      "2018-05-23T13:51:57.942387: step 11961, loss 0.08165, acc 0.96875\n",
      "2018-05-23T13:51:58.396665: step 11962, loss 0.166505, acc 0.953125\n",
      "2018-05-23T13:51:58.878967: step 11963, loss 0.120463, acc 0.921875\n",
      "2018-05-23T13:51:59.415343: step 11964, loss 0.17307, acc 0.921875\n",
      "2018-05-23T13:51:59.821340: step 11965, loss 0.134459, acc 0.953125\n",
      "2018-05-23T13:52:00.257788: step 11966, loss 0.1748, acc 0.9375\n",
      "2018-05-23T13:52:01.025729: step 11967, loss 0.103311, acc 0.953125\n",
      "2018-05-23T13:52:01.564822: step 11968, loss 0.131223, acc 0.953125\n",
      "2018-05-23T13:52:02.217079: step 11969, loss 0.146549, acc 0.953125\n",
      "2018-05-23T13:52:03.138346: step 11970, loss 0.0865917, acc 0.953125\n",
      "2018-05-23T13:52:03.611645: step 11971, loss 0.0691463, acc 0.96875\n",
      "2018-05-23T13:52:04.161688: step 11972, loss 0.0948724, acc 0.96875\n",
      "2018-05-23T13:52:04.629240: step 11973, loss 0.218035, acc 0.953125\n",
      "2018-05-23T13:52:05.155395: step 11974, loss 0.182185, acc 0.90625\n",
      "2018-05-23T13:52:05.694381: step 11975, loss 0.0801104, acc 0.96875\n",
      "2018-05-23T13:52:06.123864: step 11976, loss 0.0782131, acc 0.96875\n",
      "2018-05-23T13:52:06.533707: step 11977, loss 0.0771307, acc 0.96875\n",
      "2018-05-23T13:52:06.957118: step 11978, loss 0.0734301, acc 0.984375\n",
      "2018-05-23T13:52:07.422266: step 11979, loss 0.212277, acc 0.890625\n",
      "2018-05-23T13:52:07.968803: step 11980, loss 0.0873703, acc 0.953125\n",
      "2018-05-23T13:52:08.443533: step 11981, loss 0.238512, acc 0.890625\n",
      "2018-05-23T13:52:08.933222: step 11982, loss 0.0969115, acc 0.984375\n",
      "2018-05-23T13:52:09.505692: step 11983, loss 0.0832418, acc 0.984375\n",
      "2018-05-23T13:52:09.971241: step 11984, loss 0.0776085, acc 0.96875\n",
      "2018-05-23T13:52:10.485900: step 11985, loss 0.0749458, acc 0.984375\n",
      "2018-05-23T13:52:10.989131: step 11986, loss 0.0833824, acc 0.984375\n",
      "2018-05-23T13:52:11.493294: step 11987, loss 0.173469, acc 0.921875\n",
      "2018-05-23T13:52:11.998995: step 11988, loss 0.12103, acc 0.953125\n",
      "2018-05-23T13:52:12.463308: step 11989, loss 0.184327, acc 0.890625\n",
      "2018-05-23T13:52:12.902667: step 11990, loss 0.18792, acc 0.875\n",
      "2018-05-23T13:52:13.324828: step 11991, loss 0.147099, acc 0.96875\n",
      "2018-05-23T13:52:13.764858: step 11992, loss 0.102319, acc 0.984375\n",
      "2018-05-23T13:52:14.198002: step 11993, loss 0.0695388, acc 1\n",
      "2018-05-23T13:52:14.734879: step 11994, loss 0.0785604, acc 0.96875\n",
      "2018-05-23T13:52:15.492168: step 11995, loss 0.236458, acc 0.921875\n",
      "2018-05-23T13:52:16.086732: step 11996, loss 0.0976163, acc 0.984375\n",
      "2018-05-23T13:52:16.610936: step 11997, loss 0.124107, acc 0.9375\n",
      "2018-05-23T13:52:17.103740: step 11998, loss 0.236276, acc 0.9375\n",
      "2018-05-23T13:52:17.592807: step 11999, loss 0.0901238, acc 0.984375\n",
      "2018-05-23T13:52:18.029259: step 12000, loss 0.115193, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:52:25.471472: step 12000, loss 1.24141, acc 0.717531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12000\n",
      "\n",
      "2018-05-23T13:52:28.269335: step 12001, loss 0.145, acc 0.953125\n",
      "2018-05-23T13:52:28.837973: step 12002, loss 0.149684, acc 0.921875\n",
      "2018-05-23T13:52:29.364971: step 12003, loss 0.143431, acc 0.921875\n",
      "2018-05-23T13:52:30.157968: step 12004, loss 0.071179, acc 1\n",
      "2018-05-23T13:52:30.593313: step 12005, loss 0.0937637, acc 0.9375\n",
      "2018-05-23T13:52:31.020556: step 12006, loss 0.0840148, acc 0.953125\n",
      "2018-05-23T13:52:31.475348: step 12007, loss 0.167283, acc 0.953125\n",
      "2018-05-23T13:52:31.941161: step 12008, loss 0.195604, acc 0.921875\n",
      "2018-05-23T13:52:32.409939: step 12009, loss 0.282866, acc 0.859375\n",
      "2018-05-23T13:52:32.814912: step 12010, loss 0.280837, acc 0.84375\n",
      "2018-05-23T13:52:33.224333: step 12011, loss 0.160408, acc 0.890625\n",
      "2018-05-23T13:52:33.628777: step 12012, loss 0.0635908, acc 0.984375\n",
      "2018-05-23T13:52:34.063651: step 12013, loss 0.107848, acc 0.96875\n",
      "2018-05-23T13:52:34.577812: step 12014, loss 0.0836443, acc 0.96875\n",
      "2018-05-23T13:52:35.245909: step 12015, loss 0.0919856, acc 0.953125\n",
      "2018-05-23T13:52:35.992966: step 12016, loss 0.0803859, acc 0.953125\n",
      "2018-05-23T13:52:36.511118: step 12017, loss 0.138818, acc 0.953125\n",
      "2018-05-23T13:52:37.068808: step 12018, loss 0.226876, acc 0.90625\n",
      "2018-05-23T13:52:37.563140: step 12019, loss 0.117283, acc 0.953125\n",
      "2018-05-23T13:52:37.990023: step 12020, loss 0.0846187, acc 0.953125\n",
      "2018-05-23T13:52:38.440856: step 12021, loss 0.0911404, acc 0.953125\n",
      "2018-05-23T13:52:38.906178: step 12022, loss 0.25364, acc 0.921875\n",
      "2018-05-23T13:52:39.320582: step 12023, loss 0.0571939, acc 1\n",
      "2018-05-23T13:52:39.739932: step 12024, loss 0.110635, acc 0.96875\n",
      "2018-05-23T13:52:40.267462: step 12025, loss 0.132919, acc 0.9375\n",
      "2018-05-23T13:52:40.719253: step 12026, loss 0.121145, acc 0.953125\n",
      "2018-05-23T13:52:41.222455: step 12027, loss 0.255705, acc 0.875\n",
      "2018-05-23T13:52:41.651880: step 12028, loss 0.158648, acc 0.9375\n",
      "2018-05-23T13:52:42.079764: step 12029, loss 0.0817419, acc 0.953125\n",
      "2018-05-23T13:52:42.626839: step 12030, loss 0.152805, acc 0.90625\n",
      "2018-05-23T13:52:43.195360: step 12031, loss 0.118637, acc 0.9375\n",
      "2018-05-23T13:52:43.760878: step 12032, loss 0.245318, acc 0.921875\n",
      "2018-05-23T13:52:44.215182: step 12033, loss 0.0682139, acc 0.96875\n",
      "2018-05-23T13:52:44.661355: step 12034, loss 0.0968621, acc 0.96875\n",
      "2018-05-23T13:52:45.085570: step 12035, loss 0.100249, acc 0.953125\n",
      "2018-05-23T13:52:45.493599: step 12036, loss 0.186984, acc 0.9375\n",
      "2018-05-23T13:52:46.150172: step 12037, loss 0.13477, acc 0.9375\n",
      "2018-05-23T13:52:46.731225: step 12038, loss 0.112891, acc 0.96875\n",
      "2018-05-23T13:52:47.195541: step 12039, loss 0.0790038, acc 0.984375\n",
      "2018-05-23T13:52:47.653925: step 12040, loss 0.11161, acc 0.9375\n",
      "2018-05-23T13:52:48.080745: step 12041, loss 0.15691, acc 0.921875\n",
      "2018-05-23T13:52:48.488444: step 12042, loss 0.170509, acc 0.90625\n",
      "2018-05-23T13:52:48.944181: step 12043, loss 0.114277, acc 0.953125\n",
      "2018-05-23T13:52:49.559760: step 12044, loss 0.0927, acc 0.96875\n",
      "2018-05-23T13:52:50.062983: step 12045, loss 0.113975, acc 0.921875\n",
      "2018-05-23T13:52:50.623005: step 12046, loss 0.224423, acc 0.921875\n",
      "2018-05-23T13:52:51.169545: step 12047, loss 0.116078, acc 0.890625\n",
      "2018-05-23T13:52:51.583435: step 12048, loss 0.158605, acc 0.921875\n",
      "2018-05-23T13:52:51.945501: step 12049, loss 0.146406, acc 0.90625\n",
      "2018-05-23T13:52:52.420291: step 12050, loss 0.200217, acc 0.90625\n",
      "2018-05-23T13:52:52.838172: step 12051, loss 0.110244, acc 0.9375\n",
      "2018-05-23T13:52:53.563231: step 12052, loss 0.231683, acc 0.90625\n",
      "2018-05-23T13:52:54.231445: step 12053, loss 0.139384, acc 0.9375\n",
      "2018-05-23T13:52:54.774990: step 12054, loss 0.228813, acc 0.890625\n",
      "2018-05-23T13:52:55.804332: step 12055, loss 0.0525858, acc 1\n",
      "2018-05-23T13:52:56.398844: step 12056, loss 0.108137, acc 0.9375\n",
      "2018-05-23T13:52:56.970823: step 12057, loss 0.131963, acc 0.953125\n",
      "2018-05-23T13:52:57.808741: step 12058, loss 0.181533, acc 0.953125\n",
      "2018-05-23T13:52:58.483974: step 12059, loss 0.183966, acc 0.921875\n",
      "2018-05-23T13:52:59.156165: step 12060, loss 0.253702, acc 0.859375\n",
      "2018-05-23T13:52:59.718661: step 12061, loss 0.108816, acc 0.96875\n",
      "2018-05-23T13:53:00.439244: step 12062, loss 0.13074, acc 0.96875\n",
      "2018-05-23T13:53:01.002736: step 12063, loss 0.1577, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:53:01.654223: step 12064, loss 0.0993398, acc 0.96875\n",
      "2018-05-23T13:53:02.211826: step 12065, loss 0.243112, acc 0.953125\n",
      "2018-05-23T13:53:03.046299: step 12066, loss 0.162257, acc 0.953125\n",
      "2018-05-23T13:53:03.588848: step 12067, loss 0.0848507, acc 0.96875\n",
      "2018-05-23T13:53:04.226142: step 12068, loss 0.102522, acc 0.9375\n",
      "2018-05-23T13:53:04.686424: step 12069, loss 0.147829, acc 0.921875\n",
      "2018-05-23T13:53:05.179506: step 12070, loss 0.153834, acc 0.9375\n",
      "2018-05-23T13:53:05.631789: step 12071, loss 0.130455, acc 0.96875\n",
      "2018-05-23T13:53:06.053364: step 12072, loss 0.104418, acc 0.9375\n",
      "2018-05-23T13:53:06.487428: step 12073, loss 0.136032, acc 0.890625\n",
      "2018-05-23T13:53:06.969186: step 12074, loss 0.0562732, acc 0.984375\n",
      "2018-05-23T13:53:07.425126: step 12075, loss 0.176083, acc 0.921875\n",
      "2018-05-23T13:53:07.930439: step 12076, loss 0.0994732, acc 0.953125\n",
      "2018-05-23T13:53:08.362800: step 12077, loss 0.059775, acc 0.984375\n",
      "2018-05-23T13:53:09.046375: step 12078, loss 0.0834894, acc 0.96875\n",
      "2018-05-23T13:53:09.720611: step 12079, loss 0.115774, acc 0.96875\n",
      "2018-05-23T13:53:10.582869: step 12080, loss 0.181785, acc 0.921875\n",
      "2018-05-23T13:53:11.341838: step 12081, loss 0.0634602, acc 0.96875\n",
      "2018-05-23T13:53:12.048487: step 12082, loss 0.194322, acc 0.953125\n",
      "2018-05-23T13:53:12.701764: step 12083, loss 0.136245, acc 0.9375\n",
      "2018-05-23T13:53:13.438304: step 12084, loss 0.150684, acc 0.953125\n",
      "2018-05-23T13:53:14.213233: step 12085, loss 0.141501, acc 0.9375\n",
      "2018-05-23T13:53:14.900394: step 12086, loss 0.0982333, acc 0.984375\n",
      "2018-05-23T13:53:15.570601: step 12087, loss 0.119161, acc 0.953125\n",
      "2018-05-23T13:53:16.272723: step 12088, loss 0.235167, acc 0.90625\n",
      "2018-05-23T13:53:16.872629: step 12089, loss 0.0922321, acc 0.953125\n",
      "2018-05-23T13:53:17.634634: step 12090, loss 0.144665, acc 0.9375\n",
      "2018-05-23T13:53:18.269979: step 12091, loss 0.175224, acc 0.890625\n",
      "2018-05-23T13:53:18.864414: step 12092, loss 0.100595, acc 0.9375\n",
      "2018-05-23T13:53:19.462953: step 12093, loss 0.0997868, acc 0.9375\n",
      "2018-05-23T13:53:20.023454: step 12094, loss 0.216416, acc 0.890625\n",
      "2018-05-23T13:53:20.573005: step 12095, loss 0.0974778, acc 0.96875\n",
      "2018-05-23T13:53:21.078679: step 12096, loss 0.156278, acc 0.90625\n",
      "2018-05-23T13:53:21.722468: step 12097, loss 0.151377, acc 0.953125\n",
      "2018-05-23T13:53:22.322811: step 12098, loss 0.194013, acc 0.90625\n",
      "2018-05-23T13:53:22.949541: step 12099, loss 0.115091, acc 0.9375\n",
      "2018-05-23T13:53:23.673896: step 12100, loss 0.132835, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:53:31.277297: step 12100, loss 1.24507, acc 0.71996\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12100\n",
      "\n",
      "2018-05-23T13:53:33.475390: step 12101, loss 0.142555, acc 0.90625\n",
      "2018-05-23T13:53:34.330159: step 12102, loss 0.127117, acc 0.9375\n",
      "2018-05-23T13:53:35.029893: step 12103, loss 0.118942, acc 0.96875\n",
      "2018-05-23T13:53:35.743976: step 12104, loss 0.151901, acc 0.9375\n",
      "2018-05-23T13:53:36.339382: step 12105, loss 0.0694344, acc 0.96875\n",
      "2018-05-23T13:53:36.950749: step 12106, loss 0.193468, acc 0.921875\n",
      "2018-05-23T13:53:37.919669: step 12107, loss 0.195827, acc 0.890625\n",
      "2018-05-23T13:53:38.629768: step 12108, loss 0.0821445, acc 0.984375\n",
      "2018-05-23T13:53:39.335879: step 12109, loss 0.0991258, acc 0.953125\n",
      "2018-05-23T13:53:40.071931: step 12110, loss 0.0611654, acc 0.984375\n",
      "2018-05-23T13:53:40.754106: step 12111, loss 0.0858773, acc 0.96875\n",
      "2018-05-23T13:53:41.474180: step 12112, loss 0.129916, acc 0.90625\n",
      "2018-05-23T13:53:42.058616: step 12113, loss 0.0484053, acc 0.984375\n",
      "2018-05-23T13:53:42.602163: step 12114, loss 0.165076, acc 0.921875\n",
      "2018-05-23T13:53:43.153687: step 12115, loss 0.0779044, acc 0.984375\n",
      "2018-05-23T13:53:43.696236: step 12116, loss 0.213743, acc 0.890625\n",
      "2018-05-23T13:53:44.253288: step 12117, loss 0.152592, acc 0.953125\n",
      "2018-05-23T13:53:44.791847: step 12118, loss 0.118336, acc 0.953125\n",
      "2018-05-23T13:53:45.342720: step 12119, loss 0.0818925, acc 0.96875\n",
      "2018-05-23T13:53:45.857405: step 12120, loss 0.105297, acc 0.984375\n",
      "2018-05-23T13:53:46.382003: step 12121, loss 0.205347, acc 0.921875\n",
      "2018-05-23T13:53:47.129005: step 12122, loss 0.102732, acc 0.96875\n",
      "2018-05-23T13:53:47.749362: step 12123, loss 0.0586771, acc 1\n",
      "2018-05-23T13:53:48.339249: step 12124, loss 0.0639144, acc 0.984375\n",
      "2018-05-23T13:53:48.949599: step 12125, loss 0.135726, acc 0.96875\n",
      "2018-05-23T13:53:49.745980: step 12126, loss 0.122415, acc 0.9375\n",
      "2018-05-23T13:53:50.354352: step 12127, loss 0.146016, acc 0.921875\n",
      "2018-05-23T13:53:51.275911: step 12128, loss 0.0869402, acc 1\n",
      "2018-05-23T13:53:51.981022: step 12129, loss 0.112179, acc 0.96875\n",
      "2018-05-23T13:53:52.475723: step 12130, loss 0.129352, acc 0.921875\n",
      "2018-05-23T13:53:53.036731: step 12131, loss 0.164976, acc 0.890625\n",
      "2018-05-23T13:53:53.642118: step 12132, loss 0.160423, acc 0.921875\n",
      "2018-05-23T13:53:54.161747: step 12133, loss 0.0593811, acc 0.96875\n",
      "2018-05-23T13:53:54.682662: step 12134, loss 0.180449, acc 0.921875\n",
      "2018-05-23T13:53:55.219257: step 12135, loss 0.105398, acc 0.96875\n",
      "2018-05-23T13:53:55.891515: step 12136, loss 0.148822, acc 0.90625\n",
      "2018-05-23T13:53:56.551991: step 12137, loss 0.134693, acc 0.96875\n",
      "2018-05-23T13:53:57.037204: step 12138, loss 0.107927, acc 0.96875\n",
      "2018-05-23T13:53:57.636144: step 12139, loss 0.147212, acc 0.9375\n",
      "2018-05-23T13:53:58.268342: step 12140, loss 0.141154, acc 0.953125\n",
      "2018-05-23T13:53:58.799430: step 12141, loss 0.095729, acc 0.9375\n",
      "2018-05-23T13:53:59.552416: step 12142, loss 0.192316, acc 0.921875\n",
      "2018-05-23T13:54:00.416108: step 12143, loss 0.0988233, acc 0.953125\n",
      "2018-05-23T13:54:01.157183: step 12144, loss 0.238724, acc 0.90625\n",
      "2018-05-23T13:54:02.046803: step 12145, loss 0.249927, acc 0.859375\n",
      "2018-05-23T13:54:02.866121: step 12146, loss 0.12811, acc 0.96875\n",
      "2018-05-23T13:54:03.625090: step 12147, loss 0.203683, acc 0.9375\n",
      "2018-05-23T13:54:04.429479: step 12148, loss 0.160746, acc 0.96875\n",
      "2018-05-23T13:54:05.130605: step 12149, loss 0.0592482, acc 0.984375\n",
      "2018-05-23T13:54:05.904534: step 12150, loss 0.166815, acc 0.921875\n",
      "2018-05-23T13:54:06.578730: step 12151, loss 0.105357, acc 0.96875\n",
      "2018-05-23T13:54:07.216445: step 12152, loss 0.174591, acc 0.953125\n",
      "2018-05-23T13:54:07.798884: step 12153, loss 0.0728714, acc 0.984375\n",
      "2018-05-23T13:54:08.348453: step 12154, loss 0.0689111, acc 0.984375\n",
      "2018-05-23T13:54:08.869061: step 12155, loss 0.12848, acc 0.953125\n",
      "2018-05-23T13:54:09.358753: step 12156, loss 0.135036, acc 0.953125\n",
      "2018-05-23T13:54:09.817516: step 12157, loss 0.0953648, acc 0.9375\n",
      "2018-05-23T13:54:10.286289: step 12158, loss 0.048403, acc 0.96875\n",
      "2018-05-23T13:54:10.741566: step 12159, loss 0.100278, acc 0.9375\n",
      "2018-05-23T13:54:11.221361: step 12160, loss 0.136671, acc 0.9375\n",
      "2018-05-23T13:54:11.680532: step 12161, loss 0.143437, acc 0.953125\n",
      "2018-05-23T13:54:12.177225: step 12162, loss 0.0942997, acc 0.953125\n",
      "2018-05-23T13:54:12.702839: step 12163, loss 0.135449, acc 0.921875\n",
      "2018-05-23T13:54:13.239402: step 12164, loss 0.0982663, acc 0.953125\n",
      "2018-05-23T13:54:13.690541: step 12165, loss 0.157837, acc 0.921875\n",
      "2018-05-23T13:54:14.198695: step 12166, loss 0.160709, acc 0.90625\n",
      "2018-05-23T13:54:14.750238: step 12167, loss 0.11096, acc 0.953125\n",
      "2018-05-23T13:54:15.265860: step 12168, loss 0.0548871, acc 0.984375\n",
      "2018-05-23T13:54:15.701081: step 12169, loss 0.1273, acc 0.9375\n",
      "2018-05-23T13:54:16.172845: step 12170, loss 0.173032, acc 0.953125\n",
      "2018-05-23T13:54:16.595732: step 12171, loss 0.0459559, acc 0.984375\n",
      "2018-05-23T13:54:17.065003: step 12172, loss 0.0808742, acc 0.96875\n",
      "2018-05-23T13:54:17.663402: step 12173, loss 0.204157, acc 0.890625\n",
      "2018-05-23T13:54:18.240857: step 12174, loss 0.0862161, acc 0.96875\n",
      "2018-05-23T13:54:18.816318: step 12175, loss 0.108114, acc 0.96875\n",
      "2018-05-23T13:54:19.385795: step 12176, loss 0.233616, acc 0.9375\n",
      "2018-05-23T13:54:19.831602: step 12177, loss 0.0853308, acc 0.984375\n",
      "2018-05-23T13:54:20.332263: step 12178, loss 0.187676, acc 0.9375\n",
      "2018-05-23T13:54:20.793032: step 12179, loss 0.232188, acc 0.9375\n",
      "2018-05-23T13:54:21.273532: step 12180, loss 0.0952534, acc 0.96875\n",
      "2018-05-23T13:54:21.790175: step 12181, loss 0.0763995, acc 0.984375\n",
      "2018-05-23T13:54:22.337711: step 12182, loss 0.0707706, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:54:22.846350: step 12183, loss 0.14859, acc 0.921875\n",
      "2018-05-23T13:54:23.436282: step 12184, loss 0.176407, acc 0.921875\n",
      "2018-05-23T13:54:23.918992: step 12185, loss 0.234699, acc 0.9375\n",
      "2018-05-23T13:54:24.421199: step 12186, loss 0.175805, acc 0.90625\n",
      "2018-05-23T13:54:24.880968: step 12187, loss 0.103182, acc 0.953125\n",
      "2018-05-23T13:54:25.395105: step 12188, loss 0.106194, acc 0.9375\n",
      "2018-05-23T13:54:25.850888: step 12189, loss 0.127627, acc 0.9375\n",
      "2018-05-23T13:54:26.310537: step 12190, loss 0.187898, acc 0.921875\n",
      "2018-05-23T13:54:26.749872: step 12191, loss 0.137739, acc 0.9375\n",
      "2018-05-23T13:54:27.199669: step 12192, loss 0.153037, acc 0.9375\n",
      "2018-05-23T13:54:27.695278: step 12193, loss 0.201514, acc 0.921875\n",
      "2018-05-23T13:54:28.189955: step 12194, loss 0.120486, acc 0.96875\n",
      "2018-05-23T13:54:28.723527: step 12195, loss 0.113438, acc 0.953125\n",
      "2018-05-23T13:54:29.304481: step 12196, loss 0.134061, acc 0.9375\n",
      "2018-05-23T13:54:29.875953: step 12197, loss 0.134449, acc 0.9375\n",
      "2018-05-23T13:54:30.445430: step 12198, loss 0.090108, acc 0.953125\n",
      "2018-05-23T13:54:31.011424: step 12199, loss 0.135042, acc 0.90625\n",
      "2018-05-23T13:54:31.591876: step 12200, loss 0.153768, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:54:38.490851: step 12200, loss 1.2666, acc 0.714245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12200\n",
      "\n",
      "2018-05-23T13:54:40.500474: step 12201, loss 0.423687, acc 0.875\n",
      "2018-05-23T13:54:41.126333: step 12202, loss 0.14511, acc 0.921875\n",
      "2018-05-23T13:54:41.711767: step 12203, loss 0.187845, acc 0.9375\n",
      "2018-05-23T13:54:42.346740: step 12204, loss 0.0893241, acc 0.96875\n",
      "2018-05-23T13:54:42.956110: step 12205, loss 0.184695, acc 0.953125\n",
      "2018-05-23T13:54:43.556504: step 12206, loss 0.150607, acc 0.953125\n",
      "2018-05-23T13:54:44.147434: step 12207, loss 0.0989573, acc 0.96875\n",
      "2018-05-23T13:54:44.741844: step 12208, loss 0.0763345, acc 0.984375\n",
      "2018-05-23T13:54:45.381164: step 12209, loss 0.0383489, acc 0.984375\n",
      "2018-05-23T13:54:46.057389: step 12210, loss 0.131594, acc 0.9375\n",
      "2018-05-23T13:54:46.690695: step 12211, loss 0.0261123, acc 1\n",
      "2018-05-23T13:54:47.359904: step 12212, loss 0.103488, acc 0.96875\n",
      "2018-05-23T13:54:48.012159: step 12213, loss 0.107581, acc 0.921875\n",
      "2018-05-23T13:54:48.652445: step 12214, loss 0.11912, acc 0.921875\n",
      "2018-05-23T13:54:49.293731: step 12215, loss 0.300543, acc 0.9375\n",
      "2018-05-23T13:54:49.961943: step 12216, loss 0.140728, acc 0.90625\n",
      "2018-05-23T13:54:50.662582: step 12217, loss 0.151799, acc 0.921875\n",
      "2018-05-23T13:54:51.605061: step 12218, loss 0.144281, acc 0.9375\n",
      "2018-05-23T13:54:52.460773: step 12219, loss 0.0700229, acc 0.96875\n",
      "2018-05-23T13:54:53.424195: step 12220, loss 0.11395, acc 0.953125\n",
      "2018-05-23T13:54:54.245995: step 12221, loss 0.180396, acc 0.9375\n",
      "2018-05-23T13:54:54.881296: step 12222, loss 0.176995, acc 0.90625\n",
      "2018-05-23T13:54:55.605914: step 12223, loss 0.11545, acc 0.984375\n",
      "2018-05-23T13:54:56.232239: step 12224, loss 0.172348, acc 0.9375\n",
      "2018-05-23T13:54:57.049087: step 12225, loss 0.250877, acc 0.890625\n",
      "2018-05-23T13:54:57.743230: step 12226, loss 0.247772, acc 0.90625\n",
      "2018-05-23T13:54:58.394488: step 12227, loss 0.136199, acc 0.9375\n",
      "2018-05-23T13:54:59.042753: step 12228, loss 0.118651, acc 0.953125\n",
      "2018-05-23T13:54:59.687030: step 12229, loss 0.249749, acc 0.921875\n",
      "2018-05-23T13:55:00.347264: step 12230, loss 0.189557, acc 0.9375\n",
      "2018-05-23T13:55:01.000547: step 12231, loss 0.0794861, acc 0.953125\n",
      "2018-05-23T13:55:01.720622: step 12232, loss 0.1822, acc 0.90625\n",
      "2018-05-23T13:55:02.414766: step 12233, loss 0.164656, acc 0.9375\n",
      "2018-05-23T13:55:03.075996: step 12234, loss 0.212956, acc 0.9375\n",
      "2018-05-23T13:55:03.691350: step 12235, loss 0.0560795, acc 0.984375\n",
      "2018-05-23T13:55:04.347594: step 12236, loss 0.222879, acc 0.90625\n",
      "2018-05-23T13:55:05.008336: step 12237, loss 0.117938, acc 0.96875\n",
      "2018-05-23T13:55:05.587786: step 12238, loss 0.126631, acc 0.9375\n",
      "2018-05-23T13:55:06.111385: step 12239, loss 0.104197, acc 0.953125\n",
      "2018-05-23T13:55:06.697294: step 12240, loss 0.15888, acc 0.953125\n",
      "2018-05-23T13:55:07.247348: step 12241, loss 0.0973907, acc 0.96875\n",
      "2018-05-23T13:55:07.791891: step 12242, loss 0.104552, acc 0.96875\n",
      "2018-05-23T13:55:08.376327: step 12243, loss 0.115239, acc 0.953125\n",
      "2018-05-23T13:55:08.906418: step 12244, loss 0.201273, acc 0.84375\n",
      "2018-05-23T13:55:09.437000: step 12245, loss 0.315541, acc 0.9375\n",
      "2018-05-23T13:55:09.884802: step 12246, loss 0.190576, acc 0.921875\n",
      "2018-05-23T13:55:10.333621: step 12247, loss 0.138989, acc 0.953125\n",
      "2018-05-23T13:55:10.879191: step 12248, loss 0.226914, acc 0.90625\n",
      "2018-05-23T13:55:11.353339: step 12249, loss 0.15921, acc 0.953125\n",
      "2018-05-23T13:55:11.736338: step 12250, loss 0.274095, acc 0.90625\n",
      "2018-05-23T13:55:12.117609: step 12251, loss 0.163689, acc 0.9375\n",
      "2018-05-23T13:55:12.524542: step 12252, loss 0.110205, acc 0.96875\n",
      "2018-05-23T13:55:12.911508: step 12253, loss 0.132982, acc 0.921875\n",
      "2018-05-23T13:55:13.304474: step 12254, loss 0.0981022, acc 0.96875\n",
      "2018-05-23T13:55:13.708585: step 12255, loss 0.228937, acc 0.9375\n",
      "2018-05-23T13:55:14.099558: step 12256, loss 0.147879, acc 0.921875\n",
      "2018-05-23T13:55:14.487521: step 12257, loss 0.0367886, acc 0.984375\n",
      "2018-05-23T13:55:14.922360: step 12258, loss 0.0786782, acc 0.984375\n",
      "2018-05-23T13:55:15.338755: step 12259, loss 0.058252, acc 0.96875\n",
      "2018-05-23T13:55:15.769621: step 12260, loss 0.144963, acc 0.921875\n",
      "2018-05-23T13:55:16.183535: step 12261, loss 0.107562, acc 0.953125\n",
      "2018-05-23T13:55:16.607907: step 12262, loss 0.161198, acc 0.9375\n",
      "2018-05-23T13:55:16.985295: step 12263, loss 0.259896, acc 0.90625\n",
      "2018-05-23T13:55:17.361291: step 12264, loss 0.270521, acc 0.921875\n",
      "2018-05-23T13:55:17.856965: step 12265, loss 0.127091, acc 0.953125\n",
      "2018-05-23T13:55:18.259281: step 12266, loss 0.119288, acc 0.9375\n",
      "2018-05-23T13:55:18.654242: step 12267, loss 0.0773721, acc 0.984375\n",
      "2018-05-23T13:55:19.042371: step 12268, loss 0.139684, acc 0.953125\n",
      "2018-05-23T13:55:19.427554: step 12269, loss 0.117468, acc 0.9375\n",
      "2018-05-23T13:55:19.828491: step 12270, loss 0.128162, acc 0.9375\n",
      "2018-05-23T13:55:20.218281: step 12271, loss 0.094031, acc 0.953125\n",
      "2018-05-23T13:55:20.648659: step 12272, loss 0.108024, acc 0.953125\n",
      "2018-05-23T13:55:21.146835: step 12273, loss 0.102879, acc 0.953125\n",
      "2018-05-23T13:55:21.602633: step 12274, loss 0.102217, acc 0.9375\n",
      "2018-05-23T13:55:22.033480: step 12275, loss 0.0860703, acc 0.96875\n",
      "2018-05-23T13:55:22.413977: step 12276, loss 0.136699, acc 0.96875\n",
      "2018-05-23T13:55:22.792964: step 12277, loss 0.156305, acc 0.921875\n",
      "2018-05-23T13:55:23.261728: step 12278, loss 0.142308, acc 0.953125\n",
      "2018-05-23T13:55:23.623271: step 12279, loss 0.117312, acc 0.921875\n",
      "2018-05-23T13:55:24.082788: step 12280, loss 0.114575, acc 0.953125\n",
      "2018-05-23T13:55:24.456300: step 12281, loss 0.143128, acc 0.9375\n",
      "2018-05-23T13:55:24.838309: step 12282, loss 0.126049, acc 0.96875\n",
      "2018-05-23T13:55:25.213329: step 12283, loss 0.200403, acc 0.953125\n",
      "2018-05-23T13:55:25.577541: step 12284, loss 0.178209, acc 0.921875\n",
      "2018-05-23T13:55:25.941600: step 12285, loss 0.0960837, acc 0.953125\n",
      "2018-05-23T13:55:26.323108: step 12286, loss 0.113686, acc 0.9375\n",
      "2018-05-23T13:55:26.709586: step 12287, loss 0.152328, acc 0.953125\n",
      "2018-05-23T13:55:27.130458: step 12288, loss 0.202786, acc 0.9375\n",
      "2018-05-23T13:55:27.567327: step 12289, loss 0.0578808, acc 1\n",
      "2018-05-23T13:55:28.019628: step 12290, loss 0.0753055, acc 0.984375\n",
      "2018-05-23T13:55:28.433034: step 12291, loss 0.13821, acc 0.953125\n",
      "2018-05-23T13:55:28.810024: step 12292, loss 0.157843, acc 0.921875\n",
      "2018-05-23T13:55:29.207156: step 12293, loss 0.0749845, acc 0.96875\n",
      "2018-05-23T13:55:29.569186: step 12294, loss 0.16581, acc 0.953125\n",
      "2018-05-23T13:55:29.935206: step 12295, loss 0.234047, acc 0.890625\n",
      "2018-05-23T13:55:30.350610: step 12296, loss 0.0992685, acc 0.96875\n",
      "2018-05-23T13:55:30.754824: step 12297, loss 0.169279, acc 0.921875\n",
      "2018-05-23T13:55:31.159859: step 12298, loss 0.165311, acc 0.9375\n",
      "2018-05-23T13:55:31.593213: step 12299, loss 0.0413216, acc 1\n",
      "2018-05-23T13:55:31.978622: step 12300, loss 0.155731, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:55:37.245618: step 12300, loss 1.25432, acc 0.716388\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12300\n",
      "\n",
      "2018-05-23T13:55:39.049833: step 12301, loss 0.0929018, acc 0.96875\n",
      "2018-05-23T13:55:39.467236: step 12302, loss 0.133307, acc 0.921875\n",
      "2018-05-23T13:55:39.915548: step 12303, loss 0.19564, acc 0.9375\n",
      "2018-05-23T13:55:40.313527: step 12304, loss 0.267826, acc 0.859375\n",
      "2018-05-23T13:55:40.733600: step 12305, loss 0.0740812, acc 0.984375\n",
      "2018-05-23T13:55:41.119596: step 12306, loss 0.196424, acc 0.90625\n",
      "2018-05-23T13:55:41.541503: step 12307, loss 0.0815439, acc 0.953125\n",
      "2018-05-23T13:55:41.948596: step 12308, loss 0.19145, acc 0.921875\n",
      "2018-05-23T13:55:42.341565: step 12309, loss 0.103329, acc 0.9375\n",
      "2018-05-23T13:55:42.747479: step 12310, loss 0.117898, acc 0.953125\n",
      "2018-05-23T13:55:43.142428: step 12311, loss 0.124557, acc 0.9375\n",
      "2018-05-23T13:55:43.543355: step 12312, loss 0.083071, acc 0.953125\n",
      "2018-05-23T13:55:43.947295: step 12313, loss 0.0999091, acc 0.953125\n",
      "2018-05-23T13:55:44.389633: step 12314, loss 0.0563715, acc 0.96875\n",
      "2018-05-23T13:55:44.779607: step 12315, loss 0.12086, acc 0.921875\n",
      "2018-05-23T13:55:45.170075: step 12316, loss 0.116646, acc 0.984375\n",
      "2018-05-23T13:55:45.563023: step 12317, loss 0.0870957, acc 0.953125\n",
      "2018-05-23T13:55:46.003354: step 12318, loss 0.200228, acc 0.90625\n",
      "2018-05-23T13:55:46.398828: step 12319, loss 0.129145, acc 0.96875\n",
      "2018-05-23T13:55:46.802769: step 12320, loss 0.0421427, acc 1\n",
      "2018-05-23T13:55:47.204718: step 12321, loss 0.159213, acc 0.9375\n",
      "2018-05-23T13:55:47.652521: step 12322, loss 0.0534517, acc 0.984375\n",
      "2018-05-23T13:55:48.086266: step 12323, loss 0.0742861, acc 0.96875\n",
      "2018-05-23T13:55:48.508192: step 12324, loss 0.265612, acc 0.921875\n",
      "2018-05-23T13:55:48.964986: step 12325, loss 0.158457, acc 0.921875\n",
      "2018-05-23T13:55:49.407623: step 12326, loss 0.116922, acc 0.9375\n",
      "2018-05-23T13:55:49.791107: step 12327, loss 0.133812, acc 0.9375\n",
      "2018-05-23T13:55:50.181065: step 12328, loss 0.087889, acc 0.9375\n",
      "2018-05-23T13:55:50.583994: step 12329, loss 0.120408, acc 0.9375\n",
      "2018-05-23T13:55:50.977942: step 12330, loss 0.12922, acc 0.96875\n",
      "2018-05-23T13:55:51.363420: step 12331, loss 0.140793, acc 0.953125\n",
      "2018-05-23T13:55:51.753386: step 12332, loss 0.161708, acc 0.9375\n",
      "2018-05-23T13:55:52.144848: step 12333, loss 0.171411, acc 0.9375\n",
      "2018-05-23T13:55:52.537316: step 12334, loss 0.101162, acc 0.9375\n",
      "2018-05-23T13:55:52.930265: step 12335, loss 0.175244, acc 0.90625\n",
      "2018-05-23T13:55:53.397524: step 12336, loss 0.0710928, acc 0.96875\n",
      "2018-05-23T13:55:53.796456: step 12337, loss 0.0837506, acc 0.96875\n",
      "2018-05-23T13:55:54.188189: step 12338, loss 0.149258, acc 0.953125\n",
      "2018-05-23T13:55:54.675135: step 12339, loss 0.0751553, acc 0.96875\n",
      "2018-05-23T13:55:55.160837: step 12340, loss 0.100023, acc 0.9375\n",
      "2018-05-23T13:55:55.589204: step 12341, loss 0.119666, acc 0.953125\n",
      "2018-05-23T13:55:56.022082: step 12342, loss 0.0489136, acc 1\n",
      "2018-05-23T13:55:56.400580: step 12343, loss 0.184282, acc 0.921875\n",
      "2018-05-23T13:55:56.778576: step 12344, loss 0.0758916, acc 0.96875\n",
      "2018-05-23T13:55:57.147584: step 12345, loss 0.083766, acc 0.984375\n",
      "2018-05-23T13:55:57.572214: step 12346, loss 0.0842206, acc 0.953125\n",
      "2018-05-23T13:55:57.948208: step 12347, loss 0.0998757, acc 0.953125\n",
      "2018-05-23T13:55:58.331200: step 12348, loss 0.105653, acc 0.953125\n",
      "2018-05-23T13:55:58.699217: step 12349, loss 0.0839843, acc 0.96875\n",
      "2018-05-23T13:55:59.090696: step 12350, loss 0.184948, acc 0.890625\n",
      "2018-05-23T13:55:59.484657: step 12351, loss 0.256174, acc 0.875\n",
      "2018-05-23T13:55:59.888091: step 12352, loss 0.0987515, acc 0.96875\n",
      "2018-05-23T13:56:00.285554: step 12353, loss 0.081271, acc 0.984375\n",
      "2018-05-23T13:56:00.703736: step 12354, loss 0.0760918, acc 0.96875\n",
      "2018-05-23T13:56:01.107678: step 12355, loss 0.166631, acc 0.921875\n",
      "2018-05-23T13:56:01.560485: step 12356, loss 0.109045, acc 0.953125\n",
      "2018-05-23T13:56:01.995345: step 12357, loss 0.179869, acc 0.921875\n",
      "2018-05-23T13:56:02.396218: step 12358, loss 0.0873851, acc 0.96875\n",
      "2018-05-23T13:56:02.818090: step 12359, loss 0.165257, acc 0.9375\n",
      "2018-05-23T13:56:03.217548: step 12360, loss 0.0617794, acc 0.96875\n",
      "2018-05-23T13:56:03.619351: step 12361, loss 0.2317, acc 0.84375\n",
      "2018-05-23T13:56:04.124509: step 12362, loss 0.120245, acc 0.9375\n",
      "2018-05-23T13:56:04.536080: step 12363, loss 0.209642, acc 0.890625\n",
      "2018-05-23T13:56:04.922546: step 12364, loss 0.109051, acc 0.953125\n",
      "2018-05-23T13:56:05.310022: step 12365, loss 0.0753346, acc 0.96875\n",
      "2018-05-23T13:56:05.734420: step 12366, loss 0.0936644, acc 0.96875\n",
      "2018-05-23T13:56:06.123512: step 12367, loss 0.370799, acc 0.796875\n",
      "2018-05-23T13:56:06.585302: step 12368, loss 0.102387, acc 0.953125\n",
      "2018-05-23T13:56:07.039383: step 12369, loss 0.095257, acc 0.9375\n",
      "2018-05-23T13:56:07.427356: step 12370, loss 0.167333, acc 0.890625\n",
      "2018-05-23T13:56:07.839255: step 12371, loss 0.0763435, acc 0.96875\n",
      "2018-05-23T13:56:08.380806: step 12372, loss 0.160164, acc 0.921875\n",
      "2018-05-23T13:56:08.831590: step 12373, loss 0.0852317, acc 0.96875\n",
      "2018-05-23T13:56:09.297850: step 12374, loss 0.237347, acc 0.90625\n",
      "2018-05-23T13:56:09.699777: step 12375, loss 0.142463, acc 0.921875\n",
      "2018-05-23T13:56:10.138602: step 12376, loss 0.150535, acc 0.953125\n",
      "2018-05-23T13:56:10.535561: step 12377, loss 0.1381, acc 0.9375\n",
      "2018-05-23T13:56:10.925534: step 12378, loss 0.138505, acc 0.96875\n",
      "2018-05-23T13:56:11.323469: step 12379, loss 0.193765, acc 0.921875\n",
      "2018-05-23T13:56:11.710445: step 12380, loss 0.184734, acc 0.953125\n",
      "2018-05-23T13:56:12.090447: step 12381, loss 0.138357, acc 0.96875\n",
      "2018-05-23T13:56:12.481608: step 12382, loss 0.168355, acc 0.9375\n",
      "2018-05-23T13:56:12.877569: step 12383, loss 0.145836, acc 0.9375\n",
      "2018-05-23T13:56:13.338165: step 12384, loss 0.138522, acc 0.921875\n",
      "2018-05-23T13:56:13.720143: step 12385, loss 0.230036, acc 0.859375\n",
      "2018-05-23T13:56:14.118088: step 12386, loss 0.160312, acc 0.921875\n",
      "2018-05-23T13:56:14.510590: step 12387, loss 0.13032, acc 0.921875\n",
      "2018-05-23T13:56:14.899549: step 12388, loss 0.107032, acc 0.953125\n",
      "2018-05-23T13:56:15.355857: step 12389, loss 0.0746089, acc 0.984375\n",
      "2018-05-23T13:56:15.794193: step 12390, loss 0.0912936, acc 0.9375\n",
      "2018-05-23T13:56:16.194145: step 12391, loss 0.111183, acc 0.953125\n",
      "2018-05-23T13:56:16.581120: step 12392, loss 0.199012, acc 0.90625\n",
      "2018-05-23T13:56:16.965596: step 12393, loss 0.138442, acc 0.921875\n",
      "2018-05-23T13:56:17.403949: step 12394, loss 0.121872, acc 0.9375\n",
      "2018-05-23T13:56:17.795901: step 12395, loss 0.129796, acc 0.9375\n",
      "2018-05-23T13:56:18.190356: step 12396, loss 0.189328, acc 0.90625\n",
      "2018-05-23T13:56:18.602287: step 12397, loss 0.15205, acc 0.921875\n",
      "2018-05-23T13:56:18.991158: step 12398, loss 0.147907, acc 0.90625\n",
      "2018-05-23T13:56:19.415024: step 12399, loss 0.158143, acc 0.953125\n",
      "2018-05-23T13:56:19.808142: step 12400, loss 0.208848, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:56:25.315169: step 12400, loss 1.2791, acc 0.718531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12400\n",
      "\n",
      "2018-05-23T13:56:27.085487: step 12401, loss 0.099125, acc 0.984375\n",
      "2018-05-23T13:56:27.484450: step 12402, loss 0.193416, acc 0.921875\n",
      "2018-05-23T13:56:27.914553: step 12403, loss 0.175149, acc 0.96875\n",
      "2018-05-23T13:56:28.304558: step 12404, loss 0.121758, acc 0.96875\n",
      "2018-05-23T13:56:28.694026: step 12405, loss 0.120599, acc 0.953125\n",
      "2018-05-23T13:56:29.073493: step 12406, loss 0.170177, acc 0.953125\n",
      "2018-05-23T13:56:29.476406: step 12407, loss 0.12294, acc 0.9375\n",
      "2018-05-23T13:56:29.858719: step 12408, loss 0.10243, acc 0.953125\n",
      "2018-05-23T13:56:30.235736: step 12409, loss 0.163283, acc 0.953125\n",
      "2018-05-23T13:56:30.629686: step 12410, loss 0.213375, acc 0.9375\n",
      "2018-05-23T13:56:31.056563: step 12411, loss 0.106287, acc 0.9375\n",
      "2018-05-23T13:56:31.615450: step 12412, loss 0.0940012, acc 0.96875\n",
      "2018-05-23T13:56:32.034785: step 12413, loss 0.0666852, acc 1\n",
      "2018-05-23T13:56:32.459174: step 12414, loss 0.151839, acc 0.9375\n",
      "2018-05-23T13:56:32.845648: step 12415, loss 0.0858358, acc 0.953125\n",
      "2018-05-23T13:56:33.238176: step 12416, loss 0.113622, acc 0.9375\n",
      "2018-05-23T13:56:33.629155: step 12417, loss 0.139704, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:56:34.009137: step 12418, loss 0.231536, acc 0.90625\n",
      "2018-05-23T13:56:34.400006: step 12419, loss 0.160036, acc 0.921875\n",
      "2018-05-23T13:56:34.830305: step 12420, loss 0.107011, acc 0.953125\n",
      "2018-05-23T13:56:35.229750: step 12421, loss 0.111964, acc 0.953125\n",
      "2018-05-23T13:56:35.625701: step 12422, loss 0.213902, acc 0.921875\n",
      "2018-05-23T13:56:36.012863: step 12423, loss 0.117878, acc 0.953125\n",
      "2018-05-23T13:56:36.437233: step 12424, loss 0.0655795, acc 0.984375\n",
      "2018-05-23T13:56:36.814754: step 12425, loss 0.0651779, acc 0.953125\n",
      "2018-05-23T13:56:37.227158: step 12426, loss 0.175071, acc 0.921875\n",
      "2018-05-23T13:56:37.682963: step 12427, loss 0.138079, acc 0.9375\n",
      "2018-05-23T13:56:38.093376: step 12428, loss 0.111568, acc 0.921875\n",
      "2018-05-23T13:56:38.483355: step 12429, loss 0.167977, acc 0.90625\n",
      "2018-05-23T13:56:38.863335: step 12430, loss 0.0792942, acc 0.984375\n",
      "2018-05-23T13:56:39.251314: step 12431, loss 0.068743, acc 0.984375\n",
      "2018-05-23T13:56:39.633534: step 12432, loss 0.172534, acc 0.921875\n",
      "2018-05-23T13:56:40.028998: step 12433, loss 0.0647334, acc 0.984375\n",
      "2018-05-23T13:56:40.482296: step 12434, loss 0.0685118, acc 0.96875\n",
      "2018-05-23T13:56:40.860287: step 12435, loss 0.235716, acc 0.890625\n",
      "2018-05-23T13:56:41.260227: step 12436, loss 0.0463829, acc 0.984375\n",
      "2018-05-23T13:56:41.698158: step 12437, loss 0.209967, acc 0.90625\n",
      "2018-05-23T13:56:42.136498: step 12438, loss 0.150695, acc 0.9375\n",
      "2018-05-23T13:56:42.585438: step 12439, loss 0.0885316, acc 0.953125\n",
      "2018-05-23T13:56:42.967436: step 12440, loss 0.219324, acc 0.90625\n",
      "2018-05-23T13:56:43.364409: step 12441, loss 0.105452, acc 0.96875\n",
      "2018-05-23T13:56:43.786307: step 12442, loss 0.080492, acc 0.96875\n",
      "2018-05-23T13:56:44.169283: step 12443, loss 0.0675575, acc 0.96875\n",
      "2018-05-23T13:56:44.557783: step 12444, loss 0.160187, acc 0.9375\n",
      "2018-05-23T13:56:44.946742: step 12445, loss 0.0810527, acc 0.9375\n",
      "2018-05-23T13:56:45.339062: step 12446, loss 0.240572, acc 0.921875\n",
      "2018-05-23T13:56:45.709575: step 12447, loss 0.196651, acc 0.9375\n",
      "2018-05-23T13:56:46.094708: step 12448, loss 0.222209, acc 0.9375\n",
      "2018-05-23T13:56:46.485173: step 12449, loss 0.135437, acc 0.9375\n",
      "2018-05-23T13:56:46.863684: step 12450, loss 0.18702, acc 0.953125\n",
      "2018-05-23T13:56:47.249670: step 12451, loss 0.0966386, acc 0.96875\n",
      "2018-05-23T13:56:47.631598: step 12452, loss 0.143884, acc 0.953125\n",
      "2018-05-23T13:56:48.035026: step 12453, loss 0.125664, acc 0.953125\n",
      "2018-05-23T13:56:48.425015: step 12454, loss 0.107004, acc 0.9375\n",
      "2018-05-23T13:56:48.824986: step 12455, loss 0.267403, acc 0.96875\n",
      "2018-05-23T13:56:49.246858: step 12456, loss 0.193601, acc 0.921875\n",
      "2018-05-23T13:56:49.630831: step 12457, loss 0.258716, acc 0.875\n",
      "2018-05-23T13:56:50.068659: step 12458, loss 0.110392, acc 0.9375\n",
      "2018-05-23T13:56:50.531421: step 12459, loss 0.231421, acc 0.90625\n",
      "2018-05-23T13:56:51.014746: step 12460, loss 0.0910868, acc 0.96875\n",
      "2018-05-23T13:56:51.417692: step 12461, loss 0.0710571, acc 0.984375\n",
      "2018-05-23T13:56:51.806680: step 12462, loss 0.0442834, acc 0.984375\n",
      "2018-05-23T13:56:52.259469: step 12463, loss 0.237425, acc 0.921875\n",
      "2018-05-23T13:56:52.682415: step 12464, loss 0.067441, acc 0.984375\n",
      "2018-05-23T13:56:53.086978: step 12465, loss 0.0808975, acc 0.96875\n",
      "2018-05-23T13:56:53.472463: step 12466, loss 0.104407, acc 0.9375\n",
      "2018-05-23T13:56:53.850473: step 12467, loss 0.121465, acc 0.953125\n",
      "2018-05-23T13:56:54.289300: step 12468, loss 0.0768652, acc 0.96875\n",
      "2018-05-23T13:56:54.727128: step 12469, loss 0.173345, acc 0.96875\n",
      "2018-05-23T13:56:55.161099: step 12470, loss 0.140409, acc 0.921875\n",
      "2018-05-23T13:56:55.574014: step 12471, loss 0.0779965, acc 0.953125\n",
      "2018-05-23T13:56:55.988292: step 12472, loss 0.110745, acc 0.9375\n",
      "2018-05-23T13:56:56.438654: step 12473, loss 0.140846, acc 0.9375\n",
      "2018-05-23T13:56:56.838592: step 12474, loss 0.162303, acc 0.90625\n",
      "2018-05-23T13:56:57.233556: step 12475, loss 0.0622835, acc 0.984375\n",
      "2018-05-23T13:56:57.622516: step 12476, loss 0.134661, acc 0.9375\n",
      "2018-05-23T13:56:58.049374: step 12477, loss 0.0380937, acc 1\n",
      "2018-05-23T13:56:58.551032: step 12478, loss 0.168149, acc 0.90625\n",
      "2018-05-23T13:56:59.005827: step 12479, loss 0.136907, acc 0.984375\n",
      "2018-05-23T13:56:59.458615: step 12480, loss 0.0759984, acc 0.953125\n",
      "2018-05-23T13:56:59.866523: step 12481, loss 0.158028, acc 0.953125\n",
      "2018-05-23T13:57:00.277424: step 12482, loss 0.0591129, acc 0.984375\n",
      "2018-05-23T13:57:00.653639: step 12483, loss 0.217712, acc 0.890625\n",
      "2018-05-23T13:57:01.170295: step 12484, loss 0.131563, acc 0.953125\n",
      "2018-05-23T13:57:01.652017: step 12485, loss 0.248383, acc 0.921875\n",
      "2018-05-23T13:57:02.063916: step 12486, loss 0.103673, acc 0.984375\n",
      "2018-05-23T13:57:02.475835: step 12487, loss 0.0706454, acc 0.984375\n",
      "2018-05-23T13:57:02.957055: step 12488, loss 0.102102, acc 0.9375\n",
      "2018-05-23T13:57:03.417834: step 12489, loss 0.089394, acc 0.96875\n",
      "2018-05-23T13:57:03.848195: step 12490, loss 0.218446, acc 0.953125\n",
      "2018-05-23T13:57:04.254129: step 12491, loss 0.0618583, acc 0.984375\n",
      "2018-05-23T13:57:04.687025: step 12492, loss 0.109398, acc 0.96875\n",
      "2018-05-23T13:57:05.079489: step 12493, loss 0.184635, acc 0.90625\n",
      "2018-05-23T13:57:05.461485: step 12494, loss 0.082186, acc 0.96875\n",
      "2018-05-23T13:57:05.905296: step 12495, loss 0.149493, acc 0.921875\n",
      "2018-05-23T13:57:06.293269: step 12496, loss 0.0770866, acc 0.9375\n",
      "2018-05-23T13:57:06.678253: step 12497, loss 0.191769, acc 0.9375\n",
      "2018-05-23T13:57:07.097133: step 12498, loss 0.112464, acc 0.953125\n",
      "2018-05-23T13:57:07.483104: step 12499, loss 0.13779, acc 0.9375\n",
      "2018-05-23T13:57:07.924748: step 12500, loss 0.154267, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:57:13.442678: step 12500, loss 1.29653, acc 0.718245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12500\n",
      "\n",
      "2018-05-23T13:57:15.011019: step 12501, loss 0.136372, acc 0.9375\n",
      "2018-05-23T13:57:15.472784: step 12502, loss 0.321895, acc 0.890625\n",
      "2018-05-23T13:57:15.892661: step 12503, loss 0.135379, acc 0.953125\n",
      "2018-05-23T13:57:16.366393: step 12504, loss 0.242356, acc 0.921875\n",
      "2018-05-23T13:57:16.765346: step 12505, loss 0.091699, acc 0.953125\n",
      "2018-05-23T13:57:17.181121: step 12506, loss 0.151877, acc 0.890625\n",
      "2018-05-23T13:57:17.625956: step 12507, loss 0.149111, acc 0.890625\n",
      "2018-05-23T13:57:18.019902: step 12508, loss 0.121367, acc 0.9375\n",
      "2018-05-23T13:57:18.455739: step 12509, loss 0.134305, acc 0.96875\n",
      "2018-05-23T13:57:18.891625: step 12510, loss 0.236983, acc 0.875\n",
      "2018-05-23T13:57:19.327458: step 12511, loss 0.0524083, acc 1\n",
      "2018-05-23T13:57:19.713455: step 12512, loss 0.121803, acc 0.953125\n",
      "2018-05-23T13:57:20.143207: step 12513, loss 0.11609, acc 0.9375\n",
      "2018-05-23T13:57:20.647856: step 12514, loss 0.271004, acc 0.90625\n",
      "2018-05-23T13:57:21.064741: step 12515, loss 0.152553, acc 0.9375\n",
      "2018-05-23T13:57:21.502597: step 12516, loss 0.0647576, acc 0.984375\n",
      "2018-05-23T13:57:21.893555: step 12517, loss 0.268376, acc 0.890625\n",
      "2018-05-23T13:57:22.293939: step 12518, loss 0.227327, acc 0.90625\n",
      "2018-05-23T13:57:22.694605: step 12519, loss 0.395387, acc 0.828125\n",
      "2018-05-23T13:57:23.077603: step 12520, loss 0.0211574, acc 1\n",
      "2018-05-23T13:57:23.572301: step 12521, loss 0.186671, acc 0.921875\n",
      "2018-05-23T13:57:24.013140: step 12522, loss 0.0798265, acc 0.953125\n",
      "2018-05-23T13:57:24.465464: step 12523, loss 0.140305, acc 0.9375\n",
      "2018-05-23T13:57:24.885472: step 12524, loss 0.0989972, acc 0.953125\n",
      "2018-05-23T13:57:25.272946: step 12525, loss 0.1441, acc 0.96875\n",
      "2018-05-23T13:57:25.662942: step 12526, loss 0.159375, acc 0.921875\n",
      "2018-05-23T13:57:26.064902: step 12527, loss 0.0782485, acc 0.96875\n",
      "2018-05-23T13:57:26.452834: step 12528, loss 0.112845, acc 0.96875\n",
      "2018-05-23T13:57:26.893655: step 12529, loss 0.0619678, acc 0.96875\n",
      "2018-05-23T13:57:27.276375: step 12530, loss 0.324205, acc 0.90625\n",
      "2018-05-23T13:57:27.745190: step 12531, loss 0.104257, acc 0.953125\n",
      "2018-05-23T13:57:28.145124: step 12532, loss 0.095815, acc 0.953125\n",
      "2018-05-23T13:57:28.534439: step 12533, loss 0.183213, acc 0.921875\n",
      "2018-05-23T13:57:28.912447: step 12534, loss 0.0892433, acc 0.96875\n",
      "2018-05-23T13:57:29.301232: step 12535, loss 0.385359, acc 0.921875\n",
      "2018-05-23T13:57:29.694828: step 12536, loss 0.107279, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:57:30.134667: step 12537, loss 0.126089, acc 0.921875\n",
      "2018-05-23T13:57:30.563543: step 12538, loss 0.133329, acc 0.9375\n",
      "2018-05-23T13:57:30.961476: step 12539, loss 0.163945, acc 0.921875\n",
      "2018-05-23T13:57:31.362423: step 12540, loss 0.147412, acc 0.921875\n",
      "2018-05-23T13:57:31.766852: step 12541, loss 0.345233, acc 0.875\n",
      "2018-05-23T13:57:32.236126: step 12542, loss 0.108065, acc 0.953125\n",
      "2018-05-23T13:57:32.659370: step 12543, loss 0.168904, acc 0.953125\n",
      "2018-05-23T13:57:33.107189: step 12544, loss 0.217031, acc 0.921875\n",
      "2018-05-23T13:57:33.509115: step 12545, loss 0.159195, acc 0.9375\n",
      "2018-05-23T13:57:33.984409: step 12546, loss 0.120809, acc 0.953125\n",
      "2018-05-23T13:57:34.533088: step 12547, loss 0.198632, acc 0.921875\n",
      "2018-05-23T13:57:35.219249: step 12548, loss 0.296333, acc 0.953125\n",
      "2018-05-23T13:57:35.938327: step 12549, loss 0.0403471, acc 1\n",
      "2018-05-23T13:57:36.474472: step 12550, loss 0.18701, acc 0.890625\n",
      "2018-05-23T13:57:37.073217: step 12551, loss 0.164721, acc 0.890625\n",
      "2018-05-23T13:57:37.572881: step 12552, loss 0.103441, acc 0.953125\n",
      "2018-05-23T13:57:38.232118: step 12553, loss 0.129719, acc 0.9375\n",
      "2018-05-23T13:57:38.816580: step 12554, loss 0.0550734, acc 0.984375\n",
      "2018-05-23T13:57:39.399050: step 12555, loss 0.0721856, acc 0.96875\n",
      "2018-05-23T13:57:39.819409: step 12556, loss 0.123213, acc 0.96875\n",
      "2018-05-23T13:57:40.210888: step 12557, loss 0.126185, acc 0.953125\n",
      "2018-05-23T13:57:40.599360: step 12558, loss 0.126307, acc 0.9375\n",
      "2018-05-23T13:57:40.981337: step 12559, loss 0.144576, acc 0.9375\n",
      "2018-05-23T13:57:41.366306: step 12560, loss 0.194559, acc 0.921875\n",
      "2018-05-23T13:57:41.761073: step 12561, loss 0.122267, acc 0.9375\n",
      "2018-05-23T13:57:42.144955: step 12562, loss 0.0864061, acc 0.96875\n",
      "2018-05-23T13:57:42.588905: step 12563, loss 0.0877949, acc 0.953125\n",
      "2018-05-23T13:57:42.980125: step 12564, loss 0.32148, acc 0.921875\n",
      "2018-05-23T13:57:43.370373: step 12565, loss 0.0952645, acc 0.9375\n",
      "2018-05-23T13:57:43.798251: step 12566, loss 0.0776019, acc 0.96875\n",
      "2018-05-23T13:57:44.183237: step 12567, loss 0.0995669, acc 0.9375\n",
      "2018-05-23T13:57:44.629045: step 12568, loss 0.242734, acc 0.921875\n",
      "2018-05-23T13:57:45.019002: step 12569, loss 0.19513, acc 0.921875\n",
      "2018-05-23T13:57:45.413490: step 12570, loss 0.200124, acc 0.9375\n",
      "2018-05-23T13:57:45.835361: step 12571, loss 0.0831453, acc 0.984375\n",
      "2018-05-23T13:57:46.232300: step 12572, loss 0.196335, acc 0.9375\n",
      "2018-05-23T13:57:46.668168: step 12573, loss 0.262248, acc 0.9375\n",
      "2018-05-23T13:57:47.113943: step 12574, loss 0.119249, acc 0.953125\n",
      "2018-05-23T13:57:47.511388: step 12575, loss 0.206295, acc 0.890625\n",
      "2018-05-23T13:57:47.946224: step 12576, loss 0.0881522, acc 0.96875\n",
      "2018-05-23T13:57:48.405104: step 12577, loss 0.199633, acc 0.890625\n",
      "2018-05-23T13:57:48.897787: step 12578, loss 0.253666, acc 0.890625\n",
      "2018-05-23T13:57:49.314324: step 12579, loss 0.0922846, acc 0.953125\n",
      "2018-05-23T13:57:49.719270: step 12580, loss 0.145563, acc 0.90625\n",
      "2018-05-23T13:57:50.144134: step 12581, loss 0.182544, acc 0.9375\n",
      "2018-05-23T13:57:50.541585: step 12582, loss 0.207466, acc 0.90625\n",
      "2018-05-23T13:57:50.948007: step 12583, loss 0.191264, acc 0.90625\n",
      "2018-05-23T13:57:51.369880: step 12584, loss 0.0953162, acc 0.96875\n",
      "2018-05-23T13:57:51.836492: step 12585, loss 0.0766837, acc 0.96875\n",
      "2018-05-23T13:57:52.230970: step 12586, loss 0.078548, acc 0.96875\n",
      "2018-05-23T13:57:52.624913: step 12587, loss 0.154691, acc 0.953125\n",
      "2018-05-23T13:57:53.014870: step 12588, loss 0.10715, acc 0.953125\n",
      "2018-05-23T13:57:53.440614: step 12589, loss 0.135381, acc 0.921875\n",
      "2018-05-23T13:57:53.874479: step 12590, loss 0.117939, acc 0.953125\n",
      "2018-05-23T13:57:54.272551: step 12591, loss 0.112025, acc 0.9375\n",
      "2018-05-23T13:57:54.662019: step 12592, loss 0.170643, acc 0.9375\n",
      "2018-05-23T13:57:55.089262: step 12593, loss 0.0795391, acc 0.984375\n",
      "2018-05-23T13:57:55.514293: step 12594, loss 0.318365, acc 0.90625\n",
      "2018-05-23T13:57:55.953152: step 12595, loss 0.226164, acc 0.890625\n",
      "2018-05-23T13:57:56.360062: step 12596, loss 0.100768, acc 0.953125\n",
      "2018-05-23T13:57:56.779962: step 12597, loss 0.0658133, acc 0.96875\n",
      "2018-05-23T13:57:57.174157: step 12598, loss 0.0977848, acc 0.953125\n",
      "2018-05-23T13:57:57.567135: step 12599, loss 0.145419, acc 0.9375\n",
      "2018-05-23T13:57:57.989006: step 12600, loss 0.164319, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:58:03.683369: step 12600, loss 1.25362, acc 0.717674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12600\n",
      "\n",
      "2018-05-23T13:58:05.267173: step 12601, loss 0.176508, acc 0.96875\n",
      "2018-05-23T13:58:05.745893: step 12602, loss 0.0937521, acc 0.96875\n",
      "2018-05-23T13:58:06.130866: step 12603, loss 0.10137, acc 0.953125\n",
      "2018-05-23T13:58:06.554414: step 12604, loss 0.161642, acc 0.953125\n",
      "2018-05-23T13:58:06.944899: step 12605, loss 0.130062, acc 0.953125\n",
      "2018-05-23T13:58:07.425632: step 12606, loss 0.150933, acc 0.96875\n",
      "2018-05-23T13:58:07.846017: step 12607, loss 0.295484, acc 0.890625\n",
      "2018-05-23T13:58:08.281873: step 12608, loss 0.138247, acc 0.921875\n",
      "2018-05-23T13:58:08.692303: step 12609, loss 0.0982169, acc 0.984375\n",
      "2018-05-23T13:58:09.083258: step 12610, loss 0.0798689, acc 0.984375\n",
      "2018-05-23T13:58:09.493161: step 12611, loss 0.128133, acc 0.953125\n",
      "2018-05-23T13:58:09.883118: step 12612, loss 0.0580173, acc 1\n",
      "2018-05-23T13:58:10.269610: step 12613, loss 0.104551, acc 0.96875\n",
      "2018-05-23T13:58:10.730377: step 12614, loss 0.123491, acc 0.953125\n",
      "2018-05-23T13:58:11.129363: step 12615, loss 0.0619794, acc 0.984375\n",
      "2018-05-23T13:58:11.533246: step 12616, loss 0.157909, acc 0.953125\n",
      "2018-05-23T13:58:11.945152: step 12617, loss 0.179918, acc 0.921875\n",
      "2018-05-23T13:58:12.372520: step 12618, loss 0.226149, acc 0.90625\n",
      "2018-05-23T13:58:12.863325: step 12619, loss 0.110695, acc 0.953125\n",
      "2018-05-23T13:58:13.293174: step 12620, loss 0.184581, acc 0.9375\n",
      "2018-05-23T13:58:13.741974: step 12621, loss 0.183297, acc 0.9375\n",
      "2018-05-23T13:58:14.140417: step 12622, loss 0.170172, acc 0.9375\n",
      "2018-05-23T13:58:14.530903: step 12623, loss 0.148277, acc 0.953125\n",
      "2018-05-23T13:58:14.978241: step 12624, loss 0.187718, acc 0.890625\n",
      "2018-05-23T13:58:15.372185: step 12625, loss 0.171458, acc 0.921875\n",
      "2018-05-23T13:58:15.879337: step 12626, loss 0.204417, acc 0.96875\n",
      "2018-05-23T13:58:16.393960: step 12627, loss 0.211842, acc 0.90625\n",
      "2018-05-23T13:58:16.927534: step 12628, loss 0.163112, acc 0.96875\n",
      "2018-05-23T13:58:17.460109: step 12629, loss 0.110639, acc 0.953125\n",
      "2018-05-23T13:58:18.020253: step 12630, loss 0.227607, acc 0.953125\n",
      "2018-05-23T13:58:18.582747: step 12631, loss 0.160073, acc 0.921875\n",
      "2018-05-23T13:58:19.021089: step 12632, loss 0.138871, acc 0.96875\n",
      "2018-05-23T13:58:19.420023: step 12633, loss 0.282806, acc 0.90625\n",
      "2018-05-23T13:58:19.805308: step 12634, loss 0.116709, acc 0.9375\n",
      "2018-05-23T13:58:20.193331: step 12635, loss 0.164739, acc 0.921875\n",
      "2018-05-23T13:58:20.638143: step 12636, loss 0.0720656, acc 0.953125\n",
      "2018-05-23T13:58:21.101904: step 12637, loss 0.0691727, acc 1\n",
      "2018-05-23T13:58:21.490862: step 12638, loss 0.116661, acc 0.96875\n",
      "2018-05-23T13:58:21.879882: step 12639, loss 0.146127, acc 0.953125\n",
      "2018-05-23T13:58:22.268840: step 12640, loss 0.216782, acc 0.859375\n",
      "2018-05-23T13:58:22.665791: step 12641, loss 0.0671391, acc 0.953125\n",
      "2018-05-23T13:58:23.078688: step 12642, loss 0.255755, acc 0.890625\n",
      "2018-05-23T13:58:23.507051: step 12643, loss 0.10751, acc 0.96875\n",
      "2018-05-23T13:58:23.899510: step 12644, loss 0.139275, acc 0.921875\n",
      "2018-05-23T13:58:24.287476: step 12645, loss 0.204287, acc 0.921875\n",
      "2018-05-23T13:58:24.681931: step 12646, loss 0.136329, acc 0.9375\n",
      "2018-05-23T13:58:25.063928: step 12647, loss 0.0781305, acc 0.9375\n",
      "2018-05-23T13:58:25.477334: step 12648, loss 0.1399, acc 0.9375\n",
      "2018-05-23T13:58:25.868288: step 12649, loss 0.122101, acc 0.96875\n",
      "2018-05-23T13:58:26.291356: step 12650, loss 0.13812, acc 0.9375\n",
      "2018-05-23T13:58:26.684324: step 12651, loss 0.181183, acc 0.875\n",
      "2018-05-23T13:58:27.074297: step 12652, loss 0.121377, acc 0.921875\n",
      "2018-05-23T13:58:27.458294: step 12653, loss 0.111131, acc 0.953125\n",
      "2018-05-23T13:58:27.845274: step 12654, loss 0.158234, acc 0.953125\n",
      "2018-05-23T13:58:28.220203: step 12655, loss 0.124924, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:58:28.695949: step 12656, loss 0.0697115, acc 0.984375\n",
      "2018-05-23T13:58:29.087344: step 12657, loss 0.150039, acc 0.96875\n",
      "2018-05-23T13:58:29.486296: step 12658, loss 0.111507, acc 0.953125\n",
      "2018-05-23T13:58:29.877253: step 12659, loss 0.205072, acc 0.953125\n",
      "2018-05-23T13:58:30.262224: step 12660, loss 0.151631, acc 0.9375\n",
      "2018-05-23T13:58:30.690078: step 12661, loss 0.0960434, acc 0.9375\n",
      "2018-05-23T13:58:31.079398: step 12662, loss 0.148399, acc 0.921875\n",
      "2018-05-23T13:58:31.509267: step 12663, loss 0.13055, acc 0.953125\n",
      "2018-05-23T13:58:31.882271: step 12664, loss 0.0954946, acc 0.96875\n",
      "2018-05-23T13:58:32.281321: step 12665, loss 0.257026, acc 0.890625\n",
      "2018-05-23T13:58:32.743011: step 12666, loss 0.118159, acc 0.96875\n",
      "2018-05-23T13:58:33.134630: step 12667, loss 0.121868, acc 0.9375\n",
      "2018-05-23T13:58:33.516990: step 12668, loss 0.0700787, acc 1\n",
      "2018-05-23T13:58:33.909940: step 12669, loss 0.230586, acc 0.9375\n",
      "2018-05-23T13:58:34.303412: step 12670, loss 0.246637, acc 0.921875\n",
      "2018-05-23T13:58:34.745754: step 12671, loss 0.107856, acc 0.9375\n",
      "2018-05-23T13:58:35.115277: step 12672, loss 0.170463, acc 0.90625\n",
      "2018-05-23T13:58:35.494264: step 12673, loss 0.243091, acc 0.90625\n",
      "2018-05-23T13:58:35.940061: step 12674, loss 0.197492, acc 0.875\n",
      "2018-05-23T13:58:36.384314: step 12675, loss 0.167122, acc 0.9375\n",
      "2018-05-23T13:58:36.857981: step 12676, loss 0.250486, acc 0.9375\n",
      "2018-05-23T13:58:37.250276: step 12677, loss 0.139308, acc 0.96875\n",
      "2018-05-23T13:58:37.661427: step 12678, loss 0.0798115, acc 0.953125\n",
      "2018-05-23T13:58:38.047924: step 12679, loss 0.136187, acc 0.953125\n",
      "2018-05-23T13:58:38.450355: step 12680, loss 0.184431, acc 0.9375\n",
      "2018-05-23T13:58:38.887186: step 12681, loss 0.156981, acc 0.921875\n",
      "2018-05-23T13:58:39.277985: step 12682, loss 0.114514, acc 0.921875\n",
      "2018-05-23T13:58:39.667460: step 12683, loss 0.106691, acc 0.96875\n",
      "2018-05-23T13:58:40.052370: step 12684, loss 0.149782, acc 0.90625\n",
      "2018-05-23T13:58:40.465155: step 12685, loss 0.260187, acc 0.953125\n",
      "2018-05-23T13:58:41.038083: step 12686, loss 0.0997124, acc 0.953125\n",
      "2018-05-23T13:58:41.548229: step 12687, loss 0.219312, acc 0.90625\n",
      "2018-05-23T13:58:42.111231: step 12688, loss 0.13057, acc 0.953125\n",
      "2018-05-23T13:58:42.636826: step 12689, loss 0.111461, acc 0.953125\n",
      "2018-05-23T13:58:43.146478: step 12690, loss 0.0999237, acc 0.96875\n",
      "2018-05-23T13:58:43.691541: step 12691, loss 0.0966244, acc 0.984375\n",
      "2018-05-23T13:58:44.179260: step 12692, loss 0.232463, acc 0.921875\n",
      "2018-05-23T13:58:44.615108: step 12693, loss 0.117332, acc 0.953125\n",
      "2018-05-23T13:58:45.093828: step 12694, loss 0.124976, acc 0.953125\n",
      "2018-05-23T13:58:45.525033: step 12695, loss 0.16401, acc 0.9375\n",
      "2018-05-23T13:58:45.965872: step 12696, loss 0.211329, acc 0.90625\n",
      "2018-05-23T13:58:46.416685: step 12697, loss 0.183162, acc 0.921875\n",
      "2018-05-23T13:58:46.819606: step 12698, loss 0.065631, acc 0.984375\n",
      "2018-05-23T13:58:47.251455: step 12699, loss 0.352967, acc 0.796875\n",
      "2018-05-23T13:58:47.693000: step 12700, loss 0.168445, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:58:53.089900: step 12700, loss 1.26828, acc 0.719817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12700\n",
      "\n",
      "2018-05-23T13:58:54.828941: step 12701, loss 0.202598, acc 0.921875\n",
      "2018-05-23T13:58:55.256793: step 12702, loss 0.0770666, acc 0.96875\n",
      "2018-05-23T13:58:55.650759: step 12703, loss 0.274486, acc 0.890625\n",
      "2018-05-23T13:58:56.048698: step 12704, loss 0.193815, acc 0.921875\n",
      "2018-05-23T13:58:56.445962: step 12705, loss 0.0983276, acc 0.96875\n",
      "2018-05-23T13:58:56.884815: step 12706, loss 0.25812, acc 0.9375\n",
      "2018-05-23T13:58:57.284744: step 12707, loss 0.124143, acc 0.953125\n",
      "2018-05-23T13:58:57.673702: step 12708, loss 0.0609634, acc 0.984375\n",
      "2018-05-23T13:58:58.107543: step 12709, loss 0.114773, acc 0.921875\n",
      "2018-05-23T13:58:58.497009: step 12710, loss 0.192567, acc 0.921875\n",
      "2018-05-23T13:58:58.880004: step 12711, loss 0.304101, acc 0.859375\n",
      "2018-05-23T13:58:59.265995: step 12712, loss 0.17849, acc 0.96875\n",
      "2018-05-23T13:58:59.669921: step 12713, loss 0.125488, acc 0.9375\n",
      "2018-05-23T13:59:00.143659: step 12714, loss 0.075713, acc 1\n",
      "2018-05-23T13:59:00.551254: step 12715, loss 0.185595, acc 0.953125\n",
      "2018-05-23T13:59:00.983100: step 12716, loss 0.101167, acc 0.96875\n",
      "2018-05-23T13:59:01.445862: step 12717, loss 0.150269, acc 0.90625\n",
      "2018-05-23T13:59:01.924580: step 12718, loss 0.139645, acc 0.921875\n",
      "2018-05-23T13:59:02.326360: step 12719, loss 0.241745, acc 0.921875\n",
      "2018-05-23T13:59:02.715338: step 12720, loss 0.160794, acc 0.90625\n",
      "2018-05-23T13:59:03.123247: step 12721, loss 0.165669, acc 0.921875\n",
      "2018-05-23T13:59:03.517811: step 12722, loss 0.0675069, acc 0.96875\n",
      "2018-05-23T13:59:03.902202: step 12723, loss 0.133695, acc 0.953125\n",
      "2018-05-23T13:59:04.347012: step 12724, loss 0.11464, acc 0.9375\n",
      "2018-05-23T13:59:04.735481: step 12725, loss 0.140876, acc 0.9375\n",
      "2018-05-23T13:59:05.138912: step 12726, loss 0.0807496, acc 0.96875\n",
      "2018-05-23T13:59:05.534366: step 12727, loss 0.139103, acc 0.921875\n",
      "2018-05-23T13:59:05.950276: step 12728, loss 0.155411, acc 0.9375\n",
      "2018-05-23T13:59:06.373147: step 12729, loss 0.126487, acc 0.953125\n",
      "2018-05-23T13:59:06.799007: step 12730, loss 0.296248, acc 0.890625\n",
      "2018-05-23T13:59:07.223902: step 12731, loss 0.157194, acc 0.875\n",
      "2018-05-23T13:59:07.681680: step 12732, loss 0.100676, acc 0.96875\n",
      "2018-05-23T13:59:08.110549: step 12733, loss 0.216902, acc 0.890625\n",
      "2018-05-23T13:59:08.537955: step 12734, loss 0.151681, acc 0.921875\n",
      "2018-05-23T13:59:08.928897: step 12735, loss 0.26603, acc 0.90625\n",
      "2018-05-23T13:59:09.337297: step 12736, loss 0.173405, acc 0.953125\n",
      "2018-05-23T13:59:09.750198: step 12737, loss 0.188902, acc 0.921875\n",
      "2018-05-23T13:59:10.130201: step 12738, loss 0.209381, acc 0.90625\n",
      "2018-05-23T13:59:10.524151: step 12739, loss 0.089291, acc 0.984375\n",
      "2018-05-23T13:59:10.909121: step 12740, loss 0.0949601, acc 0.9375\n",
      "2018-05-23T13:59:11.297082: step 12741, loss 0.0813153, acc 0.96875\n",
      "2018-05-23T13:59:11.677961: step 12742, loss 0.19243, acc 0.921875\n",
      "2018-05-23T13:59:12.064926: step 12743, loss 0.144089, acc 0.953125\n",
      "2018-05-23T13:59:12.451329: step 12744, loss 0.173142, acc 0.921875\n",
      "2018-05-23T13:59:12.866755: step 12745, loss 0.097469, acc 0.953125\n",
      "2018-05-23T13:59:13.258713: step 12746, loss 0.124354, acc 0.9375\n",
      "2018-05-23T13:59:13.660637: step 12747, loss 0.137533, acc 0.953125\n",
      "2018-05-23T13:59:14.063385: step 12748, loss 0.223027, acc 0.875\n",
      "2018-05-23T13:59:14.446380: step 12749, loss 0.154111, acc 0.9375\n",
      "2018-05-23T13:59:14.918306: step 12750, loss 0.105093, acc 0.953125\n",
      "2018-05-23T13:59:15.339180: step 12751, loss 0.22592, acc 0.921875\n",
      "2018-05-23T13:59:15.828874: step 12752, loss 0.173458, acc 0.921875\n",
      "2018-05-23T13:59:16.302608: step 12753, loss 0.169784, acc 0.953125\n",
      "2018-05-23T13:59:16.713203: step 12754, loss 0.230858, acc 0.921875\n",
      "2018-05-23T13:59:17.148061: step 12755, loss 0.0734427, acc 0.984375\n",
      "2018-05-23T13:59:17.677651: step 12756, loss 0.137329, acc 0.921875\n",
      "2018-05-23T13:59:18.363394: step 12757, loss 0.123285, acc 0.953125\n",
      "2018-05-23T13:59:19.137839: step 12758, loss 0.195006, acc 0.921875\n",
      "2018-05-23T13:59:19.924731: step 12759, loss 0.205136, acc 0.890625\n",
      "2018-05-23T13:59:20.643807: step 12760, loss 0.12187, acc 0.953125\n",
      "2018-05-23T13:59:21.340942: step 12761, loss 0.138304, acc 0.921875\n",
      "2018-05-23T13:59:22.009155: step 12762, loss 0.215779, acc 0.921875\n",
      "2018-05-23T13:59:22.668470: step 12763, loss 0.0669746, acc 0.96875\n",
      "2018-05-23T13:59:23.261837: step 12764, loss 0.153032, acc 0.953125\n",
      "2018-05-23T13:59:23.809972: step 12765, loss 0.107352, acc 0.96875\n",
      "2018-05-23T13:59:24.430152: step 12766, loss 0.142352, acc 0.9375\n",
      "2018-05-23T13:59:25.007611: step 12767, loss 0.152468, acc 0.953125\n",
      "2018-05-23T13:59:25.585572: step 12768, loss 0.0776579, acc 0.984375\n",
      "2018-05-23T13:59:26.096350: step 12769, loss 0.0865928, acc 0.96875\n",
      "2018-05-23T13:59:26.660834: step 12770, loss 0.186557, acc 0.890625\n",
      "2018-05-23T13:59:27.205738: step 12771, loss 0.137329, acc 0.9375\n",
      "2018-05-23T13:59:27.600210: step 12772, loss 0.310066, acc 0.890625\n",
      "2018-05-23T13:59:27.986196: step 12773, loss 0.199075, acc 0.90625\n",
      "2018-05-23T13:59:28.445965: step 12774, loss 0.376755, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T13:59:28.824224: step 12775, loss 0.145986, acc 0.9375\n",
      "2018-05-23T13:59:29.216194: step 12776, loss 0.246468, acc 0.890625\n",
      "2018-05-23T13:59:29.604221: step 12777, loss 0.104802, acc 0.953125\n",
      "2018-05-23T13:59:29.985219: step 12778, loss 0.119867, acc 0.96875\n",
      "2018-05-23T13:59:30.388163: step 12779, loss 0.183658, acc 0.921875\n",
      "2018-05-23T13:59:30.770167: step 12780, loss 0.117073, acc 0.953125\n",
      "2018-05-23T13:59:31.150170: step 12781, loss 0.0805469, acc 1\n",
      "2018-05-23T13:59:31.590016: step 12782, loss 0.112927, acc 0.953125\n",
      "2018-05-23T13:59:32.090104: step 12783, loss 0.140809, acc 0.921875\n",
      "2018-05-23T13:59:32.574830: step 12784, loss 0.143108, acc 0.9375\n",
      "2018-05-23T13:59:32.997710: step 12785, loss 0.0973028, acc 0.96875\n",
      "2018-05-23T13:59:33.399155: step 12786, loss 0.135476, acc 0.9375\n",
      "2018-05-23T13:59:33.909529: step 12787, loss 0.172974, acc 0.90625\n",
      "2018-05-23T13:59:34.560219: step 12788, loss 0.177557, acc 0.953125\n",
      "2018-05-23T13:59:35.198511: step 12789, loss 0.0946622, acc 0.953125\n",
      "2018-05-23T13:59:36.065193: step 12790, loss 0.0875361, acc 0.96875\n",
      "2018-05-23T13:59:36.678552: step 12791, loss 0.0905565, acc 0.953125\n",
      "2018-05-23T13:59:37.516312: step 12792, loss 0.185262, acc 0.953125\n",
      "2018-05-23T13:59:38.354076: step 12793, loss 0.14826, acc 0.9375\n",
      "2018-05-23T13:59:38.982459: step 12794, loss 0.139062, acc 0.921875\n",
      "2018-05-23T13:59:39.441753: step 12795, loss 0.151936, acc 0.96875\n",
      "2018-05-23T13:59:40.014220: step 12796, loss 0.13665, acc 0.9375\n",
      "2018-05-23T13:59:40.484467: step 12797, loss 0.105137, acc 0.96875\n",
      "2018-05-23T13:59:41.003179: step 12798, loss 0.143981, acc 0.9375\n",
      "2018-05-23T13:59:41.498188: step 12799, loss 0.0721951, acc 0.984375\n",
      "2018-05-23T13:59:41.951523: step 12800, loss 0.199123, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T13:59:47.787454: step 12800, loss 1.26934, acc 0.713673\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12800\n",
      "\n",
      "2018-05-23T13:59:49.603182: step 12801, loss 0.382677, acc 0.859375\n",
      "2018-05-23T13:59:50.182744: step 12802, loss 0.164827, acc 0.921875\n",
      "2018-05-23T13:59:50.628155: step 12803, loss 0.119747, acc 0.953125\n",
      "2018-05-23T13:59:51.053719: step 12804, loss 0.110696, acc 0.96875\n",
      "2018-05-23T13:59:51.455244: step 12805, loss 0.154327, acc 0.941176\n",
      "2018-05-23T13:59:51.922561: step 12806, loss 0.107886, acc 0.96875\n",
      "2018-05-23T13:59:52.564550: step 12807, loss 0.0599729, acc 0.984375\n",
      "2018-05-23T13:59:53.270483: step 12808, loss 0.111535, acc 0.9375\n",
      "2018-05-23T13:59:53.772701: step 12809, loss 0.0752241, acc 0.984375\n",
      "2018-05-23T13:59:54.207472: step 12810, loss 0.189903, acc 0.90625\n",
      "2018-05-23T13:59:54.823605: step 12811, loss 0.0710415, acc 0.984375\n",
      "2018-05-23T13:59:55.336199: step 12812, loss 0.106765, acc 0.96875\n",
      "2018-05-23T13:59:55.930123: step 12813, loss 0.267781, acc 0.875\n",
      "2018-05-23T13:59:56.432781: step 12814, loss 0.0700625, acc 0.953125\n",
      "2018-05-23T13:59:56.894558: step 12815, loss 0.141081, acc 0.9375\n",
      "2018-05-23T13:59:57.352489: step 12816, loss 0.110026, acc 0.9375\n",
      "2018-05-23T13:59:57.767380: step 12817, loss 0.227527, acc 0.875\n",
      "2018-05-23T13:59:58.223691: step 12818, loss 0.293569, acc 0.890625\n",
      "2018-05-23T13:59:58.824637: step 12819, loss 0.0867752, acc 0.953125\n",
      "2018-05-23T13:59:59.286400: step 12820, loss 0.16296, acc 0.921875\n",
      "2018-05-23T13:59:59.764363: step 12821, loss 0.131499, acc 0.9375\n",
      "2018-05-23T14:00:00.180251: step 12822, loss 0.200964, acc 0.921875\n",
      "2018-05-23T14:00:00.587610: step 12823, loss 0.0806172, acc 0.984375\n",
      "2018-05-23T14:00:01.003653: step 12824, loss 0.022935, acc 1\n",
      "2018-05-23T14:00:01.464422: step 12825, loss 0.0565885, acc 1\n",
      "2018-05-23T14:00:01.884298: step 12826, loss 0.107268, acc 0.9375\n",
      "2018-05-23T14:00:02.286730: step 12827, loss 0.101432, acc 0.96875\n",
      "2018-05-23T14:00:02.720084: step 12828, loss 0.224391, acc 0.953125\n",
      "2018-05-23T14:00:03.139958: step 12829, loss 0.119034, acc 0.96875\n",
      "2018-05-23T14:00:03.535916: step 12830, loss 0.118012, acc 0.9375\n",
      "2018-05-23T14:00:03.979258: step 12831, loss 0.0681739, acc 0.984375\n",
      "2018-05-23T14:00:04.373248: step 12832, loss 0.059471, acc 0.984375\n",
      "2018-05-23T14:00:04.779097: step 12833, loss 0.0974038, acc 0.96875\n",
      "2018-05-23T14:00:05.181449: step 12834, loss 0.0932276, acc 0.953125\n",
      "2018-05-23T14:00:05.582345: step 12835, loss 0.221815, acc 0.921875\n",
      "2018-05-23T14:00:06.082044: step 12836, loss 0.115733, acc 0.96875\n",
      "2018-05-23T14:00:06.523861: step 12837, loss 0.11034, acc 0.9375\n",
      "2018-05-23T14:00:07.029515: step 12838, loss 0.121918, acc 0.9375\n",
      "2018-05-23T14:00:07.448838: step 12839, loss 0.156853, acc 0.9375\n",
      "2018-05-23T14:00:07.860258: step 12840, loss 0.214841, acc 0.921875\n",
      "2018-05-23T14:00:08.349965: step 12841, loss 0.110878, acc 0.953125\n",
      "2018-05-23T14:00:08.772344: step 12842, loss 0.0772837, acc 0.984375\n",
      "2018-05-23T14:00:09.195214: step 12843, loss 0.125429, acc 0.953125\n",
      "2018-05-23T14:00:09.616096: step 12844, loss 0.11192, acc 0.953125\n",
      "2018-05-23T14:00:10.033660: step 12845, loss 0.23647, acc 0.9375\n",
      "2018-05-23T14:00:10.456959: step 12846, loss 0.0602526, acc 0.96875\n",
      "2018-05-23T14:00:11.004007: step 12847, loss 0.0496553, acc 1\n",
      "2018-05-23T14:00:11.565522: step 12848, loss 0.133299, acc 0.9375\n",
      "2018-05-23T14:00:12.105078: step 12849, loss 0.187607, acc 0.921875\n",
      "2018-05-23T14:00:12.546019: step 12850, loss 0.128445, acc 0.9375\n",
      "2018-05-23T14:00:13.022281: step 12851, loss 0.182868, acc 0.9375\n",
      "2018-05-23T14:00:13.537932: step 12852, loss 0.143184, acc 0.953125\n",
      "2018-05-23T14:00:13.945847: step 12853, loss 0.127033, acc 0.921875\n",
      "2018-05-23T14:00:14.814034: step 12854, loss 0.0860054, acc 0.9375\n",
      "2018-05-23T14:00:15.880798: step 12855, loss 0.121972, acc 0.921875\n",
      "2018-05-23T14:00:16.351538: step 12856, loss 0.0988012, acc 0.96875\n",
      "2018-05-23T14:00:16.854700: step 12857, loss 0.0886011, acc 0.953125\n",
      "2018-05-23T14:00:17.343394: step 12858, loss 0.0928286, acc 0.953125\n",
      "2018-05-23T14:00:17.818635: step 12859, loss 0.0844087, acc 0.9375\n",
      "2018-05-23T14:00:18.253978: step 12860, loss 0.0947475, acc 0.96875\n",
      "2018-05-23T14:00:19.034171: step 12861, loss 0.190817, acc 0.890625\n",
      "2018-05-23T14:00:19.677449: step 12862, loss 0.106922, acc 0.953125\n",
      "2018-05-23T14:00:20.151206: step 12863, loss 0.105082, acc 0.96875\n",
      "2018-05-23T14:00:20.612514: step 12864, loss 0.0530516, acc 0.984375\n",
      "2018-05-23T14:00:21.034918: step 12865, loss 0.09329, acc 0.984375\n",
      "2018-05-23T14:00:21.837832: step 12866, loss 0.148146, acc 0.890625\n",
      "2018-05-23T14:00:22.501057: step 12867, loss 0.126212, acc 0.96875\n",
      "2018-05-23T14:00:23.086490: step 12868, loss 0.161849, acc 0.90625\n",
      "2018-05-23T14:00:23.920790: step 12869, loss 0.0888195, acc 0.96875\n",
      "2018-05-23T14:00:24.569057: step 12870, loss 0.0827798, acc 0.953125\n",
      "2018-05-23T14:00:25.039796: step 12871, loss 0.112263, acc 0.984375\n",
      "2018-05-23T14:00:25.522510: step 12872, loss 0.108568, acc 0.953125\n",
      "2018-05-23T14:00:26.115435: step 12873, loss 0.215563, acc 0.953125\n",
      "2018-05-23T14:00:26.824535: step 12874, loss 0.104723, acc 0.953125\n",
      "2018-05-23T14:00:27.343150: step 12875, loss 0.0645694, acc 0.96875\n",
      "2018-05-23T14:00:27.936176: step 12876, loss 0.132184, acc 0.9375\n",
      "2018-05-23T14:00:28.406961: step 12877, loss 0.162281, acc 0.90625\n",
      "2018-05-23T14:00:29.558901: step 12878, loss 0.0937502, acc 0.96875\n",
      "2018-05-23T14:00:30.300426: step 12879, loss 0.0991786, acc 0.9375\n",
      "2018-05-23T14:00:30.855940: step 12880, loss 0.0763175, acc 0.96875\n",
      "2018-05-23T14:00:31.549150: step 12881, loss 0.0972973, acc 0.953125\n",
      "2018-05-23T14:00:32.173015: step 12882, loss 0.0919173, acc 0.96875\n",
      "2018-05-23T14:00:32.830258: step 12883, loss 0.0650726, acc 0.96875\n",
      "2018-05-23T14:00:33.356018: step 12884, loss 0.114487, acc 0.9375\n",
      "2018-05-23T14:00:33.842226: step 12885, loss 0.100331, acc 0.953125\n",
      "2018-05-23T14:00:34.483030: step 12886, loss 0.0944546, acc 0.96875\n",
      "2018-05-23T14:00:34.933850: step 12887, loss 0.0325745, acc 1\n",
      "2018-05-23T14:00:35.353729: step 12888, loss 0.155035, acc 0.953125\n",
      "2018-05-23T14:00:35.798542: step 12889, loss 0.177339, acc 0.9375\n",
      "2018-05-23T14:00:36.279189: step 12890, loss 0.0434842, acc 0.984375\n",
      "2018-05-23T14:00:36.680136: step 12891, loss 0.101146, acc 0.953125\n",
      "2018-05-23T14:00:37.170822: step 12892, loss 0.0607006, acc 0.96875\n",
      "2018-05-23T14:00:37.801030: step 12893, loss 0.126262, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:00:38.462335: step 12894, loss 0.255276, acc 0.90625\n",
      "2018-05-23T14:00:39.151715: step 12895, loss 0.0741406, acc 0.953125\n",
      "2018-05-23T14:00:40.493621: step 12896, loss 0.170385, acc 0.9375\n",
      "2018-05-23T14:00:41.676887: step 12897, loss 0.0658911, acc 0.984375\n",
      "2018-05-23T14:00:42.341838: step 12898, loss 0.161522, acc 0.921875\n",
      "2018-05-23T14:00:42.937979: step 12899, loss 0.109416, acc 0.953125\n",
      "2018-05-23T14:00:43.479786: step 12900, loss 0.111969, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:00:49.854415: step 12900, loss 1.31176, acc 0.714959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-12900\n",
      "\n",
      "2018-05-23T14:00:52.150595: step 12901, loss 0.0773288, acc 0.9375\n",
      "2018-05-23T14:00:52.655329: step 12902, loss 0.0643405, acc 0.96875\n",
      "2018-05-23T14:00:53.177167: step 12903, loss 0.0576129, acc 0.984375\n",
      "2018-05-23T14:00:53.756851: step 12904, loss 0.151344, acc 0.9375\n",
      "2018-05-23T14:00:54.306236: step 12905, loss 0.0808969, acc 0.96875\n",
      "2018-05-23T14:00:54.805889: step 12906, loss 0.101029, acc 0.96875\n",
      "2018-05-23T14:00:55.245448: step 12907, loss 0.122276, acc 0.953125\n",
      "2018-05-23T14:00:55.681536: step 12908, loss 0.0811775, acc 0.96875\n",
      "2018-05-23T14:00:56.153518: step 12909, loss 0.039397, acc 0.984375\n",
      "2018-05-23T14:00:56.599718: step 12910, loss 0.0892439, acc 0.96875\n",
      "2018-05-23T14:00:57.074926: step 12911, loss 0.0784504, acc 0.96875\n",
      "2018-05-23T14:00:57.552088: step 12912, loss 0.11782, acc 0.96875\n",
      "2018-05-23T14:00:58.028147: step 12913, loss 0.0816602, acc 0.984375\n",
      "2018-05-23T14:00:58.506705: step 12914, loss 0.0791752, acc 0.96875\n",
      "2018-05-23T14:00:58.963260: step 12915, loss 0.0654608, acc 0.984375\n",
      "2018-05-23T14:00:59.426661: step 12916, loss 0.0404581, acc 0.984375\n",
      "2018-05-23T14:00:59.901568: step 12917, loss 0.181803, acc 0.90625\n",
      "2018-05-23T14:01:00.408775: step 12918, loss 0.145127, acc 0.921875\n",
      "2018-05-23T14:01:00.897809: step 12919, loss 0.0581382, acc 0.984375\n",
      "2018-05-23T14:01:01.366366: step 12920, loss 0.172655, acc 0.90625\n",
      "2018-05-23T14:01:01.816812: step 12921, loss 0.0826414, acc 0.953125\n",
      "2018-05-23T14:01:02.270172: step 12922, loss 0.231141, acc 0.9375\n",
      "2018-05-23T14:01:02.722836: step 12923, loss 0.210382, acc 0.90625\n",
      "2018-05-23T14:01:03.169151: step 12924, loss 0.0990882, acc 0.96875\n",
      "2018-05-23T14:01:03.619920: step 12925, loss 0.131892, acc 0.921875\n",
      "2018-05-23T14:01:04.088714: step 12926, loss 0.217015, acc 0.921875\n",
      "2018-05-23T14:01:04.588889: step 12927, loss 0.225126, acc 0.90625\n",
      "2018-05-23T14:01:05.069928: step 12928, loss 0.126438, acc 0.9375\n",
      "2018-05-23T14:01:05.559140: step 12929, loss 0.104629, acc 0.953125\n",
      "2018-05-23T14:01:06.024604: step 12930, loss 0.170847, acc 0.9375\n",
      "2018-05-23T14:01:06.475168: step 12931, loss 0.108147, acc 0.953125\n",
      "2018-05-23T14:01:06.909929: step 12932, loss 0.272167, acc 0.90625\n",
      "2018-05-23T14:01:07.367599: step 12933, loss 0.130572, acc 0.9375\n",
      "2018-05-23T14:01:07.833146: step 12934, loss 0.106378, acc 0.9375\n",
      "2018-05-23T14:01:08.285858: step 12935, loss 0.0459673, acc 0.984375\n",
      "2018-05-23T14:01:08.754246: step 12936, loss 0.0943065, acc 0.96875\n",
      "2018-05-23T14:01:09.197016: step 12937, loss 0.0563595, acc 0.984375\n",
      "2018-05-23T14:01:09.646005: step 12938, loss 0.129538, acc 0.96875\n",
      "2018-05-23T14:01:10.093531: step 12939, loss 0.25367, acc 0.90625\n",
      "2018-05-23T14:01:10.555486: step 12940, loss 0.0513606, acc 0.984375\n",
      "2018-05-23T14:01:10.991823: step 12941, loss 0.150793, acc 0.9375\n",
      "2018-05-23T14:01:11.428922: step 12942, loss 0.0918753, acc 0.96875\n",
      "2018-05-23T14:01:11.895047: step 12943, loss 0.138204, acc 0.953125\n",
      "2018-05-23T14:01:12.356690: step 12944, loss 0.125718, acc 0.9375\n",
      "2018-05-23T14:01:12.803422: step 12945, loss 0.0848905, acc 0.984375\n",
      "2018-05-23T14:01:13.258040: step 12946, loss 0.0750322, acc 0.984375\n",
      "2018-05-23T14:01:13.719456: step 12947, loss 0.128624, acc 0.96875\n",
      "2018-05-23T14:01:14.172228: step 12948, loss 0.087953, acc 0.984375\n",
      "2018-05-23T14:01:14.621187: step 12949, loss 0.10064, acc 0.96875\n",
      "2018-05-23T14:01:15.067486: step 12950, loss 0.123211, acc 0.953125\n",
      "2018-05-23T14:01:15.522878: step 12951, loss 0.0749719, acc 0.953125\n",
      "2018-05-23T14:01:15.979566: step 12952, loss 0.107874, acc 0.9375\n",
      "2018-05-23T14:01:16.428487: step 12953, loss 0.254497, acc 0.9375\n",
      "2018-05-23T14:01:16.900527: step 12954, loss 0.184868, acc 0.90625\n",
      "2018-05-23T14:01:17.365864: step 12955, loss 0.0356249, acc 1\n",
      "2018-05-23T14:01:17.828030: step 12956, loss 0.100516, acc 0.953125\n",
      "2018-05-23T14:01:18.289712: step 12957, loss 0.0668421, acc 0.96875\n",
      "2018-05-23T14:01:18.758912: step 12958, loss 0.127533, acc 0.953125\n",
      "2018-05-23T14:01:19.210341: step 12959, loss 0.0321558, acc 1\n",
      "2018-05-23T14:01:19.656992: step 12960, loss 0.108797, acc 0.90625\n",
      "2018-05-23T14:01:20.080373: step 12961, loss 0.207747, acc 0.90625\n",
      "2018-05-23T14:01:20.538384: step 12962, loss 0.0617732, acc 0.96875\n",
      "2018-05-23T14:01:20.989395: step 12963, loss 0.0716682, acc 1\n",
      "2018-05-23T14:01:21.432553: step 12964, loss 0.111423, acc 0.953125\n",
      "2018-05-23T14:01:21.880909: step 12965, loss 0.275596, acc 0.859375\n",
      "2018-05-23T14:01:22.338809: step 12966, loss 0.111727, acc 0.953125\n",
      "2018-05-23T14:01:22.771392: step 12967, loss 0.0839908, acc 0.96875\n",
      "2018-05-23T14:01:23.205808: step 12968, loss 0.0726439, acc 0.984375\n",
      "2018-05-23T14:01:23.632247: step 12969, loss 0.0718196, acc 0.96875\n",
      "2018-05-23T14:01:24.076074: step 12970, loss 0.086043, acc 0.953125\n",
      "2018-05-23T14:01:24.491951: step 12971, loss 0.133883, acc 0.953125\n",
      "2018-05-23T14:01:24.961712: step 12972, loss 0.160651, acc 0.96875\n",
      "2018-05-23T14:01:25.420116: step 12973, loss 0.116355, acc 0.9375\n",
      "2018-05-23T14:01:25.902439: step 12974, loss 0.154508, acc 0.9375\n",
      "2018-05-23T14:01:26.356625: step 12975, loss 0.0812262, acc 0.96875\n",
      "2018-05-23T14:01:26.800301: step 12976, loss 0.0992792, acc 0.96875\n",
      "2018-05-23T14:01:27.270069: step 12977, loss 0.0823695, acc 0.984375\n",
      "2018-05-23T14:01:27.731691: step 12978, loss 0.0923989, acc 0.96875\n",
      "2018-05-23T14:01:28.155676: step 12979, loss 0.0752239, acc 0.953125\n",
      "2018-05-23T14:01:28.603692: step 12980, loss 0.114206, acc 0.96875\n",
      "2018-05-23T14:01:29.038789: step 12981, loss 0.0331403, acc 1\n",
      "2018-05-23T14:01:29.491239: step 12982, loss 0.149297, acc 0.9375\n",
      "2018-05-23T14:01:29.924217: step 12983, loss 0.111698, acc 0.96875\n",
      "2018-05-23T14:01:30.373424: step 12984, loss 0.168902, acc 0.953125\n",
      "2018-05-23T14:01:30.804726: step 12985, loss 0.087606, acc 0.96875\n",
      "2018-05-23T14:01:31.219620: step 12986, loss 0.0636868, acc 0.953125\n",
      "2018-05-23T14:01:31.664124: step 12987, loss 0.212811, acc 0.890625\n",
      "2018-05-23T14:01:32.097622: step 12988, loss 0.102307, acc 0.96875\n",
      "2018-05-23T14:01:32.547828: step 12989, loss 0.200303, acc 0.9375\n",
      "2018-05-23T14:01:32.985724: step 12990, loss 0.0623705, acc 0.984375\n",
      "2018-05-23T14:01:33.423453: step 12991, loss 0.130786, acc 0.96875\n",
      "2018-05-23T14:01:33.864510: step 12992, loss 0.101663, acc 0.953125\n",
      "2018-05-23T14:01:34.319330: step 12993, loss 0.0497785, acc 0.96875\n",
      "2018-05-23T14:01:34.769210: step 12994, loss 0.092861, acc 0.96875\n",
      "2018-05-23T14:01:35.211036: step 12995, loss 0.135946, acc 0.921875\n",
      "2018-05-23T14:01:35.659105: step 12996, loss 0.0661949, acc 0.984375\n",
      "2018-05-23T14:01:36.111318: step 12997, loss 0.0745355, acc 0.96875\n",
      "2018-05-23T14:01:36.565607: step 12998, loss 0.176228, acc 0.90625\n",
      "2018-05-23T14:01:37.056107: step 12999, loss 0.105952, acc 0.953125\n",
      "2018-05-23T14:01:37.536162: step 13000, loss 0.154997, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:01:43.050604: step 13000, loss 1.3316, acc 0.715245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13000\n",
      "\n",
      "2018-05-23T14:01:44.764570: step 13001, loss 0.146151, acc 0.921875\n",
      "2018-05-23T14:01:45.219073: step 13002, loss 0.116983, acc 0.953125\n",
      "2018-05-23T14:01:45.657821: step 13003, loss 0.0817283, acc 0.984375\n",
      "2018-05-23T14:01:46.133386: step 13004, loss 0.107006, acc 0.953125\n",
      "2018-05-23T14:01:46.566336: step 13005, loss 0.0759348, acc 0.96875\n",
      "2018-05-23T14:01:46.996553: step 13006, loss 0.0789148, acc 0.984375\n",
      "2018-05-23T14:01:47.487900: step 13007, loss 0.0688534, acc 0.984375\n",
      "2018-05-23T14:01:47.909685: step 13008, loss 0.174289, acc 0.953125\n",
      "2018-05-23T14:01:48.328400: step 13009, loss 0.0838253, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:01:48.782971: step 13010, loss 0.0811532, acc 0.953125\n",
      "2018-05-23T14:01:49.193049: step 13011, loss 0.111356, acc 0.953125\n",
      "2018-05-23T14:01:49.649112: step 13012, loss 0.131451, acc 0.953125\n",
      "2018-05-23T14:01:50.082138: step 13013, loss 0.119996, acc 0.96875\n",
      "2018-05-23T14:01:50.516514: step 13014, loss 0.0320089, acc 1\n",
      "2018-05-23T14:01:50.977844: step 13015, loss 0.114112, acc 0.953125\n",
      "2018-05-23T14:01:51.413572: step 13016, loss 0.128267, acc 0.953125\n",
      "2018-05-23T14:01:51.845583: step 13017, loss 0.055715, acc 0.984375\n",
      "2018-05-23T14:01:52.288766: step 13018, loss 0.0967238, acc 0.984375\n",
      "2018-05-23T14:01:52.741171: step 13019, loss 0.112577, acc 0.953125\n",
      "2018-05-23T14:01:53.178419: step 13020, loss 0.140276, acc 0.921875\n",
      "2018-05-23T14:01:53.670298: step 13021, loss 0.111543, acc 0.9375\n",
      "2018-05-23T14:01:54.098584: step 13022, loss 0.137296, acc 0.96875\n",
      "2018-05-23T14:01:54.550886: step 13023, loss 0.314648, acc 0.9375\n",
      "2018-05-23T14:01:54.987906: step 13024, loss 0.0369687, acc 0.984375\n",
      "2018-05-23T14:01:55.440326: step 13025, loss 0.192592, acc 0.90625\n",
      "2018-05-23T14:01:55.888915: step 13026, loss 0.103307, acc 0.9375\n",
      "2018-05-23T14:01:56.332547: step 13027, loss 0.0984264, acc 0.953125\n",
      "2018-05-23T14:01:56.775743: step 13028, loss 0.126248, acc 0.953125\n",
      "2018-05-23T14:01:57.201831: step 13029, loss 0.0498264, acc 0.96875\n",
      "2018-05-23T14:01:57.646082: step 13030, loss 0.119188, acc 0.953125\n",
      "2018-05-23T14:01:58.072615: step 13031, loss 0.118172, acc 0.984375\n",
      "2018-05-23T14:01:58.511217: step 13032, loss 0.164435, acc 0.953125\n",
      "2018-05-23T14:01:58.941663: step 13033, loss 0.137409, acc 0.9375\n",
      "2018-05-23T14:01:59.386667: step 13034, loss 0.216906, acc 0.90625\n",
      "2018-05-23T14:01:59.830980: step 13035, loss 0.0644893, acc 0.984375\n",
      "2018-05-23T14:02:00.291997: step 13036, loss 0.106161, acc 0.953125\n",
      "2018-05-23T14:02:00.740088: step 13037, loss 0.173435, acc 0.921875\n",
      "2018-05-23T14:02:01.179813: step 13038, loss 0.0504675, acc 0.984375\n",
      "2018-05-23T14:02:01.689025: step 13039, loss 0.081498, acc 0.984375\n",
      "2018-05-23T14:02:02.131365: step 13040, loss 0.0939362, acc 0.9375\n",
      "2018-05-23T14:02:02.584555: step 13041, loss 0.0912819, acc 0.953125\n",
      "2018-05-23T14:02:03.036607: step 13042, loss 0.149172, acc 0.953125\n",
      "2018-05-23T14:02:03.466688: step 13043, loss 0.102734, acc 0.9375\n",
      "2018-05-23T14:02:03.903212: step 13044, loss 0.0884886, acc 0.96875\n",
      "2018-05-23T14:02:04.337245: step 13045, loss 0.154438, acc 0.953125\n",
      "2018-05-23T14:02:04.770679: step 13046, loss 0.118092, acc 0.953125\n",
      "2018-05-23T14:02:05.226439: step 13047, loss 0.215602, acc 0.890625\n",
      "2018-05-23T14:02:05.686428: step 13048, loss 0.134864, acc 0.96875\n",
      "2018-05-23T14:02:06.145905: step 13049, loss 0.136842, acc 0.921875\n",
      "2018-05-23T14:02:06.660760: step 13050, loss 0.220597, acc 0.875\n",
      "2018-05-23T14:02:07.139828: step 13051, loss 0.119847, acc 0.953125\n",
      "2018-05-23T14:02:07.595876: step 13052, loss 0.161252, acc 0.921875\n",
      "2018-05-23T14:02:08.026332: step 13053, loss 0.137954, acc 0.9375\n",
      "2018-05-23T14:02:08.498462: step 13054, loss 0.0384346, acc 1\n",
      "2018-05-23T14:02:08.939323: step 13055, loss 0.162689, acc 0.953125\n",
      "2018-05-23T14:02:09.377334: step 13056, loss 0.168269, acc 0.890625\n",
      "2018-05-23T14:02:09.815179: step 13057, loss 0.166449, acc 0.9375\n",
      "2018-05-23T14:02:10.248543: step 13058, loss 0.133845, acc 0.9375\n",
      "2018-05-23T14:02:10.672827: step 13059, loss 0.0750798, acc 0.984375\n",
      "2018-05-23T14:02:11.149050: step 13060, loss 0.0716104, acc 0.984375\n",
      "2018-05-23T14:02:11.590672: step 13061, loss 0.119033, acc 0.90625\n",
      "2018-05-23T14:02:12.066560: step 13062, loss 0.118259, acc 0.9375\n",
      "2018-05-23T14:02:12.512466: step 13063, loss 0.0746168, acc 0.984375\n",
      "2018-05-23T14:02:12.970146: step 13064, loss 0.138021, acc 0.953125\n",
      "2018-05-23T14:02:13.411497: step 13065, loss 0.0999165, acc 0.96875\n",
      "2018-05-23T14:02:13.849586: step 13066, loss 0.0767371, acc 0.96875\n",
      "2018-05-23T14:02:14.292194: step 13067, loss 0.156296, acc 0.953125\n",
      "2018-05-23T14:02:14.716849: step 13068, loss 0.182533, acc 0.921875\n",
      "2018-05-23T14:02:15.165850: step 13069, loss 0.154255, acc 0.96875\n",
      "2018-05-23T14:02:15.614536: step 13070, loss 0.0972232, acc 0.953125\n",
      "2018-05-23T14:02:16.130141: step 13071, loss 0.0607746, acc 0.984375\n",
      "2018-05-23T14:02:16.582988: step 13072, loss 0.102335, acc 0.953125\n",
      "2018-05-23T14:02:17.014973: step 13073, loss 0.0924632, acc 0.953125\n",
      "2018-05-23T14:02:17.443705: step 13074, loss 0.0636735, acc 0.984375\n",
      "2018-05-23T14:02:17.884668: step 13075, loss 0.0905122, acc 0.96875\n",
      "2018-05-23T14:02:18.334594: step 13076, loss 0.096592, acc 0.953125\n",
      "2018-05-23T14:02:18.758701: step 13077, loss 0.146843, acc 0.921875\n",
      "2018-05-23T14:02:19.188574: step 13078, loss 0.136534, acc 0.96875\n",
      "2018-05-23T14:02:19.628758: step 13079, loss 0.0772681, acc 0.96875\n",
      "2018-05-23T14:02:20.058101: step 13080, loss 0.0963499, acc 0.96875\n",
      "2018-05-23T14:02:20.496616: step 13081, loss 0.0817542, acc 0.96875\n",
      "2018-05-23T14:02:20.933888: step 13082, loss 0.0970234, acc 0.96875\n",
      "2018-05-23T14:02:21.353011: step 13083, loss 0.104156, acc 0.953125\n",
      "2018-05-23T14:02:21.786206: step 13084, loss 0.0755475, acc 0.953125\n",
      "2018-05-23T14:02:22.216225: step 13085, loss 0.0656447, acc 0.953125\n",
      "2018-05-23T14:02:22.658574: step 13086, loss 0.12528, acc 0.9375\n",
      "2018-05-23T14:02:23.117667: step 13087, loss 0.0869646, acc 0.953125\n",
      "2018-05-23T14:02:23.608619: step 13088, loss 0.0714785, acc 0.96875\n",
      "2018-05-23T14:02:24.074587: step 13089, loss 0.0985891, acc 0.921875\n",
      "2018-05-23T14:02:24.513614: step 13090, loss 0.179463, acc 0.90625\n",
      "2018-05-23T14:02:24.957635: step 13091, loss 0.144946, acc 0.9375\n",
      "2018-05-23T14:02:25.392781: step 13092, loss 0.0989754, acc 0.96875\n",
      "2018-05-23T14:02:25.829438: step 13093, loss 0.105055, acc 0.96875\n",
      "2018-05-23T14:02:26.264088: step 13094, loss 0.181843, acc 0.921875\n",
      "2018-05-23T14:02:26.687976: step 13095, loss 0.0570353, acc 1\n",
      "2018-05-23T14:02:27.144759: step 13096, loss 0.118975, acc 0.9375\n",
      "2018-05-23T14:02:27.577258: step 13097, loss 0.0272106, acc 1\n",
      "2018-05-23T14:02:28.037731: step 13098, loss 0.0611699, acc 0.984375\n",
      "2018-05-23T14:02:28.507026: step 13099, loss 0.130449, acc 0.96875\n",
      "2018-05-23T14:02:28.958811: step 13100, loss 0.0969355, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:02:34.172792: step 13100, loss 1.36419, acc 0.715959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13100\n",
      "\n",
      "2018-05-23T14:02:35.913642: step 13101, loss 0.0788281, acc 0.953125\n",
      "2018-05-23T14:02:36.345896: step 13102, loss 0.0485942, acc 0.984375\n",
      "2018-05-23T14:02:36.786580: step 13103, loss 0.17424, acc 0.953125\n",
      "2018-05-23T14:02:37.246190: step 13104, loss 0.0482566, acc 0.96875\n",
      "2018-05-23T14:02:37.692775: step 13105, loss 0.12963, acc 0.984375\n",
      "2018-05-23T14:02:38.169616: step 13106, loss 0.0803064, acc 0.96875\n",
      "2018-05-23T14:02:38.647776: step 13107, loss 0.277535, acc 0.875\n",
      "2018-05-23T14:02:39.090817: step 13108, loss 0.131665, acc 0.9375\n",
      "2018-05-23T14:02:39.532004: step 13109, loss 0.114972, acc 0.90625\n",
      "2018-05-23T14:02:39.961161: step 13110, loss 0.0699907, acc 0.984375\n",
      "2018-05-23T14:02:40.410793: step 13111, loss 0.100204, acc 0.96875\n",
      "2018-05-23T14:02:40.864834: step 13112, loss 0.168627, acc 0.921875\n",
      "2018-05-23T14:02:41.309409: step 13113, loss 0.0362366, acc 1\n",
      "2018-05-23T14:02:41.749700: step 13114, loss 0.156204, acc 0.921875\n",
      "2018-05-23T14:02:42.214813: step 13115, loss 0.127124, acc 0.96875\n",
      "2018-05-23T14:02:42.661719: step 13116, loss 0.143298, acc 0.953125\n",
      "2018-05-23T14:02:43.147174: step 13117, loss 0.0967233, acc 0.953125\n",
      "2018-05-23T14:02:43.570814: step 13118, loss 0.358483, acc 0.890625\n",
      "2018-05-23T14:02:44.015063: step 13119, loss 0.0943154, acc 0.9375\n",
      "2018-05-23T14:02:44.444150: step 13120, loss 0.0748328, acc 0.96875\n",
      "2018-05-23T14:02:44.900689: step 13121, loss 0.112368, acc 0.953125\n",
      "2018-05-23T14:02:45.347156: step 13122, loss 0.140713, acc 0.9375\n",
      "2018-05-23T14:02:45.786649: step 13123, loss 0.107029, acc 0.953125\n",
      "2018-05-23T14:02:46.211811: step 13124, loss 0.093183, acc 0.96875\n",
      "2018-05-23T14:02:46.643059: step 13125, loss 0.065162, acc 0.984375\n",
      "2018-05-23T14:02:47.091393: step 13126, loss 0.064864, acc 0.96875\n",
      "2018-05-23T14:02:47.541181: step 13127, loss 0.180294, acc 0.921875\n",
      "2018-05-23T14:02:48.038695: step 13128, loss 0.0435052, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:02:48.499880: step 13129, loss 0.103918, acc 0.953125\n",
      "2018-05-23T14:02:48.976223: step 13130, loss 0.126165, acc 0.953125\n",
      "2018-05-23T14:02:49.413101: step 13131, loss 0.172311, acc 0.9375\n",
      "2018-05-23T14:02:49.857853: step 13132, loss 0.276914, acc 0.875\n",
      "2018-05-23T14:02:50.306674: step 13133, loss 0.207791, acc 0.953125\n",
      "2018-05-23T14:02:50.764547: step 13134, loss 0.175249, acc 0.9375\n",
      "2018-05-23T14:02:51.193234: step 13135, loss 0.114439, acc 0.953125\n",
      "2018-05-23T14:02:51.654413: step 13136, loss 0.138313, acc 0.953125\n",
      "2018-05-23T14:02:52.081698: step 13137, loss 0.152594, acc 0.953125\n",
      "2018-05-23T14:02:52.574309: step 13138, loss 0.0708927, acc 0.96875\n",
      "2018-05-23T14:02:53.084918: step 13139, loss 0.222041, acc 0.9375\n",
      "2018-05-23T14:02:53.577668: step 13140, loss 0.153666, acc 0.921875\n",
      "2018-05-23T14:02:54.045723: step 13141, loss 0.140507, acc 0.921875\n",
      "2018-05-23T14:02:54.501731: step 13142, loss 0.170316, acc 0.90625\n",
      "2018-05-23T14:02:54.956947: step 13143, loss 0.109154, acc 0.96875\n",
      "2018-05-23T14:02:55.398452: step 13144, loss 0.177835, acc 0.921875\n",
      "2018-05-23T14:02:55.840998: step 13145, loss 0.0616783, acc 1\n",
      "2018-05-23T14:02:56.294331: step 13146, loss 0.0785345, acc 0.96875\n",
      "2018-05-23T14:02:56.738158: step 13147, loss 0.0640506, acc 0.953125\n",
      "2018-05-23T14:02:57.185747: step 13148, loss 0.113068, acc 0.953125\n",
      "2018-05-23T14:02:57.626785: step 13149, loss 0.111663, acc 0.9375\n",
      "2018-05-23T14:02:58.063375: step 13150, loss 0.115456, acc 0.90625\n",
      "2018-05-23T14:02:58.486919: step 13151, loss 0.10765, acc 0.953125\n",
      "2018-05-23T14:02:58.926614: step 13152, loss 0.0654318, acc 0.96875\n",
      "2018-05-23T14:02:59.378922: step 13153, loss 0.0627257, acc 0.984375\n",
      "2018-05-23T14:02:59.809271: step 13154, loss 0.0578092, acc 0.984375\n",
      "2018-05-23T14:03:00.259536: step 13155, loss 0.177107, acc 0.96875\n",
      "2018-05-23T14:03:00.706452: step 13156, loss 0.250136, acc 0.953125\n",
      "2018-05-23T14:03:01.119515: step 13157, loss 0.123411, acc 0.953125\n",
      "2018-05-23T14:03:01.555842: step 13158, loss 0.0720589, acc 0.96875\n",
      "2018-05-23T14:03:01.969921: step 13159, loss 0.113198, acc 0.953125\n",
      "2018-05-23T14:03:02.408462: step 13160, loss 0.101411, acc 0.953125\n",
      "2018-05-23T14:03:02.823142: step 13161, loss 0.231134, acc 0.90625\n",
      "2018-05-23T14:03:03.241841: step 13162, loss 0.0332351, acc 1\n",
      "2018-05-23T14:03:03.687904: step 13163, loss 0.113814, acc 0.953125\n",
      "2018-05-23T14:03:04.113369: step 13164, loss 0.12759, acc 0.96875\n",
      "2018-05-23T14:03:04.568216: step 13165, loss 0.0692266, acc 0.96875\n",
      "2018-05-23T14:03:04.995468: step 13166, loss 0.100414, acc 0.96875\n",
      "2018-05-23T14:03:05.453426: step 13167, loss 0.105814, acc 0.9375\n",
      "2018-05-23T14:03:05.924839: step 13168, loss 0.197813, acc 0.921875\n",
      "2018-05-23T14:03:06.374703: step 13169, loss 0.0895278, acc 0.984375\n",
      "2018-05-23T14:03:06.833074: step 13170, loss 0.0972213, acc 0.96875\n",
      "2018-05-23T14:03:07.265237: step 13171, loss 0.19764, acc 0.890625\n",
      "2018-05-23T14:03:07.702135: step 13172, loss 0.102344, acc 0.953125\n",
      "2018-05-23T14:03:08.139629: step 13173, loss 0.127781, acc 0.9375\n",
      "2018-05-23T14:03:08.590670: step 13174, loss 0.0476774, acc 0.984375\n",
      "2018-05-23T14:03:09.030779: step 13175, loss 0.0788977, acc 0.9375\n",
      "2018-05-23T14:03:09.484628: step 13176, loss 0.159667, acc 0.9375\n",
      "2018-05-23T14:03:09.912825: step 13177, loss 0.166941, acc 0.9375\n",
      "2018-05-23T14:03:10.345449: step 13178, loss 0.0841317, acc 0.953125\n",
      "2018-05-23T14:03:10.773455: step 13179, loss 0.237666, acc 0.921875\n",
      "2018-05-23T14:03:11.202916: step 13180, loss 0.121635, acc 0.953125\n",
      "2018-05-23T14:03:11.644009: step 13181, loss 0.113595, acc 0.953125\n",
      "2018-05-23T14:03:12.064665: step 13182, loss 0.0942015, acc 0.953125\n",
      "2018-05-23T14:03:12.506635: step 13183, loss 0.144451, acc 0.921875\n",
      "2018-05-23T14:03:12.929335: step 13184, loss 0.1031, acc 0.953125\n",
      "2018-05-23T14:03:13.355753: step 13185, loss 0.170333, acc 0.90625\n",
      "2018-05-23T14:03:13.776288: step 13186, loss 0.118574, acc 0.921875\n",
      "2018-05-23T14:03:14.213263: step 13187, loss 0.1633, acc 0.921875\n",
      "2018-05-23T14:03:14.637756: step 13188, loss 0.156218, acc 0.90625\n",
      "2018-05-23T14:03:15.058522: step 13189, loss 0.238628, acc 0.953125\n",
      "2018-05-23T14:03:15.500885: step 13190, loss 0.154169, acc 0.953125\n",
      "2018-05-23T14:03:15.936477: step 13191, loss 0.0740465, acc 0.984375\n",
      "2018-05-23T14:03:16.368549: step 13192, loss 0.335291, acc 0.921875\n",
      "2018-05-23T14:03:16.803305: step 13193, loss 0.119854, acc 0.9375\n",
      "2018-05-23T14:03:17.230354: step 13194, loss 0.082104, acc 0.96875\n",
      "2018-05-23T14:03:17.695352: step 13195, loss 0.0671886, acc 0.984375\n",
      "2018-05-23T14:03:18.190054: step 13196, loss 0.121317, acc 0.953125\n",
      "2018-05-23T14:03:18.701606: step 13197, loss 0.0556036, acc 0.984375\n",
      "2018-05-23T14:03:19.191825: step 13198, loss 0.117681, acc 0.96875\n",
      "2018-05-23T14:03:19.687531: step 13199, loss 0.158743, acc 0.9375\n",
      "2018-05-23T14:03:20.203604: step 13200, loss 0.107016, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:03:25.605863: step 13200, loss 1.36087, acc 0.718245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13200\n",
      "\n",
      "2018-05-23T14:03:27.787990: step 13201, loss 0.110738, acc 0.96875\n",
      "2018-05-23T14:03:28.229561: step 13202, loss 0.135494, acc 0.9375\n",
      "2018-05-23T14:03:28.672216: step 13203, loss 0.0916677, acc 0.953125\n",
      "2018-05-23T14:03:29.120687: step 13204, loss 0.162744, acc 0.90625\n",
      "2018-05-23T14:03:29.564257: step 13205, loss 0.143259, acc 0.90625\n",
      "2018-05-23T14:03:30.054738: step 13206, loss 0.100111, acc 0.984375\n",
      "2018-05-23T14:03:30.529605: step 13207, loss 0.110722, acc 0.953125\n",
      "2018-05-23T14:03:30.988154: step 13208, loss 0.0393304, acc 0.984375\n",
      "2018-05-23T14:03:31.439719: step 13209, loss 0.150041, acc 0.9375\n",
      "2018-05-23T14:03:31.882876: step 13210, loss 0.191488, acc 0.90625\n",
      "2018-05-23T14:03:32.340940: step 13211, loss 0.171583, acc 0.9375\n",
      "2018-05-23T14:03:32.828550: step 13212, loss 0.0982754, acc 0.9375\n",
      "2018-05-23T14:03:33.277918: step 13213, loss 0.0602523, acc 0.96875\n",
      "2018-05-23T14:03:33.735559: step 13214, loss 0.0782471, acc 0.953125\n",
      "2018-05-23T14:03:34.218275: step 13215, loss 0.0745861, acc 0.984375\n",
      "2018-05-23T14:03:34.680239: step 13216, loss 0.0633304, acc 0.96875\n",
      "2018-05-23T14:03:35.141769: step 13217, loss 0.105182, acc 0.9375\n",
      "2018-05-23T14:03:35.627418: step 13218, loss 0.104565, acc 0.921875\n",
      "2018-05-23T14:03:36.098490: step 13219, loss 0.236424, acc 0.921875\n",
      "2018-05-23T14:03:36.549617: step 13220, loss 0.0928995, acc 0.953125\n",
      "2018-05-23T14:03:37.023485: step 13221, loss 0.242563, acc 0.96875\n",
      "2018-05-23T14:03:37.484337: step 13222, loss 0.130273, acc 0.953125\n",
      "2018-05-23T14:03:37.941095: step 13223, loss 0.0917228, acc 0.953125\n",
      "2018-05-23T14:03:38.408084: step 13224, loss 0.0723014, acc 0.984375\n",
      "2018-05-23T14:03:38.864989: step 13225, loss 0.0455944, acc 0.984375\n",
      "2018-05-23T14:03:39.321650: step 13226, loss 0.17637, acc 0.9375\n",
      "2018-05-23T14:03:39.786122: step 13227, loss 0.0802802, acc 0.953125\n",
      "2018-05-23T14:03:40.235396: step 13228, loss 0.1546, acc 0.921875\n",
      "2018-05-23T14:03:40.690128: step 13229, loss 0.0242199, acc 1\n",
      "2018-05-23T14:03:41.166585: step 13230, loss 0.0751281, acc 1\n",
      "2018-05-23T14:03:41.599988: step 13231, loss 0.180965, acc 0.96875\n",
      "2018-05-23T14:03:42.057662: step 13232, loss 0.169105, acc 0.96875\n",
      "2018-05-23T14:03:42.509425: step 13233, loss 0.114526, acc 0.953125\n",
      "2018-05-23T14:03:42.945500: step 13234, loss 0.0832551, acc 0.953125\n",
      "2018-05-23T14:03:43.413568: step 13235, loss 0.112546, acc 0.953125\n",
      "2018-05-23T14:03:43.850478: step 13236, loss 0.180179, acc 0.90625\n",
      "2018-05-23T14:03:44.324491: step 13237, loss 0.111278, acc 0.9375\n",
      "2018-05-23T14:03:44.753516: step 13238, loss 0.157277, acc 0.921875\n",
      "2018-05-23T14:03:45.211643: step 13239, loss 0.169969, acc 0.9375\n",
      "2018-05-23T14:03:45.671478: step 13240, loss 0.208637, acc 0.9375\n",
      "2018-05-23T14:03:46.139204: step 13241, loss 0.115072, acc 0.953125\n",
      "2018-05-23T14:03:46.620798: step 13242, loss 0.185555, acc 0.90625\n",
      "2018-05-23T14:03:47.069860: step 13243, loss 0.142889, acc 0.9375\n",
      "2018-05-23T14:03:47.527597: step 13244, loss 0.180809, acc 0.953125\n",
      "2018-05-23T14:03:48.013947: step 13245, loss 0.06252, acc 0.984375\n",
      "2018-05-23T14:03:48.502067: step 13246, loss 0.0479205, acc 1\n",
      "2018-05-23T14:03:48.990918: step 13247, loss 0.10436, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:03:49.475475: step 13248, loss 0.116036, acc 0.953125\n",
      "2018-05-23T14:03:49.965568: step 13249, loss 0.139232, acc 0.96875\n",
      "2018-05-23T14:03:50.432172: step 13250, loss 0.10257, acc 0.9375\n",
      "2018-05-23T14:03:50.902677: step 13251, loss 0.245677, acc 0.890625\n",
      "2018-05-23T14:03:51.343061: step 13252, loss 0.121992, acc 0.984375\n",
      "2018-05-23T14:03:51.778066: step 13253, loss 0.0715658, acc 0.984375\n",
      "2018-05-23T14:03:52.222636: step 13254, loss 0.218878, acc 0.9375\n",
      "2018-05-23T14:03:52.661586: step 13255, loss 0.232368, acc 0.90625\n",
      "2018-05-23T14:03:53.120638: step 13256, loss 0.115984, acc 0.9375\n",
      "2018-05-23T14:03:53.589676: step 13257, loss 0.102861, acc 0.953125\n",
      "2018-05-23T14:03:54.055368: step 13258, loss 0.136322, acc 0.953125\n",
      "2018-05-23T14:03:54.495893: step 13259, loss 0.113228, acc 0.953125\n",
      "2018-05-23T14:03:54.916375: step 13260, loss 0.100282, acc 0.9375\n",
      "2018-05-23T14:03:55.348072: step 13261, loss 0.195539, acc 0.96875\n",
      "2018-05-23T14:03:55.784558: step 13262, loss 0.121298, acc 0.9375\n",
      "2018-05-23T14:03:56.188168: step 13263, loss 0.0713934, acc 0.96875\n",
      "2018-05-23T14:03:56.631530: step 13264, loss 0.100345, acc 0.96875\n",
      "2018-05-23T14:03:57.066522: step 13265, loss 0.187919, acc 0.921875\n",
      "2018-05-23T14:03:57.518514: step 13266, loss 0.0730388, acc 0.984375\n",
      "2018-05-23T14:03:57.959526: step 13267, loss 0.190789, acc 0.90625\n",
      "2018-05-23T14:03:58.378807: step 13268, loss 0.0874065, acc 0.96875\n",
      "2018-05-23T14:03:58.825448: step 13269, loss 0.0545506, acc 0.984375\n",
      "2018-05-23T14:03:59.260253: step 13270, loss 0.101123, acc 0.953125\n",
      "2018-05-23T14:03:59.681357: step 13271, loss 0.069138, acc 0.96875\n",
      "2018-05-23T14:04:00.114870: step 13272, loss 0.0215576, acc 1\n",
      "2018-05-23T14:04:00.545100: step 13273, loss 0.121375, acc 0.953125\n",
      "2018-05-23T14:04:00.995910: step 13274, loss 0.104113, acc 0.953125\n",
      "2018-05-23T14:04:01.442630: step 13275, loss 0.143365, acc 0.96875\n",
      "2018-05-23T14:04:01.904222: step 13276, loss 0.159511, acc 0.96875\n",
      "2018-05-23T14:04:02.374318: step 13277, loss 0.0907886, acc 0.953125\n",
      "2018-05-23T14:04:02.825230: step 13278, loss 0.104236, acc 0.953125\n",
      "2018-05-23T14:04:03.269521: step 13279, loss 0.0822964, acc 0.9375\n",
      "2018-05-23T14:04:03.759840: step 13280, loss 0.132293, acc 0.953125\n",
      "2018-05-23T14:04:04.205985: step 13281, loss 0.199729, acc 0.90625\n",
      "2018-05-23T14:04:04.651814: step 13282, loss 0.0951656, acc 0.96875\n",
      "2018-05-23T14:04:05.080332: step 13283, loss 0.0713645, acc 0.96875\n",
      "2018-05-23T14:04:05.530507: step 13284, loss 0.0711975, acc 0.984375\n",
      "2018-05-23T14:04:05.987858: step 13285, loss 0.0409806, acc 1\n",
      "2018-05-23T14:04:06.419592: step 13286, loss 0.166399, acc 0.953125\n",
      "2018-05-23T14:04:06.854011: step 13287, loss 0.107275, acc 0.96875\n",
      "2018-05-23T14:04:07.292160: step 13288, loss 0.106539, acc 0.953125\n",
      "2018-05-23T14:04:07.713907: step 13289, loss 0.11344, acc 0.953125\n",
      "2018-05-23T14:04:08.163637: step 13290, loss 0.137196, acc 0.9375\n",
      "2018-05-23T14:04:08.620329: step 13291, loss 0.152284, acc 0.921875\n",
      "2018-05-23T14:04:09.104475: step 13292, loss 0.107516, acc 0.984375\n",
      "2018-05-23T14:04:09.563485: step 13293, loss 0.109045, acc 0.953125\n",
      "2018-05-23T14:04:10.029861: step 13294, loss 0.0716297, acc 0.96875\n",
      "2018-05-23T14:04:10.463240: step 13295, loss 0.174402, acc 0.9375\n",
      "2018-05-23T14:04:10.895895: step 13296, loss 0.211297, acc 0.953125\n",
      "2018-05-23T14:04:11.332788: step 13297, loss 0.097479, acc 0.953125\n",
      "2018-05-23T14:04:11.768602: step 13298, loss 0.106433, acc 0.953125\n",
      "2018-05-23T14:04:12.229932: step 13299, loss 0.0309923, acc 1\n",
      "2018-05-23T14:04:12.669857: step 13300, loss 0.144672, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:04:18.131576: step 13300, loss 1.39516, acc 0.715674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13300\n",
      "\n",
      "2018-05-23T14:04:19.825881: step 13301, loss 0.0448087, acc 0.984375\n",
      "2018-05-23T14:04:20.260335: step 13302, loss 0.0729564, acc 0.96875\n",
      "2018-05-23T14:04:20.751638: step 13303, loss 0.0837188, acc 0.953125\n",
      "2018-05-23T14:04:21.180981: step 13304, loss 0.119574, acc 0.953125\n",
      "2018-05-23T14:04:21.623147: step 13305, loss 0.0937123, acc 0.96875\n",
      "2018-05-23T14:04:22.069189: step 13306, loss 0.0809524, acc 0.96875\n",
      "2018-05-23T14:04:22.535885: step 13307, loss 0.107971, acc 0.953125\n",
      "2018-05-23T14:04:22.992341: step 13308, loss 0.130728, acc 0.953125\n",
      "2018-05-23T14:04:23.444185: step 13309, loss 0.158797, acc 0.921875\n",
      "2018-05-23T14:04:23.887712: step 13310, loss 0.141036, acc 0.9375\n",
      "2018-05-23T14:04:24.321129: step 13311, loss 0.124026, acc 0.96875\n",
      "2018-05-23T14:04:24.738116: step 13312, loss 0.0969783, acc 0.96875\n",
      "2018-05-23T14:04:25.166172: step 13313, loss 0.0729817, acc 0.984375\n",
      "2018-05-23T14:04:25.590254: step 13314, loss 0.121787, acc 0.953125\n",
      "2018-05-23T14:04:26.032464: step 13315, loss 0.168812, acc 0.921875\n",
      "2018-05-23T14:04:26.458137: step 13316, loss 0.137139, acc 0.984375\n",
      "2018-05-23T14:04:26.907762: step 13317, loss 0.0752868, acc 0.96875\n",
      "2018-05-23T14:04:27.336753: step 13318, loss 0.167138, acc 0.953125\n",
      "2018-05-23T14:04:27.770597: step 13319, loss 0.119638, acc 0.9375\n",
      "2018-05-23T14:04:28.194649: step 13320, loss 0.110912, acc 0.9375\n",
      "2018-05-23T14:04:28.648780: step 13321, loss 0.143588, acc 0.9375\n",
      "2018-05-23T14:04:29.081226: step 13322, loss 0.0816458, acc 0.96875\n",
      "2018-05-23T14:04:29.532220: step 13323, loss 0.0882337, acc 0.953125\n",
      "2018-05-23T14:04:29.969588: step 13324, loss 0.113065, acc 0.96875\n",
      "2018-05-23T14:04:30.409089: step 13325, loss 0.143262, acc 0.9375\n",
      "2018-05-23T14:04:30.833312: step 13326, loss 0.0755176, acc 0.9375\n",
      "2018-05-23T14:04:31.270370: step 13327, loss 0.106773, acc 0.953125\n",
      "2018-05-23T14:04:31.711012: step 13328, loss 0.122147, acc 0.953125\n",
      "2018-05-23T14:04:32.132621: step 13329, loss 0.158221, acc 0.9375\n",
      "2018-05-23T14:04:32.585410: step 13330, loss 0.154802, acc 0.921875\n",
      "2018-05-23T14:04:33.051088: step 13331, loss 0.166171, acc 0.90625\n",
      "2018-05-23T14:04:33.509839: step 13332, loss 0.146141, acc 0.953125\n",
      "2018-05-23T14:04:33.979289: step 13333, loss 0.0784818, acc 0.953125\n",
      "2018-05-23T14:04:34.421566: step 13334, loss 0.23129, acc 0.921875\n",
      "2018-05-23T14:04:34.868427: step 13335, loss 0.122601, acc 0.96875\n",
      "2018-05-23T14:04:35.313742: step 13336, loss 0.137619, acc 0.9375\n",
      "2018-05-23T14:04:35.776737: step 13337, loss 0.0996116, acc 0.96875\n",
      "2018-05-23T14:04:36.212670: step 13338, loss 0.0805249, acc 0.984375\n",
      "2018-05-23T14:04:36.671334: step 13339, loss 0.16042, acc 0.953125\n",
      "2018-05-23T14:04:37.098494: step 13340, loss 0.0491488, acc 0.984375\n",
      "2018-05-23T14:04:37.531544: step 13341, loss 0.112994, acc 0.9375\n",
      "2018-05-23T14:04:38.000578: step 13342, loss 0.0429767, acc 1\n",
      "2018-05-23T14:04:38.511363: step 13343, loss 0.0835796, acc 0.953125\n",
      "2018-05-23T14:04:38.971502: step 13344, loss 0.143379, acc 0.953125\n",
      "2018-05-23T14:04:39.405210: step 13345, loss 0.193311, acc 0.90625\n",
      "2018-05-23T14:04:39.841480: step 13346, loss 0.174427, acc 0.921875\n",
      "2018-05-23T14:04:40.280449: step 13347, loss 0.218504, acc 0.890625\n",
      "2018-05-23T14:04:40.698440: step 13348, loss 0.146755, acc 0.9375\n",
      "2018-05-23T14:04:41.155390: step 13349, loss 0.115174, acc 0.96875\n",
      "2018-05-23T14:04:41.628802: step 13350, loss 0.0431435, acc 0.984375\n",
      "2018-05-23T14:04:42.098918: step 13351, loss 0.0600705, acc 0.984375\n",
      "2018-05-23T14:04:42.549308: step 13352, loss 0.08432, acc 0.953125\n",
      "2018-05-23T14:04:42.980247: step 13353, loss 0.191483, acc 0.9375\n",
      "2018-05-23T14:04:43.411413: step 13354, loss 0.0918912, acc 0.953125\n",
      "2018-05-23T14:04:43.844939: step 13355, loss 0.0501929, acc 0.96875\n",
      "2018-05-23T14:04:44.290056: step 13356, loss 0.162233, acc 0.921875\n",
      "2018-05-23T14:04:44.719091: step 13357, loss 0.261296, acc 0.953125\n",
      "2018-05-23T14:04:45.144716: step 13358, loss 0.11701, acc 0.9375\n",
      "2018-05-23T14:04:45.572222: step 13359, loss 0.112221, acc 0.953125\n",
      "2018-05-23T14:04:46.025545: step 13360, loss 0.150006, acc 0.953125\n",
      "2018-05-23T14:04:46.444835: step 13361, loss 0.100801, acc 0.96875\n",
      "2018-05-23T14:04:46.873235: step 13362, loss 0.120724, acc 0.9375\n",
      "2018-05-23T14:04:47.304535: step 13363, loss 0.152942, acc 0.921875\n",
      "2018-05-23T14:04:47.755191: step 13364, loss 0.0687373, acc 0.96875\n",
      "2018-05-23T14:04:48.196808: step 13365, loss 0.111703, acc 0.953125\n",
      "2018-05-23T14:04:48.625035: step 13366, loss 0.138823, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:04:49.064963: step 13367, loss 0.0530083, acc 1\n",
      "2018-05-23T14:04:49.498301: step 13368, loss 0.0939405, acc 0.96875\n",
      "2018-05-23T14:04:49.946855: step 13369, loss 0.0506543, acc 0.984375\n",
      "2018-05-23T14:04:50.395799: step 13370, loss 0.103502, acc 0.953125\n",
      "2018-05-23T14:04:50.931452: step 13371, loss 0.10925, acc 0.953125\n",
      "2018-05-23T14:04:51.362554: step 13372, loss 0.185138, acc 0.953125\n",
      "2018-05-23T14:04:51.783759: step 13373, loss 0.190214, acc 0.921875\n",
      "2018-05-23T14:04:52.213552: step 13374, loss 0.0458108, acc 0.984375\n",
      "2018-05-23T14:04:52.667033: step 13375, loss 0.103189, acc 0.9375\n",
      "2018-05-23T14:04:53.115069: step 13376, loss 0.0657117, acc 0.96875\n",
      "2018-05-23T14:04:53.798650: step 13377, loss 0.0844144, acc 0.9375\n",
      "2018-05-23T14:04:54.272569: step 13378, loss 0.182561, acc 0.921875\n",
      "2018-05-23T14:04:54.748223: step 13379, loss 0.120682, acc 0.953125\n",
      "2018-05-23T14:04:55.247427: step 13380, loss 0.106181, acc 0.96875\n",
      "2018-05-23T14:04:55.688978: step 13381, loss 0.147482, acc 0.921875\n",
      "2018-05-23T14:04:56.149514: step 13382, loss 0.120163, acc 0.9375\n",
      "2018-05-23T14:04:56.601958: step 13383, loss 0.0751147, acc 0.953125\n",
      "2018-05-23T14:04:57.062476: step 13384, loss 0.112183, acc 0.921875\n",
      "2018-05-23T14:04:57.544935: step 13385, loss 0.0635099, acc 0.984375\n",
      "2018-05-23T14:04:58.026286: step 13386, loss 0.0601993, acc 0.984375\n",
      "2018-05-23T14:04:58.543884: step 13387, loss 0.165236, acc 0.90625\n",
      "2018-05-23T14:04:59.025953: step 13388, loss 0.081336, acc 0.9375\n",
      "2018-05-23T14:04:59.476790: step 13389, loss 0.0575641, acc 1\n",
      "2018-05-23T14:04:59.954251: step 13390, loss 0.214758, acc 0.9375\n",
      "2018-05-23T14:05:00.492350: step 13391, loss 0.12536, acc 0.890625\n",
      "2018-05-23T14:05:01.031056: step 13392, loss 0.214682, acc 0.90625\n",
      "2018-05-23T14:05:01.538116: step 13393, loss 0.0977775, acc 0.984375\n",
      "2018-05-23T14:05:02.120091: step 13394, loss 0.229036, acc 0.875\n",
      "2018-05-23T14:05:02.564767: step 13395, loss 0.0883701, acc 0.953125\n",
      "2018-05-23T14:05:03.161864: step 13396, loss 0.135045, acc 0.953125\n",
      "2018-05-23T14:05:03.853338: step 13397, loss 0.120456, acc 0.953125\n",
      "2018-05-23T14:05:04.340758: step 13398, loss 0.126247, acc 0.953125\n",
      "2018-05-23T14:05:04.883526: step 13399, loss 0.182762, acc 0.921875\n",
      "2018-05-23T14:05:05.396880: step 13400, loss 0.219435, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:05:10.697080: step 13400, loss 1.397, acc 0.714102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13400\n",
      "\n",
      "2018-05-23T14:05:12.516173: step 13401, loss 0.0702481, acc 0.96875\n",
      "2018-05-23T14:05:12.947490: step 13402, loss 0.0928442, acc 0.953125\n",
      "2018-05-23T14:05:13.374040: step 13403, loss 0.127132, acc 0.953125\n",
      "2018-05-23T14:05:13.811359: step 13404, loss 0.153207, acc 0.90625\n",
      "2018-05-23T14:05:14.246384: step 13405, loss 0.0624446, acc 0.984375\n",
      "2018-05-23T14:05:14.685803: step 13406, loss 0.11915, acc 0.9375\n",
      "2018-05-23T14:05:15.137959: step 13407, loss 0.111938, acc 0.9375\n",
      "2018-05-23T14:05:15.559340: step 13408, loss 0.11155, acc 0.953125\n",
      "2018-05-23T14:05:16.009261: step 13409, loss 0.130473, acc 0.9375\n",
      "2018-05-23T14:05:16.434791: step 13410, loss 0.198068, acc 0.890625\n",
      "2018-05-23T14:05:16.849610: step 13411, loss 0.270172, acc 0.90625\n",
      "2018-05-23T14:05:17.279562: step 13412, loss 0.049733, acc 0.96875\n",
      "2018-05-23T14:05:17.724744: step 13413, loss 0.162464, acc 0.90625\n",
      "2018-05-23T14:05:18.158022: step 13414, loss 0.116823, acc 0.96875\n",
      "2018-05-23T14:05:18.584552: step 13415, loss 0.114744, acc 0.953125\n",
      "2018-05-23T14:05:19.028544: step 13416, loss 0.0708653, acc 0.96875\n",
      "2018-05-23T14:05:19.480211: step 13417, loss 0.118618, acc 0.9375\n",
      "2018-05-23T14:05:19.906513: step 13418, loss 0.146167, acc 0.90625\n",
      "2018-05-23T14:05:20.344000: step 13419, loss 0.166294, acc 0.9375\n",
      "2018-05-23T14:05:20.806663: step 13420, loss 0.143903, acc 0.90625\n",
      "2018-05-23T14:05:21.215573: step 13421, loss 0.146928, acc 0.921875\n",
      "2018-05-23T14:05:21.638296: step 13422, loss 0.143828, acc 0.953125\n",
      "2018-05-23T14:05:22.053943: step 13423, loss 0.0985546, acc 0.96875\n",
      "2018-05-23T14:05:22.494419: step 13424, loss 0.0309489, acc 0.984375\n",
      "2018-05-23T14:05:22.925872: step 13425, loss 0.178595, acc 0.921875\n",
      "2018-05-23T14:05:23.370975: step 13426, loss 0.216698, acc 0.921875\n",
      "2018-05-23T14:05:23.820414: step 13427, loss 0.112194, acc 0.9375\n",
      "2018-05-23T14:05:24.244173: step 13428, loss 0.148396, acc 0.9375\n",
      "2018-05-23T14:05:24.670500: step 13429, loss 0.0770843, acc 0.96875\n",
      "2018-05-23T14:05:25.102880: step 13430, loss 0.18254, acc 0.9375\n",
      "2018-05-23T14:05:25.550488: step 13431, loss 0.137259, acc 0.953125\n",
      "2018-05-23T14:05:26.021745: step 13432, loss 0.114166, acc 0.953125\n",
      "2018-05-23T14:05:26.495664: step 13433, loss 0.0972365, acc 0.96875\n",
      "2018-05-23T14:05:26.946314: step 13434, loss 0.123745, acc 0.9375\n",
      "2018-05-23T14:05:27.415499: step 13435, loss 0.0768977, acc 0.96875\n",
      "2018-05-23T14:05:27.850474: step 13436, loss 0.128436, acc 0.96875\n",
      "2018-05-23T14:05:28.254349: step 13437, loss 0.0906681, acc 0.984375\n",
      "2018-05-23T14:05:28.695960: step 13438, loss 0.054916, acc 0.96875\n",
      "2018-05-23T14:05:29.123976: step 13439, loss 0.116117, acc 0.953125\n",
      "2018-05-23T14:05:29.536011: step 13440, loss 0.159144, acc 0.953125\n",
      "2018-05-23T14:05:29.976106: step 13441, loss 0.116852, acc 0.9375\n",
      "2018-05-23T14:05:30.401559: step 13442, loss 0.128054, acc 0.96875\n",
      "2018-05-23T14:05:30.840453: step 13443, loss 0.0798547, acc 0.984375\n",
      "2018-05-23T14:05:31.258974: step 13444, loss 0.0654359, acc 1\n",
      "2018-05-23T14:05:31.709951: step 13445, loss 0.230295, acc 0.875\n",
      "2018-05-23T14:05:32.202191: step 13446, loss 0.120421, acc 0.953125\n",
      "2018-05-23T14:05:32.650743: step 13447, loss 0.161523, acc 0.953125\n",
      "2018-05-23T14:05:33.137456: step 13448, loss 0.119144, acc 0.953125\n",
      "2018-05-23T14:05:33.560253: step 13449, loss 0.100863, acc 0.953125\n",
      "2018-05-23T14:05:33.999021: step 13450, loss 0.11099, acc 0.9375\n",
      "2018-05-23T14:05:34.427937: step 13451, loss 0.152277, acc 0.9375\n",
      "2018-05-23T14:05:34.885101: step 13452, loss 0.208256, acc 0.90625\n",
      "2018-05-23T14:05:35.297577: step 13453, loss 0.247446, acc 0.921875\n",
      "2018-05-23T14:05:35.769294: step 13454, loss 0.177047, acc 0.921875\n",
      "2018-05-23T14:05:36.186496: step 13455, loss 0.12068, acc 0.921875\n",
      "2018-05-23T14:05:36.598955: step 13456, loss 0.156916, acc 0.890625\n",
      "2018-05-23T14:05:37.023317: step 13457, loss 0.183496, acc 0.96875\n",
      "2018-05-23T14:05:37.442632: step 13458, loss 0.244869, acc 0.953125\n",
      "2018-05-23T14:05:37.870586: step 13459, loss 0.122383, acc 0.96875\n",
      "2018-05-23T14:05:38.328516: step 13460, loss 0.17782, acc 0.953125\n",
      "2018-05-23T14:05:38.784528: step 13461, loss 0.124109, acc 0.953125\n",
      "2018-05-23T14:05:39.228410: step 13462, loss 0.0797953, acc 0.953125\n",
      "2018-05-23T14:05:39.668891: step 13463, loss 0.158952, acc 0.921875\n",
      "2018-05-23T14:05:40.114004: step 13464, loss 0.107125, acc 0.90625\n",
      "2018-05-23T14:05:40.558636: step 13465, loss 0.161277, acc 0.953125\n",
      "2018-05-23T14:05:40.968248: step 13466, loss 0.173741, acc 0.9375\n",
      "2018-05-23T14:05:41.422289: step 13467, loss 0.076636, acc 0.96875\n",
      "2018-05-23T14:05:41.838736: step 13468, loss 0.159383, acc 0.921875\n",
      "2018-05-23T14:05:42.257525: step 13469, loss 0.135793, acc 0.921875\n",
      "2018-05-23T14:05:42.694844: step 13470, loss 0.136687, acc 0.96875\n",
      "2018-05-23T14:05:43.126087: step 13471, loss 0.0952227, acc 0.9375\n",
      "2018-05-23T14:05:43.557494: step 13472, loss 0.134101, acc 0.921875\n",
      "2018-05-23T14:05:43.987281: step 13473, loss 0.11958, acc 0.921875\n",
      "2018-05-23T14:05:44.444740: step 13474, loss 0.1026, acc 0.953125\n",
      "2018-05-23T14:05:44.888271: step 13475, loss 0.118379, acc 0.9375\n",
      "2018-05-23T14:05:45.350370: step 13476, loss 0.140337, acc 0.9375\n",
      "2018-05-23T14:05:45.802586: step 13477, loss 0.0430356, acc 0.984375\n",
      "2018-05-23T14:05:46.270324: step 13478, loss 0.0783507, acc 0.96875\n",
      "2018-05-23T14:05:46.740020: step 13479, loss 0.0660318, acc 0.984375\n",
      "2018-05-23T14:05:47.200335: step 13480, loss 0.0379498, acc 1\n",
      "2018-05-23T14:05:47.649506: step 13481, loss 0.146793, acc 0.953125\n",
      "2018-05-23T14:05:48.067133: step 13482, loss 0.106927, acc 0.96875\n",
      "2018-05-23T14:05:48.491493: step 13483, loss 0.116479, acc 0.953125\n",
      "2018-05-23T14:05:48.921927: step 13484, loss 0.276017, acc 0.9375\n",
      "2018-05-23T14:05:49.353026: step 13485, loss 0.182568, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:05:49.792721: step 13486, loss 0.16706, acc 0.90625\n",
      "2018-05-23T14:05:50.224045: step 13487, loss 0.0778957, acc 0.96875\n",
      "2018-05-23T14:05:50.666378: step 13488, loss 0.091463, acc 0.96875\n",
      "2018-05-23T14:05:51.082601: step 13489, loss 0.0670042, acc 0.984375\n",
      "2018-05-23T14:05:51.527327: step 13490, loss 0.104346, acc 0.96875\n",
      "2018-05-23T14:05:51.966242: step 13491, loss 0.0830355, acc 0.953125\n",
      "2018-05-23T14:05:52.407510: step 13492, loss 0.140046, acc 0.96875\n",
      "2018-05-23T14:05:52.869071: step 13493, loss 0.106776, acc 0.96875\n",
      "2018-05-23T14:05:53.308148: step 13494, loss 0.135506, acc 0.921875\n",
      "2018-05-23T14:05:53.801161: step 13495, loss 0.169938, acc 0.90625\n",
      "2018-05-23T14:05:54.259178: step 13496, loss 0.140424, acc 0.9375\n",
      "2018-05-23T14:05:54.688060: step 13497, loss 0.161557, acc 0.9375\n",
      "2018-05-23T14:05:55.138726: step 13498, loss 0.0908527, acc 0.96875\n",
      "2018-05-23T14:05:55.566749: step 13499, loss 0.147214, acc 0.9375\n",
      "2018-05-23T14:05:56.009174: step 13500, loss 0.233078, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:06:01.136702: step 13500, loss 1.39785, acc 0.713102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13500\n",
      "\n",
      "2018-05-23T14:06:02.740837: step 13501, loss 0.192637, acc 0.96875\n",
      "2018-05-23T14:06:03.182932: step 13502, loss 0.129694, acc 0.9375\n",
      "2018-05-23T14:06:03.643032: step 13503, loss 0.152677, acc 0.9375\n",
      "2018-05-23T14:06:04.070632: step 13504, loss 0.189716, acc 0.953125\n",
      "2018-05-23T14:06:04.508248: step 13505, loss 0.149091, acc 0.9375\n",
      "2018-05-23T14:06:04.968857: step 13506, loss 0.120181, acc 0.921875\n",
      "2018-05-23T14:06:05.429678: step 13507, loss 0.0504643, acc 0.96875\n",
      "2018-05-23T14:06:05.969521: step 13508, loss 0.0998108, acc 0.96875\n",
      "2018-05-23T14:06:06.439629: step 13509, loss 0.0633821, acc 0.96875\n",
      "2018-05-23T14:06:06.890544: step 13510, loss 0.159784, acc 0.953125\n",
      "2018-05-23T14:06:07.324512: step 13511, loss 0.303742, acc 0.921875\n",
      "2018-05-23T14:06:07.775119: step 13512, loss 0.149551, acc 0.953125\n",
      "2018-05-23T14:06:08.220071: step 13513, loss 0.179985, acc 0.921875\n",
      "2018-05-23T14:06:08.692457: step 13514, loss 0.309785, acc 0.84375\n",
      "2018-05-23T14:06:09.146022: step 13515, loss 0.0618542, acc 0.984375\n",
      "2018-05-23T14:06:09.599985: step 13516, loss 0.0873693, acc 0.9375\n",
      "2018-05-23T14:06:10.052747: step 13517, loss 0.139362, acc 0.9375\n",
      "2018-05-23T14:06:10.469456: step 13518, loss 0.155826, acc 0.890625\n",
      "2018-05-23T14:06:10.918226: step 13519, loss 0.142459, acc 0.9375\n",
      "2018-05-23T14:06:11.376384: step 13520, loss 0.111401, acc 0.953125\n",
      "2018-05-23T14:06:11.820240: step 13521, loss 0.157848, acc 0.890625\n",
      "2018-05-23T14:06:12.253816: step 13522, loss 0.0931678, acc 0.96875\n",
      "2018-05-23T14:06:12.699487: step 13523, loss 0.141894, acc 0.9375\n",
      "2018-05-23T14:06:13.137789: step 13524, loss 0.105151, acc 0.953125\n",
      "2018-05-23T14:06:13.578568: step 13525, loss 0.0929474, acc 0.9375\n",
      "2018-05-23T14:06:14.020274: step 13526, loss 0.147644, acc 0.921875\n",
      "2018-05-23T14:06:14.485938: step 13527, loss 0.161463, acc 0.921875\n",
      "2018-05-23T14:06:14.910121: step 13528, loss 0.091785, acc 0.953125\n",
      "2018-05-23T14:06:15.339870: step 13529, loss 0.114303, acc 0.9375\n",
      "2018-05-23T14:06:15.796391: step 13530, loss 0.133971, acc 0.921875\n",
      "2018-05-23T14:06:16.207935: step 13531, loss 0.171112, acc 0.9375\n",
      "2018-05-23T14:06:16.648261: step 13532, loss 0.0502189, acc 0.96875\n",
      "2018-05-23T14:06:17.063385: step 13533, loss 0.273112, acc 0.875\n",
      "2018-05-23T14:06:17.508307: step 13534, loss 0.0756122, acc 0.984375\n",
      "2018-05-23T14:06:17.945595: step 13535, loss 0.193703, acc 0.921875\n",
      "2018-05-23T14:06:18.374119: step 13536, loss 0.140962, acc 0.9375\n",
      "2018-05-23T14:06:18.847899: step 13537, loss 0.150405, acc 0.9375\n",
      "2018-05-23T14:06:19.303672: step 13538, loss 0.106696, acc 0.953125\n",
      "2018-05-23T14:06:19.773430: step 13539, loss 0.0895435, acc 0.96875\n",
      "2018-05-23T14:06:20.259182: step 13540, loss 0.0991933, acc 0.953125\n",
      "2018-05-23T14:06:20.696564: step 13541, loss 0.0906298, acc 0.953125\n",
      "2018-05-23T14:06:21.128317: step 13542, loss 0.17964, acc 0.90625\n",
      "2018-05-23T14:06:21.577495: step 13543, loss 0.158967, acc 0.953125\n",
      "2018-05-23T14:06:22.026163: step 13544, loss 0.127531, acc 0.984375\n",
      "2018-05-23T14:06:22.447345: step 13545, loss 0.0425132, acc 1\n",
      "2018-05-23T14:06:22.871015: step 13546, loss 0.173747, acc 0.9375\n",
      "2018-05-23T14:06:23.296549: step 13547, loss 0.0475118, acc 0.984375\n",
      "2018-05-23T14:06:23.799820: step 13548, loss 0.102176, acc 0.96875\n",
      "2018-05-23T14:06:24.235624: step 13549, loss 0.138268, acc 0.921875\n",
      "2018-05-23T14:06:24.674155: step 13550, loss 0.201355, acc 0.9375\n",
      "2018-05-23T14:06:25.115929: step 13551, loss 0.168153, acc 0.9375\n",
      "2018-05-23T14:06:25.572369: step 13552, loss 0.101327, acc 0.96875\n",
      "2018-05-23T14:06:26.048010: step 13553, loss 0.126687, acc 0.921875\n",
      "2018-05-23T14:06:26.522687: step 13554, loss 0.0723006, acc 0.984375\n",
      "2018-05-23T14:06:27.021645: step 13555, loss 0.166786, acc 0.96875\n",
      "2018-05-23T14:06:27.460911: step 13556, loss 0.171569, acc 0.90625\n",
      "2018-05-23T14:06:27.880974: step 13557, loss 0.128557, acc 0.96875\n",
      "2018-05-23T14:06:28.309259: step 13558, loss 0.34365, acc 0.921875\n",
      "2018-05-23T14:06:28.736133: step 13559, loss 0.137739, acc 0.9375\n",
      "2018-05-23T14:06:29.165610: step 13560, loss 0.0651578, acc 0.953125\n",
      "2018-05-23T14:06:29.612698: step 13561, loss 0.186557, acc 0.921875\n",
      "2018-05-23T14:06:30.037390: step 13562, loss 0.0984974, acc 0.96875\n",
      "2018-05-23T14:06:30.487334: step 13563, loss 0.0781495, acc 0.96875\n",
      "2018-05-23T14:06:30.908119: step 13564, loss 0.197139, acc 0.921875\n",
      "2018-05-23T14:06:31.361506: step 13565, loss 0.121239, acc 0.9375\n",
      "2018-05-23T14:06:31.789245: step 13566, loss 0.195786, acc 0.921875\n",
      "2018-05-23T14:06:32.247383: step 13567, loss 0.131827, acc 0.921875\n",
      "2018-05-23T14:06:32.688567: step 13568, loss 0.0980722, acc 0.9375\n",
      "2018-05-23T14:06:33.110465: step 13569, loss 0.145065, acc 0.921875\n",
      "2018-05-23T14:06:33.565400: step 13570, loss 0.125512, acc 0.9375\n",
      "2018-05-23T14:06:33.995626: step 13571, loss 0.0523898, acc 0.984375\n",
      "2018-05-23T14:06:34.420416: step 13572, loss 0.192751, acc 0.921875\n",
      "2018-05-23T14:06:34.853672: step 13573, loss 0.161408, acc 0.953125\n",
      "2018-05-23T14:06:35.277386: step 13574, loss 0.135606, acc 0.953125\n",
      "2018-05-23T14:06:35.727517: step 13575, loss 0.143653, acc 0.9375\n",
      "2018-05-23T14:06:36.149486: step 13576, loss 0.119796, acc 0.9375\n",
      "2018-05-23T14:06:36.589963: step 13577, loss 0.220893, acc 0.90625\n",
      "2018-05-23T14:06:37.013633: step 13578, loss 0.0955002, acc 0.96875\n",
      "2018-05-23T14:06:37.475887: step 13579, loss 0.193479, acc 0.921875\n",
      "2018-05-23T14:06:38.017408: step 13580, loss 0.140742, acc 0.9375\n",
      "2018-05-23T14:06:38.542923: step 13581, loss 0.103066, acc 0.953125\n",
      "2018-05-23T14:06:38.955230: step 13582, loss 0.0744947, acc 0.96875\n",
      "2018-05-23T14:06:39.382329: step 13583, loss 0.121799, acc 0.96875\n",
      "2018-05-23T14:06:39.829604: step 13584, loss 0.174645, acc 0.9375\n",
      "2018-05-23T14:06:40.258279: step 13585, loss 0.0859361, acc 0.984375\n",
      "2018-05-23T14:06:40.701346: step 13586, loss 0.136813, acc 0.921875\n",
      "2018-05-23T14:06:41.131839: step 13587, loss 0.188382, acc 0.890625\n",
      "2018-05-23T14:06:41.564282: step 13588, loss 0.140058, acc 0.9375\n",
      "2018-05-23T14:06:41.994659: step 13589, loss 0.160225, acc 0.9375\n",
      "2018-05-23T14:06:42.438419: step 13590, loss 0.144739, acc 0.921875\n",
      "2018-05-23T14:06:42.886735: step 13591, loss 0.179246, acc 0.90625\n",
      "2018-05-23T14:06:43.325198: step 13592, loss 0.213531, acc 0.90625\n",
      "2018-05-23T14:06:43.762586: step 13593, loss 0.264872, acc 0.9375\n",
      "2018-05-23T14:06:44.179483: step 13594, loss 0.113359, acc 0.921875\n",
      "2018-05-23T14:06:44.605486: step 13595, loss 0.0760642, acc 0.96875\n",
      "2018-05-23T14:06:45.079177: step 13596, loss 0.076497, acc 0.984375\n",
      "2018-05-23T14:06:45.542803: step 13597, loss 0.127612, acc 0.9375\n",
      "2018-05-23T14:06:45.998821: step 13598, loss 0.129935, acc 0.953125\n",
      "2018-05-23T14:06:46.429514: step 13599, loss 0.125112, acc 0.9375\n",
      "2018-05-23T14:06:46.856568: step 13600, loss 0.164722, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:06:52.097614: step 13600, loss 1.39347, acc 0.711102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13600\n",
      "\n",
      "2018-05-23T14:06:54.021326: step 13601, loss 0.173171, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:06:54.492357: step 13602, loss 0.113422, acc 0.921875\n",
      "2018-05-23T14:06:54.940612: step 13603, loss 0.132255, acc 0.9375\n",
      "2018-05-23T14:06:55.383167: step 13604, loss 0.0939739, acc 0.953125\n",
      "2018-05-23T14:06:55.829656: step 13605, loss 0.1301, acc 0.953125\n",
      "2018-05-23T14:06:56.272313: step 13606, loss 0.0591935, acc 0.984375\n",
      "2018-05-23T14:06:56.721680: step 13607, loss 0.0584283, acc 0.984375\n",
      "2018-05-23T14:06:57.186795: step 13608, loss 0.213454, acc 0.9375\n",
      "2018-05-23T14:06:57.644610: step 13609, loss 0.13847, acc 0.953125\n",
      "2018-05-23T14:06:58.140605: step 13610, loss 0.185264, acc 0.921875\n",
      "2018-05-23T14:06:58.623667: step 13611, loss 0.063469, acc 0.984375\n",
      "2018-05-23T14:06:59.091831: step 13612, loss 0.151134, acc 0.9375\n",
      "2018-05-23T14:06:59.585543: step 13613, loss 0.156575, acc 0.90625\n",
      "2018-05-23T14:07:00.063634: step 13614, loss 0.106686, acc 0.9375\n",
      "2018-05-23T14:07:00.501424: step 13615, loss 0.283682, acc 0.921875\n",
      "2018-05-23T14:07:01.014650: step 13616, loss 0.163377, acc 0.9375\n",
      "2018-05-23T14:07:01.536351: step 13617, loss 0.217299, acc 0.921875\n",
      "2018-05-23T14:07:02.028177: step 13618, loss 0.250649, acc 0.890625\n",
      "2018-05-23T14:07:02.491054: step 13619, loss 0.0737527, acc 0.96875\n",
      "2018-05-23T14:07:02.913883: step 13620, loss 0.091768, acc 0.9375\n",
      "2018-05-23T14:07:03.349707: step 13621, loss 0.117194, acc 0.96875\n",
      "2018-05-23T14:07:03.800115: step 13622, loss 0.0599631, acc 0.96875\n",
      "2018-05-23T14:07:04.244837: step 13623, loss 0.156866, acc 0.9375\n",
      "2018-05-23T14:07:04.690844: step 13624, loss 0.185643, acc 0.9375\n",
      "2018-05-23T14:07:05.153430: step 13625, loss 0.0826132, acc 0.953125\n",
      "2018-05-23T14:07:05.625770: step 13626, loss 0.134133, acc 0.9375\n",
      "2018-05-23T14:07:06.110030: step 13627, loss 0.0942694, acc 0.96875\n",
      "2018-05-23T14:07:06.534817: step 13628, loss 0.10968, acc 0.953125\n",
      "2018-05-23T14:07:06.961897: step 13629, loss 0.0708504, acc 0.96875\n",
      "2018-05-23T14:07:07.386731: step 13630, loss 0.0695711, acc 0.96875\n",
      "2018-05-23T14:07:07.810210: step 13631, loss 0.115735, acc 0.921875\n",
      "2018-05-23T14:07:08.238845: step 13632, loss 0.267473, acc 0.90625\n",
      "2018-05-23T14:07:08.699927: step 13633, loss 0.100414, acc 0.953125\n",
      "2018-05-23T14:07:09.133277: step 13634, loss 0.241004, acc 0.90625\n",
      "2018-05-23T14:07:09.570673: step 13635, loss 0.154108, acc 0.953125\n",
      "2018-05-23T14:07:10.011413: step 13636, loss 0.0799711, acc 0.953125\n",
      "2018-05-23T14:07:10.475320: step 13637, loss 0.129587, acc 0.96875\n",
      "2018-05-23T14:07:10.942038: step 13638, loss 0.0706824, acc 0.96875\n",
      "2018-05-23T14:07:11.401425: step 13639, loss 0.115966, acc 0.9375\n",
      "2018-05-23T14:07:11.856779: step 13640, loss 0.111509, acc 0.9375\n",
      "2018-05-23T14:07:12.288187: step 13641, loss 0.120763, acc 0.9375\n",
      "2018-05-23T14:07:12.713053: step 13642, loss 0.150238, acc 0.9375\n",
      "2018-05-23T14:07:13.160900: step 13643, loss 0.149191, acc 0.921875\n",
      "2018-05-23T14:07:13.579016: step 13644, loss 0.352149, acc 0.859375\n",
      "2018-05-23T14:07:14.014156: step 13645, loss 0.135123, acc 0.921875\n",
      "2018-05-23T14:07:14.429344: step 13646, loss 0.176254, acc 0.90625\n",
      "2018-05-23T14:07:14.842068: step 13647, loss 0.166494, acc 0.859375\n",
      "2018-05-23T14:07:15.282552: step 13648, loss 0.206474, acc 0.9375\n",
      "2018-05-23T14:07:15.708361: step 13649, loss 0.12329, acc 0.9375\n",
      "2018-05-23T14:07:16.120832: step 13650, loss 0.193098, acc 0.921875\n",
      "2018-05-23T14:07:16.563711: step 13651, loss 0.112191, acc 0.9375\n",
      "2018-05-23T14:07:17.043196: step 13652, loss 0.11518, acc 0.96875\n",
      "2018-05-23T14:07:17.509531: step 13653, loss 0.142582, acc 0.953125\n",
      "2018-05-23T14:07:17.952271: step 13654, loss 0.118068, acc 0.953125\n",
      "2018-05-23T14:07:18.372894: step 13655, loss 0.118154, acc 0.953125\n",
      "2018-05-23T14:07:18.803262: step 13656, loss 0.113965, acc 0.953125\n",
      "2018-05-23T14:07:19.227846: step 13657, loss 0.165492, acc 0.953125\n",
      "2018-05-23T14:07:19.659601: step 13658, loss 0.0933674, acc 0.9375\n",
      "2018-05-23T14:07:20.080893: step 13659, loss 0.0786169, acc 0.96875\n",
      "2018-05-23T14:07:20.504446: step 13660, loss 0.175212, acc 0.9375\n",
      "2018-05-23T14:07:20.966609: step 13661, loss 0.137798, acc 0.9375\n",
      "2018-05-23T14:07:21.405678: step 13662, loss 0.0869032, acc 0.96875\n",
      "2018-05-23T14:07:21.838792: step 13663, loss 0.358745, acc 0.890625\n",
      "2018-05-23T14:07:22.268778: step 13664, loss 0.0931719, acc 0.921875\n",
      "2018-05-23T14:07:22.680769: step 13665, loss 0.0944697, acc 0.96875\n",
      "2018-05-23T14:07:23.110976: step 13666, loss 0.0939583, acc 0.96875\n",
      "2018-05-23T14:07:23.589156: step 13667, loss 0.0400777, acc 1\n",
      "2018-05-23T14:07:24.029523: step 13668, loss 0.0588912, acc 0.953125\n",
      "2018-05-23T14:07:24.467301: step 13669, loss 0.106057, acc 0.96875\n",
      "2018-05-23T14:07:24.945149: step 13670, loss 0.0849183, acc 0.96875\n",
      "2018-05-23T14:07:25.399527: step 13671, loss 0.109757, acc 0.953125\n",
      "2018-05-23T14:07:25.864533: step 13672, loss 0.152375, acc 0.921875\n",
      "2018-05-23T14:07:26.307669: step 13673, loss 0.260551, acc 0.921875\n",
      "2018-05-23T14:07:26.758932: step 13674, loss 0.19274, acc 0.890625\n",
      "2018-05-23T14:07:27.172955: step 13675, loss 0.146689, acc 0.9375\n",
      "2018-05-23T14:07:27.619329: step 13676, loss 0.161723, acc 0.9375\n",
      "2018-05-23T14:07:28.078104: step 13677, loss 0.073594, acc 0.96875\n",
      "2018-05-23T14:07:28.547492: step 13678, loss 0.121816, acc 0.96875\n",
      "2018-05-23T14:07:29.005218: step 13679, loss 0.0585754, acc 0.984375\n",
      "2018-05-23T14:07:29.423627: step 13680, loss 0.0474293, acc 1\n",
      "2018-05-23T14:07:29.859622: step 13681, loss 0.15946, acc 0.9375\n",
      "2018-05-23T14:07:30.292169: step 13682, loss 0.122055, acc 0.953125\n",
      "2018-05-23T14:07:30.724287: step 13683, loss 0.0599743, acc 0.984375\n",
      "2018-05-23T14:07:31.169934: step 13684, loss 0.0718213, acc 0.984375\n",
      "2018-05-23T14:07:31.599101: step 13685, loss 0.0905873, acc 0.953125\n",
      "2018-05-23T14:07:32.071424: step 13686, loss 0.0523409, acc 1\n",
      "2018-05-23T14:07:32.531801: step 13687, loss 0.103906, acc 0.96875\n",
      "2018-05-23T14:07:32.998353: step 13688, loss 0.183323, acc 0.953125\n",
      "2018-05-23T14:07:33.436219: step 13689, loss 0.117945, acc 0.953125\n",
      "2018-05-23T14:07:33.875801: step 13690, loss 0.180958, acc 0.9375\n",
      "2018-05-23T14:07:34.302525: step 13691, loss 0.149083, acc 0.953125\n",
      "2018-05-23T14:07:34.735838: step 13692, loss 0.279684, acc 0.875\n",
      "2018-05-23T14:07:35.167094: step 13693, loss 0.101757, acc 0.9375\n",
      "2018-05-23T14:07:35.582302: step 13694, loss 0.0759301, acc 0.953125\n",
      "2018-05-23T14:07:36.091595: step 13695, loss 0.220272, acc 0.921875\n",
      "2018-05-23T14:07:36.576528: step 13696, loss 0.113638, acc 0.9375\n",
      "2018-05-23T14:07:37.079605: step 13697, loss 0.0899505, acc 0.953125\n",
      "2018-05-23T14:07:37.536410: step 13698, loss 0.27797, acc 0.875\n",
      "2018-05-23T14:07:37.978588: step 13699, loss 0.171758, acc 0.90625\n",
      "2018-05-23T14:07:38.417245: step 13700, loss 0.308863, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:07:45.146497: step 13700, loss 1.37407, acc 0.719531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13700\n",
      "\n",
      "2018-05-23T14:07:47.056346: step 13701, loss 0.130443, acc 0.921875\n",
      "2018-05-23T14:07:47.972812: step 13702, loss 0.0685972, acc 0.96875\n",
      "2018-05-23T14:07:48.490661: step 13703, loss 0.13729, acc 0.9375\n",
      "2018-05-23T14:07:49.050322: step 13704, loss 0.188846, acc 0.890625\n",
      "2018-05-23T14:07:49.596560: step 13705, loss 0.264265, acc 0.90625\n",
      "2018-05-23T14:07:50.231505: step 13706, loss 0.145392, acc 0.9375\n",
      "2018-05-23T14:07:50.977849: step 13707, loss 0.153008, acc 0.9375\n",
      "2018-05-23T14:07:51.474298: step 13708, loss 0.239111, acc 0.921875\n",
      "2018-05-23T14:07:51.969125: step 13709, loss 0.168433, acc 0.953125\n",
      "2018-05-23T14:07:52.447912: step 13710, loss 0.0885709, acc 0.96875\n",
      "2018-05-23T14:07:52.934748: step 13711, loss 0.175731, acc 0.96875\n",
      "2018-05-23T14:07:53.467121: step 13712, loss 0.131331, acc 0.96875\n",
      "2018-05-23T14:07:53.981060: step 13713, loss 0.202684, acc 0.921875\n",
      "2018-05-23T14:07:54.429584: step 13714, loss 0.133492, acc 0.953125\n",
      "2018-05-23T14:07:54.892526: step 13715, loss 0.176569, acc 0.90625\n",
      "2018-05-23T14:07:55.340385: step 13716, loss 0.169395, acc 0.953125\n",
      "2018-05-23T14:07:55.816030: step 13717, loss 0.0901675, acc 0.953125\n",
      "2018-05-23T14:07:56.259874: step 13718, loss 0.180774, acc 0.921875\n",
      "2018-05-23T14:07:56.709431: step 13719, loss 0.0928022, acc 0.984375\n",
      "2018-05-23T14:07:57.150453: step 13720, loss 0.164335, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:07:57.604330: step 13721, loss 0.149162, acc 0.953125\n",
      "2018-05-23T14:07:58.059268: step 13722, loss 0.108417, acc 0.96875\n",
      "2018-05-23T14:07:58.526099: step 13723, loss 0.0906433, acc 0.953125\n",
      "2018-05-23T14:07:59.116354: step 13724, loss 0.125006, acc 0.921875\n",
      "2018-05-23T14:07:59.867396: step 13725, loss 0.204604, acc 0.9375\n",
      "2018-05-23T14:08:00.892039: step 13726, loss 0.104129, acc 0.96875\n",
      "2018-05-23T14:08:02.012366: step 13727, loss 0.101741, acc 0.96875\n",
      "2018-05-23T14:08:02.518101: step 13728, loss 0.0952569, acc 0.984375\n",
      "2018-05-23T14:08:03.009921: step 13729, loss 0.099757, acc 0.953125\n",
      "2018-05-23T14:08:03.496987: step 13730, loss 0.152015, acc 0.921875\n",
      "2018-05-23T14:08:04.039133: step 13731, loss 0.109558, acc 0.953125\n",
      "2018-05-23T14:08:04.504191: step 13732, loss 0.305187, acc 0.921875\n",
      "2018-05-23T14:08:04.983824: step 13733, loss 0.0667412, acc 0.96875\n",
      "2018-05-23T14:08:05.415665: step 13734, loss 0.193083, acc 0.921875\n",
      "2018-05-23T14:08:05.874916: step 13735, loss 0.234668, acc 0.921875\n",
      "2018-05-23T14:08:06.352512: step 13736, loss 0.103385, acc 0.953125\n",
      "2018-05-23T14:08:06.838330: step 13737, loss 0.153277, acc 0.921875\n",
      "2018-05-23T14:08:07.308534: step 13738, loss 0.14237, acc 0.9375\n",
      "2018-05-23T14:08:07.768472: step 13739, loss 0.159181, acc 0.9375\n",
      "2018-05-23T14:08:08.244880: step 13740, loss 0.38226, acc 0.875\n",
      "2018-05-23T14:08:08.713545: step 13741, loss 0.0526987, acc 0.984375\n",
      "2018-05-23T14:08:09.176444: step 13742, loss 0.207519, acc 0.90625\n",
      "2018-05-23T14:08:09.642764: step 13743, loss 0.13486, acc 0.9375\n",
      "2018-05-23T14:08:10.097480: step 13744, loss 0.0239639, acc 1\n",
      "2018-05-23T14:08:10.534866: step 13745, loss 0.119657, acc 0.953125\n",
      "2018-05-23T14:08:11.047333: step 13746, loss 0.0877643, acc 0.96875\n",
      "2018-05-23T14:08:11.562505: step 13747, loss 0.0979283, acc 0.96875\n",
      "2018-05-23T14:08:12.063842: step 13748, loss 0.109891, acc 0.96875\n",
      "2018-05-23T14:08:12.516120: step 13749, loss 0.0870814, acc 0.984375\n",
      "2018-05-23T14:08:12.983782: step 13750, loss 0.109057, acc 0.953125\n",
      "2018-05-23T14:08:13.458669: step 13751, loss 0.191762, acc 0.9375\n",
      "2018-05-23T14:08:13.918009: step 13752, loss 0.121315, acc 0.9375\n",
      "2018-05-23T14:08:14.380439: step 13753, loss 0.134083, acc 0.921875\n",
      "2018-05-23T14:08:14.830546: step 13754, loss 0.168367, acc 0.921875\n",
      "2018-05-23T14:08:15.274992: step 13755, loss 0.0895389, acc 0.96875\n",
      "2018-05-23T14:08:15.731331: step 13756, loss 0.0417949, acc 0.984375\n",
      "2018-05-23T14:08:16.231927: step 13757, loss 0.150042, acc 0.953125\n",
      "2018-05-23T14:08:16.717205: step 13758, loss 0.124106, acc 0.96875\n",
      "2018-05-23T14:08:17.178813: step 13759, loss 0.104812, acc 0.96875\n",
      "2018-05-23T14:08:17.677762: step 13760, loss 0.0509179, acc 0.984375\n",
      "2018-05-23T14:08:18.139598: step 13761, loss 0.188954, acc 0.921875\n",
      "2018-05-23T14:08:18.619561: step 13762, loss 0.120258, acc 0.953125\n",
      "2018-05-23T14:08:19.066144: step 13763, loss 0.0814483, acc 0.953125\n",
      "2018-05-23T14:08:19.519856: step 13764, loss 0.175062, acc 0.953125\n",
      "2018-05-23T14:08:19.983709: step 13765, loss 0.0754692, acc 0.953125\n",
      "2018-05-23T14:08:20.424359: step 13766, loss 0.0871168, acc 0.953125\n",
      "2018-05-23T14:08:20.887575: step 13767, loss 0.170136, acc 0.90625\n",
      "2018-05-23T14:08:21.339432: step 13768, loss 0.0935764, acc 0.984375\n",
      "2018-05-23T14:08:21.843542: step 13769, loss 0.0869248, acc 0.96875\n",
      "2018-05-23T14:08:22.287517: step 13770, loss 0.147425, acc 0.9375\n",
      "2018-05-23T14:08:22.725693: step 13771, loss 0.0574117, acc 0.984375\n",
      "2018-05-23T14:08:23.182069: step 13772, loss 0.215723, acc 0.875\n",
      "2018-05-23T14:08:23.646352: step 13773, loss 0.0701118, acc 0.96875\n",
      "2018-05-23T14:08:24.109853: step 13774, loss 0.0849473, acc 0.9375\n",
      "2018-05-23T14:08:24.578563: step 13775, loss 0.254559, acc 0.921875\n",
      "2018-05-23T14:08:25.032593: step 13776, loss 0.15293, acc 0.953125\n",
      "2018-05-23T14:08:25.477418: step 13777, loss 0.122291, acc 0.9375\n",
      "2018-05-23T14:08:25.952544: step 13778, loss 0.224548, acc 0.890625\n",
      "2018-05-23T14:08:26.398498: step 13779, loss 0.131345, acc 0.9375\n",
      "2018-05-23T14:08:26.852579: step 13780, loss 0.119414, acc 0.953125\n",
      "2018-05-23T14:08:27.315216: step 13781, loss 0.232986, acc 0.9375\n",
      "2018-05-23T14:08:27.746421: step 13782, loss 0.131303, acc 0.921875\n",
      "2018-05-23T14:08:28.197183: step 13783, loss 0.263152, acc 0.890625\n",
      "2018-05-23T14:08:28.639696: step 13784, loss 0.131091, acc 0.921875\n",
      "2018-05-23T14:08:29.095799: step 13785, loss 0.066144, acc 0.984375\n",
      "2018-05-23T14:08:29.556293: step 13786, loss 0.14941, acc 0.921875\n",
      "2018-05-23T14:08:30.005878: step 13787, loss 0.0693937, acc 0.96875\n",
      "2018-05-23T14:08:30.460929: step 13788, loss 0.169674, acc 0.90625\n",
      "2018-05-23T14:08:30.919798: step 13789, loss 0.0879231, acc 0.96875\n",
      "2018-05-23T14:08:31.257123: step 13790, loss 0.0781893, acc 0.941176\n",
      "2018-05-23T14:08:31.753921: step 13791, loss 0.13721, acc 0.921875\n",
      "2018-05-23T14:08:32.193424: step 13792, loss 0.123892, acc 0.953125\n",
      "2018-05-23T14:08:32.663279: step 13793, loss 0.0652501, acc 0.984375\n",
      "2018-05-23T14:08:33.115051: step 13794, loss 0.135533, acc 0.9375\n",
      "2018-05-23T14:08:33.597563: step 13795, loss 0.166595, acc 0.90625\n",
      "2018-05-23T14:08:34.204558: step 13796, loss 0.147679, acc 0.953125\n",
      "2018-05-23T14:08:34.986869: step 13797, loss 0.0724479, acc 0.953125\n",
      "2018-05-23T14:08:35.586266: step 13798, loss 0.055308, acc 0.984375\n",
      "2018-05-23T14:08:36.397972: step 13799, loss 0.0794408, acc 0.96875\n",
      "2018-05-23T14:08:37.012676: step 13800, loss 0.105923, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:08:44.103947: step 13800, loss 1.40416, acc 0.717817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13800\n",
      "\n",
      "2018-05-23T14:08:46.899089: step 13801, loss 0.0789184, acc 0.984375\n",
      "2018-05-23T14:08:47.593278: step 13802, loss 0.129703, acc 0.90625\n",
      "2018-05-23T14:08:48.209841: step 13803, loss 0.13274, acc 0.96875\n",
      "2018-05-23T14:08:48.855422: step 13804, loss 0.0750611, acc 0.984375\n",
      "2018-05-23T14:08:49.451162: step 13805, loss 0.130554, acc 0.9375\n",
      "2018-05-23T14:08:50.090344: step 13806, loss 0.109006, acc 0.953125\n",
      "2018-05-23T14:08:50.860493: step 13807, loss 0.104879, acc 0.953125\n",
      "2018-05-23T14:08:51.506467: step 13808, loss 0.207965, acc 0.9375\n",
      "2018-05-23T14:08:52.147248: step 13809, loss 0.0503334, acc 1\n",
      "2018-05-23T14:08:52.813910: step 13810, loss 0.0622978, acc 0.96875\n",
      "2018-05-23T14:08:53.558935: step 13811, loss 0.0699593, acc 0.953125\n",
      "2018-05-23T14:08:54.186595: step 13812, loss 0.181124, acc 0.984375\n",
      "2018-05-23T14:08:54.870961: step 13813, loss 0.0513097, acc 0.984375\n",
      "2018-05-23T14:08:55.529355: step 13814, loss 0.0776131, acc 0.953125\n",
      "2018-05-23T14:08:56.189222: step 13815, loss 0.0368846, acc 0.984375\n",
      "2018-05-23T14:08:56.839565: step 13816, loss 0.102618, acc 0.9375\n",
      "2018-05-23T14:08:57.475335: step 13817, loss 0.0741618, acc 0.953125\n",
      "2018-05-23T14:08:58.217470: step 13818, loss 0.0679375, acc 0.984375\n",
      "2018-05-23T14:08:59.020309: step 13819, loss 0.112825, acc 0.9375\n",
      "2018-05-23T14:08:59.670224: step 13820, loss 0.0758437, acc 0.984375\n",
      "2018-05-23T14:09:00.372910: step 13821, loss 0.103841, acc 0.9375\n",
      "2018-05-23T14:09:01.010535: step 13822, loss 0.0965363, acc 0.953125\n",
      "2018-05-23T14:09:01.654726: step 13823, loss 0.035786, acc 0.984375\n",
      "2018-05-23T14:09:02.289022: step 13824, loss 0.0952202, acc 0.9375\n",
      "2018-05-23T14:09:02.992299: step 13825, loss 0.0998263, acc 0.9375\n",
      "2018-05-23T14:09:03.671876: step 13826, loss 0.239309, acc 0.890625\n",
      "2018-05-23T14:09:04.318626: step 13827, loss 0.0799059, acc 0.953125\n",
      "2018-05-23T14:09:04.924436: step 13828, loss 0.124854, acc 0.953125\n",
      "2018-05-23T14:09:05.552416: step 13829, loss 0.111088, acc 0.9375\n",
      "2018-05-23T14:09:06.326821: step 13830, loss 0.268627, acc 0.953125\n",
      "2018-05-23T14:09:06.948779: step 13831, loss 0.0565671, acc 0.984375\n",
      "2018-05-23T14:09:07.694694: step 13832, loss 0.120925, acc 0.953125\n",
      "2018-05-23T14:09:08.399031: step 13833, loss 0.0744144, acc 0.96875\n",
      "2018-05-23T14:09:09.141062: step 13834, loss 0.0832405, acc 0.96875\n",
      "2018-05-23T14:09:09.814619: step 13835, loss 0.0576983, acc 0.984375\n",
      "2018-05-23T14:09:10.509005: step 13836, loss 0.0869802, acc 0.96875\n",
      "2018-05-23T14:09:11.082456: step 13837, loss 0.0989959, acc 0.9375\n",
      "2018-05-23T14:09:11.710212: step 13838, loss 0.0975629, acc 0.96875\n",
      "2018-05-23T14:09:12.329682: step 13839, loss 0.206285, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:09:12.952000: step 13840, loss 0.14251, acc 0.921875\n",
      "2018-05-23T14:09:13.695112: step 13841, loss 0.255725, acc 0.9375\n",
      "2018-05-23T14:09:14.334580: step 13842, loss 0.0545055, acc 0.984375\n",
      "2018-05-23T14:09:15.024153: step 13843, loss 0.0531535, acc 1\n",
      "2018-05-23T14:09:15.743060: step 13844, loss 0.21566, acc 0.90625\n",
      "2018-05-23T14:09:16.400935: step 13845, loss 0.0570038, acc 0.984375\n",
      "2018-05-23T14:09:17.098190: step 13846, loss 0.129411, acc 0.96875\n",
      "2018-05-23T14:09:17.839753: step 13847, loss 0.138655, acc 0.9375\n",
      "2018-05-23T14:09:18.712774: step 13848, loss 0.0939752, acc 0.96875\n",
      "2018-05-23T14:09:19.447052: step 13849, loss 0.0892714, acc 0.953125\n",
      "2018-05-23T14:09:20.112513: step 13850, loss 0.193199, acc 0.890625\n",
      "2018-05-23T14:09:20.902285: step 13851, loss 0.0858448, acc 0.96875\n",
      "2018-05-23T14:09:21.606358: step 13852, loss 0.0900537, acc 0.953125\n",
      "2018-05-23T14:09:22.346195: step 13853, loss 0.0432413, acc 1\n",
      "2018-05-23T14:09:23.124850: step 13854, loss 0.109737, acc 0.96875\n",
      "2018-05-23T14:09:23.627977: step 13855, loss 0.088139, acc 0.96875\n",
      "2018-05-23T14:09:24.057152: step 13856, loss 0.0373054, acc 1\n",
      "2018-05-23T14:09:24.500418: step 13857, loss 0.107748, acc 0.96875\n",
      "2018-05-23T14:09:24.939786: step 13858, loss 0.0417129, acc 0.96875\n",
      "2018-05-23T14:09:25.387132: step 13859, loss 0.0892617, acc 0.9375\n",
      "2018-05-23T14:09:25.813979: step 13860, loss 0.145503, acc 0.953125\n",
      "2018-05-23T14:09:26.274482: step 13861, loss 0.152434, acc 0.921875\n",
      "2018-05-23T14:09:26.729704: step 13862, loss 0.106901, acc 0.953125\n",
      "2018-05-23T14:09:27.164871: step 13863, loss 0.0564835, acc 0.96875\n",
      "2018-05-23T14:09:27.612053: step 13864, loss 0.157001, acc 0.953125\n",
      "2018-05-23T14:09:28.065840: step 13865, loss 0.0596499, acc 0.96875\n",
      "2018-05-23T14:09:28.521941: step 13866, loss 0.112086, acc 0.953125\n",
      "2018-05-23T14:09:28.950976: step 13867, loss 0.0814664, acc 0.984375\n",
      "2018-05-23T14:09:29.403781: step 13868, loss 0.117489, acc 0.953125\n",
      "2018-05-23T14:09:29.864773: step 13869, loss 0.0494173, acc 0.984375\n",
      "2018-05-23T14:09:30.304773: step 13870, loss 0.0899451, acc 0.953125\n",
      "2018-05-23T14:09:30.753607: step 13871, loss 0.0593441, acc 0.984375\n",
      "2018-05-23T14:09:31.184961: step 13872, loss 0.193814, acc 0.90625\n",
      "2018-05-23T14:09:31.620742: step 13873, loss 0.134244, acc 0.9375\n",
      "2018-05-23T14:09:32.075215: step 13874, loss 0.0369066, acc 1\n",
      "2018-05-23T14:09:32.530202: step 13875, loss 0.100116, acc 0.9375\n",
      "2018-05-23T14:09:32.978613: step 13876, loss 0.096042, acc 0.9375\n",
      "2018-05-23T14:09:33.418822: step 13877, loss 0.114204, acc 0.9375\n",
      "2018-05-23T14:09:33.855134: step 13878, loss 0.115244, acc 0.9375\n",
      "2018-05-23T14:09:34.320928: step 13879, loss 0.119072, acc 0.96875\n",
      "2018-05-23T14:09:34.755575: step 13880, loss 0.103203, acc 0.96875\n",
      "2018-05-23T14:09:35.209133: step 13881, loss 0.106009, acc 0.96875\n",
      "2018-05-23T14:09:35.667341: step 13882, loss 0.0966604, acc 0.96875\n",
      "2018-05-23T14:09:36.124501: step 13883, loss 0.096041, acc 0.953125\n",
      "2018-05-23T14:09:36.569025: step 13884, loss 0.194349, acc 0.890625\n",
      "2018-05-23T14:09:37.051694: step 13885, loss 0.0677665, acc 0.953125\n",
      "2018-05-23T14:09:37.532015: step 13886, loss 0.0723438, acc 0.96875\n",
      "2018-05-23T14:09:38.020274: step 13887, loss 0.108946, acc 0.9375\n",
      "2018-05-23T14:09:38.463710: step 13888, loss 0.0599261, acc 0.96875\n",
      "2018-05-23T14:09:38.938649: step 13889, loss 0.107174, acc 0.953125\n",
      "2018-05-23T14:09:39.376062: step 13890, loss 0.144241, acc 0.921875\n",
      "2018-05-23T14:09:40.022518: step 13891, loss 0.0845139, acc 0.9375\n",
      "2018-05-23T14:09:40.577744: step 13892, loss 0.0746462, acc 0.984375\n",
      "2018-05-23T14:09:41.077293: step 13893, loss 0.277001, acc 0.9375\n",
      "2018-05-23T14:09:41.552795: step 13894, loss 0.0699564, acc 0.96875\n",
      "2018-05-23T14:09:42.011159: step 13895, loss 0.139153, acc 0.90625\n",
      "2018-05-23T14:09:42.469047: step 13896, loss 0.191902, acc 0.9375\n",
      "2018-05-23T14:09:42.931454: step 13897, loss 0.200036, acc 0.90625\n",
      "2018-05-23T14:09:43.384785: step 13898, loss 0.0781987, acc 0.984375\n",
      "2018-05-23T14:09:43.823719: step 13899, loss 0.069024, acc 0.984375\n",
      "2018-05-23T14:09:44.273395: step 13900, loss 0.07996, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:09:49.826302: step 13900, loss 1.44619, acc 0.71053\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-13900\n",
      "\n",
      "2018-05-23T14:09:52.097175: step 13901, loss 0.103484, acc 0.953125\n",
      "2018-05-23T14:09:52.557063: step 13902, loss 0.110299, acc 0.921875\n",
      "2018-05-23T14:09:53.016300: step 13903, loss 0.207985, acc 0.921875\n",
      "2018-05-23T14:09:53.508594: step 13904, loss 0.165365, acc 0.921875\n",
      "2018-05-23T14:09:53.979749: step 13905, loss 0.0767736, acc 0.984375\n",
      "2018-05-23T14:09:54.450302: step 13906, loss 0.12496, acc 0.90625\n",
      "2018-05-23T14:09:54.905213: step 13907, loss 0.102496, acc 0.921875\n",
      "2018-05-23T14:09:55.333252: step 13908, loss 0.0699987, acc 0.96875\n",
      "2018-05-23T14:09:55.797856: step 13909, loss 0.0615062, acc 0.984375\n",
      "2018-05-23T14:09:56.231159: step 13910, loss 0.110209, acc 0.953125\n",
      "2018-05-23T14:09:56.684526: step 13911, loss 0.124133, acc 0.9375\n",
      "2018-05-23T14:09:57.119830: step 13912, loss 0.0803703, acc 0.984375\n",
      "2018-05-23T14:09:57.579442: step 13913, loss 0.0598321, acc 0.96875\n",
      "2018-05-23T14:09:58.051059: step 13914, loss 0.1235, acc 0.921875\n",
      "2018-05-23T14:09:58.478719: step 13915, loss 0.0603532, acc 0.984375\n",
      "2018-05-23T14:09:58.943874: step 13916, loss 0.0871624, acc 0.96875\n",
      "2018-05-23T14:09:59.385314: step 13917, loss 0.0786206, acc 0.96875\n",
      "2018-05-23T14:09:59.831647: step 13918, loss 0.0894579, acc 0.96875\n",
      "2018-05-23T14:10:00.304840: step 13919, loss 0.0351224, acc 1\n",
      "2018-05-23T14:10:00.750713: step 13920, loss 0.0855567, acc 0.96875\n",
      "2018-05-23T14:10:01.188494: step 13921, loss 0.0810013, acc 0.96875\n",
      "2018-05-23T14:10:01.653522: step 13922, loss 0.262579, acc 0.921875\n",
      "2018-05-23T14:10:02.155837: step 13923, loss 0.108935, acc 0.953125\n",
      "2018-05-23T14:10:02.634970: step 13924, loss 0.104521, acc 0.953125\n",
      "2018-05-23T14:10:03.135564: step 13925, loss 0.0562315, acc 0.984375\n",
      "2018-05-23T14:10:03.591445: step 13926, loss 0.109649, acc 0.96875\n",
      "2018-05-23T14:10:04.052025: step 13927, loss 0.113997, acc 0.953125\n",
      "2018-05-23T14:10:04.499433: step 13928, loss 0.112535, acc 0.953125\n",
      "2018-05-23T14:10:05.117156: step 13929, loss 0.118841, acc 0.921875\n",
      "2018-05-23T14:10:05.558098: step 13930, loss 0.0772804, acc 0.984375\n",
      "2018-05-23T14:10:06.061825: step 13931, loss 0.116536, acc 0.9375\n",
      "2018-05-23T14:10:06.528898: step 13932, loss 0.112519, acc 0.953125\n",
      "2018-05-23T14:10:07.027762: step 13933, loss 0.134134, acc 0.9375\n",
      "2018-05-23T14:10:07.488088: step 13934, loss 0.0738894, acc 0.96875\n",
      "2018-05-23T14:10:07.933862: step 13935, loss 0.106135, acc 0.9375\n",
      "2018-05-23T14:10:08.390757: step 13936, loss 0.209413, acc 0.90625\n",
      "2018-05-23T14:10:08.903233: step 13937, loss 0.0529587, acc 0.984375\n",
      "2018-05-23T14:10:09.350857: step 13938, loss 0.0605704, acc 0.96875\n",
      "2018-05-23T14:10:09.823408: step 13939, loss 0.121427, acc 0.9375\n",
      "2018-05-23T14:10:10.271787: step 13940, loss 0.197366, acc 0.90625\n",
      "2018-05-23T14:10:10.722988: step 13941, loss 0.0674821, acc 1\n",
      "2018-05-23T14:10:11.162238: step 13942, loss 0.0789344, acc 0.953125\n",
      "2018-05-23T14:10:11.790644: step 13943, loss 0.0751202, acc 0.984375\n",
      "2018-05-23T14:10:12.254869: step 13944, loss 0.133298, acc 0.921875\n",
      "2018-05-23T14:10:12.750465: step 13945, loss 0.115245, acc 0.953125\n",
      "2018-05-23T14:10:13.308444: step 13946, loss 0.10394, acc 0.953125\n",
      "2018-05-23T14:10:13.846661: step 13947, loss 0.114381, acc 0.9375\n",
      "2018-05-23T14:10:14.367146: step 13948, loss 0.0746698, acc 0.984375\n",
      "2018-05-23T14:10:14.849867: step 13949, loss 0.0773526, acc 0.96875\n",
      "2018-05-23T14:10:15.301322: step 13950, loss 0.0892265, acc 0.96875\n",
      "2018-05-23T14:10:15.771158: step 13951, loss 0.177911, acc 0.921875\n",
      "2018-05-23T14:10:16.277148: step 13952, loss 0.145866, acc 0.9375\n",
      "2018-05-23T14:10:16.738595: step 13953, loss 0.0367529, acc 1\n",
      "2018-05-23T14:10:17.237626: step 13954, loss 0.21155, acc 0.921875\n",
      "2018-05-23T14:10:17.695036: step 13955, loss 0.170767, acc 0.953125\n",
      "2018-05-23T14:10:18.187132: step 13956, loss 0.0658574, acc 0.96875\n",
      "2018-05-23T14:10:18.634704: step 13957, loss 0.160601, acc 0.921875\n",
      "2018-05-23T14:10:19.110897: step 13958, loss 0.068455, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:10:19.557212: step 13959, loss 0.185729, acc 0.921875\n",
      "2018-05-23T14:10:20.038500: step 13960, loss 0.0898572, acc 0.984375\n",
      "2018-05-23T14:10:20.497677: step 13961, loss 0.131369, acc 0.96875\n",
      "2018-05-23T14:10:20.957932: step 13962, loss 0.0486173, acc 1\n",
      "2018-05-23T14:10:21.399464: step 13963, loss 0.0867396, acc 0.96875\n",
      "2018-05-23T14:10:21.842015: step 13964, loss 0.128396, acc 0.9375\n",
      "2018-05-23T14:10:22.315767: step 13965, loss 0.0789028, acc 0.96875\n",
      "2018-05-23T14:10:22.751981: step 13966, loss 0.0768216, acc 0.953125\n",
      "2018-05-23T14:10:23.214838: step 13967, loss 0.110662, acc 0.953125\n",
      "2018-05-23T14:10:23.666472: step 13968, loss 0.183793, acc 0.890625\n",
      "2018-05-23T14:10:24.124815: step 13969, loss 0.12451, acc 0.921875\n",
      "2018-05-23T14:10:24.582538: step 13970, loss 0.0901855, acc 0.96875\n",
      "2018-05-23T14:10:25.039416: step 13971, loss 0.125031, acc 0.9375\n",
      "2018-05-23T14:10:25.488364: step 13972, loss 0.066523, acc 0.96875\n",
      "2018-05-23T14:10:25.915016: step 13973, loss 0.0986004, acc 0.96875\n",
      "2018-05-23T14:10:26.381913: step 13974, loss 0.197105, acc 0.9375\n",
      "2018-05-23T14:10:26.817685: step 13975, loss 0.107295, acc 0.953125\n",
      "2018-05-23T14:10:27.265232: step 13976, loss 0.151461, acc 0.9375\n",
      "2018-05-23T14:10:27.715282: step 13977, loss 0.138186, acc 0.953125\n",
      "2018-05-23T14:10:28.170375: step 13978, loss 0.227528, acc 0.890625\n",
      "2018-05-23T14:10:28.630191: step 13979, loss 0.184184, acc 0.953125\n",
      "2018-05-23T14:10:29.068027: step 13980, loss 0.188203, acc 0.90625\n",
      "2018-05-23T14:10:29.515493: step 13981, loss 0.126993, acc 0.96875\n",
      "2018-05-23T14:10:29.978890: step 13982, loss 0.0943235, acc 0.953125\n",
      "2018-05-23T14:10:30.437109: step 13983, loss 0.0913262, acc 0.96875\n",
      "2018-05-23T14:10:30.890515: step 13984, loss 0.103392, acc 0.9375\n",
      "2018-05-23T14:10:31.345724: step 13985, loss 0.237249, acc 0.875\n",
      "2018-05-23T14:10:31.791021: step 13986, loss 0.159594, acc 0.9375\n",
      "2018-05-23T14:10:32.235151: step 13987, loss 0.0986155, acc 0.96875\n",
      "2018-05-23T14:10:32.712071: step 13988, loss 0.0687238, acc 0.96875\n",
      "2018-05-23T14:10:33.152869: step 13989, loss 0.13678, acc 0.953125\n",
      "2018-05-23T14:10:33.621318: step 13990, loss 0.042337, acc 0.984375\n",
      "2018-05-23T14:10:34.056282: step 13991, loss 0.064486, acc 0.984375\n",
      "2018-05-23T14:10:34.500299: step 13992, loss 0.109796, acc 0.953125\n",
      "2018-05-23T14:10:35.503507: step 13993, loss 0.0607175, acc 0.984375\n",
      "2018-05-23T14:10:36.250797: step 13994, loss 0.15524, acc 0.9375\n",
      "2018-05-23T14:10:36.916336: step 13995, loss 0.122668, acc 0.9375\n",
      "2018-05-23T14:10:37.514921: step 13996, loss 0.0657423, acc 0.96875\n",
      "2018-05-23T14:10:38.150572: step 13997, loss 0.109492, acc 0.96875\n",
      "2018-05-23T14:10:38.813244: step 13998, loss 0.0595011, acc 1\n",
      "2018-05-23T14:10:39.440722: step 13999, loss 0.0588025, acc 0.96875\n",
      "2018-05-23T14:10:40.122912: step 14000, loss 0.0591965, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:10:47.249775: step 14000, loss 1.45743, acc 0.713959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14000\n",
      "\n",
      "2018-05-23T14:10:49.833717: step 14001, loss 0.0994945, acc 0.953125\n",
      "2018-05-23T14:10:50.435305: step 14002, loss 0.118676, acc 0.96875\n",
      "2018-05-23T14:10:51.109024: step 14003, loss 0.127686, acc 0.921875\n",
      "2018-05-23T14:10:51.790514: step 14004, loss 0.0423323, acc 1\n",
      "2018-05-23T14:10:52.395603: step 14005, loss 0.0584138, acc 0.984375\n",
      "2018-05-23T14:10:53.017138: step 14006, loss 0.0707525, acc 0.984375\n",
      "2018-05-23T14:10:53.676189: step 14007, loss 0.08009, acc 0.953125\n",
      "2018-05-23T14:10:54.350217: step 14008, loss 0.0885487, acc 0.96875\n",
      "2018-05-23T14:10:54.974428: step 14009, loss 0.0563672, acc 0.984375\n",
      "2018-05-23T14:10:55.578540: step 14010, loss 0.158027, acc 0.921875\n",
      "2018-05-23T14:10:56.223124: step 14011, loss 0.0635315, acc 0.953125\n",
      "2018-05-23T14:10:56.840844: step 14012, loss 0.127753, acc 0.953125\n",
      "2018-05-23T14:10:57.505261: step 14013, loss 0.0669852, acc 0.96875\n",
      "2018-05-23T14:10:58.149048: step 14014, loss 0.14232, acc 0.96875\n",
      "2018-05-23T14:10:58.775986: step 14015, loss 0.074892, acc 0.96875\n",
      "2018-05-23T14:10:59.381856: step 14016, loss 0.0841284, acc 0.953125\n",
      "2018-05-23T14:11:00.086763: step 14017, loss 0.0990323, acc 0.953125\n",
      "2018-05-23T14:11:00.684284: step 14018, loss 0.142759, acc 0.953125\n",
      "2018-05-23T14:11:01.330305: step 14019, loss 0.0899468, acc 0.953125\n",
      "2018-05-23T14:11:02.010512: step 14020, loss 0.0915729, acc 0.96875\n",
      "2018-05-23T14:11:02.652127: step 14021, loss 0.232537, acc 0.921875\n",
      "2018-05-23T14:11:03.273762: step 14022, loss 0.150081, acc 0.921875\n",
      "2018-05-23T14:11:03.926431: step 14023, loss 0.12187, acc 0.9375\n",
      "2018-05-23T14:11:04.636785: step 14024, loss 0.152883, acc 0.90625\n",
      "2018-05-23T14:11:05.253706: step 14025, loss 0.0397605, acc 0.984375\n",
      "2018-05-23T14:11:05.954577: step 14026, loss 0.154636, acc 0.9375\n",
      "2018-05-23T14:11:06.605326: step 14027, loss 0.378308, acc 0.859375\n",
      "2018-05-23T14:11:07.265265: step 14028, loss 0.123697, acc 0.953125\n",
      "2018-05-23T14:11:07.871162: step 14029, loss 0.119705, acc 0.921875\n",
      "2018-05-23T14:11:08.528849: step 14030, loss 0.157296, acc 0.921875\n",
      "2018-05-23T14:11:09.183473: step 14031, loss 0.129496, acc 0.9375\n",
      "2018-05-23T14:11:09.812780: step 14032, loss 0.155919, acc 0.953125\n",
      "2018-05-23T14:11:10.527269: step 14033, loss 0.101876, acc 0.953125\n",
      "2018-05-23T14:11:11.192655: step 14034, loss 0.0791705, acc 0.9375\n",
      "2018-05-23T14:11:11.843707: step 14035, loss 0.102966, acc 0.953125\n",
      "2018-05-23T14:11:12.518515: step 14036, loss 0.0907464, acc 0.984375\n",
      "2018-05-23T14:11:13.208535: step 14037, loss 0.0664305, acc 0.984375\n",
      "2018-05-23T14:11:13.913812: step 14038, loss 0.0571304, acc 1\n",
      "2018-05-23T14:11:14.537302: step 14039, loss 0.0532259, acc 0.96875\n",
      "2018-05-23T14:11:15.184969: step 14040, loss 0.149982, acc 0.90625\n",
      "2018-05-23T14:11:15.853758: step 14041, loss 0.0864579, acc 0.984375\n",
      "2018-05-23T14:11:16.645529: step 14042, loss 0.231472, acc 0.90625\n",
      "2018-05-23T14:11:17.344057: step 14043, loss 0.0876293, acc 0.96875\n",
      "2018-05-23T14:11:17.980353: step 14044, loss 0.118824, acc 0.9375\n",
      "2018-05-23T14:11:18.631382: step 14045, loss 0.0698358, acc 0.953125\n",
      "2018-05-23T14:11:19.278298: step 14046, loss 0.0933224, acc 0.953125\n",
      "2018-05-23T14:11:19.931200: step 14047, loss 0.220944, acc 0.921875\n",
      "2018-05-23T14:11:20.541912: step 14048, loss 0.10226, acc 0.953125\n",
      "2018-05-23T14:11:21.227089: step 14049, loss 0.105095, acc 0.953125\n",
      "2018-05-23T14:11:21.877212: step 14050, loss 0.137499, acc 0.921875\n",
      "2018-05-23T14:11:22.485707: step 14051, loss 0.0460534, acc 0.984375\n",
      "2018-05-23T14:11:23.115501: step 14052, loss 0.0939291, acc 0.953125\n",
      "2018-05-23T14:11:23.856904: step 14053, loss 0.177946, acc 0.953125\n",
      "2018-05-23T14:11:24.469965: step 14054, loss 0.0728936, acc 0.96875\n",
      "2018-05-23T14:11:25.132978: step 14055, loss 0.078479, acc 0.984375\n",
      "2018-05-23T14:11:25.998889: step 14056, loss 0.075764, acc 0.96875\n",
      "2018-05-23T14:11:26.682791: step 14057, loss 0.04958, acc 1\n",
      "2018-05-23T14:11:27.326207: step 14058, loss 0.0780299, acc 0.96875\n",
      "2018-05-23T14:11:27.963488: step 14059, loss 0.0671485, acc 0.96875\n",
      "2018-05-23T14:11:28.715955: step 14060, loss 0.108134, acc 0.96875\n",
      "2018-05-23T14:11:29.432712: step 14061, loss 0.148881, acc 0.9375\n",
      "2018-05-23T14:11:30.072521: step 14062, loss 0.114028, acc 0.953125\n",
      "2018-05-23T14:11:30.672697: step 14063, loss 0.136157, acc 0.9375\n",
      "2018-05-23T14:11:31.288294: step 14064, loss 0.0404324, acc 1\n",
      "2018-05-23T14:11:31.912492: step 14065, loss 0.243267, acc 0.890625\n",
      "2018-05-23T14:11:32.524670: step 14066, loss 0.190276, acc 0.9375\n",
      "2018-05-23T14:11:33.206965: step 14067, loss 0.0513607, acc 0.96875\n",
      "2018-05-23T14:11:33.811097: step 14068, loss 0.0559208, acc 0.984375\n",
      "2018-05-23T14:11:34.444876: step 14069, loss 0.116759, acc 0.96875\n",
      "2018-05-23T14:11:35.109549: step 14070, loss 0.0574753, acc 0.984375\n",
      "2018-05-23T14:11:35.744373: step 14071, loss 0.0930623, acc 0.953125\n",
      "2018-05-23T14:11:36.453955: step 14072, loss 0.0883049, acc 0.96875\n",
      "2018-05-23T14:11:37.075056: step 14073, loss 0.159225, acc 0.90625\n",
      "2018-05-23T14:11:37.748745: step 14074, loss 0.213068, acc 0.9375\n",
      "2018-05-23T14:11:38.384813: step 14075, loss 0.0546981, acc 0.96875\n",
      "2018-05-23T14:11:39.004807: step 14076, loss 0.0864176, acc 0.9375\n",
      "2018-05-23T14:11:39.646103: step 14077, loss 0.11709, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:11:40.331260: step 14078, loss 0.135778, acc 0.921875\n",
      "2018-05-23T14:11:40.930607: step 14079, loss 0.178705, acc 0.921875\n",
      "2018-05-23T14:11:41.550490: step 14080, loss 0.118532, acc 0.9375\n",
      "2018-05-23T14:11:42.177111: step 14081, loss 0.113957, acc 0.921875\n",
      "2018-05-23T14:11:42.893962: step 14082, loss 0.120315, acc 0.953125\n",
      "2018-05-23T14:11:43.592175: step 14083, loss 0.123931, acc 0.953125\n",
      "2018-05-23T14:11:44.297690: step 14084, loss 0.106638, acc 0.953125\n",
      "2018-05-23T14:11:44.914018: step 14085, loss 0.173086, acc 0.953125\n",
      "2018-05-23T14:11:45.521594: step 14086, loss 0.106523, acc 0.96875\n",
      "2018-05-23T14:11:46.111265: step 14087, loss 0.198353, acc 0.953125\n",
      "2018-05-23T14:11:46.731871: step 14088, loss 0.0429132, acc 0.984375\n",
      "2018-05-23T14:11:47.352402: step 14089, loss 0.119941, acc 0.953125\n",
      "2018-05-23T14:11:47.982191: step 14090, loss 0.0856327, acc 0.96875\n",
      "2018-05-23T14:11:48.612099: step 14091, loss 0.102952, acc 0.9375\n",
      "2018-05-23T14:11:49.226650: step 14092, loss 0.0945969, acc 0.9375\n",
      "2018-05-23T14:11:49.882218: step 14093, loss 0.147181, acc 0.984375\n",
      "2018-05-23T14:11:50.498127: step 14094, loss 0.0907546, acc 0.953125\n",
      "2018-05-23T14:11:51.148300: step 14095, loss 0.21805, acc 0.90625\n",
      "2018-05-23T14:11:51.753028: step 14096, loss 0.107849, acc 0.953125\n",
      "2018-05-23T14:11:52.437402: step 14097, loss 0.0683466, acc 0.984375\n",
      "2018-05-23T14:11:53.088539: step 14098, loss 0.0359865, acc 1\n",
      "2018-05-23T14:11:53.757423: step 14099, loss 0.130543, acc 0.96875\n",
      "2018-05-23T14:11:54.393132: step 14100, loss 0.182804, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:12:01.250460: step 14100, loss 1.4901, acc 0.712959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14100\n",
      "\n",
      "2018-05-23T14:12:03.877637: step 14101, loss 0.0826056, acc 0.9375\n",
      "2018-05-23T14:12:04.527390: step 14102, loss 0.223878, acc 0.9375\n",
      "2018-05-23T14:12:05.161210: step 14103, loss 0.125533, acc 0.9375\n",
      "2018-05-23T14:12:05.802638: step 14104, loss 0.143735, acc 0.90625\n",
      "2018-05-23T14:12:06.444959: step 14105, loss 0.0493514, acc 0.984375\n",
      "2018-05-23T14:12:07.074189: step 14106, loss 0.151115, acc 0.921875\n",
      "2018-05-23T14:12:07.685774: step 14107, loss 0.0694771, acc 0.96875\n",
      "2018-05-23T14:12:08.456008: step 14108, loss 0.0947184, acc 0.9375\n",
      "2018-05-23T14:12:09.234399: step 14109, loss 0.317887, acc 0.859375\n",
      "2018-05-23T14:12:09.853880: step 14110, loss 0.0885827, acc 0.96875\n",
      "2018-05-23T14:12:10.502162: step 14111, loss 0.12894, acc 0.9375\n",
      "2018-05-23T14:12:11.112379: step 14112, loss 0.120266, acc 0.9375\n",
      "2018-05-23T14:12:11.777025: step 14113, loss 0.0973127, acc 0.9375\n",
      "2018-05-23T14:12:12.411890: step 14114, loss 0.0854514, acc 0.96875\n",
      "2018-05-23T14:12:13.223462: step 14115, loss 0.0836406, acc 0.984375\n",
      "2018-05-23T14:12:13.866926: step 14116, loss 0.0494168, acc 1\n",
      "2018-05-23T14:12:14.494788: step 14117, loss 0.0605718, acc 0.96875\n",
      "2018-05-23T14:12:15.149634: step 14118, loss 0.107696, acc 0.9375\n",
      "2018-05-23T14:12:15.777634: step 14119, loss 0.10547, acc 0.953125\n",
      "2018-05-23T14:12:16.395332: step 14120, loss 0.0800683, acc 0.96875\n",
      "2018-05-23T14:12:17.016260: step 14121, loss 0.126424, acc 0.953125\n",
      "2018-05-23T14:12:17.636955: step 14122, loss 0.140593, acc 0.921875\n",
      "2018-05-23T14:12:18.269373: step 14123, loss 0.108301, acc 0.953125\n",
      "2018-05-23T14:12:18.922372: step 14124, loss 0.0700484, acc 0.984375\n",
      "2018-05-23T14:12:19.634808: step 14125, loss 0.112571, acc 0.953125\n",
      "2018-05-23T14:12:20.343885: step 14126, loss 0.0846656, acc 0.984375\n",
      "2018-05-23T14:12:21.011126: step 14127, loss 0.0668833, acc 0.984375\n",
      "2018-05-23T14:12:21.644413: step 14128, loss 0.143742, acc 0.921875\n",
      "2018-05-23T14:12:22.319369: step 14129, loss 0.0657397, acc 0.984375\n",
      "2018-05-23T14:12:23.039865: step 14130, loss 0.0910166, acc 0.96875\n",
      "2018-05-23T14:12:23.638764: step 14131, loss 0.0558939, acc 0.96875\n",
      "2018-05-23T14:12:24.348974: step 14132, loss 0.0691851, acc 0.984375\n",
      "2018-05-23T14:12:25.012691: step 14133, loss 0.0867508, acc 0.96875\n",
      "2018-05-23T14:12:25.913174: step 14134, loss 0.318235, acc 0.875\n",
      "2018-05-23T14:12:26.682958: step 14135, loss 0.158887, acc 0.9375\n",
      "2018-05-23T14:12:27.471315: step 14136, loss 0.0754852, acc 0.984375\n",
      "2018-05-23T14:12:28.093753: step 14137, loss 0.144532, acc 0.96875\n",
      "2018-05-23T14:12:28.820848: step 14138, loss 0.114783, acc 0.953125\n",
      "2018-05-23T14:12:29.570945: step 14139, loss 0.130521, acc 0.9375\n",
      "2018-05-23T14:12:30.302524: step 14140, loss 0.0806208, acc 0.96875\n",
      "2018-05-23T14:12:31.021894: step 14141, loss 0.0749092, acc 0.984375\n",
      "2018-05-23T14:12:31.739095: step 14142, loss 0.0537059, acc 0.984375\n",
      "2018-05-23T14:12:32.468263: step 14143, loss 0.0707747, acc 0.953125\n",
      "2018-05-23T14:12:33.149120: step 14144, loss 0.164855, acc 0.90625\n",
      "2018-05-23T14:12:33.829756: step 14145, loss 0.28759, acc 0.921875\n",
      "2018-05-23T14:12:34.551023: step 14146, loss 0.0599294, acc 0.96875\n",
      "2018-05-23T14:12:35.289158: step 14147, loss 0.0802567, acc 0.984375\n",
      "2018-05-23T14:12:36.084873: step 14148, loss 0.0914848, acc 0.953125\n",
      "2018-05-23T14:12:36.824738: step 14149, loss 0.0569108, acc 0.984375\n",
      "2018-05-23T14:12:37.536935: step 14150, loss 0.143544, acc 0.921875\n",
      "2018-05-23T14:12:38.254454: step 14151, loss 0.0898303, acc 0.9375\n",
      "2018-05-23T14:12:39.055073: step 14152, loss 0.0926255, acc 0.953125\n",
      "2018-05-23T14:12:39.788499: step 14153, loss 0.22194, acc 0.890625\n",
      "2018-05-23T14:12:40.564512: step 14154, loss 0.115186, acc 0.96875\n",
      "2018-05-23T14:12:41.278857: step 14155, loss 0.166372, acc 0.953125\n",
      "2018-05-23T14:12:41.994696: step 14156, loss 0.134805, acc 0.9375\n",
      "2018-05-23T14:12:42.689331: step 14157, loss 0.171674, acc 0.9375\n",
      "2018-05-23T14:12:43.392054: step 14158, loss 0.192485, acc 0.921875\n",
      "2018-05-23T14:12:44.120240: step 14159, loss 0.17673, acc 0.921875\n",
      "2018-05-23T14:12:44.770618: step 14160, loss 0.198327, acc 0.9375\n",
      "2018-05-23T14:12:45.500951: step 14161, loss 0.204274, acc 0.90625\n",
      "2018-05-23T14:12:46.199012: step 14162, loss 0.146111, acc 0.96875\n",
      "2018-05-23T14:12:46.932684: step 14163, loss 0.144269, acc 0.9375\n",
      "2018-05-23T14:12:47.658051: step 14164, loss 0.195974, acc 0.90625\n",
      "2018-05-23T14:12:48.387929: step 14165, loss 0.112676, acc 0.90625\n",
      "2018-05-23T14:12:49.117102: step 14166, loss 0.133112, acc 0.953125\n",
      "2018-05-23T14:12:49.822859: step 14167, loss 0.0812855, acc 0.984375\n",
      "2018-05-23T14:12:50.584611: step 14168, loss 0.170907, acc 0.921875\n",
      "2018-05-23T14:12:51.356797: step 14169, loss 0.0809955, acc 0.96875\n",
      "2018-05-23T14:12:52.119939: step 14170, loss 0.100887, acc 0.9375\n",
      "2018-05-23T14:12:52.728938: step 14171, loss 0.134294, acc 0.9375\n",
      "2018-05-23T14:12:53.432900: step 14172, loss 0.0910509, acc 0.953125\n",
      "2018-05-23T14:12:54.135280: step 14173, loss 0.0434773, acc 1\n",
      "2018-05-23T14:12:54.973977: step 14174, loss 0.103989, acc 0.953125\n",
      "2018-05-23T14:12:55.594765: step 14175, loss 0.127433, acc 0.953125\n",
      "2018-05-23T14:12:56.345495: step 14176, loss 0.170521, acc 0.953125\n",
      "2018-05-23T14:12:57.093738: step 14177, loss 0.2227, acc 0.953125\n",
      "2018-05-23T14:12:57.796742: step 14178, loss 0.0592985, acc 1\n",
      "2018-05-23T14:12:58.561446: step 14179, loss 0.0837284, acc 0.953125\n",
      "2018-05-23T14:12:59.322414: step 14180, loss 0.0934773, acc 0.96875\n",
      "2018-05-23T14:13:00.050803: step 14181, loss 0.0895959, acc 0.9375\n",
      "2018-05-23T14:13:00.804412: step 14182, loss 0.0682952, acc 0.96875\n",
      "2018-05-23T14:13:01.519972: step 14183, loss 0.0502064, acc 0.984375\n",
      "2018-05-23T14:13:02.283534: step 14184, loss 0.153995, acc 0.90625\n",
      "2018-05-23T14:13:03.066869: step 14185, loss 0.125851, acc 0.953125\n",
      "2018-05-23T14:13:03.802665: step 14186, loss 0.0912284, acc 0.953125\n",
      "2018-05-23T14:13:04.491953: step 14187, loss 0.186229, acc 0.921875\n",
      "2018-05-23T14:13:05.168457: step 14188, loss 0.0862753, acc 0.953125\n",
      "2018-05-23T14:13:06.013000: step 14189, loss 0.0926216, acc 0.96875\n",
      "2018-05-23T14:13:06.726966: step 14190, loss 0.0414227, acc 0.984375\n",
      "2018-05-23T14:13:07.444541: step 14191, loss 0.0661906, acc 0.984375\n",
      "2018-05-23T14:13:08.183747: step 14192, loss 0.0814997, acc 0.96875\n",
      "2018-05-23T14:13:08.963342: step 14193, loss 0.0362491, acc 0.984375\n",
      "2018-05-23T14:13:09.718497: step 14194, loss 0.0825997, acc 0.953125\n",
      "2018-05-23T14:13:10.425068: step 14195, loss 0.0445488, acc 0.984375\n",
      "2018-05-23T14:13:11.112304: step 14196, loss 0.207999, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:13:11.848890: step 14197, loss 0.0890257, acc 0.96875\n",
      "2018-05-23T14:13:12.592707: step 14198, loss 0.0723512, acc 0.984375\n",
      "2018-05-23T14:13:13.279481: step 14199, loss 0.0990933, acc 0.984375\n",
      "2018-05-23T14:13:14.024881: step 14200, loss 0.0308568, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:13:21.549712: step 14200, loss 1.48095, acc 0.710816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14200\n",
      "\n",
      "2018-05-23T14:13:24.095887: step 14201, loss 0.0871979, acc 0.96875\n",
      "2018-05-23T14:13:24.849092: step 14202, loss 0.108118, acc 0.953125\n",
      "2018-05-23T14:13:25.486762: step 14203, loss 0.0677657, acc 0.984375\n",
      "2018-05-23T14:13:26.125370: step 14204, loss 0.118495, acc 0.953125\n",
      "2018-05-23T14:13:26.894777: step 14205, loss 0.0763279, acc 0.953125\n",
      "2018-05-23T14:13:27.491827: step 14206, loss 0.087358, acc 0.9375\n",
      "2018-05-23T14:13:28.089832: step 14207, loss 0.0887072, acc 0.953125\n",
      "2018-05-23T14:13:28.707983: step 14208, loss 0.18668, acc 0.90625\n",
      "2018-05-23T14:13:29.310965: step 14209, loss 0.0450182, acc 1\n",
      "2018-05-23T14:13:29.958519: step 14210, loss 0.154556, acc 0.9375\n",
      "2018-05-23T14:13:30.612627: step 14211, loss 0.1448, acc 0.953125\n",
      "2018-05-23T14:13:31.300593: step 14212, loss 0.199235, acc 0.921875\n",
      "2018-05-23T14:13:31.943137: step 14213, loss 0.0812496, acc 0.953125\n",
      "2018-05-23T14:13:32.561852: step 14214, loss 0.0940024, acc 0.953125\n",
      "2018-05-23T14:13:33.182304: step 14215, loss 0.100645, acc 0.96875\n",
      "2018-05-23T14:13:33.809107: step 14216, loss 0.0657516, acc 0.96875\n",
      "2018-05-23T14:13:34.449032: step 14217, loss 0.0731773, acc 0.984375\n",
      "2018-05-23T14:13:35.093754: step 14218, loss 0.0634157, acc 0.984375\n",
      "2018-05-23T14:13:35.729624: step 14219, loss 0.0764238, acc 0.96875\n",
      "2018-05-23T14:13:36.334864: step 14220, loss 0.0559061, acc 0.984375\n",
      "2018-05-23T14:13:36.959905: step 14221, loss 0.113019, acc 0.953125\n",
      "2018-05-23T14:13:37.626078: step 14222, loss 0.102851, acc 0.984375\n",
      "2018-05-23T14:13:38.322525: step 14223, loss 0.238632, acc 0.921875\n",
      "2018-05-23T14:13:39.058662: step 14224, loss 0.0912155, acc 0.953125\n",
      "2018-05-23T14:13:39.713993: step 14225, loss 0.199183, acc 0.921875\n",
      "2018-05-23T14:13:40.373826: step 14226, loss 0.0468954, acc 1\n",
      "2018-05-23T14:13:41.007594: step 14227, loss 0.138122, acc 0.921875\n",
      "2018-05-23T14:13:41.740922: step 14228, loss 0.0557799, acc 0.96875\n",
      "2018-05-23T14:13:42.375563: step 14229, loss 0.127773, acc 0.90625\n",
      "2018-05-23T14:13:43.047852: step 14230, loss 0.100157, acc 0.953125\n",
      "2018-05-23T14:13:43.826492: step 14231, loss 0.111052, acc 0.953125\n",
      "2018-05-23T14:13:44.567711: step 14232, loss 0.149351, acc 0.90625\n",
      "2018-05-23T14:13:45.243779: step 14233, loss 0.118731, acc 0.96875\n",
      "2018-05-23T14:13:45.898839: step 14234, loss 0.190321, acc 0.90625\n",
      "2018-05-23T14:13:46.501429: step 14235, loss 0.0396214, acc 1\n",
      "2018-05-23T14:13:47.208440: step 14236, loss 0.1032, acc 0.9375\n",
      "2018-05-23T14:13:47.925117: step 14237, loss 0.0928546, acc 0.9375\n",
      "2018-05-23T14:13:48.626868: step 14238, loss 0.129101, acc 0.9375\n",
      "2018-05-23T14:13:49.345296: step 14239, loss 0.138675, acc 0.9375\n",
      "2018-05-23T14:13:50.058514: step 14240, loss 0.096278, acc 0.953125\n",
      "2018-05-23T14:13:50.711666: step 14241, loss 0.145829, acc 0.921875\n",
      "2018-05-23T14:13:51.441149: step 14242, loss 0.0968013, acc 0.953125\n",
      "2018-05-23T14:13:52.109947: step 14243, loss 0.11718, acc 0.953125\n",
      "2018-05-23T14:13:52.811122: step 14244, loss 0.0553205, acc 0.96875\n",
      "2018-05-23T14:13:53.483683: step 14245, loss 0.140776, acc 0.921875\n",
      "2018-05-23T14:13:54.232190: step 14246, loss 0.045077, acc 1\n",
      "2018-05-23T14:13:54.913815: step 14247, loss 0.0646655, acc 0.96875\n",
      "2018-05-23T14:13:55.623880: step 14248, loss 0.110279, acc 0.96875\n",
      "2018-05-23T14:13:56.350120: step 14249, loss 0.215799, acc 0.921875\n",
      "2018-05-23T14:13:57.015609: step 14250, loss 0.141875, acc 0.953125\n",
      "2018-05-23T14:13:57.709438: step 14251, loss 0.135045, acc 0.9375\n",
      "2018-05-23T14:13:58.420109: step 14252, loss 0.0511878, acc 1\n",
      "2018-05-23T14:13:59.097329: step 14253, loss 0.0912364, acc 0.96875\n",
      "2018-05-23T14:13:59.790001: step 14254, loss 0.0822486, acc 0.953125\n",
      "2018-05-23T14:14:00.543769: step 14255, loss 0.177973, acc 0.9375\n",
      "2018-05-23T14:14:01.246872: step 14256, loss 0.178621, acc 0.9375\n",
      "2018-05-23T14:14:02.036824: step 14257, loss 0.18413, acc 0.921875\n",
      "2018-05-23T14:14:02.759456: step 14258, loss 0.117097, acc 0.953125\n",
      "2018-05-23T14:14:03.467929: step 14259, loss 0.13833, acc 0.9375\n",
      "2018-05-23T14:14:04.210424: step 14260, loss 0.0444435, acc 0.984375\n",
      "2018-05-23T14:14:04.940662: step 14261, loss 0.0473988, acc 0.96875\n",
      "2018-05-23T14:14:05.669736: step 14262, loss 0.124917, acc 0.9375\n",
      "2018-05-23T14:14:06.469941: step 14263, loss 0.190095, acc 0.921875\n",
      "2018-05-23T14:14:07.218673: step 14264, loss 0.0943764, acc 0.96875\n",
      "2018-05-23T14:14:07.937718: step 14265, loss 0.130326, acc 0.921875\n",
      "2018-05-23T14:14:08.663681: step 14266, loss 0.0428604, acc 0.984375\n",
      "2018-05-23T14:14:09.418572: step 14267, loss 0.176629, acc 0.921875\n",
      "2018-05-23T14:14:10.004378: step 14268, loss 0.103826, acc 0.96875\n",
      "2018-05-23T14:14:10.625484: step 14269, loss 0.0837521, acc 0.96875\n",
      "2018-05-23T14:14:11.234409: step 14270, loss 0.117575, acc 0.953125\n",
      "2018-05-23T14:14:11.864546: step 14271, loss 0.233803, acc 0.90625\n",
      "2018-05-23T14:14:12.513806: step 14272, loss 0.0726399, acc 0.96875\n",
      "2018-05-23T14:14:13.106949: step 14273, loss 0.0680461, acc 1\n",
      "2018-05-23T14:14:13.745467: step 14274, loss 0.114115, acc 0.953125\n",
      "2018-05-23T14:14:14.377323: step 14275, loss 0.203145, acc 0.921875\n",
      "2018-05-23T14:14:14.964463: step 14276, loss 0.131281, acc 0.90625\n",
      "2018-05-23T14:14:15.591728: step 14277, loss 0.0985734, acc 0.9375\n",
      "2018-05-23T14:14:16.203868: step 14278, loss 0.131626, acc 0.921875\n",
      "2018-05-23T14:14:16.813241: step 14279, loss 0.0469692, acc 1\n",
      "2018-05-23T14:14:17.497660: step 14280, loss 0.122463, acc 0.953125\n",
      "2018-05-23T14:14:18.187639: step 14281, loss 0.172875, acc 0.90625\n",
      "2018-05-23T14:14:19.102800: step 14282, loss 0.0989975, acc 0.9375\n",
      "2018-05-23T14:14:19.859332: step 14283, loss 0.0405058, acc 1\n",
      "2018-05-23T14:14:20.502258: step 14284, loss 0.0594914, acc 1\n",
      "2018-05-23T14:14:21.143627: step 14285, loss 0.273226, acc 0.90625\n",
      "2018-05-23T14:14:21.715577: step 14286, loss 0.0932636, acc 0.953125\n",
      "2018-05-23T14:14:22.320836: step 14287, loss 0.163616, acc 0.9375\n",
      "2018-05-23T14:14:22.993858: step 14288, loss 0.20506, acc 0.9375\n",
      "2018-05-23T14:14:23.627451: step 14289, loss 0.143645, acc 0.90625\n",
      "2018-05-23T14:14:24.533724: step 14290, loss 0.075677, acc 0.96875\n",
      "2018-05-23T14:14:25.220511: step 14291, loss 0.131746, acc 0.96875\n",
      "2018-05-23T14:14:25.950352: step 14292, loss 0.118126, acc 0.953125\n",
      "2018-05-23T14:14:26.653552: step 14293, loss 0.0607241, acc 0.984375\n",
      "2018-05-23T14:14:27.294614: step 14294, loss 0.27899, acc 0.890625\n",
      "2018-05-23T14:14:28.109083: step 14295, loss 0.0766198, acc 0.953125\n",
      "2018-05-23T14:14:28.758554: step 14296, loss 0.204476, acc 0.921875\n",
      "2018-05-23T14:14:29.468869: step 14297, loss 0.0748644, acc 0.96875\n",
      "2018-05-23T14:14:30.177338: step 14298, loss 0.185812, acc 0.890625\n",
      "2018-05-23T14:14:30.844069: step 14299, loss 0.0801314, acc 0.9375\n",
      "2018-05-23T14:14:31.475285: step 14300, loss 0.119754, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:14:38.336709: step 14300, loss 1.50318, acc 0.707815\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14300\n",
      "\n",
      "2018-05-23T14:14:40.929503: step 14301, loss 0.134273, acc 0.921875\n",
      "2018-05-23T14:14:41.556708: step 14302, loss 0.138249, acc 0.96875\n",
      "2018-05-23T14:14:42.212534: step 14303, loss 0.119778, acc 0.921875\n",
      "2018-05-23T14:14:42.864009: step 14304, loss 0.162684, acc 0.953125\n",
      "2018-05-23T14:14:43.498758: step 14305, loss 0.177002, acc 0.9375\n",
      "2018-05-23T14:14:44.174522: step 14306, loss 0.193177, acc 0.96875\n",
      "2018-05-23T14:14:44.784717: step 14307, loss 0.194814, acc 0.921875\n",
      "2018-05-23T14:14:45.427949: step 14308, loss 0.21074, acc 0.921875\n",
      "2018-05-23T14:14:46.049106: step 14309, loss 0.244931, acc 0.90625\n",
      "2018-05-23T14:14:46.672670: step 14310, loss 0.14185, acc 0.921875\n",
      "2018-05-23T14:14:47.266285: step 14311, loss 0.0967648, acc 0.96875\n",
      "2018-05-23T14:14:47.889434: step 14312, loss 0.137185, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:14:48.613990: step 14313, loss 0.166546, acc 0.9375\n",
      "2018-05-23T14:14:49.372656: step 14314, loss 0.107117, acc 0.984375\n",
      "2018-05-23T14:14:49.967502: step 14315, loss 0.0625617, acc 0.96875\n",
      "2018-05-23T14:14:50.624920: step 14316, loss 0.164534, acc 0.90625\n",
      "2018-05-23T14:14:51.256845: step 14317, loss 0.0899861, acc 0.96875\n",
      "2018-05-23T14:14:51.890768: step 14318, loss 0.0866576, acc 0.9375\n",
      "2018-05-23T14:14:52.488043: step 14319, loss 0.112211, acc 0.9375\n",
      "2018-05-23T14:14:53.111064: step 14320, loss 0.0800045, acc 0.953125\n",
      "2018-05-23T14:14:53.751714: step 14321, loss 0.128964, acc 0.9375\n",
      "2018-05-23T14:14:54.478290: step 14322, loss 0.0562704, acc 0.96875\n",
      "2018-05-23T14:14:55.087832: step 14323, loss 0.0679015, acc 0.984375\n",
      "2018-05-23T14:14:55.682911: step 14324, loss 0.113667, acc 0.9375\n",
      "2018-05-23T14:14:56.285153: step 14325, loss 0.136097, acc 0.9375\n",
      "2018-05-23T14:14:56.910248: step 14326, loss 0.138734, acc 0.921875\n",
      "2018-05-23T14:14:57.540383: step 14327, loss 0.0849033, acc 0.953125\n",
      "2018-05-23T14:14:58.266819: step 14328, loss 0.0982712, acc 0.96875\n",
      "2018-05-23T14:14:58.951278: step 14329, loss 0.120901, acc 0.953125\n",
      "2018-05-23T14:14:59.534530: step 14330, loss 0.163218, acc 0.9375\n",
      "2018-05-23T14:15:00.167231: step 14331, loss 0.0858812, acc 0.953125\n",
      "2018-05-23T14:15:00.843993: step 14332, loss 0.0898682, acc 0.953125\n",
      "2018-05-23T14:15:01.496538: step 14333, loss 0.120704, acc 0.9375\n",
      "2018-05-23T14:15:02.119740: step 14334, loss 0.123979, acc 0.953125\n",
      "2018-05-23T14:15:02.721283: step 14335, loss 0.163095, acc 0.921875\n",
      "2018-05-23T14:15:03.350758: step 14336, loss 0.0660812, acc 0.984375\n",
      "2018-05-23T14:15:03.953226: step 14337, loss 0.0516688, acc 1\n",
      "2018-05-23T14:15:04.589936: step 14338, loss 0.0911597, acc 0.953125\n",
      "2018-05-23T14:15:05.313087: step 14339, loss 0.0414031, acc 1\n",
      "2018-05-23T14:15:06.105889: step 14340, loss 0.076946, acc 0.984375\n",
      "2018-05-23T14:15:06.795163: step 14341, loss 0.123609, acc 0.953125\n",
      "2018-05-23T14:15:07.741998: step 14342, loss 0.175419, acc 0.9375\n",
      "2018-05-23T14:15:08.464790: step 14343, loss 0.0746366, acc 0.953125\n",
      "2018-05-23T14:15:09.158802: step 14344, loss 0.190821, acc 0.953125\n",
      "2018-05-23T14:15:09.847419: step 14345, loss 0.0774116, acc 0.953125\n",
      "2018-05-23T14:15:10.468175: step 14346, loss 0.115704, acc 0.953125\n",
      "2018-05-23T14:15:11.108546: step 14347, loss 0.118662, acc 0.921875\n",
      "2018-05-23T14:15:11.778655: step 14348, loss 0.127307, acc 0.953125\n",
      "2018-05-23T14:15:12.420986: step 14349, loss 0.213274, acc 0.953125\n",
      "2018-05-23T14:15:13.110981: step 14350, loss 0.145342, acc 0.890625\n",
      "2018-05-23T14:15:13.727417: step 14351, loss 0.108633, acc 0.96875\n",
      "2018-05-23T14:15:14.350278: step 14352, loss 0.0737946, acc 0.96875\n",
      "2018-05-23T14:15:14.985355: step 14353, loss 0.112428, acc 0.953125\n",
      "2018-05-23T14:15:15.580827: step 14354, loss 0.0771911, acc 1\n",
      "2018-05-23T14:15:16.217897: step 14355, loss 0.0717911, acc 0.96875\n",
      "2018-05-23T14:15:16.908799: step 14356, loss 0.0789496, acc 0.953125\n",
      "2018-05-23T14:15:17.591576: step 14357, loss 0.0668721, acc 0.984375\n",
      "2018-05-23T14:15:18.270577: step 14358, loss 0.0906943, acc 0.96875\n",
      "2018-05-23T14:15:18.880779: step 14359, loss 0.0389258, acc 1\n",
      "2018-05-23T14:15:19.519827: step 14360, loss 0.140902, acc 0.90625\n",
      "2018-05-23T14:15:20.163670: step 14361, loss 0.081405, acc 0.96875\n",
      "2018-05-23T14:15:20.844120: step 14362, loss 0.0874174, acc 0.96875\n",
      "2018-05-23T14:15:21.537483: step 14363, loss 0.0808437, acc 0.9375\n",
      "2018-05-23T14:15:22.213945: step 14364, loss 0.0755827, acc 0.96875\n",
      "2018-05-23T14:15:22.866977: step 14365, loss 0.0407262, acc 0.984375\n",
      "2018-05-23T14:15:24.099782: step 14366, loss 0.0978518, acc 0.953125\n",
      "2018-05-23T14:15:25.105728: step 14367, loss 0.122298, acc 0.953125\n",
      "2018-05-23T14:15:25.751357: step 14368, loss 0.0479893, acc 0.984375\n",
      "2018-05-23T14:15:26.445883: step 14369, loss 0.0656921, acc 0.96875\n",
      "2018-05-23T14:15:27.099421: step 14370, loss 0.118664, acc 0.921875\n",
      "2018-05-23T14:15:27.728418: step 14371, loss 0.0645148, acc 0.984375\n",
      "2018-05-23T14:15:28.345943: step 14372, loss 0.172627, acc 0.890625\n",
      "2018-05-23T14:15:28.967403: step 14373, loss 0.0765687, acc 0.9375\n",
      "2018-05-23T14:15:29.599965: step 14374, loss 0.135701, acc 0.921875\n",
      "2018-05-23T14:15:30.237098: step 14375, loss 0.175371, acc 0.90625\n",
      "2018-05-23T14:15:30.933851: step 14376, loss 0.0800974, acc 0.984375\n",
      "2018-05-23T14:15:31.574022: step 14377, loss 0.0408448, acc 1\n",
      "2018-05-23T14:15:32.235731: step 14378, loss 0.296105, acc 0.875\n",
      "2018-05-23T14:15:32.881922: step 14379, loss 0.103258, acc 0.953125\n",
      "2018-05-23T14:15:33.502514: step 14380, loss 0.259141, acc 0.875\n",
      "2018-05-23T14:15:34.151553: step 14381, loss 0.109721, acc 0.953125\n",
      "2018-05-23T14:15:34.777954: step 14382, loss 0.0723398, acc 0.96875\n",
      "2018-05-23T14:15:35.391654: step 14383, loss 0.109127, acc 0.953125\n",
      "2018-05-23T14:15:36.042489: step 14384, loss 0.204082, acc 0.90625\n",
      "2018-05-23T14:15:36.667536: step 14385, loss 0.193193, acc 0.921875\n",
      "2018-05-23T14:15:37.279139: step 14386, loss 0.126366, acc 0.953125\n",
      "2018-05-23T14:15:37.880295: step 14387, loss 0.077038, acc 0.984375\n",
      "2018-05-23T14:15:38.560291: step 14388, loss 0.0607547, acc 0.96875\n",
      "2018-05-23T14:15:39.223074: step 14389, loss 0.148184, acc 0.921875\n",
      "2018-05-23T14:15:39.896755: step 14390, loss 0.161916, acc 0.9375\n",
      "2018-05-23T14:15:40.595587: step 14391, loss 0.150533, acc 0.9375\n",
      "2018-05-23T14:15:41.204686: step 14392, loss 0.0636941, acc 0.96875\n",
      "2018-05-23T14:15:41.840606: step 14393, loss 0.106088, acc 0.953125\n",
      "2018-05-23T14:15:42.464544: step 14394, loss 0.19063, acc 0.953125\n",
      "2018-05-23T14:15:43.095869: step 14395, loss 0.105288, acc 0.953125\n",
      "2018-05-23T14:15:43.739582: step 14396, loss 0.172449, acc 0.90625\n",
      "2018-05-23T14:15:44.381249: step 14397, loss 0.190506, acc 0.9375\n",
      "2018-05-23T14:15:45.013315: step 14398, loss 0.0951272, acc 0.953125\n",
      "2018-05-23T14:15:45.691268: step 14399, loss 0.0458492, acc 0.984375\n",
      "2018-05-23T14:15:46.330209: step 14400, loss 0.196644, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:15:53.130609: step 14400, loss 1.52095, acc 0.71053\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14400\n",
      "\n",
      "2018-05-23T14:15:55.891974: step 14401, loss 0.130309, acc 0.9375\n",
      "2018-05-23T14:15:56.563937: step 14402, loss 0.15094, acc 0.9375\n",
      "2018-05-23T14:15:57.278705: step 14403, loss 0.0756247, acc 0.984375\n",
      "2018-05-23T14:15:57.915785: step 14404, loss 0.107399, acc 0.953125\n",
      "2018-05-23T14:15:58.648520: step 14405, loss 0.140496, acc 0.921875\n",
      "2018-05-23T14:15:59.272255: step 14406, loss 0.149387, acc 0.953125\n",
      "2018-05-23T14:16:00.056019: step 14407, loss 0.137132, acc 0.921875\n",
      "2018-05-23T14:16:00.643804: step 14408, loss 0.0251438, acc 1\n",
      "2018-05-23T14:16:01.308714: step 14409, loss 0.0435157, acc 0.984375\n",
      "2018-05-23T14:16:01.970149: step 14410, loss 0.116415, acc 0.9375\n",
      "2018-05-23T14:16:02.621903: step 14411, loss 0.124539, acc 0.953125\n",
      "2018-05-23T14:16:03.268836: step 14412, loss 0.167348, acc 0.9375\n",
      "2018-05-23T14:16:03.858778: step 14413, loss 0.154186, acc 0.921875\n",
      "2018-05-23T14:16:04.514083: step 14414, loss 0.12502, acc 0.921875\n",
      "2018-05-23T14:16:05.163025: step 14415, loss 0.0471321, acc 0.984375\n",
      "2018-05-23T14:16:05.793329: step 14416, loss 0.103957, acc 0.984375\n",
      "2018-05-23T14:16:06.432231: step 14417, loss 0.160583, acc 0.921875\n",
      "2018-05-23T14:16:07.020222: step 14418, loss 0.151328, acc 0.953125\n",
      "2018-05-23T14:16:07.685058: step 14419, loss 0.121077, acc 0.921875\n",
      "2018-05-23T14:16:08.272112: step 14420, loss 0.160481, acc 0.921875\n",
      "2018-05-23T14:16:08.902518: step 14421, loss 0.10694, acc 0.96875\n",
      "2018-05-23T14:16:09.561252: step 14422, loss 0.0660762, acc 0.96875\n",
      "2018-05-23T14:16:10.178129: step 14423, loss 0.0640647, acc 1\n",
      "2018-05-23T14:16:10.813428: step 14424, loss 0.126887, acc 0.9375\n",
      "2018-05-23T14:16:11.513758: step 14425, loss 0.0957859, acc 0.953125\n",
      "2018-05-23T14:16:12.194139: step 14426, loss 0.119057, acc 0.921875\n",
      "2018-05-23T14:16:12.843440: step 14427, loss 0.156452, acc 0.921875\n",
      "2018-05-23T14:16:13.513847: step 14428, loss 0.116917, acc 0.9375\n",
      "2018-05-23T14:16:14.207650: step 14429, loss 0.185648, acc 0.953125\n",
      "2018-05-23T14:16:14.835642: step 14430, loss 0.038916, acc 1\n",
      "2018-05-23T14:16:15.489273: step 14431, loss 0.145384, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:16:16.232056: step 14432, loss 0.0815432, acc 0.953125\n",
      "2018-05-23T14:16:17.273686: step 14433, loss 0.0971328, acc 0.953125\n",
      "2018-05-23T14:16:18.081468: step 14434, loss 0.145268, acc 0.9375\n",
      "2018-05-23T14:16:18.743210: step 14435, loss 0.052453, acc 0.96875\n",
      "2018-05-23T14:16:19.428118: step 14436, loss 0.0831616, acc 0.953125\n",
      "2018-05-23T14:16:20.169239: step 14437, loss 0.0544211, acc 0.96875\n",
      "2018-05-23T14:16:20.956470: step 14438, loss 0.0624244, acc 0.984375\n",
      "2018-05-23T14:16:21.785774: step 14439, loss 0.0669434, acc 0.984375\n",
      "2018-05-23T14:16:22.460491: step 14440, loss 0.156018, acc 0.921875\n",
      "2018-05-23T14:16:23.270968: step 14441, loss 0.148757, acc 0.9375\n",
      "2018-05-23T14:16:23.920102: step 14442, loss 0.20644, acc 0.90625\n",
      "2018-05-23T14:16:24.595601: step 14443, loss 0.260514, acc 0.90625\n",
      "2018-05-23T14:16:25.273828: step 14444, loss 0.149055, acc 0.9375\n",
      "2018-05-23T14:16:25.922624: step 14445, loss 0.133949, acc 0.9375\n",
      "2018-05-23T14:16:26.566204: step 14446, loss 0.117812, acc 0.953125\n",
      "2018-05-23T14:16:27.316238: step 14447, loss 0.150314, acc 0.9375\n",
      "2018-05-23T14:16:27.987034: step 14448, loss 0.206567, acc 0.90625\n",
      "2018-05-23T14:16:28.650559: step 14449, loss 0.0796208, acc 0.984375\n",
      "2018-05-23T14:16:29.315222: step 14450, loss 0.143545, acc 0.96875\n",
      "2018-05-23T14:16:29.978656: step 14451, loss 0.0846418, acc 0.96875\n",
      "2018-05-23T14:16:30.632581: step 14452, loss 0.12401, acc 0.984375\n",
      "2018-05-23T14:16:31.320205: step 14453, loss 0.14738, acc 0.953125\n",
      "2018-05-23T14:16:32.034814: step 14454, loss 0.0894167, acc 0.96875\n",
      "2018-05-23T14:16:32.709230: step 14455, loss 0.0813699, acc 0.953125\n",
      "2018-05-23T14:16:33.401913: step 14456, loss 0.125008, acc 0.953125\n",
      "2018-05-23T14:16:34.131296: step 14457, loss 0.115105, acc 0.9375\n",
      "2018-05-23T14:16:34.754577: step 14458, loss 0.0791007, acc 0.96875\n",
      "2018-05-23T14:16:35.485083: step 14459, loss 0.111859, acc 0.921875\n",
      "2018-05-23T14:16:36.067355: step 14460, loss 0.0721694, acc 0.9375\n",
      "2018-05-23T14:16:36.687989: step 14461, loss 0.047273, acc 0.96875\n",
      "2018-05-23T14:16:37.319326: step 14462, loss 0.0874573, acc 0.984375\n",
      "2018-05-23T14:16:37.943538: step 14463, loss 0.107129, acc 0.9375\n",
      "2018-05-23T14:16:38.609336: step 14464, loss 0.184364, acc 0.921875\n",
      "2018-05-23T14:16:39.271264: step 14465, loss 0.127031, acc 0.921875\n",
      "2018-05-23T14:16:39.891734: step 14466, loss 0.125193, acc 0.90625\n",
      "2018-05-23T14:16:40.509088: step 14467, loss 0.149256, acc 0.96875\n",
      "2018-05-23T14:16:41.159904: step 14468, loss 0.0970424, acc 0.953125\n",
      "2018-05-23T14:16:41.868877: step 14469, loss 0.160701, acc 0.9375\n",
      "2018-05-23T14:16:42.589989: step 14470, loss 0.173799, acc 0.953125\n",
      "2018-05-23T14:16:43.347845: step 14471, loss 0.0958527, acc 0.9375\n",
      "2018-05-23T14:16:44.027552: step 14472, loss 0.113503, acc 0.9375\n",
      "2018-05-23T14:16:44.680195: step 14473, loss 0.160751, acc 0.953125\n",
      "2018-05-23T14:16:45.387335: step 14474, loss 0.231719, acc 0.953125\n",
      "2018-05-23T14:16:46.304472: step 14475, loss 0.0734619, acc 0.953125\n",
      "2018-05-23T14:16:47.050099: step 14476, loss 0.0699908, acc 0.984375\n",
      "2018-05-23T14:16:47.733196: step 14477, loss 0.165229, acc 0.921875\n",
      "2018-05-23T14:16:48.405175: step 14478, loss 0.0703481, acc 0.984375\n",
      "2018-05-23T14:16:49.111580: step 14479, loss 0.0779256, acc 0.9375\n",
      "2018-05-23T14:16:49.785398: step 14480, loss 0.117927, acc 0.953125\n",
      "2018-05-23T14:16:50.545644: step 14481, loss 0.210816, acc 0.921875\n",
      "2018-05-23T14:16:51.497277: step 14482, loss 0.0796064, acc 0.96875\n",
      "2018-05-23T14:16:52.270797: step 14483, loss 0.118468, acc 0.9375\n",
      "2018-05-23T14:16:52.997512: step 14484, loss 0.108352, acc 0.953125\n",
      "2018-05-23T14:16:53.675534: step 14485, loss 0.192876, acc 0.875\n",
      "2018-05-23T14:16:54.339998: step 14486, loss 0.191917, acc 0.953125\n",
      "2018-05-23T14:16:54.971875: step 14487, loss 0.200701, acc 0.921875\n",
      "2018-05-23T14:16:55.704922: step 14488, loss 0.125007, acc 0.90625\n",
      "2018-05-23T14:16:56.731825: step 14489, loss 0.067846, acc 0.984375\n",
      "2018-05-23T14:16:57.547140: step 14490, loss 0.144267, acc 0.953125\n",
      "2018-05-23T14:16:58.226621: step 14491, loss 0.168304, acc 0.9375\n",
      "2018-05-23T14:16:59.039395: step 14492, loss 0.079151, acc 0.96875\n",
      "2018-05-23T14:17:00.168793: step 14493, loss 0.166583, acc 0.953125\n",
      "2018-05-23T14:17:01.026744: step 14494, loss 0.0786778, acc 0.96875\n",
      "2018-05-23T14:17:02.018670: step 14495, loss 0.110586, acc 0.96875\n",
      "2018-05-23T14:17:02.850437: step 14496, loss 0.0577519, acc 0.984375\n",
      "2018-05-23T14:17:03.608566: step 14497, loss 0.138645, acc 0.921875\n",
      "2018-05-23T14:17:04.389180: step 14498, loss 0.0812444, acc 0.953125\n",
      "2018-05-23T14:17:05.093157: step 14499, loss 0.21325, acc 0.9375\n",
      "2018-05-23T14:17:05.720234: step 14500, loss 0.141179, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:17:13.607088: step 14500, loss 1.55149, acc 0.712959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14500\n",
      "\n",
      "2018-05-23T14:17:16.209494: step 14501, loss 0.0801821, acc 0.984375\n",
      "2018-05-23T14:17:16.901097: step 14502, loss 0.196238, acc 0.953125\n",
      "2018-05-23T14:17:17.568168: step 14503, loss 0.105953, acc 0.953125\n",
      "2018-05-23T14:17:18.251765: step 14504, loss 0.0563867, acc 0.96875\n",
      "2018-05-23T14:17:18.951853: step 14505, loss 0.102537, acc 0.9375\n",
      "2018-05-23T14:17:19.606637: step 14506, loss 0.127363, acc 0.953125\n",
      "2018-05-23T14:17:20.279576: step 14507, loss 0.0874073, acc 0.96875\n",
      "2018-05-23T14:17:20.949927: step 14508, loss 0.0609786, acc 0.96875\n",
      "2018-05-23T14:17:21.593011: step 14509, loss 0.104534, acc 0.9375\n",
      "2018-05-23T14:17:22.257588: step 14510, loss 0.0489796, acc 0.984375\n",
      "2018-05-23T14:17:22.935459: step 14511, loss 0.108162, acc 0.921875\n",
      "2018-05-23T14:17:23.620248: step 14512, loss 0.123931, acc 0.9375\n",
      "2018-05-23T14:17:24.365611: step 14513, loss 0.226622, acc 0.90625\n",
      "2018-05-23T14:17:25.055271: step 14514, loss 0.0686178, acc 0.96875\n",
      "2018-05-23T14:17:25.765823: step 14515, loss 0.0984077, acc 0.96875\n",
      "2018-05-23T14:17:26.491643: step 14516, loss 0.0356842, acc 1\n",
      "2018-05-23T14:17:27.160126: step 14517, loss 0.0808064, acc 0.953125\n",
      "2018-05-23T14:17:27.864049: step 14518, loss 0.0562903, acc 0.984375\n",
      "2018-05-23T14:17:29.017685: step 14519, loss 0.110804, acc 0.953125\n",
      "2018-05-23T14:17:29.743907: step 14520, loss 0.07348, acc 0.96875\n",
      "2018-05-23T14:17:30.456954: step 14521, loss 0.118628, acc 0.953125\n",
      "2018-05-23T14:17:31.224115: step 14522, loss 0.0754177, acc 0.984375\n",
      "2018-05-23T14:17:33.245961: step 14523, loss 0.107148, acc 0.9375\n",
      "2018-05-23T14:17:40.273748: step 14524, loss 0.0820034, acc 0.96875\n",
      "2018-05-23T14:49:52.547352: step 14525, loss 0.162517, acc 0.90625\n",
      "2018-05-23T14:49:54.016422: step 14526, loss 0.0855783, acc 0.953125\n",
      "2018-05-23T14:49:55.312483: step 14527, loss 0.0916605, acc 0.96875\n",
      "2018-05-23T14:49:56.504295: step 14528, loss 0.0294248, acc 1\n",
      "2018-05-23T14:49:57.818834: step 14529, loss 0.108761, acc 0.953125\n",
      "2018-05-23T14:49:59.419604: step 14530, loss 0.12164, acc 0.953125\n",
      "2018-05-23T14:50:00.659630: step 14531, loss 0.0946201, acc 0.9375\n",
      "2018-05-23T14:50:01.719343: step 14532, loss 0.151823, acc 0.9375\n",
      "2018-05-23T14:50:02.852745: step 14533, loss 0.0593079, acc 1\n",
      "2018-05-23T14:50:03.716558: step 14534, loss 0.0643011, acc 0.984375\n",
      "2018-05-23T14:50:04.700936: step 14535, loss 0.16396, acc 0.953125\n",
      "2018-05-23T14:50:05.584370: step 14536, loss 0.11417, acc 0.953125\n",
      "2018-05-23T14:50:06.365475: step 14537, loss 0.238585, acc 0.875\n",
      "2018-05-23T14:50:07.315127: step 14538, loss 0.11128, acc 0.921875\n",
      "2018-05-23T14:50:08.198045: step 14539, loss 0.0983076, acc 0.9375\n",
      "2018-05-23T14:50:09.069402: step 14540, loss 0.102042, acc 0.953125\n",
      "2018-05-23T14:50:09.766553: step 14541, loss 0.312436, acc 0.921875\n",
      "2018-05-23T14:50:10.488746: step 14542, loss 0.262453, acc 0.921875\n",
      "2018-05-23T14:50:11.264003: step 14543, loss 0.0626957, acc 0.96875\n",
      "2018-05-23T14:50:11.989115: step 14544, loss 0.0780678, acc 0.984375\n",
      "2018-05-23T14:50:12.735367: step 14545, loss 0.167292, acc 0.96875\n",
      "2018-05-23T14:50:13.484722: step 14546, loss 0.149056, acc 0.953125\n",
      "2018-05-23T14:50:14.170156: step 14547, loss 0.180076, acc 0.890625\n",
      "2018-05-23T14:50:14.856007: step 14548, loss 0.158101, acc 0.9375\n",
      "2018-05-23T14:50:15.511894: step 14549, loss 0.0735345, acc 0.96875\n",
      "2018-05-23T14:50:16.218046: step 14550, loss 0.0660731, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:50:16.927164: step 14551, loss 0.120181, acc 0.96875\n",
      "2018-05-23T14:50:17.638900: step 14552, loss 0.0820835, acc 0.96875\n",
      "2018-05-23T14:50:18.321167: step 14553, loss 0.101447, acc 0.953125\n",
      "2018-05-23T14:50:18.990725: step 14554, loss 0.0614841, acc 0.96875\n",
      "2018-05-23T14:50:19.736010: step 14555, loss 0.102173, acc 0.96875\n",
      "2018-05-23T14:50:20.420966: step 14556, loss 0.12208, acc 0.953125\n",
      "2018-05-23T14:50:21.093232: step 14557, loss 0.0886427, acc 0.96875\n",
      "2018-05-23T14:50:21.867391: step 14558, loss 0.0834684, acc 0.96875\n",
      "2018-05-23T14:50:22.572952: step 14559, loss 0.169033, acc 0.953125\n",
      "2018-05-23T14:50:23.344973: step 14560, loss 0.12835, acc 0.9375\n",
      "2018-05-23T14:50:24.021374: step 14561, loss 0.0827211, acc 0.953125\n",
      "2018-05-23T14:50:24.657743: step 14562, loss 0.0829934, acc 0.96875\n",
      "2018-05-23T14:50:25.388671: step 14563, loss 0.0552954, acc 0.96875\n",
      "2018-05-23T14:50:26.187712: step 14564, loss 0.138054, acc 0.9375\n",
      "2018-05-23T14:50:26.961737: step 14565, loss 0.168184, acc 0.921875\n",
      "2018-05-23T14:50:27.829752: step 14566, loss 0.0454962, acc 0.96875\n",
      "2018-05-23T14:50:28.479736: step 14567, loss 0.133757, acc 0.96875\n",
      "2018-05-23T14:50:29.222749: step 14568, loss 0.2015, acc 0.9375\n",
      "2018-05-23T14:50:29.765298: step 14569, loss 0.108762, acc 0.953125\n",
      "2018-05-23T14:50:30.445477: step 14570, loss 0.0731599, acc 0.96875\n",
      "2018-05-23T14:50:30.890289: step 14571, loss 0.100008, acc 0.953125\n",
      "2018-05-23T14:50:31.342114: step 14572, loss 0.113831, acc 0.96875\n",
      "2018-05-23T14:50:32.254638: step 14573, loss 0.0375891, acc 1\n",
      "2018-05-23T14:50:33.092398: step 14574, loss 0.167568, acc 0.921875\n",
      "2018-05-23T14:50:33.843926: step 14575, loss 0.0791754, acc 0.984375\n",
      "2018-05-23T14:50:34.510509: step 14576, loss 0.0419921, acc 0.984375\n",
      "2018-05-23T14:50:35.223458: step 14577, loss 0.0769476, acc 0.96875\n",
      "2018-05-23T14:50:35.921025: step 14578, loss 0.0626516, acc 1\n",
      "2018-05-23T14:50:36.588795: step 14579, loss 0.114043, acc 0.953125\n",
      "2018-05-23T14:50:37.252096: step 14580, loss 0.0241529, acc 1\n",
      "2018-05-23T14:50:37.848698: step 14581, loss 0.0760866, acc 0.984375\n",
      "2018-05-23T14:50:38.454730: step 14582, loss 0.0564827, acc 0.96875\n",
      "2018-05-23T14:50:39.081292: step 14583, loss 0.125015, acc 0.890625\n",
      "2018-05-23T14:50:39.703901: step 14584, loss 0.0672605, acc 0.984375\n",
      "2018-05-23T14:50:40.313264: step 14585, loss 0.125393, acc 0.921875\n",
      "2018-05-23T14:50:40.940848: step 14586, loss 0.0939603, acc 0.984375\n",
      "2018-05-23T14:50:41.556175: step 14587, loss 0.162098, acc 0.9375\n",
      "2018-05-23T14:50:42.173261: step 14588, loss 0.0613099, acc 0.96875\n",
      "2018-05-23T14:50:42.823863: step 14589, loss 0.179342, acc 0.953125\n",
      "2018-05-23T14:50:43.442025: step 14590, loss 0.223995, acc 0.921875\n",
      "2018-05-23T14:50:44.067060: step 14591, loss 0.0634889, acc 0.96875\n",
      "2018-05-23T14:50:44.699129: step 14592, loss 0.203904, acc 0.9375\n",
      "2018-05-23T14:50:45.388461: step 14593, loss 0.133546, acc 0.9375\n",
      "2018-05-23T14:50:46.040202: step 14594, loss 0.138205, acc 0.953125\n",
      "2018-05-23T14:50:46.707802: step 14595, loss 0.0502698, acc 0.984375\n",
      "2018-05-23T14:50:47.324378: step 14596, loss 0.1712, acc 0.96875\n",
      "2018-05-23T14:50:47.926790: step 14597, loss 0.0515421, acc 0.984375\n",
      "2018-05-23T14:50:48.570635: step 14598, loss 0.117165, acc 0.9375\n",
      "2018-05-23T14:50:49.199470: step 14599, loss 0.116188, acc 0.953125\n",
      "2018-05-23T14:50:49.951872: step 14600, loss 0.239305, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:50:57.420085: step 14600, loss 1.56665, acc 0.712959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14600\n",
      "\n",
      "2018-05-23T14:50:58.968941: step 14601, loss 0.197141, acc 0.984375\n",
      "2018-05-23T14:50:59.337953: step 14602, loss 0.221274, acc 0.890625\n",
      "2018-05-23T14:50:59.669067: step 14603, loss 0.0907057, acc 0.96875\n",
      "2018-05-23T14:51:00.004171: step 14604, loss 0.157614, acc 0.9375\n",
      "2018-05-23T14:51:00.353237: step 14605, loss 0.156722, acc 0.90625\n",
      "2018-05-23T14:51:00.757158: step 14606, loss 0.110696, acc 0.9375\n",
      "2018-05-23T14:51:01.058352: step 14607, loss 0.0978605, acc 0.953125\n",
      "2018-05-23T14:51:01.362538: step 14608, loss 0.143322, acc 0.9375\n",
      "2018-05-23T14:51:01.687323: step 14609, loss 0.115679, acc 0.96875\n",
      "2018-05-23T14:51:02.018435: step 14610, loss 0.157786, acc 0.890625\n",
      "2018-05-23T14:51:02.352542: step 14611, loss 0.068623, acc 0.984375\n",
      "2018-05-23T14:51:02.682661: step 14612, loss 0.119574, acc 0.953125\n",
      "2018-05-23T14:51:02.991835: step 14613, loss 0.131663, acc 0.9375\n",
      "2018-05-23T14:51:03.291032: step 14614, loss 0.0983845, acc 0.953125\n",
      "2018-05-23T14:51:03.590232: step 14615, loss 0.121754, acc 0.953125\n",
      "2018-05-23T14:51:03.892424: step 14616, loss 0.213551, acc 0.921875\n",
      "2018-05-23T14:51:04.183691: step 14617, loss 0.102464, acc 0.96875\n",
      "2018-05-23T14:51:04.521787: step 14618, loss 0.190445, acc 0.921875\n",
      "2018-05-23T14:51:04.891797: step 14619, loss 0.190024, acc 0.90625\n",
      "2018-05-23T14:51:05.350570: step 14620, loss 0.155269, acc 0.9375\n",
      "2018-05-23T14:51:06.031748: step 14621, loss 0.111961, acc 0.953125\n",
      "2018-05-23T14:51:06.657075: step 14622, loss 0.110915, acc 0.953125\n",
      "2018-05-23T14:51:07.165716: step 14623, loss 0.154138, acc 0.90625\n",
      "2018-05-23T14:51:07.741175: step 14624, loss 0.0470834, acc 0.984375\n",
      "2018-05-23T14:51:08.382460: step 14625, loss 0.139804, acc 0.921875\n",
      "2018-05-23T14:51:08.953932: step 14626, loss 0.124657, acc 0.953125\n",
      "2018-05-23T14:51:09.401733: step 14627, loss 0.144566, acc 0.953125\n",
      "2018-05-23T14:51:09.867488: step 14628, loss 0.162306, acc 0.953125\n",
      "2018-05-23T14:51:10.287363: step 14629, loss 0.120699, acc 0.953125\n",
      "2018-05-23T14:51:10.680311: step 14630, loss 0.139768, acc 0.9375\n",
      "2018-05-23T14:51:11.121135: step 14631, loss 0.0621426, acc 0.984375\n",
      "2018-05-23T14:51:11.516076: step 14632, loss 0.109628, acc 0.984375\n",
      "2018-05-23T14:51:11.905035: step 14633, loss 0.179426, acc 0.921875\n",
      "2018-05-23T14:51:12.426640: step 14634, loss 0.0955807, acc 0.96875\n",
      "2018-05-23T14:51:12.900374: step 14635, loss 0.276444, acc 0.9375\n",
      "2018-05-23T14:51:13.459877: step 14636, loss 0.0618526, acc 0.96875\n",
      "2018-05-23T14:51:14.234385: step 14637, loss 0.135773, acc 0.9375\n",
      "2018-05-23T14:51:14.770950: step 14638, loss 0.158729, acc 0.921875\n",
      "2018-05-23T14:51:15.204792: step 14639, loss 0.118534, acc 0.953125\n",
      "2018-05-23T14:51:15.659574: step 14640, loss 0.335768, acc 0.921875\n",
      "2018-05-23T14:51:16.123331: step 14641, loss 0.194075, acc 0.921875\n",
      "2018-05-23T14:51:16.623992: step 14642, loss 0.0612723, acc 0.96875\n",
      "2018-05-23T14:51:17.126647: step 14643, loss 0.151173, acc 0.9375\n",
      "2018-05-23T14:51:17.596390: step 14644, loss 0.0926427, acc 0.9375\n",
      "2018-05-23T14:51:18.132955: step 14645, loss 0.167298, acc 0.9375\n",
      "2018-05-23T14:51:18.453099: step 14646, loss 0.0943475, acc 0.984375\n",
      "2018-05-23T14:51:18.758281: step 14647, loss 0.0914568, acc 0.96875\n",
      "2018-05-23T14:51:19.058424: step 14648, loss 0.211874, acc 0.9375\n",
      "2018-05-23T14:51:19.360615: step 14649, loss 0.162978, acc 0.9375\n",
      "2018-05-23T14:51:19.660812: step 14650, loss 0.0935268, acc 0.96875\n",
      "2018-05-23T14:51:19.999904: step 14651, loss 0.0599788, acc 0.984375\n",
      "2018-05-23T14:51:20.458676: step 14652, loss 0.229956, acc 0.921875\n",
      "2018-05-23T14:51:20.852624: step 14653, loss 0.0878531, acc 0.953125\n",
      "2018-05-23T14:51:21.366249: step 14654, loss 0.109803, acc 0.96875\n",
      "2018-05-23T14:51:21.871896: step 14655, loss 0.140214, acc 0.953125\n",
      "2018-05-23T14:51:22.181069: step 14656, loss 0.137168, acc 0.90625\n",
      "2018-05-23T14:51:22.937046: step 14657, loss 0.182109, acc 0.921875\n",
      "2018-05-23T14:51:23.313040: step 14658, loss 0.0644728, acc 0.984375\n",
      "2018-05-23T14:51:23.654130: step 14659, loss 0.178899, acc 0.953125\n",
      "2018-05-23T14:51:24.000202: step 14660, loss 0.0849848, acc 0.984375\n",
      "2018-05-23T14:51:24.333327: step 14661, loss 0.0529402, acc 0.984375\n",
      "2018-05-23T14:51:24.643495: step 14662, loss 0.058243, acc 0.984375\n",
      "2018-05-23T14:51:24.982790: step 14663, loss 0.0656004, acc 0.984375\n",
      "2018-05-23T14:51:25.355591: step 14664, loss 0.118151, acc 0.9375\n",
      "2018-05-23T14:51:25.665760: step 14665, loss 0.13989, acc 0.953125\n",
      "2018-05-23T14:51:26.085638: step 14666, loss 0.0668419, acc 0.96875\n",
      "2018-05-23T14:51:26.548399: step 14667, loss 0.0610626, acc 1\n",
      "2018-05-23T14:51:27.044073: step 14668, loss 0.0860378, acc 0.96875\n",
      "2018-05-23T14:51:27.560692: step 14669, loss 0.102375, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:51:27.994532: step 14670, loss 0.0804012, acc 0.96875\n",
      "2018-05-23T14:51:28.458291: step 14671, loss 0.0294428, acc 1\n",
      "2018-05-23T14:51:28.892130: step 14672, loss 0.163793, acc 0.9375\n",
      "2018-05-23T14:51:29.374839: step 14673, loss 0.0563482, acc 0.96875\n",
      "2018-05-23T14:51:29.877494: step 14674, loss 0.144229, acc 0.953125\n",
      "2018-05-23T14:51:30.302358: step 14675, loss 0.0798321, acc 0.953125\n",
      "2018-05-23T14:51:30.769108: step 14676, loss 0.0885302, acc 0.953125\n",
      "2018-05-23T14:51:31.456271: step 14677, loss 0.153581, acc 0.921875\n",
      "2018-05-23T14:51:31.954937: step 14678, loss 0.195278, acc 0.890625\n",
      "2018-05-23T14:51:32.384786: step 14679, loss 0.160162, acc 0.9375\n",
      "2018-05-23T14:51:32.808653: step 14680, loss 0.260815, acc 0.9375\n",
      "2018-05-23T14:51:33.358182: step 14681, loss 0.0779655, acc 0.984375\n",
      "2018-05-23T14:51:33.840891: step 14682, loss 0.100038, acc 0.953125\n",
      "2018-05-23T14:51:34.457242: step 14683, loss 0.0571878, acc 1\n",
      "2018-05-23T14:51:35.091546: step 14684, loss 0.0985449, acc 0.96875\n",
      "2018-05-23T14:51:35.752385: step 14685, loss 0.091226, acc 0.96875\n",
      "2018-05-23T14:51:36.328847: step 14686, loss 0.133997, acc 0.953125\n",
      "2018-05-23T14:51:36.981062: step 14687, loss 0.120522, acc 0.90625\n",
      "2018-05-23T14:51:37.807851: step 14688, loss 0.117971, acc 0.96875\n",
      "2018-05-23T14:51:38.366357: step 14689, loss 0.14778, acc 0.921875\n",
      "2018-05-23T14:51:39.102388: step 14690, loss 0.160066, acc 0.9375\n",
      "2018-05-23T14:51:39.843405: step 14691, loss 0.225431, acc 0.90625\n",
      "2018-05-23T14:51:40.505635: step 14692, loss 0.192648, acc 0.9375\n",
      "2018-05-23T14:51:41.126972: step 14693, loss 0.151296, acc 0.921875\n",
      "2018-05-23T14:51:41.681488: step 14694, loss 0.100131, acc 0.96875\n",
      "2018-05-23T14:51:42.507279: step 14695, loss 0.0832378, acc 0.96875\n",
      "2018-05-23T14:51:43.009934: step 14696, loss 0.155621, acc 0.953125\n",
      "2018-05-23T14:51:43.668174: step 14697, loss 0.16493, acc 0.953125\n",
      "2018-05-23T14:51:44.600679: step 14698, loss 0.161168, acc 0.9375\n",
      "2018-05-23T14:51:45.281858: step 14699, loss 0.0888405, acc 0.96875\n",
      "2018-05-23T14:51:45.961039: step 14700, loss 0.158586, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:51:52.791766: step 14700, loss 1.53248, acc 0.711816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14700\n",
      "\n",
      "2018-05-23T14:51:55.862551: step 14701, loss 0.136092, acc 0.921875\n",
      "2018-05-23T14:51:56.348253: step 14702, loss 0.152741, acc 0.9375\n",
      "2018-05-23T14:51:56.905761: step 14703, loss 0.0804005, acc 0.953125\n",
      "2018-05-23T14:51:57.341595: step 14704, loss 0.105794, acc 0.953125\n",
      "2018-05-23T14:51:57.916057: step 14705, loss 0.125654, acc 0.96875\n",
      "2018-05-23T14:51:58.365853: step 14706, loss 0.0683051, acc 0.96875\n",
      "2018-05-23T14:51:58.786729: step 14707, loss 0.0862633, acc 0.96875\n",
      "2018-05-23T14:51:59.182670: step 14708, loss 0.127204, acc 0.9375\n",
      "2018-05-23T14:51:59.590578: step 14709, loss 0.182468, acc 0.890625\n",
      "2018-05-23T14:52:00.134124: step 14710, loss 0.189757, acc 0.96875\n",
      "2018-05-23T14:52:00.658721: step 14711, loss 0.097291, acc 0.984375\n",
      "2018-05-23T14:52:01.068625: step 14712, loss 0.0883102, acc 0.96875\n",
      "2018-05-23T14:52:01.499472: step 14713, loss 0.14649, acc 0.953125\n",
      "2018-05-23T14:52:01.912367: step 14714, loss 0.0984431, acc 0.9375\n",
      "2018-05-23T14:52:02.328254: step 14715, loss 0.139826, acc 0.96875\n",
      "2018-05-23T14:52:02.759101: step 14716, loss 0.136743, acc 0.9375\n",
      "2018-05-23T14:52:03.141081: step 14717, loss 0.184168, acc 0.953125\n",
      "2018-05-23T14:52:03.534027: step 14718, loss 0.210391, acc 0.9375\n",
      "2018-05-23T14:52:03.955900: step 14719, loss 0.198786, acc 0.90625\n",
      "2018-05-23T14:52:04.437612: step 14720, loss 0.212451, acc 0.96875\n",
      "2018-05-23T14:52:04.873446: step 14721, loss 0.191101, acc 0.90625\n",
      "2018-05-23T14:52:05.289334: step 14722, loss 0.147137, acc 0.890625\n",
      "2018-05-23T14:52:05.691258: step 14723, loss 0.203092, acc 0.921875\n",
      "2018-05-23T14:52:06.109140: step 14724, loss 0.188196, acc 0.9375\n",
      "2018-05-23T14:52:06.548963: step 14725, loss 0.160214, acc 0.953125\n",
      "2018-05-23T14:52:06.898030: step 14726, loss 0.180104, acc 0.921875\n",
      "2018-05-23T14:52:07.246097: step 14727, loss 0.155858, acc 0.9375\n",
      "2018-05-23T14:52:07.637052: step 14728, loss 0.0763242, acc 0.96875\n",
      "2018-05-23T14:52:08.120759: step 14729, loss 0.0965126, acc 0.9375\n",
      "2018-05-23T14:52:08.475807: step 14730, loss 0.0936843, acc 0.953125\n",
      "2018-05-23T14:52:08.836842: step 14731, loss 0.0953864, acc 0.96875\n",
      "2018-05-23T14:52:09.246746: step 14732, loss 0.137698, acc 0.953125\n",
      "2018-05-23T14:52:09.569880: step 14733, loss 0.118589, acc 0.9375\n",
      "2018-05-23T14:52:09.885038: step 14734, loss 0.0877482, acc 0.953125\n",
      "2018-05-23T14:52:10.184237: step 14735, loss 0.0956535, acc 0.96875\n",
      "2018-05-23T14:52:10.487391: step 14736, loss 0.109488, acc 0.9375\n",
      "2018-05-23T14:52:10.809530: step 14737, loss 0.164232, acc 0.9375\n",
      "2018-05-23T14:52:11.140643: step 14738, loss 0.0796717, acc 0.96875\n",
      "2018-05-23T14:52:11.446824: step 14739, loss 0.248888, acc 0.890625\n",
      "2018-05-23T14:52:11.755001: step 14740, loss 0.0847678, acc 0.96875\n",
      "2018-05-23T14:52:12.054201: step 14741, loss 0.086494, acc 0.953125\n",
      "2018-05-23T14:52:12.392294: step 14742, loss 0.284961, acc 0.921875\n",
      "2018-05-23T14:52:12.751336: step 14743, loss 0.0481277, acc 0.984375\n",
      "2018-05-23T14:52:13.070480: step 14744, loss 0.253405, acc 0.875\n",
      "2018-05-23T14:52:13.467419: step 14745, loss 0.245017, acc 0.953125\n",
      "2018-05-23T14:52:13.806513: step 14746, loss 0.0604189, acc 0.96875\n",
      "2018-05-23T14:52:14.176523: step 14747, loss 0.0946379, acc 0.96875\n",
      "2018-05-23T14:52:14.571465: step 14748, loss 0.178248, acc 0.90625\n",
      "2018-05-23T14:52:14.899587: step 14749, loss 0.12336, acc 0.9375\n",
      "2018-05-23T14:52:15.242671: step 14750, loss 0.327395, acc 0.90625\n",
      "2018-05-23T14:52:15.529901: step 14751, loss 0.112192, acc 0.9375\n",
      "2018-05-23T14:52:15.847054: step 14752, loss 0.181599, acc 0.953125\n",
      "2018-05-23T14:52:16.150242: step 14753, loss 0.0607271, acc 0.984375\n",
      "2018-05-23T14:52:16.490332: step 14754, loss 0.114183, acc 0.953125\n",
      "2018-05-23T14:52:16.869317: step 14755, loss 0.0948344, acc 0.984375\n",
      "2018-05-23T14:52:17.187468: step 14756, loss 0.115507, acc 0.953125\n",
      "2018-05-23T14:52:17.494645: step 14757, loss 0.0730046, acc 0.984375\n",
      "2018-05-23T14:52:17.813791: step 14758, loss 0.11733, acc 0.921875\n",
      "2018-05-23T14:52:18.198761: step 14759, loss 0.128241, acc 0.96875\n",
      "2018-05-23T14:52:18.536857: step 14760, loss 0.0967264, acc 0.96875\n",
      "2018-05-23T14:52:18.951748: step 14761, loss 0.0465769, acc 1\n",
      "2018-05-23T14:52:19.282862: step 14762, loss 0.177067, acc 0.953125\n",
      "2018-05-23T14:52:19.628935: step 14763, loss 0.0757948, acc 0.96875\n",
      "2018-05-23T14:52:19.998945: step 14764, loss 0.0733764, acc 0.96875\n",
      "2018-05-23T14:52:20.512572: step 14765, loss 0.0717859, acc 0.96875\n",
      "2018-05-23T14:52:20.834710: step 14766, loss 0.112242, acc 0.953125\n",
      "2018-05-23T14:52:21.155850: step 14767, loss 0.0711051, acc 0.96875\n",
      "2018-05-23T14:52:21.507908: step 14768, loss 0.125966, acc 0.953125\n",
      "2018-05-23T14:52:21.836031: step 14769, loss 0.042443, acc 1\n",
      "2018-05-23T14:52:22.368606: step 14770, loss 0.116453, acc 0.953125\n",
      "2018-05-23T14:52:22.861289: step 14771, loss 0.0811726, acc 0.953125\n",
      "2018-05-23T14:52:23.300115: step 14772, loss 0.0704876, acc 0.984375\n",
      "2018-05-23T14:52:23.692065: step 14773, loss 0.084975, acc 0.953125\n",
      "2018-05-23T14:52:24.124908: step 14774, loss 0.209491, acc 0.875\n",
      "2018-05-23T14:52:24.384214: step 14775, loss 0.240349, acc 0.941176\n",
      "2018-05-23T14:52:24.733282: step 14776, loss 0.126687, acc 0.96875\n",
      "2018-05-23T14:52:25.065391: step 14777, loss 0.0636029, acc 0.984375\n",
      "2018-05-23T14:52:25.400495: step 14778, loss 0.0712379, acc 0.96875\n",
      "2018-05-23T14:52:25.748564: step 14779, loss 0.0864817, acc 0.984375\n",
      "2018-05-23T14:52:26.087656: step 14780, loss 0.116776, acc 0.953125\n",
      "2018-05-23T14:52:26.459662: step 14781, loss 0.040191, acc 1\n",
      "2018-05-23T14:52:26.838648: step 14782, loss 0.0359238, acc 0.984375\n",
      "2018-05-23T14:52:27.197687: step 14783, loss 0.0870891, acc 0.96875\n",
      "2018-05-23T14:52:27.569693: step 14784, loss 0.122148, acc 0.953125\n",
      "2018-05-23T14:52:27.909782: step 14785, loss 0.134287, acc 0.953125\n",
      "2018-05-23T14:52:28.305725: step 14786, loss 0.142848, acc 0.890625\n",
      "2018-05-23T14:52:28.704656: step 14787, loss 0.180562, acc 0.90625\n",
      "2018-05-23T14:52:29.100599: step 14788, loss 0.154937, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:52:29.486564: step 14789, loss 0.0861515, acc 0.953125\n",
      "2018-05-23T14:52:30.070004: step 14790, loss 0.0885274, acc 0.953125\n",
      "2018-05-23T14:52:30.852910: step 14791, loss 0.177992, acc 0.921875\n",
      "2018-05-23T14:52:31.306695: step 14792, loss 0.0645712, acc 0.96875\n",
      "2018-05-23T14:52:31.772449: step 14793, loss 0.0687614, acc 0.96875\n",
      "2018-05-23T14:52:32.225238: step 14794, loss 0.092375, acc 0.96875\n",
      "2018-05-23T14:52:32.608214: step 14795, loss 0.113538, acc 0.96875\n",
      "2018-05-23T14:52:32.968250: step 14796, loss 0.207122, acc 0.90625\n",
      "2018-05-23T14:52:33.323300: step 14797, loss 0.0805672, acc 0.953125\n",
      "2018-05-23T14:52:33.687327: step 14798, loss 0.0707132, acc 0.96875\n",
      "2018-05-23T14:52:34.041379: step 14799, loss 0.0492344, acc 0.984375\n",
      "2018-05-23T14:52:34.523092: step 14800, loss 0.144734, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:52:40.522042: step 14800, loss 1.58277, acc 0.706244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14800\n",
      "\n",
      "2018-05-23T14:52:42.641373: step 14801, loss 0.0955539, acc 0.96875\n",
      "2018-05-23T14:52:43.186913: step 14802, loss 0.0949627, acc 0.953125\n",
      "2018-05-23T14:52:43.683586: step 14803, loss 0.106125, acc 0.953125\n",
      "2018-05-23T14:52:44.089499: step 14804, loss 0.0793151, acc 0.953125\n",
      "2018-05-23T14:52:44.499402: step 14805, loss 0.113951, acc 0.984375\n",
      "2018-05-23T14:52:44.914293: step 14806, loss 0.0636224, acc 0.984375\n",
      "2018-05-23T14:52:45.358109: step 14807, loss 0.11944, acc 0.9375\n",
      "2018-05-23T14:52:45.833832: step 14808, loss 0.145839, acc 0.90625\n",
      "2018-05-23T14:52:46.204840: step 14809, loss 0.0661715, acc 0.96875\n",
      "2018-05-23T14:52:46.757362: step 14810, loss 0.0973642, acc 0.9375\n",
      "2018-05-23T14:52:47.266000: step 14811, loss 0.089832, acc 0.953125\n",
      "2018-05-23T14:52:47.648976: step 14812, loss 0.0735142, acc 0.953125\n",
      "2018-05-23T14:52:48.022976: step 14813, loss 0.0769292, acc 0.96875\n",
      "2018-05-23T14:52:48.496708: step 14814, loss 0.0287139, acc 1\n",
      "2018-05-23T14:52:48.831812: step 14815, loss 0.088806, acc 0.953125\n",
      "2018-05-23T14:52:49.324494: step 14816, loss 0.10366, acc 0.9375\n",
      "2018-05-23T14:52:50.026616: step 14817, loss 0.126201, acc 0.953125\n",
      "2018-05-23T14:52:50.591106: step 14818, loss 0.12418, acc 0.953125\n",
      "2018-05-23T14:52:51.229399: step 14819, loss 0.0682303, acc 0.96875\n",
      "2018-05-23T14:52:51.721083: step 14820, loss 0.108139, acc 0.953125\n",
      "2018-05-23T14:52:52.228725: step 14821, loss 0.0240208, acc 1\n",
      "2018-05-23T14:52:52.836100: step 14822, loss 0.135518, acc 0.984375\n",
      "2018-05-23T14:52:53.342744: step 14823, loss 0.0609285, acc 0.96875\n",
      "2018-05-23T14:52:53.719735: step 14824, loss 0.0882262, acc 0.96875\n",
      "2018-05-23T14:52:54.073789: step 14825, loss 0.106675, acc 0.953125\n",
      "2018-05-23T14:52:54.458759: step 14826, loss 0.0648756, acc 0.96875\n",
      "2018-05-23T14:52:54.828769: step 14827, loss 0.171312, acc 0.9375\n",
      "2018-05-23T14:52:55.180828: step 14828, loss 0.0424742, acc 0.96875\n",
      "2018-05-23T14:52:55.572779: step 14829, loss 0.0633801, acc 0.984375\n",
      "2018-05-23T14:52:55.931819: step 14830, loss 0.112974, acc 0.984375\n",
      "2018-05-23T14:52:56.280884: step 14831, loss 0.0524744, acc 0.984375\n",
      "2018-05-23T14:52:56.701758: step 14832, loss 0.0640044, acc 0.96875\n",
      "2018-05-23T14:52:57.064787: step 14833, loss 0.112376, acc 0.953125\n",
      "2018-05-23T14:52:57.420834: step 14834, loss 0.119445, acc 0.9375\n",
      "2018-05-23T14:52:57.768903: step 14835, loss 0.032865, acc 0.984375\n",
      "2018-05-23T14:52:58.124952: step 14836, loss 0.0770157, acc 0.953125\n",
      "2018-05-23T14:52:58.581729: step 14837, loss 0.0636457, acc 0.96875\n",
      "2018-05-23T14:52:59.041499: step 14838, loss 0.0775086, acc 0.96875\n",
      "2018-05-23T14:52:59.608980: step 14839, loss 0.0897241, acc 0.96875\n",
      "2018-05-23T14:53:00.179456: step 14840, loss 0.407733, acc 0.875\n",
      "2018-05-23T14:53:00.641219: step 14841, loss 0.0537451, acc 0.96875\n",
      "2018-05-23T14:53:01.189752: step 14842, loss 0.084318, acc 0.953125\n",
      "2018-05-23T14:53:01.722329: step 14843, loss 0.0623682, acc 0.96875\n",
      "2018-05-23T14:53:02.297792: step 14844, loss 0.268791, acc 0.953125\n",
      "2018-05-23T14:53:02.756561: step 14845, loss 0.0378254, acc 1\n",
      "2018-05-23T14:53:03.303099: step 14846, loss 0.0896728, acc 0.953125\n",
      "2018-05-23T14:53:03.865595: step 14847, loss 0.107721, acc 0.96875\n",
      "2018-05-23T14:53:04.368252: step 14848, loss 0.0664567, acc 0.984375\n",
      "2018-05-23T14:53:05.016516: step 14849, loss 0.0338177, acc 0.984375\n",
      "2018-05-23T14:53:05.519171: step 14850, loss 0.0745827, acc 0.953125\n",
      "2018-05-23T14:53:06.024821: step 14851, loss 0.0876309, acc 0.953125\n",
      "2018-05-23T14:53:06.402808: step 14852, loss 0.087641, acc 0.953125\n",
      "2018-05-23T14:53:06.840636: step 14853, loss 0.0853301, acc 0.9375\n",
      "2018-05-23T14:53:07.235580: step 14854, loss 0.0755303, acc 0.984375\n",
      "2018-05-23T14:53:07.692356: step 14855, loss 0.0701279, acc 0.984375\n",
      "2018-05-23T14:53:08.080319: step 14856, loss 0.129839, acc 0.953125\n",
      "2018-05-23T14:53:08.474265: step 14857, loss 0.0744328, acc 0.953125\n",
      "2018-05-23T14:53:08.958968: step 14858, loss 0.104155, acc 0.921875\n",
      "2018-05-23T14:53:09.401784: step 14859, loss 0.0948478, acc 0.9375\n",
      "2018-05-23T14:53:10.076978: step 14860, loss 0.0265878, acc 0.984375\n",
      "2018-05-23T14:53:10.468928: step 14861, loss 0.0721629, acc 0.96875\n",
      "2018-05-23T14:53:10.835948: step 14862, loss 0.116459, acc 0.9375\n",
      "2018-05-23T14:53:11.268789: step 14863, loss 0.115591, acc 0.96875\n",
      "2018-05-23T14:53:11.671711: step 14864, loss 0.0680953, acc 0.96875\n",
      "2018-05-23T14:53:12.069646: step 14865, loss 0.0700152, acc 0.984375\n",
      "2018-05-23T14:53:12.478553: step 14866, loss 0.0858106, acc 0.96875\n",
      "2018-05-23T14:53:12.961261: step 14867, loss 0.0654267, acc 0.953125\n",
      "2018-05-23T14:53:13.309331: step 14868, loss 0.121729, acc 0.9375\n",
      "2018-05-23T14:53:13.681334: step 14869, loss 0.133237, acc 0.953125\n",
      "2018-05-23T14:53:14.035387: step 14870, loss 0.093694, acc 0.953125\n",
      "2018-05-23T14:53:14.395424: step 14871, loss 0.0908392, acc 0.984375\n",
      "2018-05-23T14:53:14.776407: step 14872, loss 0.0602303, acc 0.984375\n",
      "2018-05-23T14:53:15.224208: step 14873, loss 0.208484, acc 0.9375\n",
      "2018-05-23T14:53:15.628128: step 14874, loss 0.0650085, acc 0.984375\n",
      "2018-05-23T14:53:16.035038: step 14875, loss 0.102821, acc 0.96875\n",
      "2018-05-23T14:53:16.422003: step 14876, loss 0.097079, acc 0.953125\n",
      "2018-05-23T14:53:16.949592: step 14877, loss 0.0330158, acc 1\n",
      "2018-05-23T14:53:17.331569: step 14878, loss 0.0798286, acc 0.984375\n",
      "2018-05-23T14:53:17.715543: step 14879, loss 0.0408468, acc 1\n",
      "2018-05-23T14:53:18.102507: step 14880, loss 0.057743, acc 0.96875\n",
      "2018-05-23T14:53:18.542332: step 14881, loss 0.0724704, acc 0.96875\n",
      "2018-05-23T14:53:18.963204: step 14882, loss 0.0231834, acc 0.984375\n",
      "2018-05-23T14:53:19.327231: step 14883, loss 0.118196, acc 0.921875\n",
      "2018-05-23T14:53:19.702227: step 14884, loss 0.0623389, acc 0.984375\n",
      "2018-05-23T14:53:20.167981: step 14885, loss 0.0524258, acc 0.984375\n",
      "2018-05-23T14:53:20.534002: step 14886, loss 0.150231, acc 0.9375\n",
      "2018-05-23T14:53:20.901023: step 14887, loss 0.0282975, acc 1\n",
      "2018-05-23T14:53:21.260060: step 14888, loss 0.172444, acc 0.953125\n",
      "2018-05-23T14:53:21.663980: step 14889, loss 0.101921, acc 0.96875\n",
      "2018-05-23T14:53:22.096822: step 14890, loss 0.137409, acc 0.9375\n",
      "2018-05-23T14:53:22.439904: step 14891, loss 0.117314, acc 0.9375\n",
      "2018-05-23T14:53:22.808916: step 14892, loss 0.218308, acc 0.953125\n",
      "2018-05-23T14:53:23.180921: step 14893, loss 0.139974, acc 0.9375\n",
      "2018-05-23T14:53:23.550933: step 14894, loss 0.104963, acc 0.953125\n",
      "2018-05-23T14:53:24.003721: step 14895, loss 0.130969, acc 0.921875\n",
      "2018-05-23T14:53:24.395672: step 14896, loss 0.128291, acc 0.921875\n",
      "2018-05-23T14:53:24.765682: step 14897, loss 0.0272704, acc 0.984375\n",
      "2018-05-23T14:53:25.118738: step 14898, loss 0.0592449, acc 1\n",
      "2018-05-23T14:53:25.470795: step 14899, loss 0.166908, acc 0.953125\n",
      "2018-05-23T14:53:25.887682: step 14900, loss 0.238892, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:53:30.944154: step 14900, loss 1.58574, acc 0.714959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-14900\n",
      "\n",
      "2018-05-23T14:53:32.700455: step 14901, loss 0.0649942, acc 0.984375\n",
      "2018-05-23T14:53:33.168204: step 14902, loss 0.0919959, acc 0.96875\n",
      "2018-05-23T14:53:33.566140: step 14903, loss 0.0613755, acc 0.984375\n",
      "2018-05-23T14:53:33.935153: step 14904, loss 0.0870883, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:53:34.281226: step 14905, loss 0.115455, acc 0.9375\n",
      "2018-05-23T14:53:34.654228: step 14906, loss 0.119098, acc 0.953125\n",
      "2018-05-23T14:53:35.208745: step 14907, loss 0.171696, acc 0.90625\n",
      "2018-05-23T14:53:35.675496: step 14908, loss 0.108201, acc 0.9375\n",
      "2018-05-23T14:53:36.080413: step 14909, loss 0.0942321, acc 0.96875\n",
      "2018-05-23T14:53:36.501287: step 14910, loss 0.0420997, acc 0.984375\n",
      "2018-05-23T14:53:37.037851: step 14911, loss 0.0760722, acc 0.953125\n",
      "2018-05-23T14:53:37.414844: step 14912, loss 0.0863531, acc 0.96875\n",
      "2018-05-23T14:53:37.792832: step 14913, loss 0.0706278, acc 0.984375\n",
      "2018-05-23T14:53:38.135913: step 14914, loss 0.0866454, acc 0.953125\n",
      "2018-05-23T14:53:38.453067: step 14915, loss 0.107748, acc 0.96875\n",
      "2018-05-23T14:53:38.810110: step 14916, loss 0.154218, acc 0.9375\n",
      "2018-05-23T14:53:39.155187: step 14917, loss 0.210945, acc 0.9375\n",
      "2018-05-23T14:53:39.487300: step 14918, loss 0.137108, acc 0.90625\n",
      "2018-05-23T14:53:39.824397: step 14919, loss 0.0991597, acc 0.953125\n",
      "2018-05-23T14:53:40.148533: step 14920, loss 0.0732587, acc 0.984375\n",
      "2018-05-23T14:53:40.473659: step 14921, loss 0.0562868, acc 1\n",
      "2018-05-23T14:53:40.845666: step 14922, loss 0.060812, acc 0.96875\n",
      "2018-05-23T14:53:41.189745: step 14923, loss 0.127134, acc 0.96875\n",
      "2018-05-23T14:53:41.552773: step 14924, loss 0.0869296, acc 0.96875\n",
      "2018-05-23T14:53:41.927465: step 14925, loss 0.0921577, acc 0.96875\n",
      "2018-05-23T14:53:42.260779: step 14926, loss 0.0888681, acc 0.953125\n",
      "2018-05-23T14:53:42.594187: step 14927, loss 0.0618846, acc 0.984375\n",
      "2018-05-23T14:53:42.987137: step 14928, loss 0.06887, acc 0.984375\n",
      "2018-05-23T14:53:43.305832: step 14929, loss 0.0564535, acc 0.96875\n",
      "2018-05-23T14:53:43.632921: step 14930, loss 0.0751653, acc 0.953125\n",
      "2018-05-23T14:53:43.962042: step 14931, loss 0.123989, acc 0.953125\n",
      "2018-05-23T14:53:44.280193: step 14932, loss 0.145931, acc 0.9375\n",
      "2018-05-23T14:53:44.596854: step 14933, loss 0.322664, acc 0.875\n",
      "2018-05-23T14:53:44.984348: step 14934, loss 0.13419, acc 0.953125\n",
      "2018-05-23T14:53:45.334370: step 14935, loss 0.161447, acc 0.875\n",
      "2018-05-23T14:53:45.750138: step 14936, loss 0.0645307, acc 0.96875\n",
      "2018-05-23T14:53:46.115672: step 14937, loss 0.108806, acc 0.953125\n",
      "2018-05-23T14:53:46.591425: step 14938, loss 0.125102, acc 0.9375\n",
      "2018-05-23T14:53:47.053207: step 14939, loss 0.0662617, acc 0.96875\n",
      "2018-05-23T14:53:47.474090: step 14940, loss 0.0723754, acc 0.953125\n",
      "2018-05-23T14:53:47.917066: step 14941, loss 0.153492, acc 0.921875\n",
      "2018-05-23T14:53:48.434177: step 14942, loss 0.142009, acc 0.953125\n",
      "2018-05-23T14:53:48.894967: step 14943, loss 0.0702908, acc 0.96875\n",
      "2018-05-23T14:53:49.416570: step 14944, loss 0.123319, acc 0.9375\n",
      "2018-05-23T14:53:49.944158: step 14945, loss 0.207349, acc 0.953125\n",
      "2018-05-23T14:53:50.439685: step 14946, loss 0.125084, acc 0.90625\n",
      "2018-05-23T14:53:50.914110: step 14947, loss 0.0929352, acc 0.953125\n",
      "2018-05-23T14:53:51.429265: step 14948, loss 0.139492, acc 0.953125\n",
      "2018-05-23T14:53:52.133889: step 14949, loss 0.0674725, acc 0.96875\n",
      "2018-05-23T14:53:52.597660: step 14950, loss 0.100121, acc 0.9375\n",
      "2018-05-23T14:53:53.206023: step 14951, loss 0.119567, acc 0.921875\n",
      "2018-05-23T14:53:53.777492: step 14952, loss 0.0730693, acc 0.984375\n",
      "2018-05-23T14:53:54.354949: step 14953, loss 0.125289, acc 0.953125\n",
      "2018-05-23T14:53:54.867579: step 14954, loss 0.0697289, acc 0.984375\n",
      "2018-05-23T14:53:55.580669: step 14955, loss 0.0367744, acc 0.984375\n",
      "2018-05-23T14:53:56.229932: step 14956, loss 0.0931866, acc 0.953125\n",
      "2018-05-23T14:53:56.897149: step 14957, loss 0.135933, acc 0.921875\n",
      "2018-05-23T14:53:57.588298: step 14958, loss 0.125861, acc 0.953125\n",
      "2018-05-23T14:53:58.075031: step 14959, loss 0.0933418, acc 0.96875\n",
      "2018-05-23T14:53:58.655445: step 14960, loss 0.14068, acc 0.9375\n",
      "2018-05-23T14:53:59.153113: step 14961, loss 0.0981185, acc 0.96875\n",
      "2018-05-23T14:53:59.582963: step 14962, loss 0.0509561, acc 0.984375\n",
      "2018-05-23T14:54:00.042732: step 14963, loss 0.125099, acc 0.9375\n",
      "2018-05-23T14:54:00.577302: step 14964, loss 0.146139, acc 0.9375\n",
      "2018-05-23T14:54:01.096910: step 14965, loss 0.0743076, acc 0.96875\n",
      "2018-05-23T14:54:01.573637: step 14966, loss 0.0401243, acc 0.984375\n",
      "2018-05-23T14:54:02.284734: step 14967, loss 0.0769975, acc 0.96875\n",
      "2018-05-23T14:54:02.921032: step 14968, loss 0.143102, acc 0.890625\n",
      "2018-05-23T14:54:03.389777: step 14969, loss 0.221828, acc 0.90625\n",
      "2018-05-23T14:54:03.835586: step 14970, loss 0.0853515, acc 0.96875\n",
      "2018-05-23T14:54:04.407069: step 14971, loss 0.102693, acc 0.953125\n",
      "2018-05-23T14:54:04.812970: step 14972, loss 0.120391, acc 0.96875\n",
      "2018-05-23T14:54:05.306652: step 14973, loss 0.12483, acc 0.953125\n",
      "2018-05-23T14:54:05.854188: step 14974, loss 0.156335, acc 0.921875\n",
      "2018-05-23T14:54:06.375790: step 14975, loss 0.134786, acc 0.953125\n",
      "2018-05-23T14:54:06.846533: step 14976, loss 0.0877344, acc 0.953125\n",
      "2018-05-23T14:54:07.284358: step 14977, loss 0.0790484, acc 0.96875\n",
      "2018-05-23T14:54:07.672323: step 14978, loss 0.127682, acc 0.9375\n",
      "2018-05-23T14:54:08.035363: step 14979, loss 0.102688, acc 0.953125\n",
      "2018-05-23T14:54:08.448246: step 14980, loss 0.276412, acc 0.859375\n",
      "2018-05-23T14:54:08.844186: step 14981, loss 0.0574762, acc 0.984375\n",
      "2018-05-23T14:54:09.233178: step 14982, loss 0.0773175, acc 0.96875\n",
      "2018-05-23T14:54:09.598169: step 14983, loss 0.151288, acc 0.921875\n",
      "2018-05-23T14:54:09.961231: step 14984, loss 0.121825, acc 0.953125\n",
      "2018-05-23T14:54:10.345170: step 14985, loss 0.0904314, acc 0.96875\n",
      "2018-05-23T14:54:10.725155: step 14986, loss 0.0723796, acc 0.984375\n",
      "2018-05-23T14:54:11.158994: step 14987, loss 0.0946068, acc 0.9375\n",
      "2018-05-23T14:54:11.524018: step 14988, loss 0.175712, acc 0.9375\n",
      "2018-05-23T14:54:11.889041: step 14989, loss 0.128523, acc 0.921875\n",
      "2018-05-23T14:54:12.251071: step 14990, loss 0.132848, acc 0.921875\n",
      "2018-05-23T14:54:12.651002: step 14991, loss 0.124696, acc 0.9375\n",
      "2018-05-23T14:54:13.004065: step 14992, loss 0.0716896, acc 0.96875\n",
      "2018-05-23T14:54:13.376895: step 14993, loss 0.104243, acc 0.953125\n",
      "2018-05-23T14:54:13.759870: step 14994, loss 0.0856744, acc 0.953125\n",
      "2018-05-23T14:54:14.143843: step 14995, loss 0.0738562, acc 0.953125\n",
      "2018-05-23T14:54:14.590647: step 14996, loss 0.0523053, acc 0.984375\n",
      "2018-05-23T14:54:14.957667: step 14997, loss 0.0576814, acc 0.984375\n",
      "2018-05-23T14:54:15.326681: step 14998, loss 0.137714, acc 0.96875\n",
      "2018-05-23T14:54:15.726611: step 14999, loss 0.138437, acc 0.953125\n",
      "2018-05-23T14:54:16.104632: step 15000, loss 0.160891, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:54:21.326628: step 15000, loss 1.61049, acc 0.71353\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15000\n",
      "\n",
      "2018-05-23T14:54:23.179670: step 15001, loss 0.0969607, acc 0.953125\n",
      "2018-05-23T14:54:23.746156: step 15002, loss 0.122858, acc 0.9375\n",
      "2018-05-23T14:54:24.159052: step 15003, loss 0.10576, acc 0.9375\n",
      "2018-05-23T14:54:24.548010: step 15004, loss 0.0867952, acc 0.953125\n",
      "2018-05-23T14:54:24.940958: step 15005, loss 0.0688961, acc 0.984375\n",
      "2018-05-23T14:54:25.325931: step 15006, loss 0.0565749, acc 0.984375\n",
      "2018-05-23T14:54:25.706910: step 15007, loss 0.0580264, acc 0.96875\n",
      "2018-05-23T14:54:26.091879: step 15008, loss 0.102425, acc 0.953125\n",
      "2018-05-23T14:54:26.470866: step 15009, loss 0.115859, acc 0.9375\n",
      "2018-05-23T14:54:26.818936: step 15010, loss 0.108359, acc 0.96875\n",
      "2018-05-23T14:54:27.206900: step 15011, loss 0.144263, acc 0.9375\n",
      "2018-05-23T14:54:27.564941: step 15012, loss 0.0529768, acc 0.96875\n",
      "2018-05-23T14:54:27.920988: step 15013, loss 0.195909, acc 0.921875\n",
      "2018-05-23T14:54:28.270055: step 15014, loss 0.131194, acc 0.890625\n",
      "2018-05-23T14:54:28.612140: step 15015, loss 0.046279, acc 0.984375\n",
      "2018-05-23T14:54:28.983147: step 15016, loss 0.0957943, acc 0.96875\n",
      "2018-05-23T14:54:29.359142: step 15017, loss 0.187266, acc 0.921875\n",
      "2018-05-23T14:54:29.739124: step 15018, loss 0.0693737, acc 0.984375\n",
      "2018-05-23T14:54:30.193907: step 15019, loss 0.108406, acc 0.90625\n",
      "2018-05-23T14:54:30.719500: step 15020, loss 0.138816, acc 0.921875\n",
      "2018-05-23T14:54:31.109456: step 15021, loss 0.114935, acc 0.96875\n",
      "2018-05-23T14:54:31.462512: step 15022, loss 0.0713607, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:54:31.965168: step 15023, loss 0.145239, acc 0.984375\n",
      "2018-05-23T14:54:32.594485: step 15024, loss 0.0564933, acc 0.984375\n",
      "2018-05-23T14:54:33.145012: step 15025, loss 0.0468833, acc 1\n",
      "2018-05-23T14:54:33.595805: step 15026, loss 0.0664213, acc 0.984375\n",
      "2018-05-23T14:54:34.149327: step 15027, loss 0.216638, acc 0.953125\n",
      "2018-05-23T14:54:34.756700: step 15028, loss 0.147494, acc 0.953125\n",
      "2018-05-23T14:54:35.723115: step 15029, loss 0.0844057, acc 0.96875\n",
      "2018-05-23T14:54:36.241728: step 15030, loss 0.0883304, acc 0.9375\n",
      "2018-05-23T14:54:36.852094: step 15031, loss 0.0488419, acc 0.984375\n",
      "2018-05-23T14:54:37.301895: step 15032, loss 0.0409303, acc 1\n",
      "2018-05-23T14:54:37.912259: step 15033, loss 0.0180189, acc 0.984375\n",
      "2018-05-23T14:54:38.714114: step 15034, loss 0.02813, acc 1\n",
      "2018-05-23T14:54:39.520955: step 15035, loss 0.0795136, acc 0.96875\n",
      "2018-05-23T14:54:40.470415: step 15036, loss 0.273633, acc 0.90625\n",
      "2018-05-23T14:54:40.905253: step 15037, loss 0.160197, acc 0.9375\n",
      "2018-05-23T14:54:41.306180: step 15038, loss 0.139729, acc 0.921875\n",
      "2018-05-23T14:54:41.651290: step 15039, loss 0.109444, acc 0.96875\n",
      "2018-05-23T14:54:42.121997: step 15040, loss 0.068719, acc 1\n",
      "2018-05-23T14:54:42.557831: step 15041, loss 0.0576386, acc 0.984375\n",
      "2018-05-23T14:54:42.952785: step 15042, loss 0.0787629, acc 0.96875\n",
      "2018-05-23T14:54:43.440471: step 15043, loss 0.0921653, acc 0.9375\n",
      "2018-05-23T14:54:43.857353: step 15044, loss 0.0567267, acc 1\n",
      "2018-05-23T14:54:44.369982: step 15045, loss 0.242925, acc 0.921875\n",
      "2018-05-23T14:54:44.747971: step 15046, loss 0.137734, acc 0.953125\n",
      "2018-05-23T14:54:45.120974: step 15047, loss 0.0245221, acc 1\n",
      "2018-05-23T14:54:45.730349: step 15048, loss 0.1296, acc 0.9375\n",
      "2018-05-23T14:54:46.177150: step 15049, loss 0.286023, acc 0.875\n",
      "2018-05-23T14:54:46.533197: step 15050, loss 0.0397059, acc 1\n",
      "2018-05-23T14:54:47.213377: step 15051, loss 0.0623878, acc 0.96875\n",
      "2018-05-23T14:54:47.723014: step 15052, loss 0.293513, acc 0.890625\n",
      "2018-05-23T14:54:48.157850: step 15053, loss 0.0715403, acc 0.953125\n",
      "2018-05-23T14:54:48.621610: step 15054, loss 0.0614828, acc 0.96875\n",
      "2018-05-23T14:54:49.123266: step 15055, loss 0.091902, acc 0.984375\n",
      "2018-05-23T14:54:49.542147: step 15056, loss 0.0936331, acc 0.96875\n",
      "2018-05-23T14:54:49.980973: step 15057, loss 0.0198247, acc 1\n",
      "2018-05-23T14:54:50.315080: step 15058, loss 0.190875, acc 0.90625\n",
      "2018-05-23T14:54:50.771857: step 15059, loss 0.070589, acc 0.96875\n",
      "2018-05-23T14:54:51.172786: step 15060, loss 0.150625, acc 0.921875\n",
      "2018-05-23T14:54:51.504895: step 15061, loss 0.141157, acc 0.90625\n",
      "2018-05-23T14:54:51.839003: step 15062, loss 0.0843799, acc 0.96875\n",
      "2018-05-23T14:54:52.163134: step 15063, loss 0.0886279, acc 0.953125\n",
      "2018-05-23T14:54:52.603954: step 15064, loss 0.101826, acc 0.984375\n",
      "2018-05-23T14:54:52.971972: step 15065, loss 0.180342, acc 0.953125\n",
      "2018-05-23T14:54:53.374892: step 15066, loss 0.115485, acc 0.953125\n",
      "2018-05-23T14:54:53.892508: step 15067, loss 0.140277, acc 0.96875\n",
      "2018-05-23T14:54:54.353275: step 15068, loss 0.112467, acc 0.953125\n",
      "2018-05-23T14:54:54.923750: step 15069, loss 0.221332, acc 0.921875\n",
      "2018-05-23T14:54:55.279797: step 15070, loss 0.136943, acc 0.90625\n",
      "2018-05-23T14:54:55.962969: step 15071, loss 0.0434569, acc 0.984375\n",
      "2018-05-23T14:54:56.335972: step 15072, loss 0.0974499, acc 0.953125\n",
      "2018-05-23T14:54:56.897470: step 15073, loss 0.0923874, acc 0.96875\n",
      "2018-05-23T14:54:57.437026: step 15074, loss 0.0457365, acc 0.96875\n",
      "2018-05-23T14:54:58.029442: step 15075, loss 0.0446626, acc 0.984375\n",
      "2018-05-23T14:54:58.494197: step 15076, loss 0.056517, acc 0.984375\n",
      "2018-05-23T14:54:59.070656: step 15077, loss 0.0720307, acc 0.984375\n",
      "2018-05-23T14:54:59.581290: step 15078, loss 0.122777, acc 0.921875\n",
      "2018-05-23T14:55:00.090926: step 15079, loss 0.127475, acc 0.953125\n",
      "2018-05-23T14:55:00.536733: step 15080, loss 0.0566524, acc 0.984375\n",
      "2018-05-23T14:55:01.012462: step 15081, loss 0.0597923, acc 0.984375\n",
      "2018-05-23T14:55:01.476223: step 15082, loss 0.117395, acc 0.9375\n",
      "2018-05-23T14:55:02.005803: step 15083, loss 0.101464, acc 0.953125\n",
      "2018-05-23T14:55:02.391771: step 15084, loss 0.0777256, acc 0.96875\n",
      "2018-05-23T14:55:02.779732: step 15085, loss 0.0784169, acc 0.984375\n",
      "2018-05-23T14:55:03.119825: step 15086, loss 0.0842315, acc 0.953125\n",
      "2018-05-23T14:55:03.444953: step 15087, loss 0.0857188, acc 0.953125\n",
      "2018-05-23T14:55:03.816959: step 15088, loss 0.0370739, acc 0.984375\n",
      "2018-05-23T14:55:04.167023: step 15089, loss 0.200871, acc 0.921875\n",
      "2018-05-23T14:55:04.496142: step 15090, loss 0.0769144, acc 0.96875\n",
      "2018-05-23T14:55:04.862163: step 15091, loss 0.0624679, acc 0.96875\n",
      "2018-05-23T14:55:05.238156: step 15092, loss 0.223475, acc 0.921875\n",
      "2018-05-23T14:55:05.561291: step 15093, loss 0.273079, acc 0.859375\n",
      "2018-05-23T14:55:05.889416: step 15094, loss 0.167455, acc 0.953125\n",
      "2018-05-23T14:55:06.204571: step 15095, loss 0.146438, acc 0.953125\n",
      "2018-05-23T14:55:06.551644: step 15096, loss 0.115574, acc 0.921875\n",
      "2018-05-23T14:55:06.927637: step 15097, loss 0.164093, acc 0.921875\n",
      "2018-05-23T14:55:07.319590: step 15098, loss 0.0665239, acc 0.984375\n",
      "2018-05-23T14:55:07.657683: step 15099, loss 0.0507374, acc 0.984375\n",
      "2018-05-23T14:55:08.006752: step 15100, loss 0.162087, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:55:13.780305: step 15100, loss 1.6084, acc 0.710387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15100\n",
      "\n",
      "2018-05-23T14:55:15.813864: step 15101, loss 0.0828751, acc 0.96875\n",
      "2018-05-23T14:55:16.211801: step 15102, loss 0.0965857, acc 0.984375\n",
      "2018-05-23T14:55:16.567849: step 15103, loss 0.130794, acc 0.953125\n",
      "2018-05-23T14:55:16.933870: step 15104, loss 0.0813529, acc 0.96875\n",
      "2018-05-23T14:55:17.320834: step 15105, loss 0.0322109, acc 0.984375\n",
      "2018-05-23T14:55:17.663915: step 15106, loss 0.0657945, acc 0.984375\n",
      "2018-05-23T14:55:18.067834: step 15107, loss 0.159319, acc 0.9375\n",
      "2018-05-23T14:55:18.444828: step 15108, loss 0.089274, acc 0.96875\n",
      "2018-05-23T14:55:18.796884: step 15109, loss 0.11877, acc 0.9375\n",
      "2018-05-23T14:55:19.136976: step 15110, loss 0.0624721, acc 0.984375\n",
      "2018-05-23T14:55:19.472078: step 15111, loss 0.107764, acc 0.953125\n",
      "2018-05-23T14:55:19.822142: step 15112, loss 0.0679388, acc 0.984375\n",
      "2018-05-23T14:55:20.552190: step 15113, loss 0.314269, acc 0.875\n",
      "2018-05-23T14:55:21.064817: step 15114, loss 0.166253, acc 0.921875\n",
      "2018-05-23T14:55:21.550518: step 15115, loss 0.123749, acc 0.96875\n",
      "2018-05-23T14:55:22.226709: step 15116, loss 0.0613543, acc 0.96875\n",
      "2018-05-23T14:55:22.659552: step 15117, loss 0.109074, acc 0.953125\n",
      "2018-05-23T14:55:23.117325: step 15118, loss 0.0309296, acc 0.984375\n",
      "2018-05-23T14:55:23.647907: step 15119, loss 0.151043, acc 0.9375\n",
      "2018-05-23T14:55:24.099701: step 15120, loss 0.0839396, acc 0.96875\n",
      "2018-05-23T14:55:24.493648: step 15121, loss 0.157788, acc 0.890625\n",
      "2018-05-23T14:55:24.892613: step 15122, loss 0.0963367, acc 0.953125\n",
      "2018-05-23T14:55:25.282535: step 15123, loss 0.0990013, acc 0.953125\n",
      "2018-05-23T14:55:25.666506: step 15124, loss 0.133105, acc 0.953125\n",
      "2018-05-23T14:55:26.095360: step 15125, loss 0.235161, acc 0.890625\n",
      "2018-05-23T14:55:26.552137: step 15126, loss 0.0503895, acc 0.984375\n",
      "2018-05-23T14:55:26.953067: step 15127, loss 0.113107, acc 0.96875\n",
      "2018-05-23T14:55:27.401865: step 15128, loss 0.127856, acc 0.9375\n",
      "2018-05-23T14:55:27.824733: step 15129, loss 0.12211, acc 0.90625\n",
      "2018-05-23T14:55:28.304451: step 15130, loss 0.108596, acc 0.984375\n",
      "2018-05-23T14:55:28.807104: step 15131, loss 0.0790925, acc 0.953125\n",
      "2018-05-23T14:55:29.239947: step 15132, loss 0.0576032, acc 0.96875\n",
      "2018-05-23T14:55:29.645861: step 15133, loss 0.0654781, acc 0.96875\n",
      "2018-05-23T14:55:30.031830: step 15134, loss 0.0514251, acc 0.96875\n",
      "2018-05-23T14:55:30.468660: step 15135, loss 0.0499639, acc 1\n",
      "2018-05-23T14:55:31.049108: step 15136, loss 0.152937, acc 0.90625\n",
      "2018-05-23T14:55:31.473970: step 15137, loss 0.0448708, acc 0.984375\n",
      "2018-05-23T14:55:32.908726: step 15138, loss 0.0445055, acc 0.96875\n",
      "2018-05-23T14:55:34.214575: step 15139, loss 0.0546965, acc 0.984375\n",
      "2018-05-23T14:55:35.169023: step 15140, loss 0.108844, acc 0.9375\n",
      "2018-05-23T14:55:36.002790: step 15141, loss 0.112764, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:55:36.407710: step 15142, loss 0.130176, acc 0.953125\n",
      "2018-05-23T14:55:36.775723: step 15143, loss 0.0265935, acc 0.984375\n",
      "2018-05-23T14:55:37.126786: step 15144, loss 0.131609, acc 0.9375\n",
      "2018-05-23T14:55:37.660359: step 15145, loss 0.0921095, acc 0.953125\n",
      "2018-05-23T14:55:38.157028: step 15146, loss 0.0738917, acc 0.96875\n",
      "2018-05-23T14:55:38.517090: step 15147, loss 0.147321, acc 0.921875\n",
      "2018-05-23T14:55:39.199267: step 15148, loss 0.148243, acc 0.953125\n",
      "2018-05-23T14:55:39.733836: step 15149, loss 0.192294, acc 0.875\n",
      "2018-05-23T14:55:40.228513: step 15150, loss 0.162005, acc 0.9375\n",
      "2018-05-23T14:55:40.611489: step 15151, loss 0.0527989, acc 0.984375\n",
      "2018-05-23T14:55:41.168000: step 15152, loss 0.0566313, acc 0.984375\n",
      "2018-05-23T14:55:41.952900: step 15153, loss 0.120623, acc 0.984375\n",
      "2018-05-23T14:55:42.881956: step 15154, loss 0.0963105, acc 0.953125\n",
      "2018-05-23T14:55:44.104685: step 15155, loss 0.134467, acc 0.953125\n",
      "2018-05-23T14:55:44.857075: step 15156, loss 0.0859019, acc 0.96875\n",
      "2018-05-23T14:55:46.284303: step 15157, loss 0.059984, acc 0.984375\n",
      "2018-05-23T14:55:47.082677: step 15158, loss 0.0983459, acc 0.953125\n",
      "2018-05-23T14:55:47.550425: step 15159, loss 0.0903228, acc 0.953125\n",
      "2018-05-23T14:55:48.052084: step 15160, loss 0.124668, acc 0.96875\n",
      "2018-05-23T14:55:48.851943: step 15161, loss 0.0965471, acc 0.953125\n",
      "2018-05-23T14:55:49.455331: step 15162, loss 0.0649355, acc 0.96875\n",
      "2018-05-23T14:55:49.943024: step 15163, loss 0.0379932, acc 1\n",
      "2018-05-23T14:55:50.493571: step 15164, loss 0.215725, acc 0.984375\n",
      "2018-05-23T14:55:51.040126: step 15165, loss 0.0586715, acc 0.984375\n",
      "2018-05-23T14:55:51.731240: step 15166, loss 0.138749, acc 0.96875\n",
      "2018-05-23T14:55:52.230905: step 15167, loss 0.109538, acc 0.953125\n",
      "2018-05-23T14:55:52.782429: step 15168, loss 0.111397, acc 0.9375\n",
      "2018-05-23T14:55:53.617197: step 15169, loss 0.30294, acc 0.953125\n",
      "2018-05-23T14:55:54.300368: step 15170, loss 0.0894131, acc 0.96875\n",
      "2018-05-23T14:55:54.963593: step 15171, loss 0.0313394, acc 1\n",
      "2018-05-23T14:55:55.801352: step 15172, loss 0.07819, acc 0.96875\n",
      "2018-05-23T14:55:56.554339: step 15173, loss 0.0453041, acc 0.96875\n",
      "2018-05-23T14:55:57.946614: step 15174, loss 0.155667, acc 0.9375\n",
      "2018-05-23T14:55:58.737498: step 15175, loss 0.129334, acc 0.953125\n",
      "2018-05-23T14:55:59.810627: step 15176, loss 0.115868, acc 0.953125\n",
      "2018-05-23T14:56:00.659357: step 15177, loss 0.0696728, acc 0.96875\n",
      "2018-05-23T14:56:01.318594: step 15178, loss 0.121847, acc 0.953125\n",
      "2018-05-23T14:56:02.004757: step 15179, loss 0.149844, acc 0.90625\n",
      "2018-05-23T14:56:02.693914: step 15180, loss 0.111612, acc 0.9375\n",
      "2018-05-23T14:56:03.264387: step 15181, loss 0.116407, acc 0.9375\n",
      "2018-05-23T14:56:03.889715: step 15182, loss 0.0576835, acc 0.984375\n",
      "2018-05-23T14:56:04.658658: step 15183, loss 0.0840714, acc 0.96875\n",
      "2018-05-23T14:56:05.433584: step 15184, loss 0.196874, acc 0.90625\n",
      "2018-05-23T14:56:06.001066: step 15185, loss 0.119051, acc 0.96875\n",
      "2018-05-23T14:56:06.637364: step 15186, loss 0.0357652, acc 1\n",
      "2018-05-23T14:56:07.137029: step 15187, loss 0.086421, acc 0.9375\n",
      "2018-05-23T14:56:07.636691: step 15188, loss 0.128371, acc 0.9375\n",
      "2018-05-23T14:56:08.388679: step 15189, loss 0.0916507, acc 0.96875\n",
      "2018-05-23T14:56:09.050908: step 15190, loss 0.112498, acc 0.96875\n",
      "2018-05-23T14:56:09.589467: step 15191, loss 0.0929751, acc 0.953125\n",
      "2018-05-23T14:56:10.163930: step 15192, loss 0.168455, acc 0.890625\n",
      "2018-05-23T14:56:10.754351: step 15193, loss 0.180958, acc 0.890625\n",
      "2018-05-23T14:56:11.286925: step 15194, loss 0.155085, acc 0.953125\n",
      "2018-05-23T14:56:11.967105: step 15195, loss 0.0805115, acc 0.953125\n",
      "2018-05-23T14:56:12.448816: step 15196, loss 0.151212, acc 0.9375\n",
      "2018-05-23T14:56:12.943494: step 15197, loss 0.1478, acc 0.9375\n",
      "2018-05-23T14:56:13.444154: step 15198, loss 0.0959582, acc 0.953125\n",
      "2018-05-23T14:56:14.001664: step 15199, loss 0.121654, acc 0.96875\n",
      "2018-05-23T14:56:14.549197: step 15200, loss 0.0671068, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:56:21.324074: step 15200, loss 1.64571, acc 0.707101\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15200\n",
      "\n",
      "2018-05-23T14:56:22.850989: step 15201, loss 0.191263, acc 0.953125\n",
      "2018-05-23T14:56:23.245932: step 15202, loss 0.160589, acc 0.90625\n",
      "2018-05-23T14:56:23.622924: step 15203, loss 0.169286, acc 0.9375\n",
      "2018-05-23T14:56:24.002907: step 15204, loss 0.0274362, acc 0.984375\n",
      "2018-05-23T14:56:24.357959: step 15205, loss 0.0660719, acc 0.984375\n",
      "2018-05-23T14:56:24.868592: step 15206, loss 0.227531, acc 0.890625\n",
      "2018-05-23T14:56:25.271513: step 15207, loss 0.135914, acc 0.96875\n",
      "2018-05-23T14:56:25.751233: step 15208, loss 0.201311, acc 0.921875\n",
      "2018-05-23T14:56:26.205018: step 15209, loss 0.057588, acc 0.984375\n",
      "2018-05-23T14:56:26.734600: step 15210, loss 0.0926506, acc 0.96875\n",
      "2018-05-23T14:56:27.119569: step 15211, loss 0.124491, acc 0.921875\n",
      "2018-05-23T14:56:27.490577: step 15212, loss 0.0867057, acc 0.984375\n",
      "2018-05-23T14:56:27.870562: step 15213, loss 0.0703149, acc 0.984375\n",
      "2018-05-23T14:56:28.223616: step 15214, loss 0.0814745, acc 0.96875\n",
      "2018-05-23T14:56:28.578667: step 15215, loss 0.168493, acc 0.953125\n",
      "2018-05-23T14:56:28.936709: step 15216, loss 0.162287, acc 0.953125\n",
      "2018-05-23T14:56:29.275700: step 15217, loss 0.152285, acc 0.9375\n",
      "2018-05-23T14:56:29.631746: step 15218, loss 0.198046, acc 0.921875\n",
      "2018-05-23T14:56:29.986797: step 15219, loss 0.104045, acc 0.953125\n",
      "2018-05-23T14:56:30.326886: step 15220, loss 0.0923562, acc 0.96875\n",
      "2018-05-23T14:56:30.682934: step 15221, loss 0.153556, acc 0.921875\n",
      "2018-05-23T14:56:31.032998: step 15222, loss 0.122284, acc 0.96875\n",
      "2018-05-23T14:56:31.400018: step 15223, loss 0.0950442, acc 0.953125\n",
      "2018-05-23T14:56:31.800945: step 15224, loss 0.0926725, acc 0.9375\n",
      "2018-05-23T14:56:32.304596: step 15225, loss 0.0674659, acc 0.984375\n",
      "2018-05-23T14:56:32.679592: step 15226, loss 0.0588899, acc 0.96875\n",
      "2018-05-23T14:56:33.168286: step 15227, loss 0.0947738, acc 0.953125\n",
      "2018-05-23T14:56:33.593148: step 15228, loss 0.174283, acc 0.9375\n",
      "2018-05-23T14:56:34.025990: step 15229, loss 0.151054, acc 0.9375\n",
      "2018-05-23T14:56:34.652351: step 15230, loss 0.102451, acc 0.96875\n",
      "2018-05-23T14:56:35.546922: step 15231, loss 0.139832, acc 0.90625\n",
      "2018-05-23T14:56:36.336809: step 15232, loss 0.075129, acc 0.984375\n",
      "2018-05-23T14:56:36.979091: step 15233, loss 0.0659064, acc 0.96875\n",
      "2018-05-23T14:56:37.587464: step 15234, loss 0.0670606, acc 0.96875\n",
      "2018-05-23T14:56:38.195837: step 15235, loss 0.118951, acc 0.9375\n",
      "2018-05-23T14:56:38.808198: step 15236, loss 0.0498919, acc 0.984375\n",
      "2018-05-23T14:56:39.391637: step 15237, loss 0.0835442, acc 0.984375\n",
      "2018-05-23T14:56:40.081790: step 15238, loss 0.0864133, acc 0.96875\n",
      "2018-05-23T14:56:40.723075: step 15239, loss 0.0754311, acc 0.953125\n",
      "2018-05-23T14:56:41.540888: step 15240, loss 0.121548, acc 0.953125\n",
      "2018-05-23T14:56:42.391612: step 15241, loss 0.221088, acc 0.953125\n",
      "2018-05-23T14:56:43.107697: step 15242, loss 0.104702, acc 0.9375\n",
      "2018-05-23T14:56:43.724047: step 15243, loss 0.209415, acc 0.9375\n",
      "2018-05-23T14:56:44.364335: step 15244, loss 0.0797836, acc 0.96875\n",
      "2018-05-23T14:56:45.126297: step 15245, loss 0.117788, acc 0.96875\n",
      "2018-05-23T14:56:45.780546: step 15246, loss 0.136884, acc 0.96875\n",
      "2018-05-23T14:56:46.394903: step 15247, loss 0.0396298, acc 0.984375\n",
      "2018-05-23T14:56:47.235654: step 15248, loss 0.153551, acc 0.984375\n",
      "2018-05-23T14:56:47.925807: step 15249, loss 0.0479599, acc 1\n",
      "2018-05-23T14:56:48.585043: step 15250, loss 0.132951, acc 0.921875\n",
      "2018-05-23T14:56:49.373933: step 15251, loss 0.0832319, acc 0.96875\n",
      "2018-05-23T14:56:50.043142: step 15252, loss 0.111212, acc 0.953125\n",
      "2018-05-23T14:56:50.700385: step 15253, loss 0.0993784, acc 0.96875\n",
      "2018-05-23T14:56:51.282827: step 15254, loss 0.0873576, acc 0.96875\n",
      "2018-05-23T14:56:52.022846: step 15255, loss 0.168798, acc 0.96875\n",
      "2018-05-23T14:56:52.870578: step 15256, loss 0.113104, acc 0.9375\n",
      "2018-05-23T14:56:53.665960: step 15257, loss 0.123843, acc 0.953125\n",
      "2018-05-23T14:56:54.425927: step 15258, loss 0.0920251, acc 0.953125\n",
      "2018-05-23T14:56:55.089152: step 15259, loss 0.080214, acc 0.96875\n",
      "2018-05-23T14:56:56.406628: step 15260, loss 0.056002, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:56:57.181554: step 15261, loss 0.066833, acc 0.96875\n",
      "2018-05-23T14:56:57.949501: step 15262, loss 0.094043, acc 0.96875\n",
      "2018-05-23T14:56:58.976752: step 15263, loss 0.0722556, acc 0.96875\n",
      "2018-05-23T14:56:59.723754: step 15264, loss 0.104509, acc 0.9375\n",
      "2018-05-23T14:57:00.436847: step 15265, loss 0.230537, acc 0.921875\n",
      "2018-05-23T14:57:01.034249: step 15266, loss 0.0716582, acc 1\n",
      "2018-05-23T14:57:01.645613: step 15267, loss 0.124496, acc 0.96875\n",
      "2018-05-23T14:57:02.212098: step 15268, loss 0.0843995, acc 0.96875\n",
      "2018-05-23T14:57:02.812492: step 15269, loss 0.119938, acc 0.921875\n",
      "2018-05-23T14:57:03.582431: step 15270, loss 0.0829269, acc 0.984375\n",
      "2018-05-23T14:57:04.304499: step 15271, loss 0.0503447, acc 1\n",
      "2018-05-23T14:57:04.957752: step 15272, loss 0.0853558, acc 0.96875\n",
      "2018-05-23T14:57:05.619980: step 15273, loss 0.12404, acc 0.921875\n",
      "2018-05-23T14:57:06.145576: step 15274, loss 0.0670722, acc 0.96875\n",
      "2018-05-23T14:57:06.661196: step 15275, loss 0.0897821, acc 0.953125\n",
      "2018-05-23T14:57:07.392239: step 15276, loss 0.18501, acc 0.9375\n",
      "2018-05-23T14:57:08.091369: step 15277, loss 0.0706239, acc 0.96875\n",
      "2018-05-23T14:57:08.694788: step 15278, loss 0.0531126, acc 0.984375\n",
      "2018-05-23T14:57:09.460761: step 15279, loss 0.0477854, acc 0.984375\n",
      "2018-05-23T14:57:10.188853: step 15280, loss 0.101453, acc 0.9375\n",
      "2018-05-23T14:57:11.104404: step 15281, loss 0.0804185, acc 0.96875\n",
      "2018-05-23T14:57:11.947149: step 15282, loss 0.0795874, acc 0.96875\n",
      "2018-05-23T14:57:12.553527: step 15283, loss 0.120434, acc 0.9375\n",
      "2018-05-23T14:57:13.097072: step 15284, loss 0.20752, acc 0.875\n",
      "2018-05-23T14:57:13.641617: step 15285, loss 0.0493035, acc 0.984375\n",
      "2018-05-23T14:57:14.144272: step 15286, loss 0.0416138, acc 1\n",
      "2018-05-23T14:57:14.747658: step 15287, loss 0.105562, acc 0.953125\n",
      "2018-05-23T14:57:15.305166: step 15288, loss 0.0633803, acc 0.984375\n",
      "2018-05-23T14:57:16.028232: step 15289, loss 0.076557, acc 0.96875\n",
      "2018-05-23T14:57:16.629622: step 15290, loss 0.10619, acc 0.953125\n",
      "2018-05-23T14:57:17.152225: step 15291, loss 0.191326, acc 0.921875\n",
      "2018-05-23T14:57:17.746635: step 15292, loss 0.0559079, acc 0.984375\n",
      "2018-05-23T14:57:18.281205: step 15293, loss 0.219193, acc 0.921875\n",
      "2018-05-23T14:57:18.788846: step 15294, loss 0.0560386, acc 0.96875\n",
      "2018-05-23T14:57:19.290505: step 15295, loss 0.0438934, acc 0.984375\n",
      "2018-05-23T14:57:19.851005: step 15296, loss 0.0720912, acc 0.96875\n",
      "2018-05-23T14:57:20.399538: step 15297, loss 0.106504, acc 0.96875\n",
      "2018-05-23T14:57:20.974999: step 15298, loss 0.0686345, acc 0.96875\n",
      "2018-05-23T14:57:21.704049: step 15299, loss 0.0703499, acc 0.984375\n",
      "2018-05-23T14:57:22.312421: step 15300, loss 0.141827, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:57:28.572673: step 15300, loss 1.63561, acc 0.714102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15300\n",
      "\n",
      "2018-05-23T14:57:30.728905: step 15301, loss 0.143923, acc 0.90625\n",
      "2018-05-23T14:57:31.249512: step 15302, loss 0.106811, acc 0.9375\n",
      "2018-05-23T14:57:31.733217: step 15303, loss 0.0461938, acc 1\n",
      "2018-05-23T14:57:32.225899: step 15304, loss 0.119134, acc 0.953125\n",
      "2018-05-23T14:57:32.708608: step 15305, loss 0.152836, acc 0.9375\n",
      "2018-05-23T14:57:33.218245: step 15306, loss 0.07778, acc 0.984375\n",
      "2018-05-23T14:57:33.692976: step 15307, loss 0.0304451, acc 1\n",
      "2018-05-23T14:57:34.146761: step 15308, loss 0.187104, acc 0.9375\n",
      "2018-05-23T14:57:34.603539: step 15309, loss 0.0679259, acc 0.953125\n",
      "2018-05-23T14:57:35.079267: step 15310, loss 0.0999821, acc 0.96875\n",
      "2018-05-23T14:57:35.647746: step 15311, loss 0.251116, acc 0.90625\n",
      "2018-05-23T14:57:36.122476: step 15312, loss 0.0609235, acc 0.96875\n",
      "2018-05-23T14:57:36.586235: step 15313, loss 0.0691799, acc 0.96875\n",
      "2018-05-23T14:57:37.047999: step 15314, loss 0.181393, acc 0.921875\n",
      "2018-05-23T14:57:37.541679: step 15315, loss 0.0527631, acc 0.984375\n",
      "2018-05-23T14:57:37.976515: step 15316, loss 0.165967, acc 0.921875\n",
      "2018-05-23T14:57:38.419330: step 15317, loss 0.151965, acc 0.96875\n",
      "2018-05-23T14:57:38.847186: step 15318, loss 0.0412588, acc 0.984375\n",
      "2018-05-23T14:57:39.321916: step 15319, loss 0.0908991, acc 0.96875\n",
      "2018-05-23T14:57:39.808614: step 15320, loss 0.128065, acc 0.921875\n",
      "2018-05-23T14:57:40.284342: step 15321, loss 0.0833043, acc 0.96875\n",
      "2018-05-23T14:57:40.746106: step 15322, loss 0.0582756, acc 1\n",
      "2018-05-23T14:57:41.319573: step 15323, loss 0.0924794, acc 0.953125\n",
      "2018-05-23T14:57:41.895034: step 15324, loss 0.0449279, acc 1\n",
      "2018-05-23T14:57:42.481464: step 15325, loss 0.213508, acc 0.953125\n",
      "2018-05-23T14:57:43.136711: step 15326, loss 0.0748159, acc 0.96875\n",
      "2018-05-23T14:57:43.898673: step 15327, loss 0.350323, acc 0.859375\n",
      "2018-05-23T14:57:44.647180: step 15328, loss 0.12868, acc 0.9375\n",
      "2018-05-23T14:57:45.285473: step 15329, loss 0.180709, acc 0.890625\n",
      "2018-05-23T14:57:45.959669: step 15330, loss 0.126087, acc 0.953125\n",
      "2018-05-23T14:57:46.628879: step 15331, loss 0.127529, acc 0.9375\n",
      "2018-05-23T14:57:47.515506: step 15332, loss 0.0762513, acc 0.96875\n",
      "2018-05-23T14:57:48.249543: step 15333, loss 0.0426411, acc 0.984375\n",
      "2018-05-23T14:57:49.168121: step 15334, loss 0.066306, acc 0.96875\n",
      "2018-05-23T14:57:49.918115: step 15335, loss 0.116943, acc 0.9375\n",
      "2018-05-23T14:57:50.748892: step 15336, loss 0.030458, acc 0.984375\n",
      "2018-05-23T14:57:51.371227: step 15337, loss 0.187044, acc 0.9375\n",
      "2018-05-23T14:57:52.206992: step 15338, loss 0.107213, acc 0.953125\n",
      "2018-05-23T14:57:53.006853: step 15339, loss 0.141063, acc 0.953125\n",
      "2018-05-23T14:57:53.670076: step 15340, loss 0.0519941, acc 0.984375\n",
      "2018-05-23T14:57:54.475921: step 15341, loss 0.126637, acc 0.9375\n",
      "2018-05-23T14:57:55.249851: step 15342, loss 0.0704061, acc 0.96875\n",
      "2018-05-23T14:57:55.991866: step 15343, loss 0.108725, acc 0.921875\n",
      "2018-05-23T14:57:56.545386: step 15344, loss 0.124843, acc 0.9375\n",
      "2018-05-23T14:57:57.080952: step 15345, loss 0.053519, acc 0.96875\n",
      "2018-05-23T14:57:57.661399: step 15346, loss 0.0892041, acc 0.96875\n",
      "2018-05-23T14:57:58.194972: step 15347, loss 0.0822332, acc 0.96875\n",
      "2018-05-23T14:57:58.768438: step 15348, loss 0.119922, acc 0.921875\n",
      "2018-05-23T14:57:59.343899: step 15349, loss 0.161603, acc 0.9375\n",
      "2018-05-23T14:57:59.854533: step 15350, loss 0.109444, acc 0.953125\n",
      "2018-05-23T14:58:00.393092: step 15351, loss 0.184249, acc 0.9375\n",
      "2018-05-23T14:58:00.957581: step 15352, loss 0.0798953, acc 0.953125\n",
      "2018-05-23T14:58:01.483175: step 15353, loss 0.188625, acc 0.890625\n",
      "2018-05-23T14:58:02.076589: step 15354, loss 0.0768232, acc 0.953125\n",
      "2018-05-23T14:58:02.619137: step 15355, loss 0.160342, acc 0.953125\n",
      "2018-05-23T14:58:03.127776: step 15356, loss 0.154266, acc 0.90625\n",
      "2018-05-23T14:58:03.633422: step 15357, loss 0.172154, acc 0.953125\n",
      "2018-05-23T14:58:04.124110: step 15358, loss 0.0509873, acc 0.96875\n",
      "2018-05-23T14:58:04.633747: step 15359, loss 0.129321, acc 0.9375\n",
      "2018-05-23T14:58:05.146377: step 15360, loss 0.135768, acc 0.921875\n",
      "2018-05-23T14:58:05.691915: step 15361, loss 0.0959291, acc 0.953125\n",
      "2018-05-23T14:58:06.225488: step 15362, loss 0.161902, acc 0.953125\n",
      "2018-05-23T14:58:06.769035: step 15363, loss 0.129551, acc 0.921875\n",
      "2018-05-23T14:58:07.271690: step 15364, loss 0.0697218, acc 0.96875\n",
      "2018-05-23T14:58:07.761380: step 15365, loss 0.109645, acc 0.953125\n",
      "2018-05-23T14:58:08.283982: step 15366, loss 0.0509714, acc 0.984375\n",
      "2018-05-23T14:58:08.778659: step 15367, loss 0.0332046, acc 0.984375\n",
      "2018-05-23T14:58:09.301261: step 15368, loss 0.125232, acc 0.9375\n",
      "2018-05-23T14:58:09.828849: step 15369, loss 0.111141, acc 0.96875\n",
      "2018-05-23T14:58:10.336492: step 15370, loss 0.111701, acc 0.953125\n",
      "2018-05-23T14:58:10.823189: step 15371, loss 0.0552418, acc 0.96875\n",
      "2018-05-23T14:58:11.350778: step 15372, loss 0.0947093, acc 0.96875\n",
      "2018-05-23T14:58:11.881358: step 15373, loss 0.149781, acc 0.9375\n",
      "2018-05-23T14:58:12.393986: step 15374, loss 0.0709791, acc 1\n",
      "2018-05-23T14:58:12.915592: step 15375, loss 0.118375, acc 0.96875\n",
      "2018-05-23T14:58:13.403287: step 15376, loss 0.0838867, acc 0.953125\n",
      "2018-05-23T14:58:13.886994: step 15377, loss 0.0997652, acc 0.984375\n",
      "2018-05-23T14:58:14.432533: step 15378, loss 0.0688665, acc 0.96875\n",
      "2018-05-23T14:58:14.964111: step 15379, loss 0.0937085, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:58:15.478735: step 15380, loss 0.115413, acc 0.9375\n",
      "2018-05-23T14:58:16.034248: step 15381, loss 0.0497278, acc 0.96875\n",
      "2018-05-23T14:58:16.551864: step 15382, loss 0.138387, acc 0.921875\n",
      "2018-05-23T14:58:17.067484: step 15383, loss 0.0821595, acc 0.96875\n",
      "2018-05-23T14:58:17.585100: step 15384, loss 0.0581498, acc 0.96875\n",
      "2018-05-23T14:58:17.996998: step 15385, loss 0.161847, acc 0.921875\n",
      "2018-05-23T14:58:18.424854: step 15386, loss 0.112697, acc 0.984375\n",
      "2018-05-23T14:58:18.907561: step 15387, loss 0.105321, acc 0.921875\n",
      "2018-05-23T14:58:19.327439: step 15388, loss 0.250127, acc 0.921875\n",
      "2018-05-23T14:58:19.805162: step 15389, loss 0.0859628, acc 0.96875\n",
      "2018-05-23T14:58:20.244984: step 15390, loss 0.121379, acc 0.953125\n",
      "2018-05-23T14:58:20.853365: step 15391, loss 0.0693555, acc 0.96875\n",
      "2018-05-23T14:58:21.478683: step 15392, loss 0.0950798, acc 0.953125\n",
      "2018-05-23T14:58:22.122960: step 15393, loss 0.0950348, acc 0.953125\n",
      "2018-05-23T14:58:22.751279: step 15394, loss 0.0749055, acc 0.984375\n",
      "2018-05-23T14:58:23.320756: step 15395, loss 0.0717304, acc 0.96875\n",
      "2018-05-23T14:58:23.963038: step 15396, loss 0.080277, acc 0.96875\n",
      "2018-05-23T14:58:24.481649: step 15397, loss 0.101679, acc 0.921875\n",
      "2018-05-23T14:58:24.928455: step 15398, loss 0.0839173, acc 0.96875\n",
      "2018-05-23T14:58:25.441084: step 15399, loss 0.0596572, acc 0.953125\n",
      "2018-05-23T14:58:25.864950: step 15400, loss 0.103942, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:58:31.353267: step 15400, loss 1.67419, acc 0.713245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15400\n",
      "\n",
      "2018-05-23T14:58:33.268144: step 15401, loss 0.0890694, acc 0.953125\n",
      "2018-05-23T14:58:33.711957: step 15402, loss 0.328921, acc 0.890625\n",
      "2018-05-23T14:58:34.153775: step 15403, loss 0.0423799, acc 0.984375\n",
      "2018-05-23T14:58:34.604569: step 15404, loss 0.155286, acc 0.890625\n",
      "2018-05-23T14:58:35.032424: step 15405, loss 0.0550733, acc 0.953125\n",
      "2018-05-23T14:58:35.486210: step 15406, loss 0.0670982, acc 0.984375\n",
      "2018-05-23T14:58:36.107549: step 15407, loss 0.170756, acc 0.859375\n",
      "2018-05-23T14:58:36.712929: step 15408, loss 0.290596, acc 0.90625\n",
      "2018-05-23T14:58:37.352220: step 15409, loss 0.149229, acc 0.9375\n",
      "2018-05-23T14:58:37.810994: step 15410, loss 0.0619012, acc 1\n",
      "2018-05-23T14:58:38.450282: step 15411, loss 0.110502, acc 0.921875\n",
      "2018-05-23T14:58:39.167363: step 15412, loss 0.231571, acc 0.90625\n",
      "2018-05-23T14:58:39.871480: step 15413, loss 0.0582443, acc 0.96875\n",
      "2018-05-23T14:58:40.596540: step 15414, loss 0.104266, acc 0.9375\n",
      "2018-05-23T14:58:41.222865: step 15415, loss 0.226003, acc 0.9375\n",
      "2018-05-23T14:58:41.789349: step 15416, loss 0.0941789, acc 0.96875\n",
      "2018-05-23T14:58:42.299983: step 15417, loss 0.153493, acc 0.921875\n",
      "2018-05-23T14:58:43.012078: step 15418, loss 0.0962908, acc 0.953125\n",
      "2018-05-23T14:58:43.577567: step 15419, loss 0.049432, acc 0.96875\n",
      "2018-05-23T14:58:44.115128: step 15420, loss 0.338687, acc 0.9375\n",
      "2018-05-23T14:58:44.761398: step 15421, loss 0.190081, acc 0.90625\n",
      "2018-05-23T14:58:45.412689: step 15422, loss 0.0969136, acc 0.96875\n",
      "2018-05-23T14:58:45.928277: step 15423, loss 0.0962145, acc 0.984375\n",
      "2018-05-23T14:58:46.524680: step 15424, loss 0.0731797, acc 0.953125\n",
      "2018-05-23T14:58:47.056259: step 15425, loss 0.0662794, acc 0.96875\n",
      "2018-05-23T14:58:47.568888: step 15426, loss 0.0570428, acc 0.984375\n",
      "2018-05-23T14:58:48.189227: step 15427, loss 0.0765742, acc 0.9375\n",
      "2018-05-23T14:58:48.586166: step 15428, loss 0.0746819, acc 0.96875\n",
      "2018-05-23T14:58:48.969142: step 15429, loss 0.0454018, acc 0.984375\n",
      "2018-05-23T14:58:49.402983: step 15430, loss 0.100258, acc 0.96875\n",
      "2018-05-23T14:58:49.844798: step 15431, loss 0.130824, acc 0.921875\n",
      "2018-05-23T14:58:50.225778: step 15432, loss 0.0998187, acc 0.96875\n",
      "2018-05-23T14:58:50.630697: step 15433, loss 0.0803051, acc 0.953125\n",
      "2018-05-23T14:58:51.071516: step 15434, loss 0.0535231, acc 0.984375\n",
      "2018-05-23T14:58:51.436542: step 15435, loss 0.175925, acc 0.90625\n",
      "2018-05-23T14:58:51.812537: step 15436, loss 0.141237, acc 0.921875\n",
      "2018-05-23T14:58:52.188528: step 15437, loss 0.0308655, acc 0.984375\n",
      "2018-05-23T14:58:52.609405: step 15438, loss 0.193288, acc 0.921875\n",
      "2018-05-23T14:58:53.032274: step 15439, loss 0.112754, acc 0.953125\n",
      "2018-05-23T14:58:53.410260: step 15440, loss 0.0361872, acc 1\n",
      "2018-05-23T14:58:53.831134: step 15441, loss 0.12164, acc 0.921875\n",
      "2018-05-23T14:58:54.216104: step 15442, loss 0.123665, acc 0.96875\n",
      "2018-05-23T14:58:54.591101: step 15443, loss 0.0737935, acc 0.96875\n",
      "2018-05-23T14:58:54.991031: step 15444, loss 0.115608, acc 0.953125\n",
      "2018-05-23T14:58:55.424870: step 15445, loss 0.155585, acc 0.984375\n",
      "2018-05-23T14:58:55.798870: step 15446, loss 0.107863, acc 0.984375\n",
      "2018-05-23T14:58:56.166885: step 15447, loss 0.0678513, acc 0.984375\n",
      "2018-05-23T14:58:56.552443: step 15448, loss 0.0485537, acc 0.984375\n",
      "2018-05-23T14:58:57.003238: step 15449, loss 0.246804, acc 0.90625\n",
      "2018-05-23T14:58:57.381225: step 15450, loss 0.0767281, acc 0.96875\n",
      "2018-05-23T14:58:57.765198: step 15451, loss 0.130128, acc 0.9375\n",
      "2018-05-23T14:58:58.127265: step 15452, loss 0.122283, acc 0.90625\n",
      "2018-05-23T14:58:58.562067: step 15453, loss 0.170534, acc 0.953125\n",
      "2018-05-23T14:58:58.947037: step 15454, loss 0.075434, acc 0.96875\n",
      "2018-05-23T14:58:59.334004: step 15455, loss 0.0605742, acc 0.953125\n",
      "2018-05-23T14:58:59.715979: step 15456, loss 0.127388, acc 0.921875\n",
      "2018-05-23T14:59:00.137853: step 15457, loss 0.185691, acc 0.96875\n",
      "2018-05-23T14:59:00.510853: step 15458, loss 0.113829, acc 0.9375\n",
      "2018-05-23T14:59:00.881861: step 15459, loss 0.112032, acc 0.96875\n",
      "2018-05-23T14:59:01.253865: step 15460, loss 0.212211, acc 0.96875\n",
      "2018-05-23T14:59:01.687707: step 15461, loss 0.0703358, acc 0.984375\n",
      "2018-05-23T14:59:02.063700: step 15462, loss 0.0757796, acc 0.96875\n",
      "2018-05-23T14:59:02.427725: step 15463, loss 0.0771484, acc 0.96875\n",
      "2018-05-23T14:59:02.826658: step 15464, loss 0.068975, acc 0.96875\n",
      "2018-05-23T14:59:03.236562: step 15465, loss 0.130575, acc 0.953125\n",
      "2018-05-23T14:59:03.609563: step 15466, loss 0.0947876, acc 0.96875\n",
      "2018-05-23T14:59:03.984562: step 15467, loss 0.120656, acc 0.921875\n",
      "2018-05-23T14:59:04.349586: step 15468, loss 0.0835574, acc 0.96875\n",
      "2018-05-23T14:59:04.786415: step 15469, loss 0.0808687, acc 0.9375\n",
      "2018-05-23T14:59:05.165402: step 15470, loss 0.211289, acc 0.890625\n",
      "2018-05-23T14:59:05.566330: step 15471, loss 0.063268, acc 0.96875\n",
      "2018-05-23T14:59:05.957283: step 15472, loss 0.133982, acc 0.9375\n",
      "2018-05-23T14:59:06.351228: step 15473, loss 0.116712, acc 0.984375\n",
      "2018-05-23T14:59:06.731725: step 15474, loss 0.289473, acc 0.921875\n",
      "2018-05-23T14:59:07.118689: step 15475, loss 0.0991523, acc 0.96875\n",
      "2018-05-23T14:59:07.524604: step 15476, loss 0.0964811, acc 0.9375\n",
      "2018-05-23T14:59:07.910572: step 15477, loss 0.0573118, acc 0.984375\n",
      "2018-05-23T14:59:08.277589: step 15478, loss 0.128758, acc 0.921875\n",
      "2018-05-23T14:59:08.640618: step 15479, loss 0.0897463, acc 0.96875\n",
      "2018-05-23T14:59:09.050522: step 15480, loss 0.149454, acc 0.953125\n",
      "2018-05-23T14:59:09.421529: step 15481, loss 0.16199, acc 0.9375\n",
      "2018-05-23T14:59:09.796530: step 15482, loss 0.0759828, acc 0.96875\n",
      "2018-05-23T14:59:10.170525: step 15483, loss 0.0457833, acc 0.984375\n",
      "2018-05-23T14:59:10.590402: step 15484, loss 0.108702, acc 0.921875\n",
      "2018-05-23T14:59:10.970386: step 15485, loss 0.0882925, acc 0.96875\n",
      "2018-05-23T14:59:11.338401: step 15486, loss 0.360434, acc 0.921875\n",
      "2018-05-23T14:59:11.708411: step 15487, loss 0.0587097, acc 0.984375\n",
      "2018-05-23T14:59:12.150229: step 15488, loss 0.182219, acc 0.921875\n",
      "2018-05-23T14:59:12.521803: step 15489, loss 0.0643667, acc 0.96875\n",
      "2018-05-23T14:59:12.897798: step 15490, loss 0.100181, acc 0.953125\n",
      "2018-05-23T14:59:13.302712: step 15491, loss 0.0264701, acc 1\n",
      "2018-05-23T14:59:13.680701: step 15492, loss 0.0644353, acc 0.96875\n",
      "2018-05-23T14:59:14.057692: step 15493, loss 0.136684, acc 0.9375\n",
      "2018-05-23T14:59:14.422718: step 15494, loss 0.0616953, acc 0.96875\n",
      "2018-05-23T14:59:14.838167: step 15495, loss 0.121617, acc 0.9375\n",
      "2018-05-23T14:59:15.219148: step 15496, loss 0.0678197, acc 0.984375\n",
      "2018-05-23T14:59:15.595140: step 15497, loss 0.177789, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T14:59:15.972134: step 15498, loss 0.281247, acc 0.9375\n",
      "2018-05-23T14:59:16.391011: step 15499, loss 0.101273, acc 0.953125\n",
      "2018-05-23T14:59:16.761327: step 15500, loss 0.114828, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T14:59:22.142930: step 15500, loss 1.67031, acc 0.706529\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15500\n",
      "\n",
      "2018-05-23T14:59:23.650897: step 15501, loss 0.166297, acc 0.9375\n",
      "2018-05-23T14:59:24.102689: step 15502, loss 0.190938, acc 0.9375\n",
      "2018-05-23T14:59:24.494639: step 15503, loss 0.0840552, acc 0.96875\n",
      "2018-05-23T14:59:24.864650: step 15504, loss 0.111755, acc 0.9375\n",
      "2018-05-23T14:59:25.255603: step 15505, loss 0.0995985, acc 0.9375\n",
      "2018-05-23T14:59:25.645593: step 15506, loss 0.108528, acc 0.953125\n",
      "2018-05-23T14:59:26.030530: step 15507, loss 0.0955775, acc 0.921875\n",
      "2018-05-23T14:59:26.397550: step 15508, loss 0.0918471, acc 0.953125\n",
      "2018-05-23T14:59:26.817425: step 15509, loss 0.0933845, acc 0.921875\n",
      "2018-05-23T14:59:27.189430: step 15510, loss 0.102317, acc 0.953125\n",
      "2018-05-23T14:59:27.567419: step 15511, loss 0.206978, acc 0.9375\n",
      "2018-05-23T14:59:27.943414: step 15512, loss 0.136726, acc 0.921875\n",
      "2018-05-23T14:59:28.364287: step 15513, loss 0.0938786, acc 0.96875\n",
      "2018-05-23T14:59:28.730308: step 15514, loss 0.105665, acc 0.9375\n",
      "2018-05-23T14:59:29.269865: step 15515, loss 0.0856078, acc 0.953125\n",
      "2018-05-23T14:59:29.881229: step 15516, loss 0.164736, acc 0.96875\n",
      "2018-05-23T14:59:30.386877: step 15517, loss 0.155171, acc 0.953125\n",
      "2018-05-23T14:59:30.895517: step 15518, loss 0.105296, acc 0.953125\n",
      "2018-05-23T14:59:31.450033: step 15519, loss 0.107554, acc 0.96875\n",
      "2018-05-23T14:59:32.050426: step 15520, loss 0.0916928, acc 0.96875\n",
      "2018-05-23T14:59:32.652836: step 15521, loss 0.0916916, acc 0.953125\n",
      "2018-05-23T14:59:33.268293: step 15522, loss 0.134378, acc 0.953125\n",
      "2018-05-23T14:59:34.259671: step 15523, loss 0.0482771, acc 0.96875\n",
      "2018-05-23T14:59:34.845103: step 15524, loss 0.0607447, acc 0.984375\n",
      "2018-05-23T14:59:35.351747: step 15525, loss 0.115805, acc 0.953125\n",
      "2018-05-23T14:59:36.197487: step 15526, loss 0.158125, acc 0.921875\n",
      "2018-05-23T14:59:36.683187: step 15527, loss 0.0891033, acc 0.96875\n",
      "2018-05-23T14:59:37.239697: step 15528, loss 0.093101, acc 0.96875\n",
      "2018-05-23T14:59:37.641624: step 15529, loss 0.0987478, acc 0.96875\n",
      "2018-05-23T14:59:38.026593: step 15530, loss 0.0668941, acc 0.96875\n",
      "2018-05-23T14:59:38.403618: step 15531, loss 0.16907, acc 0.96875\n",
      "2018-05-23T14:59:38.821468: step 15532, loss 0.0775173, acc 0.96875\n",
      "2018-05-23T14:59:39.191641: step 15533, loss 0.187623, acc 0.921875\n",
      "2018-05-23T14:59:39.564673: step 15534, loss 0.0196246, acc 1\n",
      "2018-05-23T14:59:39.935647: step 15535, loss 0.102132, acc 0.984375\n",
      "2018-05-23T14:59:40.370484: step 15536, loss 0.126234, acc 0.9375\n",
      "2018-05-23T14:59:40.751467: step 15537, loss 0.0678832, acc 0.984375\n",
      "2018-05-23T14:59:41.110507: step 15538, loss 0.124683, acc 0.953125\n",
      "2018-05-23T14:59:41.488493: step 15539, loss 0.295592, acc 0.875\n",
      "2018-05-23T14:59:41.904380: step 15540, loss 0.0722874, acc 0.984375\n",
      "2018-05-23T14:59:42.305308: step 15541, loss 0.181792, acc 0.875\n",
      "2018-05-23T14:59:42.674322: step 15542, loss 0.123882, acc 0.953125\n",
      "2018-05-23T14:59:43.061288: step 15543, loss 0.0801788, acc 0.96875\n",
      "2018-05-23T14:59:43.477173: step 15544, loss 0.274972, acc 0.921875\n",
      "2018-05-23T14:59:43.860150: step 15545, loss 0.0472657, acc 0.984375\n",
      "2018-05-23T14:59:44.251102: step 15546, loss 0.0657491, acc 0.953125\n",
      "2018-05-23T14:59:44.689930: step 15547, loss 0.080845, acc 0.953125\n",
      "2018-05-23T14:59:45.058943: step 15548, loss 0.167319, acc 0.921875\n",
      "2018-05-23T14:59:45.430946: step 15549, loss 0.132307, acc 0.9375\n",
      "2018-05-23T14:59:45.808938: step 15550, loss 0.178126, acc 0.921875\n",
      "2018-05-23T14:59:46.231804: step 15551, loss 0.0916615, acc 0.953125\n",
      "2018-05-23T14:59:46.613782: step 15552, loss 0.110948, acc 0.96875\n",
      "2018-05-23T14:59:47.025680: step 15553, loss 0.0535755, acc 1\n",
      "2018-05-23T14:59:47.394697: step 15554, loss 0.0798314, acc 0.984375\n",
      "2018-05-23T14:59:47.821551: step 15555, loss 0.102614, acc 0.9375\n",
      "2018-05-23T14:59:48.208517: step 15556, loss 0.120542, acc 0.953125\n",
      "2018-05-23T14:59:48.581519: step 15557, loss 0.0936982, acc 0.96875\n",
      "2018-05-23T14:59:49.007380: step 15558, loss 0.0768197, acc 0.953125\n",
      "2018-05-23T14:59:49.382376: step 15559, loss 0.076402, acc 0.96875\n",
      "2018-05-23T14:59:49.771337: step 15560, loss 0.329976, acc 0.921875\n",
      "2018-05-23T14:59:50.150322: step 15561, loss 0.100781, acc 0.96875\n",
      "2018-05-23T14:59:50.577179: step 15562, loss 0.114893, acc 0.953125\n",
      "2018-05-23T14:59:50.960155: step 15563, loss 0.134523, acc 0.9375\n",
      "2018-05-23T14:59:51.337147: step 15564, loss 0.188299, acc 0.953125\n",
      "2018-05-23T14:59:51.707160: step 15565, loss 0.0501364, acc 1\n",
      "2018-05-23T14:59:52.135013: step 15566, loss 0.128639, acc 0.953125\n",
      "2018-05-23T14:59:52.505022: step 15567, loss 0.11647, acc 0.96875\n",
      "2018-05-23T14:59:52.927891: step 15568, loss 0.0970299, acc 0.96875\n",
      "2018-05-23T14:59:53.300895: step 15569, loss 0.0995554, acc 0.953125\n",
      "2018-05-23T14:59:53.713789: step 15570, loss 0.0538197, acc 1\n",
      "2018-05-23T14:59:54.091777: step 15571, loss 0.152784, acc 0.9375\n",
      "2018-05-23T14:59:54.464819: step 15572, loss 0.0529116, acc 0.984375\n",
      "2018-05-23T14:59:54.863715: step 15573, loss 0.233822, acc 0.90625\n",
      "2018-05-23T14:59:55.245693: step 15574, loss 0.0716856, acc 0.96875\n",
      "2018-05-23T14:59:55.631657: step 15575, loss 0.104468, acc 0.96875\n",
      "2018-05-23T14:59:56.014634: step 15576, loss 0.0841697, acc 0.9375\n",
      "2018-05-23T14:59:56.434512: step 15577, loss 0.131816, acc 0.96875\n",
      "2018-05-23T14:59:56.809506: step 15578, loss 0.0518979, acc 0.96875\n",
      "2018-05-23T14:59:57.180319: step 15579, loss 0.0772208, acc 0.96875\n",
      "2018-05-23T14:59:57.569418: step 15580, loss 0.062174, acc 0.96875\n",
      "2018-05-23T14:59:58.002261: step 15581, loss 0.154001, acc 0.953125\n",
      "2018-05-23T14:59:58.377257: step 15582, loss 0.0792388, acc 0.984375\n",
      "2018-05-23T14:59:58.756244: step 15583, loss 0.134879, acc 0.9375\n",
      "2018-05-23T14:59:59.124260: step 15584, loss 0.0846976, acc 0.953125\n",
      "2018-05-23T14:59:59.551119: step 15585, loss 0.105514, acc 0.953125\n",
      "2018-05-23T14:59:59.924121: step 15586, loss 0.0995624, acc 0.984375\n",
      "2018-05-23T15:00:00.337014: step 15587, loss 0.0575268, acc 0.984375\n",
      "2018-05-23T15:00:00.768858: step 15588, loss 0.137211, acc 0.96875\n",
      "2018-05-23T15:00:01.172778: step 15589, loss 0.239433, acc 0.90625\n",
      "2018-05-23T15:00:01.651497: step 15590, loss 0.0336987, acc 1\n",
      "2018-05-23T15:00:02.218980: step 15591, loss 0.0492529, acc 0.984375\n",
      "2018-05-23T15:00:02.643842: step 15592, loss 0.148568, acc 0.953125\n",
      "2018-05-23T15:00:03.019839: step 15593, loss 0.130122, acc 0.90625\n",
      "2018-05-23T15:00:03.401816: step 15594, loss 0.0555635, acc 1\n",
      "2018-05-23T15:00:03.841639: step 15595, loss 0.129793, acc 0.921875\n",
      "2018-05-23T15:00:04.222652: step 15596, loss 0.0649093, acc 0.984375\n",
      "2018-05-23T15:00:04.593627: step 15597, loss 0.276681, acc 0.90625\n",
      "2018-05-23T15:00:04.974610: step 15598, loss 0.17045, acc 0.890625\n",
      "2018-05-23T15:00:05.400471: step 15599, loss 0.0799426, acc 0.96875\n",
      "2018-05-23T15:00:05.787433: step 15600, loss 0.123369, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:00:11.058333: step 15600, loss 1.65818, acc 0.71153\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15600\n",
      "\n",
      "2018-05-23T15:00:12.565303: step 15601, loss 0.211013, acc 0.953125\n",
      "2018-05-23T15:00:13.008119: step 15602, loss 0.167613, acc 0.921875\n",
      "2018-05-23T15:00:13.598537: step 15603, loss 0.136545, acc 0.921875\n",
      "2018-05-23T15:00:14.303655: step 15604, loss 0.147495, acc 0.9375\n",
      "2018-05-23T15:00:14.880108: step 15605, loss 0.109966, acc 0.953125\n",
      "2018-05-23T15:00:15.375785: step 15606, loss 0.269678, acc 0.921875\n",
      "2018-05-23T15:00:15.814608: step 15607, loss 0.0465916, acc 0.984375\n",
      "2018-05-23T15:00:16.169660: step 15608, loss 0.0637165, acc 0.96875\n",
      "2018-05-23T15:00:16.519724: step 15609, loss 0.15312, acc 0.953125\n",
      "2018-05-23T15:00:16.899733: step 15610, loss 0.226913, acc 0.90625\n",
      "2018-05-23T15:00:17.745445: step 15611, loss 0.182513, acc 0.9375\n",
      "2018-05-23T15:00:18.298963: step 15612, loss 0.0652724, acc 0.96875\n",
      "2018-05-23T15:00:18.812588: step 15613, loss 0.140672, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:00:19.304291: step 15614, loss 0.0736913, acc 0.96875\n",
      "2018-05-23T15:00:19.738115: step 15615, loss 0.124818, acc 0.953125\n",
      "2018-05-23T15:00:20.345489: step 15616, loss 0.129843, acc 0.953125\n",
      "2018-05-23T15:00:20.921945: step 15617, loss 0.165079, acc 0.9375\n",
      "2018-05-23T15:00:21.749732: step 15618, loss 0.0626341, acc 0.984375\n",
      "2018-05-23T15:00:22.393012: step 15619, loss 0.107589, acc 0.96875\n",
      "2018-05-23T15:00:23.068205: step 15620, loss 0.16072, acc 0.921875\n",
      "2018-05-23T15:00:23.748385: step 15621, loss 0.0759926, acc 0.953125\n",
      "2018-05-23T15:00:24.287942: step 15622, loss 0.242707, acc 0.921875\n",
      "2018-05-23T15:00:24.938202: step 15623, loss 0.0843542, acc 0.953125\n",
      "2018-05-23T15:00:25.524633: step 15624, loss 0.0953891, acc 0.96875\n",
      "2018-05-23T15:00:26.212792: step 15625, loss 0.0552697, acc 0.96875\n",
      "2018-05-23T15:00:27.022625: step 15626, loss 0.165086, acc 0.9375\n",
      "2018-05-23T15:00:27.871355: step 15627, loss 0.284146, acc 0.953125\n",
      "2018-05-23T15:00:28.347081: step 15628, loss 0.137405, acc 0.9375\n",
      "2018-05-23T15:00:28.819818: step 15629, loss 0.114662, acc 0.953125\n",
      "2018-05-23T15:00:29.322473: step 15630, loss 0.0631445, acc 0.984375\n",
      "2018-05-23T15:00:29.942814: step 15631, loss 0.0637767, acc 0.96875\n",
      "2018-05-23T15:00:30.600055: step 15632, loss 0.255954, acc 0.890625\n",
      "2018-05-23T15:00:31.183494: step 15633, loss 0.108923, acc 0.96875\n",
      "2018-05-23T15:00:31.697120: step 15634, loss 0.0980589, acc 0.921875\n",
      "2018-05-23T15:00:32.795184: step 15635, loss 0.149877, acc 0.9375\n",
      "2018-05-23T15:00:33.555151: step 15636, loss 0.253324, acc 0.875\n",
      "2018-05-23T15:00:34.336061: step 15637, loss 0.179197, acc 0.953125\n",
      "2018-05-23T15:00:35.300480: step 15638, loss 0.184622, acc 0.90625\n",
      "2018-05-23T15:00:36.096352: step 15639, loss 0.210102, acc 0.890625\n",
      "2018-05-23T15:00:36.694752: step 15640, loss 0.139984, acc 0.921875\n",
      "2018-05-23T15:00:37.304121: step 15641, loss 0.0893606, acc 0.953125\n",
      "2018-05-23T15:00:37.870607: step 15642, loss 0.149097, acc 0.921875\n",
      "2018-05-23T15:00:38.427117: step 15643, loss 0.158367, acc 0.984375\n",
      "2018-05-23T15:00:39.467334: step 15644, loss 0.0800503, acc 0.953125\n",
      "2018-05-23T15:00:40.209348: step 15645, loss 0.0978552, acc 0.96875\n",
      "2018-05-23T15:00:41.107945: step 15646, loss 0.148919, acc 0.953125\n",
      "2018-05-23T15:00:41.706344: step 15647, loss 0.101449, acc 0.953125\n",
      "2018-05-23T15:00:42.221964: step 15648, loss 0.151214, acc 0.96875\n",
      "2018-05-23T15:00:42.887184: step 15649, loss 0.126342, acc 0.953125\n",
      "2018-05-23T15:00:43.513509: step 15650, loss 0.167588, acc 0.921875\n",
      "2018-05-23T15:00:43.943361: step 15651, loss 0.221682, acc 0.9375\n",
      "2018-05-23T15:00:44.533780: step 15652, loss 0.202159, acc 0.96875\n",
      "2018-05-23T15:00:44.992552: step 15653, loss 0.157818, acc 0.953125\n",
      "2018-05-23T15:00:45.640818: step 15654, loss 0.18405, acc 0.90625\n",
      "2018-05-23T15:00:46.082636: step 15655, loss 0.145921, acc 0.921875\n",
      "2018-05-23T15:00:46.437688: step 15656, loss 0.147393, acc 0.9375\n",
      "2018-05-23T15:00:47.036085: step 15657, loss 0.110009, acc 0.953125\n",
      "2018-05-23T15:00:47.500844: step 15658, loss 0.131479, acc 0.9375\n",
      "2018-05-23T15:00:47.977566: step 15659, loss 0.112019, acc 0.96875\n",
      "2018-05-23T15:00:48.542059: step 15660, loss 0.205892, acc 0.90625\n",
      "2018-05-23T15:00:49.124500: step 15661, loss 0.176428, acc 0.90625\n",
      "2018-05-23T15:00:49.627154: step 15662, loss 0.14043, acc 0.96875\n",
      "2018-05-23T15:00:50.291890: step 15663, loss 0.0774043, acc 0.953125\n",
      "2018-05-23T15:00:50.874333: step 15664, loss 0.109863, acc 0.96875\n",
      "2018-05-23T15:00:51.369009: step 15665, loss 0.099273, acc 0.953125\n",
      "2018-05-23T15:00:51.862688: step 15666, loss 0.052101, acc 1\n",
      "2018-05-23T15:00:52.416211: step 15667, loss 0.0618548, acc 0.96875\n",
      "2018-05-23T15:00:52.905897: step 15668, loss 0.0682939, acc 0.984375\n",
      "2018-05-23T15:00:53.347716: step 15669, loss 0.0894327, acc 0.96875\n",
      "2018-05-23T15:00:53.779560: step 15670, loss 0.0385344, acc 0.984375\n",
      "2018-05-23T15:00:54.167555: step 15671, loss 0.089645, acc 0.953125\n",
      "2018-05-23T15:00:54.542521: step 15672, loss 0.120026, acc 0.953125\n",
      "2018-05-23T15:00:54.922502: step 15673, loss 0.121766, acc 0.921875\n",
      "2018-05-23T15:00:55.298496: step 15674, loss 0.109638, acc 0.96875\n",
      "2018-05-23T15:00:55.673495: step 15675, loss 0.0609235, acc 1\n",
      "2018-05-23T15:00:56.048493: step 15676, loss 0.248427, acc 0.90625\n",
      "2018-05-23T15:00:56.431467: step 15677, loss 0.0720069, acc 0.984375\n",
      "2018-05-23T15:00:56.809456: step 15678, loss 0.196485, acc 0.921875\n",
      "2018-05-23T15:00:57.262243: step 15679, loss 0.0899554, acc 0.984375\n",
      "2018-05-23T15:00:57.644260: step 15680, loss 0.183335, acc 0.921875\n",
      "2018-05-23T15:00:58.147915: step 15681, loss 0.274367, acc 0.90625\n",
      "2018-05-23T15:00:58.698442: step 15682, loss 0.0613127, acc 0.984375\n",
      "2018-05-23T15:00:59.178156: step 15683, loss 0.057256, acc 0.96875\n",
      "2018-05-23T15:00:59.673832: step 15684, loss 0.0591399, acc 1\n",
      "2018-05-23T15:01:00.182474: step 15685, loss 0.0274979, acc 0.984375\n",
      "2018-05-23T15:01:00.749954: step 15686, loss 0.14151, acc 0.9375\n",
      "2018-05-23T15:01:01.298486: step 15687, loss 0.244133, acc 0.953125\n",
      "2018-05-23T15:01:01.688441: step 15688, loss 0.12709, acc 0.9375\n",
      "2018-05-23T15:01:02.077401: step 15689, loss 0.0409263, acc 0.984375\n",
      "2018-05-23T15:01:02.451515: step 15690, loss 0.121573, acc 0.953125\n",
      "2018-05-23T15:01:02.840473: step 15691, loss 0.11659, acc 0.953125\n",
      "2018-05-23T15:01:03.215469: step 15692, loss 0.0468066, acc 1\n",
      "2018-05-23T15:01:03.589471: step 15693, loss 0.106246, acc 0.953125\n",
      "2018-05-23T15:01:03.972445: step 15694, loss 0.0631185, acc 0.96875\n",
      "2018-05-23T15:01:04.342456: step 15695, loss 0.0849508, acc 0.953125\n",
      "2018-05-23T15:01:04.719446: step 15696, loss 0.0801516, acc 0.953125\n",
      "2018-05-23T15:01:05.099430: step 15697, loss 0.0355725, acc 1\n",
      "2018-05-23T15:01:05.468444: step 15698, loss 0.0397081, acc 1\n",
      "2018-05-23T15:01:05.947162: step 15699, loss 0.108199, acc 0.96875\n",
      "2018-05-23T15:01:06.359061: step 15700, loss 0.071063, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:01:13.186308: step 15700, loss 1.65528, acc 0.711959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15700\n",
      "\n",
      "2018-05-23T15:01:14.766081: step 15701, loss 0.233474, acc 0.9375\n",
      "2018-05-23T15:01:15.307633: step 15702, loss 0.217783, acc 0.921875\n",
      "2018-05-23T15:01:15.788346: step 15703, loss 0.172093, acc 0.9375\n",
      "2018-05-23T15:01:16.246122: step 15704, loss 0.0472886, acc 0.984375\n",
      "2018-05-23T15:01:16.693924: step 15705, loss 0.145663, acc 0.953125\n",
      "2018-05-23T15:01:17.136739: step 15706, loss 0.152723, acc 0.921875\n",
      "2018-05-23T15:01:17.601497: step 15707, loss 0.0881642, acc 0.9375\n",
      "2018-05-23T15:01:18.035338: step 15708, loss 0.139968, acc 0.953125\n",
      "2018-05-23T15:01:18.457208: step 15709, loss 0.153965, acc 0.9375\n",
      "2018-05-23T15:01:19.087522: step 15710, loss 0.129621, acc 0.953125\n",
      "2018-05-23T15:01:19.692901: step 15711, loss 0.144882, acc 0.953125\n",
      "2018-05-23T15:01:20.409983: step 15712, loss 0.331958, acc 0.921875\n",
      "2018-05-23T15:01:21.175935: step 15713, loss 0.0658234, acc 0.984375\n",
      "2018-05-23T15:01:21.765357: step 15714, loss 0.04598, acc 0.984375\n",
      "2018-05-23T15:01:22.231111: step 15715, loss 0.0478652, acc 1\n",
      "2018-05-23T15:01:22.690882: step 15716, loss 0.343591, acc 0.90625\n",
      "2018-05-23T15:01:23.134694: step 15717, loss 0.0938741, acc 0.96875\n",
      "2018-05-23T15:01:23.569532: step 15718, loss 0.0949933, acc 0.953125\n",
      "2018-05-23T15:01:24.021322: step 15719, loss 0.123237, acc 0.96875\n",
      "2018-05-23T15:01:24.445189: step 15720, loss 0.0982397, acc 0.984375\n",
      "2018-05-23T15:01:24.905956: step 15721, loss 0.0906665, acc 0.96875\n",
      "2018-05-23T15:01:25.391656: step 15722, loss 0.0898301, acc 0.96875\n",
      "2018-05-23T15:01:25.820508: step 15723, loss 0.299721, acc 0.890625\n",
      "2018-05-23T15:01:26.251359: step 15724, loss 0.0902801, acc 0.953125\n",
      "2018-05-23T15:01:26.686192: step 15725, loss 0.158157, acc 0.921875\n",
      "2018-05-23T15:01:27.124021: step 15726, loss 0.0824165, acc 0.96875\n",
      "2018-05-23T15:01:27.538911: step 15727, loss 0.133838, acc 0.953125\n",
      "2018-05-23T15:01:27.979733: step 15728, loss 0.103059, acc 0.953125\n",
      "2018-05-23T15:01:28.408586: step 15729, loss 0.225662, acc 0.921875\n",
      "2018-05-23T15:01:28.846414: step 15730, loss 0.183107, acc 0.9375\n",
      "2018-05-23T15:01:29.378990: step 15731, loss 0.0645682, acc 1\n",
      "2018-05-23T15:01:29.797870: step 15732, loss 0.174306, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:01:30.240685: step 15733, loss 0.146423, acc 0.9375\n",
      "2018-05-23T15:01:30.674523: step 15734, loss 0.126215, acc 0.953125\n",
      "2018-05-23T15:01:31.130303: step 15735, loss 0.195853, acc 0.90625\n",
      "2018-05-23T15:01:31.532229: step 15736, loss 0.078799, acc 0.953125\n",
      "2018-05-23T15:01:31.971055: step 15737, loss 0.162955, acc 0.9375\n",
      "2018-05-23T15:01:32.443791: step 15738, loss 0.0850559, acc 0.921875\n",
      "2018-05-23T15:01:32.892591: step 15739, loss 0.242146, acc 0.9375\n",
      "2018-05-23T15:01:33.328424: step 15740, loss 0.189922, acc 0.953125\n",
      "2018-05-23T15:01:33.747303: step 15741, loss 0.120874, acc 0.96875\n",
      "2018-05-23T15:01:34.276887: step 15742, loss 0.0680885, acc 0.96875\n",
      "2018-05-23T15:01:34.697762: step 15743, loss 0.169774, acc 0.875\n",
      "2018-05-23T15:01:35.127611: step 15744, loss 0.138195, acc 0.984375\n",
      "2018-05-23T15:01:35.561454: step 15745, loss 0.0741662, acc 0.96875\n",
      "2018-05-23T15:01:36.011248: step 15746, loss 0.191006, acc 0.9375\n",
      "2018-05-23T15:01:36.446084: step 15747, loss 0.0380545, acc 0.984375\n",
      "2018-05-23T15:01:36.894884: step 15748, loss 0.0538386, acc 0.96875\n",
      "2018-05-23T15:01:37.330717: step 15749, loss 0.130094, acc 0.953125\n",
      "2018-05-23T15:01:37.757575: step 15750, loss 0.149875, acc 0.9375\n",
      "2018-05-23T15:01:38.190418: step 15751, loss 0.254202, acc 0.953125\n",
      "2018-05-23T15:01:38.647195: step 15752, loss 0.16028, acc 0.9375\n",
      "2018-05-23T15:01:39.131898: step 15753, loss 0.0296922, acc 0.984375\n",
      "2018-05-23T15:01:39.565737: step 15754, loss 0.17244, acc 0.90625\n",
      "2018-05-23T15:01:39.998580: step 15755, loss 0.226584, acc 0.875\n",
      "2018-05-23T15:01:40.430424: step 15756, loss 0.0589972, acc 0.984375\n",
      "2018-05-23T15:01:40.864263: step 15757, loss 0.104462, acc 0.96875\n",
      "2018-05-23T15:01:41.292121: step 15758, loss 0.0960137, acc 0.96875\n",
      "2018-05-23T15:01:41.731943: step 15759, loss 0.120914, acc 0.96875\n",
      "2018-05-23T15:01:42.053083: step 15760, loss 0.0184832, acc 1\n",
      "2018-05-23T15:01:42.542774: step 15761, loss 0.0522029, acc 0.984375\n",
      "2018-05-23T15:01:43.049418: step 15762, loss 0.124808, acc 0.921875\n",
      "2018-05-23T15:01:43.492234: step 15763, loss 0.0655606, acc 0.96875\n",
      "2018-05-23T15:01:43.915102: step 15764, loss 0.126273, acc 0.9375\n",
      "2018-05-23T15:01:44.345949: step 15765, loss 0.162388, acc 0.921875\n",
      "2018-05-23T15:01:44.788764: step 15766, loss 0.168693, acc 0.921875\n",
      "2018-05-23T15:01:45.222604: step 15767, loss 0.097602, acc 0.96875\n",
      "2018-05-23T15:01:45.657442: step 15768, loss 0.188965, acc 0.953125\n",
      "2018-05-23T15:01:46.078315: step 15769, loss 0.0494669, acc 0.984375\n",
      "2018-05-23T15:01:46.545067: step 15770, loss 0.083369, acc 0.96875\n",
      "2018-05-23T15:01:46.992868: step 15771, loss 0.0493118, acc 0.984375\n",
      "2018-05-23T15:01:47.478570: step 15772, loss 0.0893415, acc 0.9375\n",
      "2018-05-23T15:01:47.999176: step 15773, loss 0.0538401, acc 0.984375\n",
      "2018-05-23T15:01:48.420050: step 15774, loss 0.197902, acc 0.921875\n",
      "2018-05-23T15:01:48.873837: step 15775, loss 0.142317, acc 0.90625\n",
      "2018-05-23T15:01:49.446305: step 15776, loss 0.0873109, acc 0.953125\n",
      "2018-05-23T15:01:50.204278: step 15777, loss 0.14597, acc 0.984375\n",
      "2018-05-23T15:01:50.970229: step 15778, loss 0.179782, acc 0.9375\n",
      "2018-05-23T15:01:51.610517: step 15779, loss 0.0905735, acc 0.96875\n",
      "2018-05-23T15:01:52.322612: step 15780, loss 0.0965223, acc 0.953125\n",
      "2018-05-23T15:01:53.212264: step 15781, loss 0.0320861, acc 1\n",
      "2018-05-23T15:01:53.816716: step 15782, loss 0.0988607, acc 0.9375\n",
      "2018-05-23T15:01:54.405142: step 15783, loss 0.0948646, acc 0.96875\n",
      "2018-05-23T15:01:55.188047: step 15784, loss 0.0505346, acc 0.984375\n",
      "2018-05-23T15:01:55.819359: step 15785, loss 0.0910171, acc 0.9375\n",
      "2018-05-23T15:01:56.430724: step 15786, loss 0.0977371, acc 0.953125\n",
      "2018-05-23T15:01:56.924402: step 15787, loss 0.1651, acc 0.9375\n",
      "2018-05-23T15:01:57.535768: step 15788, loss 0.0932279, acc 0.96875\n",
      "2018-05-23T15:01:58.293774: step 15789, loss 0.0798949, acc 0.96875\n",
      "2018-05-23T15:01:59.019342: step 15790, loss 0.096874, acc 0.96875\n",
      "2018-05-23T15:01:59.692542: step 15791, loss 0.0451554, acc 1\n",
      "2018-05-23T15:02:00.271991: step 15792, loss 0.0934421, acc 0.953125\n",
      "2018-05-23T15:02:00.872386: step 15793, loss 0.0612469, acc 0.984375\n",
      "2018-05-23T15:02:01.538604: step 15794, loss 0.079737, acc 0.953125\n",
      "2018-05-23T15:02:02.278623: step 15795, loss 0.0361812, acc 1\n",
      "2018-05-23T15:02:02.902954: step 15796, loss 0.0918994, acc 0.953125\n",
      "2018-05-23T15:02:03.592110: step 15797, loss 0.146087, acc 0.953125\n",
      "2018-05-23T15:02:04.175550: step 15798, loss 0.112054, acc 0.953125\n",
      "2018-05-23T15:02:04.881660: step 15799, loss 0.110096, acc 0.953125\n",
      "2018-05-23T15:02:05.789232: step 15800, loss 0.0904422, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:02:13.901083: step 15800, loss 1.63997, acc 0.707958\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15800\n",
      "\n",
      "2018-05-23T15:02:16.197939: step 15801, loss 0.0832378, acc 0.96875\n",
      "2018-05-23T15:02:16.700594: step 15802, loss 0.150114, acc 0.9375\n",
      "2018-05-23T15:02:17.190283: step 15803, loss 0.0740968, acc 0.96875\n",
      "2018-05-23T15:02:17.653045: step 15804, loss 0.0499416, acc 0.984375\n",
      "2018-05-23T15:02:18.112815: step 15805, loss 0.147256, acc 0.953125\n",
      "2018-05-23T15:02:18.592533: step 15806, loss 0.113239, acc 0.921875\n",
      "2018-05-23T15:02:19.027371: step 15807, loss 0.073094, acc 0.984375\n",
      "2018-05-23T15:02:19.454228: step 15808, loss 0.196732, acc 0.921875\n",
      "2018-05-23T15:02:19.897042: step 15809, loss 0.203653, acc 0.9375\n",
      "2018-05-23T15:02:20.325895: step 15810, loss 0.023838, acc 1\n",
      "2018-05-23T15:02:20.849495: step 15811, loss 0.0484475, acc 1\n",
      "2018-05-23T15:02:21.271366: step 15812, loss 0.0480645, acc 1\n",
      "2018-05-23T15:02:21.702213: step 15813, loss 0.0952662, acc 0.96875\n",
      "2018-05-23T15:02:22.133063: step 15814, loss 0.0574906, acc 0.96875\n",
      "2018-05-23T15:02:22.569894: step 15815, loss 0.0791263, acc 0.953125\n",
      "2018-05-23T15:02:23.005727: step 15816, loss 0.0529021, acc 0.984375\n",
      "2018-05-23T15:02:23.439565: step 15817, loss 0.105424, acc 0.984375\n",
      "2018-05-23T15:02:23.906318: step 15818, loss 0.0789516, acc 0.96875\n",
      "2018-05-23T15:02:24.347138: step 15819, loss 0.057817, acc 0.96875\n",
      "2018-05-23T15:02:24.833835: step 15820, loss 0.103331, acc 0.9375\n",
      "2018-05-23T15:02:25.259697: step 15821, loss 0.05277, acc 0.984375\n",
      "2018-05-23T15:02:25.704507: step 15822, loss 0.17922, acc 0.890625\n",
      "2018-05-23T15:02:26.155301: step 15823, loss 0.106172, acc 0.96875\n",
      "2018-05-23T15:02:26.647982: step 15824, loss 0.0977891, acc 0.953125\n",
      "2018-05-23T15:02:27.089801: step 15825, loss 0.240749, acc 0.9375\n",
      "2018-05-23T15:02:27.525635: step 15826, loss 0.0804005, acc 0.953125\n",
      "2018-05-23T15:02:27.961468: step 15827, loss 0.130817, acc 0.953125\n",
      "2018-05-23T15:02:28.391318: step 15828, loss 0.219327, acc 0.921875\n",
      "2018-05-23T15:02:28.842112: step 15829, loss 0.0918908, acc 0.9375\n",
      "2018-05-23T15:02:29.477414: step 15830, loss 0.0860891, acc 0.984375\n",
      "2018-05-23T15:02:30.065839: step 15831, loss 0.110978, acc 0.953125\n",
      "2018-05-23T15:02:30.626340: step 15832, loss 0.0568901, acc 0.984375\n",
      "2018-05-23T15:02:31.425203: step 15833, loss 0.0849609, acc 0.953125\n",
      "2018-05-23T15:02:32.021608: step 15834, loss 0.0649098, acc 0.96875\n",
      "2018-05-23T15:02:32.587095: step 15835, loss 0.0851532, acc 0.984375\n",
      "2018-05-23T15:02:33.311158: step 15836, loss 0.0442658, acc 0.96875\n",
      "2018-05-23T15:02:33.853708: step 15837, loss 0.134582, acc 0.9375\n",
      "2018-05-23T15:02:34.451109: step 15838, loss 0.0797091, acc 0.953125\n",
      "2018-05-23T15:02:34.966729: step 15839, loss 0.0518503, acc 0.984375\n",
      "2018-05-23T15:02:35.530221: step 15840, loss 0.104463, acc 0.953125\n",
      "2018-05-23T15:02:36.415851: step 15841, loss 0.0622199, acc 0.96875\n",
      "2018-05-23T15:02:36.955408: step 15842, loss 0.0547523, acc 0.984375\n",
      "2018-05-23T15:02:37.549819: step 15843, loss 0.0603415, acc 0.96875\n",
      "2018-05-23T15:02:38.054468: step 15844, loss 0.0307657, acc 1\n",
      "2018-05-23T15:02:38.690766: step 15845, loss 0.143624, acc 0.96875\n",
      "2018-05-23T15:02:39.338035: step 15846, loss 0.250355, acc 0.90625\n",
      "2018-05-23T15:02:40.099997: step 15847, loss 0.103739, acc 0.953125\n",
      "2018-05-23T15:02:40.764220: step 15848, loss 0.225564, acc 0.921875\n",
      "2018-05-23T15:02:41.285825: step 15849, loss 0.170719, acc 0.9375\n",
      "2018-05-23T15:02:41.756565: step 15850, loss 0.069656, acc 0.953125\n",
      "2018-05-23T15:02:42.214340: step 15851, loss 0.102373, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:02:42.798777: step 15852, loss 0.136526, acc 0.9375\n",
      "2018-05-23T15:02:43.263534: step 15853, loss 0.0830658, acc 0.96875\n",
      "2018-05-23T15:02:43.887863: step 15854, loss 0.131587, acc 0.953125\n",
      "2018-05-23T15:02:44.401489: step 15855, loss 0.0887556, acc 0.953125\n",
      "2018-05-23T15:02:44.846298: step 15856, loss 0.0664144, acc 0.96875\n",
      "2018-05-23T15:02:45.340977: step 15857, loss 0.0983528, acc 0.96875\n",
      "2018-05-23T15:02:45.978270: step 15858, loss 0.0629835, acc 0.96875\n",
      "2018-05-23T15:02:46.466964: step 15859, loss 0.0260581, acc 0.984375\n",
      "2018-05-23T15:02:47.108248: step 15860, loss 0.244275, acc 0.921875\n",
      "2018-05-23T15:02:47.497207: step 15861, loss 0.110073, acc 0.953125\n",
      "2018-05-23T15:02:47.856248: step 15862, loss 0.311222, acc 0.890625\n",
      "2018-05-23T15:02:48.357906: step 15863, loss 0.0952471, acc 0.96875\n",
      "2018-05-23T15:02:49.064016: step 15864, loss 0.130212, acc 0.96875\n",
      "2018-05-23T15:02:49.435022: step 15865, loss 0.0325661, acc 1\n",
      "2018-05-23T15:02:49.787081: step 15866, loss 0.0419893, acc 1\n",
      "2018-05-23T15:02:50.174047: step 15867, loss 0.0472378, acc 0.96875\n",
      "2018-05-23T15:02:50.530094: step 15868, loss 0.0664402, acc 0.96875\n",
      "2018-05-23T15:02:50.958947: step 15869, loss 0.12398, acc 0.9375\n",
      "2018-05-23T15:02:51.351895: step 15870, loss 0.11101, acc 0.96875\n",
      "2018-05-23T15:02:51.693980: step 15871, loss 0.269659, acc 0.921875\n",
      "2018-05-23T15:02:52.038060: step 15872, loss 0.0295442, acc 1\n",
      "2018-05-23T15:02:52.401089: step 15873, loss 0.0726774, acc 0.953125\n",
      "2018-05-23T15:02:52.879807: step 15874, loss 0.130351, acc 0.96875\n",
      "2018-05-23T15:02:53.271760: step 15875, loss 0.0829787, acc 0.984375\n",
      "2018-05-23T15:02:53.711582: step 15876, loss 0.0811286, acc 0.984375\n",
      "2018-05-23T15:02:54.136447: step 15877, loss 0.104431, acc 0.96875\n",
      "2018-05-23T15:02:54.580260: step 15878, loss 0.0606802, acc 0.984375\n",
      "2018-05-23T15:02:55.004127: step 15879, loss 0.210182, acc 0.9375\n",
      "2018-05-23T15:02:55.361417: step 15880, loss 0.161382, acc 0.953125\n",
      "2018-05-23T15:02:55.750344: step 15881, loss 0.0722549, acc 0.984375\n",
      "2018-05-23T15:02:56.174206: step 15882, loss 0.0438676, acc 0.96875\n",
      "2018-05-23T15:02:56.539231: step 15883, loss 0.121156, acc 0.9375\n",
      "2018-05-23T15:02:56.909240: step 15884, loss 0.127174, acc 0.953125\n",
      "2018-05-23T15:02:57.277789: step 15885, loss 0.0791799, acc 0.96875\n",
      "2018-05-23T15:02:57.854250: step 15886, loss 0.0305129, acc 1\n",
      "2018-05-23T15:02:58.319004: step 15887, loss 0.121476, acc 0.953125\n",
      "2018-05-23T15:02:58.841606: step 15888, loss 0.116717, acc 0.984375\n",
      "2018-05-23T15:02:59.386149: step 15889, loss 0.038877, acc 0.984375\n",
      "2018-05-23T15:02:59.877833: step 15890, loss 0.0630569, acc 0.953125\n",
      "2018-05-23T15:03:00.418388: step 15891, loss 0.127179, acc 0.9375\n",
      "2018-05-23T15:03:01.210270: step 15892, loss 0.211026, acc 0.921875\n",
      "2018-05-23T15:03:01.731875: step 15893, loss 0.0352332, acc 0.984375\n",
      "2018-05-23T15:03:02.250487: step 15894, loss 0.0541273, acc 0.984375\n",
      "2018-05-23T15:03:02.821959: step 15895, loss 0.0770794, acc 0.953125\n",
      "2018-05-23T15:03:03.265772: step 15896, loss 0.144032, acc 0.921875\n",
      "2018-05-23T15:03:03.780395: step 15897, loss 0.173585, acc 0.90625\n",
      "2018-05-23T15:03:04.442621: step 15898, loss 0.0776769, acc 0.96875\n",
      "2018-05-23T15:03:04.970211: step 15899, loss 0.0763122, acc 0.96875\n",
      "2018-05-23T15:03:05.422002: step 15900, loss 0.0491693, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:03:10.775679: step 15900, loss 1.67519, acc 0.706244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-15900\n",
      "\n",
      "2018-05-23T15:03:12.206851: step 15901, loss 0.0678143, acc 0.953125\n",
      "2018-05-23T15:03:12.705518: step 15902, loss 0.11324, acc 0.953125\n",
      "2018-05-23T15:03:13.098465: step 15903, loss 0.104043, acc 0.96875\n",
      "2018-05-23T15:03:13.663952: step 15904, loss 0.0662725, acc 0.984375\n",
      "2018-05-23T15:03:14.029974: step 15905, loss 0.0861664, acc 0.953125\n",
      "2018-05-23T15:03:14.399983: step 15906, loss 0.0253398, acc 1\n",
      "2018-05-23T15:03:14.743418: step 15907, loss 0.0314812, acc 1\n",
      "2018-05-23T15:03:15.080517: step 15908, loss 0.266789, acc 0.90625\n",
      "2018-05-23T15:03:15.435567: step 15909, loss 0.0636013, acc 0.96875\n",
      "2018-05-23T15:03:15.819538: step 15910, loss 0.0846609, acc 0.96875\n",
      "2018-05-23T15:03:16.166609: step 15911, loss 0.0707144, acc 0.96875\n",
      "2018-05-23T15:03:16.532631: step 15912, loss 0.0514668, acc 1\n",
      "2018-05-23T15:03:16.938544: step 15913, loss 0.117031, acc 0.984375\n",
      "2018-05-23T15:03:17.486079: step 15914, loss 0.0815691, acc 0.96875\n",
      "2018-05-23T15:03:17.925904: step 15915, loss 0.0779611, acc 0.984375\n",
      "2018-05-23T15:03:18.329822: step 15916, loss 0.0856077, acc 0.96875\n",
      "2018-05-23T15:03:18.682877: step 15917, loss 0.128671, acc 0.953125\n",
      "2018-05-23T15:03:19.031944: step 15918, loss 0.0603532, acc 0.984375\n",
      "2018-05-23T15:03:19.379018: step 15919, loss 0.106384, acc 0.9375\n",
      "2018-05-23T15:03:19.780941: step 15920, loss 0.0608084, acc 0.96875\n",
      "2018-05-23T15:03:20.134993: step 15921, loss 0.122938, acc 0.9375\n",
      "2018-05-23T15:03:20.477080: step 15922, loss 0.0423766, acc 0.984375\n",
      "2018-05-23T15:03:20.937846: step 15923, loss 0.119751, acc 0.953125\n",
      "2018-05-23T15:03:21.315837: step 15924, loss 0.0776025, acc 0.9375\n",
      "2018-05-23T15:03:21.756539: step 15925, loss 0.154171, acc 0.921875\n",
      "2018-05-23T15:03:22.174420: step 15926, loss 0.177915, acc 0.9375\n",
      "2018-05-23T15:03:22.525482: step 15927, loss 0.115543, acc 0.9375\n",
      "2018-05-23T15:03:22.888512: step 15928, loss 0.166658, acc 0.9375\n",
      "2018-05-23T15:03:23.250542: step 15929, loss 0.0466687, acc 0.96875\n",
      "2018-05-23T15:03:23.635511: step 15930, loss 0.197036, acc 0.9375\n",
      "2018-05-23T15:03:24.101267: step 15931, loss 0.058567, acc 0.96875\n",
      "2018-05-23T15:03:24.545081: step 15932, loss 0.0602881, acc 0.984375\n",
      "2018-05-23T15:03:24.937031: step 15933, loss 0.069245, acc 0.96875\n",
      "2018-05-23T15:03:25.296069: step 15934, loss 0.0349628, acc 0.984375\n",
      "2018-05-23T15:03:25.759829: step 15935, loss 0.101774, acc 0.9375\n",
      "2018-05-23T15:03:26.223590: step 15936, loss 0.0777353, acc 0.984375\n",
      "2018-05-23T15:03:26.823985: step 15937, loss 0.271158, acc 0.9375\n",
      "2018-05-23T15:03:27.385480: step 15938, loss 0.171694, acc 0.953125\n",
      "2018-05-23T15:03:28.083614: step 15939, loss 0.106519, acc 0.9375\n",
      "2018-05-23T15:03:28.687996: step 15940, loss 0.0806453, acc 0.96875\n",
      "2018-05-23T15:03:29.288901: step 15941, loss 0.234099, acc 0.921875\n",
      "2018-05-23T15:03:29.928190: step 15942, loss 0.131073, acc 0.953125\n",
      "2018-05-23T15:03:30.742602: step 15943, loss 0.0527116, acc 0.96875\n",
      "2018-05-23T15:03:31.430763: step 15944, loss 0.0768213, acc 0.984375\n",
      "2018-05-23T15:03:32.752377: step 15945, loss 0.0664296, acc 0.984375\n",
      "2018-05-23T15:03:33.353769: step 15946, loss 0.126562, acc 0.921875\n",
      "2018-05-23T15:03:34.124705: step 15947, loss 0.223046, acc 0.921875\n",
      "2018-05-23T15:03:34.901140: step 15948, loss 0.0790122, acc 0.953125\n",
      "2018-05-23T15:03:35.567357: step 15949, loss 0.120169, acc 0.953125\n",
      "2018-05-23T15:03:36.164759: step 15950, loss 0.104797, acc 0.953125\n",
      "2018-05-23T15:03:36.872863: step 15951, loss 0.0565848, acc 0.984375\n",
      "2018-05-23T15:03:37.461290: step 15952, loss 0.070942, acc 0.96875\n",
      "2018-05-23T15:03:37.985887: step 15953, loss 0.0768604, acc 0.96875\n",
      "2018-05-23T15:03:38.528436: step 15954, loss 0.0532916, acc 0.984375\n",
      "2018-05-23T15:03:39.056024: step 15955, loss 0.0678682, acc 0.984375\n",
      "2018-05-23T15:03:39.568652: step 15956, loss 0.205498, acc 0.90625\n",
      "2018-05-23T15:03:40.244844: step 15957, loss 0.0560028, acc 0.984375\n",
      "2018-05-23T15:03:40.810333: step 15958, loss 0.102622, acc 0.96875\n",
      "2018-05-23T15:03:41.209265: step 15959, loss 0.0762001, acc 0.953125\n",
      "2018-05-23T15:03:41.586256: step 15960, loss 0.118958, acc 0.96875\n",
      "2018-05-23T15:03:41.962251: step 15961, loss 0.121595, acc 0.9375\n",
      "2018-05-23T15:03:42.475876: step 15962, loss 0.0551963, acc 0.96875\n",
      "2018-05-23T15:03:42.926715: step 15963, loss 0.156821, acc 0.984375\n",
      "2018-05-23T15:03:43.538079: step 15964, loss 0.117438, acc 0.96875\n",
      "2018-05-23T15:03:44.091598: step 15965, loss 0.0984251, acc 0.953125\n",
      "2018-05-23T15:03:44.662073: step 15966, loss 0.102166, acc 0.953125\n",
      "2018-05-23T15:03:45.179690: step 15967, loss 0.0854371, acc 0.96875\n",
      "2018-05-23T15:03:45.688326: step 15968, loss 0.0852175, acc 0.953125\n",
      "2018-05-23T15:03:46.208934: step 15969, loss 0.0684728, acc 0.96875\n",
      "2018-05-23T15:03:46.809329: step 15970, loss 0.201695, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:03:47.303009: step 15971, loss 0.148137, acc 0.9375\n",
      "2018-05-23T15:03:47.703933: step 15972, loss 0.110884, acc 0.953125\n",
      "2018-05-23T15:03:48.117826: step 15973, loss 0.209976, acc 0.90625\n",
      "2018-05-23T15:03:48.500805: step 15974, loss 0.22403, acc 0.921875\n",
      "2018-05-23T15:03:48.892755: step 15975, loss 0.1036, acc 0.9375\n",
      "2018-05-23T15:03:49.260769: step 15976, loss 0.038079, acc 1\n",
      "2018-05-23T15:03:49.650726: step 15977, loss 0.0218672, acc 1\n",
      "2018-05-23T15:03:50.111495: step 15978, loss 0.185289, acc 0.96875\n",
      "2018-05-23T15:03:50.491478: step 15979, loss 0.102528, acc 0.96875\n",
      "2018-05-23T15:03:50.919332: step 15980, loss 0.038274, acc 1\n",
      "2018-05-23T15:03:51.322290: step 15981, loss 0.0860425, acc 0.96875\n",
      "2018-05-23T15:03:51.707226: step 15982, loss 0.1366, acc 0.953125\n",
      "2018-05-23T15:03:52.084200: step 15983, loss 0.106112, acc 0.953125\n",
      "2018-05-23T15:03:52.455206: step 15984, loss 0.0723458, acc 0.96875\n",
      "2018-05-23T15:03:52.850150: step 15985, loss 0.115221, acc 0.984375\n",
      "2018-05-23T15:03:53.227140: step 15986, loss 0.111226, acc 0.96875\n",
      "2018-05-23T15:03:53.607124: step 15987, loss 0.112493, acc 0.953125\n",
      "2018-05-23T15:03:54.034979: step 15988, loss 0.0674114, acc 0.984375\n",
      "2018-05-23T15:03:54.447875: step 15989, loss 0.162802, acc 0.90625\n",
      "2018-05-23T15:03:54.884708: step 15990, loss 0.0561294, acc 0.984375\n",
      "2018-05-23T15:03:55.268679: step 15991, loss 0.142246, acc 0.90625\n",
      "2018-05-23T15:03:55.651655: step 15992, loss 0.126894, acc 0.96875\n",
      "2018-05-23T15:03:56.095466: step 15993, loss 0.10061, acc 0.96875\n",
      "2018-05-23T15:03:56.503375: step 15994, loss 0.0934919, acc 0.953125\n",
      "2018-05-23T15:03:56.906297: step 15995, loss 0.166056, acc 0.9375\n",
      "2018-05-23T15:03:57.275313: step 15996, loss 0.147978, acc 0.921875\n",
      "2018-05-23T15:03:57.662274: step 15997, loss 0.11416, acc 0.921875\n",
      "2018-05-23T15:03:58.052964: step 15998, loss 0.0506535, acc 0.96875\n",
      "2018-05-23T15:03:58.422975: step 15999, loss 0.116598, acc 0.921875\n",
      "2018-05-23T15:03:58.822905: step 16000, loss 0.0527032, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:04:04.360092: step 16000, loss 1.6678, acc 0.709816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16000\n",
      "\n",
      "2018-05-23T15:04:05.898974: step 16001, loss 0.0519849, acc 0.984375\n",
      "2018-05-23T15:04:06.320845: step 16002, loss 0.057195, acc 0.96875\n",
      "2018-05-23T15:04:06.724766: step 16003, loss 0.0437865, acc 0.984375\n",
      "2018-05-23T15:04:07.199496: step 16004, loss 0.0560015, acc 0.984375\n",
      "2018-05-23T15:04:07.584465: step 16005, loss 0.135782, acc 0.921875\n",
      "2018-05-23T15:04:07.958464: step 16006, loss 0.0970933, acc 0.96875\n",
      "2018-05-23T15:04:08.342438: step 16007, loss 0.155741, acc 0.90625\n",
      "2018-05-23T15:04:08.739379: step 16008, loss 0.0771806, acc 0.96875\n",
      "2018-05-23T15:04:09.109388: step 16009, loss 0.0959326, acc 0.96875\n",
      "2018-05-23T15:04:09.491364: step 16010, loss 0.170288, acc 0.921875\n",
      "2018-05-23T15:04:09.881320: step 16011, loss 0.113408, acc 0.96875\n",
      "2018-05-23T15:04:10.359043: step 16012, loss 0.09088, acc 0.96875\n",
      "2018-05-23T15:04:10.754984: step 16013, loss 0.217269, acc 0.921875\n",
      "2018-05-23T15:04:11.132974: step 16014, loss 0.0578403, acc 0.984375\n",
      "2018-05-23T15:04:11.506974: step 16015, loss 0.187792, acc 0.921875\n",
      "2018-05-23T15:04:11.892939: step 16016, loss 0.0488961, acc 1\n",
      "2018-05-23T15:04:12.382629: step 16017, loss 0.126546, acc 0.953125\n",
      "2018-05-23T15:04:12.769594: step 16018, loss 0.0848317, acc 0.953125\n",
      "2018-05-23T15:04:13.145590: step 16019, loss 0.090356, acc 0.9375\n",
      "2018-05-23T15:04:13.528566: step 16020, loss 0.0942822, acc 0.953125\n",
      "2018-05-23T15:04:13.906552: step 16021, loss 0.0725091, acc 0.96875\n",
      "2018-05-23T15:04:14.289530: step 16022, loss 0.125191, acc 0.921875\n",
      "2018-05-23T15:04:14.686467: step 16023, loss 0.0908977, acc 0.953125\n",
      "2018-05-23T15:04:15.085399: step 16024, loss 0.0824846, acc 0.953125\n",
      "2018-05-23T15:04:15.559132: step 16025, loss 0.104673, acc 0.953125\n",
      "2018-05-23T15:04:15.942108: step 16026, loss 0.0748721, acc 0.96875\n",
      "2018-05-23T15:04:16.320111: step 16027, loss 0.0771328, acc 0.96875\n",
      "2018-05-23T15:04:16.709057: step 16028, loss 0.09626, acc 0.9375\n",
      "2018-05-23T15:04:17.113972: step 16029, loss 0.118829, acc 0.96875\n",
      "2018-05-23T15:04:17.745283: step 16030, loss 0.0483222, acc 0.984375\n",
      "2018-05-23T15:04:18.171144: step 16031, loss 0.0568395, acc 0.984375\n",
      "2018-05-23T15:04:18.638893: step 16032, loss 0.0565725, acc 0.96875\n",
      "2018-05-23T15:04:19.104649: step 16033, loss 0.244624, acc 0.953125\n",
      "2018-05-23T15:04:19.517544: step 16034, loss 0.0395604, acc 0.984375\n",
      "2018-05-23T15:04:19.974320: step 16035, loss 0.0508231, acc 0.984375\n",
      "2018-05-23T15:04:20.406167: step 16036, loss 0.0629448, acc 0.984375\n",
      "2018-05-23T15:04:20.861947: step 16037, loss 0.0700944, acc 0.96875\n",
      "2018-05-23T15:04:21.235947: step 16038, loss 0.0484596, acc 0.984375\n",
      "2018-05-23T15:04:21.610941: step 16039, loss 0.111673, acc 0.953125\n",
      "2018-05-23T15:04:21.989928: step 16040, loss 0.0219735, acc 1\n",
      "2018-05-23T15:04:22.373901: step 16041, loss 0.172428, acc 0.921875\n",
      "2018-05-23T15:04:22.786804: step 16042, loss 0.134478, acc 0.953125\n",
      "2018-05-23T15:04:23.243574: step 16043, loss 0.0770764, acc 0.96875\n",
      "2018-05-23T15:04:23.681405: step 16044, loss 0.0943856, acc 0.96875\n",
      "2018-05-23T15:04:24.071361: step 16045, loss 0.135766, acc 0.9375\n",
      "2018-05-23T15:04:24.445358: step 16046, loss 0.270356, acc 0.9375\n",
      "2018-05-23T15:04:24.863242: step 16047, loss 0.0381598, acc 1\n",
      "2018-05-23T15:04:25.289102: step 16048, loss 0.0765182, acc 0.953125\n",
      "2018-05-23T15:04:25.672080: step 16049, loss 0.255411, acc 0.953125\n",
      "2018-05-23T15:04:26.109907: step 16050, loss 0.0937337, acc 0.96875\n",
      "2018-05-23T15:04:26.491884: step 16051, loss 0.139802, acc 0.96875\n",
      "2018-05-23T15:04:26.918746: step 16052, loss 0.131481, acc 0.953125\n",
      "2018-05-23T15:04:27.443344: step 16053, loss 0.0693104, acc 0.96875\n",
      "2018-05-23T15:04:28.128510: step 16054, loss 0.0581267, acc 0.953125\n",
      "2018-05-23T15:04:28.647123: step 16055, loss 0.0253876, acc 0.984375\n",
      "2018-05-23T15:04:29.208622: step 16056, loss 0.159282, acc 0.953125\n",
      "2018-05-23T15:04:29.745187: step 16057, loss 0.109813, acc 0.953125\n",
      "2018-05-23T15:04:30.312667: step 16058, loss 0.183465, acc 0.921875\n",
      "2018-05-23T15:04:30.879153: step 16059, loss 0.124168, acc 0.9375\n",
      "2018-05-23T15:04:31.305014: step 16060, loss 0.239498, acc 0.859375\n",
      "2018-05-23T15:04:31.690982: step 16061, loss 0.327683, acc 0.890625\n",
      "2018-05-23T15:04:32.060992: step 16062, loss 0.189515, acc 0.9375\n",
      "2018-05-23T15:04:32.485857: step 16063, loss 0.119797, acc 0.9375\n",
      "2018-05-23T15:04:32.871822: step 16064, loss 0.0783386, acc 0.953125\n",
      "2018-05-23T15:04:33.258819: step 16065, loss 0.0310501, acc 1\n",
      "2018-05-23T15:04:33.640767: step 16066, loss 0.125711, acc 0.953125\n",
      "2018-05-23T15:04:34.106518: step 16067, loss 0.110309, acc 0.953125\n",
      "2018-05-23T15:04:34.507445: step 16068, loss 0.130182, acc 0.9375\n",
      "2018-05-23T15:04:34.907377: step 16069, loss 0.12995, acc 0.953125\n",
      "2018-05-23T15:04:35.279380: step 16070, loss 0.0807572, acc 0.953125\n",
      "2018-05-23T15:04:35.689284: step 16071, loss 0.0688645, acc 0.96875\n",
      "2018-05-23T15:04:36.145067: step 16072, loss 0.227825, acc 0.953125\n",
      "2018-05-23T15:04:36.527043: step 16073, loss 0.146657, acc 0.953125\n",
      "2018-05-23T15:04:36.918996: step 16074, loss 0.0549766, acc 0.96875\n",
      "2018-05-23T15:04:37.297980: step 16075, loss 0.0376848, acc 0.984375\n",
      "2018-05-23T15:04:37.696945: step 16076, loss 0.0409365, acc 1\n",
      "2018-05-23T15:04:38.064929: step 16077, loss 0.14309, acc 0.90625\n",
      "2018-05-23T15:04:38.439925: step 16078, loss 0.0824182, acc 0.953125\n",
      "2018-05-23T15:04:38.905679: step 16079, loss 0.145572, acc 0.921875\n",
      "2018-05-23T15:04:39.292644: step 16080, loss 0.144956, acc 0.921875\n",
      "2018-05-23T15:04:39.713520: step 16081, loss 0.157444, acc 0.90625\n",
      "2018-05-23T15:04:40.091507: step 16082, loss 0.0850189, acc 0.953125\n",
      "2018-05-23T15:04:40.476016: step 16083, loss 0.0980118, acc 0.953125\n",
      "2018-05-23T15:04:40.872952: step 16084, loss 0.053525, acc 0.96875\n",
      "2018-05-23T15:04:41.241968: step 16085, loss 0.0662163, acc 0.953125\n",
      "2018-05-23T15:04:41.624940: step 16086, loss 0.0424297, acc 0.984375\n",
      "2018-05-23T15:04:42.121612: step 16087, loss 0.0770021, acc 0.953125\n",
      "2018-05-23T15:04:42.505587: step 16088, loss 0.166186, acc 0.921875\n",
      "2018-05-23T15:04:42.903663: step 16089, loss 0.0764959, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:04:43.272670: step 16090, loss 0.0967536, acc 0.96875\n",
      "2018-05-23T15:04:43.654647: step 16091, loss 0.139015, acc 0.9375\n",
      "2018-05-23T15:04:44.030125: step 16092, loss 0.13305, acc 0.96875\n",
      "2018-05-23T15:04:44.405121: step 16093, loss 0.0907649, acc 0.96875\n",
      "2018-05-23T15:04:44.869390: step 16094, loss 0.112426, acc 0.96875\n",
      "2018-05-23T15:04:45.258352: step 16095, loss 0.0933411, acc 0.953125\n",
      "2018-05-23T15:04:45.644319: step 16096, loss 0.12947, acc 0.9375\n",
      "2018-05-23T15:04:46.036270: step 16097, loss 0.0764273, acc 0.953125\n",
      "2018-05-23T15:04:46.410270: step 16098, loss 0.0673232, acc 1\n",
      "2018-05-23T15:04:46.791249: step 16099, loss 0.0918578, acc 0.9375\n",
      "2018-05-23T15:04:47.183201: step 16100, loss 0.0738978, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:04:52.550842: step 16100, loss 1.6797, acc 0.715102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16100\n",
      "\n",
      "2018-05-23T15:04:54.159538: step 16101, loss 0.1272, acc 0.9375\n",
      "2018-05-23T15:04:54.638258: step 16102, loss 0.109295, acc 0.953125\n",
      "2018-05-23T15:04:55.212721: step 16103, loss 0.0161436, acc 1\n",
      "2018-05-23T15:04:55.906863: step 16104, loss 0.0251298, acc 0.984375\n",
      "2018-05-23T15:04:56.565104: step 16105, loss 0.0796894, acc 0.9375\n",
      "2018-05-23T15:04:57.187437: step 16106, loss 0.147418, acc 0.9375\n",
      "2018-05-23T15:04:57.806824: step 16107, loss 0.0579984, acc 0.984375\n",
      "2018-05-23T15:04:58.414197: step 16108, loss 0.0990815, acc 0.96875\n",
      "2018-05-23T15:04:59.035538: step 16109, loss 0.185701, acc 0.90625\n",
      "2018-05-23T15:04:59.548167: step 16110, loss 0.130516, acc 0.953125\n",
      "2018-05-23T15:05:00.113652: step 16111, loss 0.0757194, acc 0.953125\n",
      "2018-05-23T15:05:00.683128: step 16112, loss 0.0651595, acc 0.96875\n",
      "2018-05-23T15:05:01.202740: step 16113, loss 0.0898085, acc 0.984375\n",
      "2018-05-23T15:05:01.725341: step 16114, loss 0.0754617, acc 0.953125\n",
      "2018-05-23T15:05:02.310774: step 16115, loss 0.0747345, acc 0.984375\n",
      "2018-05-23T15:05:02.857311: step 16116, loss 0.0523352, acc 0.984375\n",
      "2018-05-23T15:05:03.410830: step 16117, loss 0.0353739, acc 1\n",
      "2018-05-23T15:05:03.813602: step 16118, loss 0.189361, acc 0.921875\n",
      "2018-05-23T15:05:04.217523: step 16119, loss 0.130069, acc 0.921875\n",
      "2018-05-23T15:05:04.598502: step 16120, loss 0.0739558, acc 0.96875\n",
      "2018-05-23T15:05:05.305650: step 16121, loss 0.0241584, acc 1\n",
      "2018-05-23T15:05:05.950922: step 16122, loss 0.143139, acc 0.9375\n",
      "2018-05-23T15:05:06.541343: step 16123, loss 0.142942, acc 0.921875\n",
      "2018-05-23T15:05:07.222521: step 16124, loss 0.0558958, acc 0.984375\n",
      "2018-05-23T15:05:07.831930: step 16125, loss 0.0898637, acc 0.9375\n",
      "2018-05-23T15:05:08.436314: step 16126, loss 0.0973973, acc 0.9375\n",
      "2018-05-23T15:05:09.090605: step 16127, loss 0.0667631, acc 0.96875\n",
      "2018-05-23T15:05:09.704963: step 16128, loss 0.0708889, acc 0.984375\n",
      "2018-05-23T15:05:10.301365: step 16129, loss 0.107984, acc 0.953125\n",
      "2018-05-23T15:05:11.049873: step 16130, loss 0.135365, acc 0.9375\n",
      "2018-05-23T15:05:11.766467: step 16131, loss 0.0558549, acc 0.984375\n",
      "2018-05-23T15:05:12.865593: step 16132, loss 0.141222, acc 0.921875\n",
      "2018-05-23T15:05:13.624585: step 16133, loss 0.136665, acc 0.9375\n",
      "2018-05-23T15:05:14.573559: step 16134, loss 0.0413001, acc 1\n",
      "2018-05-23T15:05:15.341080: step 16135, loss 0.0820163, acc 0.953125\n",
      "2018-05-23T15:05:16.168864: step 16136, loss 0.0800074, acc 0.96875\n",
      "2018-05-23T15:05:16.779233: step 16137, loss 0.189976, acc 0.890625\n",
      "2018-05-23T15:05:17.397580: step 16138, loss 0.129869, acc 0.9375\n",
      "2018-05-23T15:05:17.876299: step 16139, loss 0.0876278, acc 0.96875\n",
      "2018-05-23T15:05:18.372968: step 16140, loss 0.0971773, acc 0.96875\n",
      "2018-05-23T15:05:18.901556: step 16141, loss 0.101089, acc 0.953125\n",
      "2018-05-23T15:05:19.300487: step 16142, loss 0.0827971, acc 0.96875\n",
      "2018-05-23T15:05:19.721362: step 16143, loss 0.0658321, acc 0.953125\n",
      "2018-05-23T15:05:20.284857: step 16144, loss 0.178272, acc 0.921875\n",
      "2018-05-23T15:05:20.919157: step 16145, loss 0.0943746, acc 0.953125\n",
      "2018-05-23T15:05:21.396879: step 16146, loss 0.100495, acc 0.9375\n",
      "2018-05-23T15:05:21.807780: step 16147, loss 0.157479, acc 0.921875\n",
      "2018-05-23T15:05:22.405182: step 16148, loss 0.0632161, acc 0.9375\n",
      "2018-05-23T15:05:22.790152: step 16149, loss 0.0667791, acc 0.984375\n",
      "2018-05-23T15:05:23.151185: step 16150, loss 0.139418, acc 0.96875\n",
      "2018-05-23T15:05:23.573059: step 16151, loss 0.0805261, acc 0.984375\n",
      "2018-05-23T15:05:23.971027: step 16152, loss 0.204701, acc 0.9375\n",
      "2018-05-23T15:05:24.372917: step 16153, loss 0.106509, acc 0.9375\n",
      "2018-05-23T15:05:24.727968: step 16154, loss 0.0485646, acc 1\n",
      "2018-05-23T15:05:25.089002: step 16155, loss 0.123376, acc 0.96875\n",
      "2018-05-23T15:05:25.441060: step 16156, loss 0.0486736, acc 1\n",
      "2018-05-23T15:05:25.827030: step 16157, loss 0.189543, acc 0.96875\n",
      "2018-05-23T15:05:26.288795: step 16158, loss 0.0504091, acc 0.984375\n",
      "2018-05-23T15:05:26.667779: step 16159, loss 0.085145, acc 0.984375\n",
      "2018-05-23T15:05:27.102617: step 16160, loss 0.0580942, acc 0.953125\n",
      "2018-05-23T15:05:27.584369: step 16161, loss 0.149009, acc 0.953125\n",
      "2018-05-23T15:05:27.962360: step 16162, loss 0.0607009, acc 0.984375\n",
      "2018-05-23T15:05:28.386185: step 16163, loss 0.386076, acc 0.921875\n",
      "2018-05-23T15:05:28.934719: step 16164, loss 0.0852595, acc 0.984375\n",
      "2018-05-23T15:05:29.663807: step 16165, loss 0.258016, acc 0.953125\n",
      "2018-05-23T15:05:30.045786: step 16166, loss 0.0714337, acc 0.953125\n",
      "2018-05-23T15:05:30.512538: step 16167, loss 0.198364, acc 0.9375\n",
      "2018-05-23T15:05:31.125897: step 16168, loss 0.259216, acc 0.921875\n",
      "2018-05-23T15:05:31.852952: step 16169, loss 0.20752, acc 0.953125\n",
      "2018-05-23T15:05:32.761523: step 16170, loss 0.0762375, acc 0.984375\n",
      "2018-05-23T15:05:33.340972: step 16171, loss 0.0956132, acc 0.953125\n",
      "2018-05-23T15:05:33.946353: step 16172, loss 0.108613, acc 0.96875\n",
      "2018-05-23T15:05:34.561706: step 16173, loss 0.0951859, acc 0.96875\n",
      "2018-05-23T15:05:35.236900: step 16174, loss 0.146697, acc 0.96875\n",
      "2018-05-23T15:05:35.935032: step 16175, loss 0.0917509, acc 0.96875\n",
      "2018-05-23T15:05:36.863548: step 16176, loss 0.0657069, acc 1\n",
      "2018-05-23T15:05:37.431031: step 16177, loss 0.0600576, acc 0.96875\n",
      "2018-05-23T15:05:37.936678: step 16178, loss 0.0975082, acc 0.953125\n",
      "2018-05-23T15:05:38.402432: step 16179, loss 0.11142, acc 0.96875\n",
      "2018-05-23T15:05:38.852229: step 16180, loss 0.0878649, acc 0.953125\n",
      "2018-05-23T15:05:39.407741: step 16181, loss 0.104376, acc 0.953125\n",
      "2018-05-23T15:05:40.193154: step 16182, loss 0.0960157, acc 0.96875\n",
      "2018-05-23T15:05:40.804514: step 16183, loss 0.17269, acc 0.921875\n",
      "2018-05-23T15:05:41.350055: step 16184, loss 0.0815224, acc 0.953125\n",
      "2018-05-23T15:05:41.885165: step 16185, loss 0.119684, acc 0.9375\n",
      "2018-05-23T15:05:42.557906: step 16186, loss 0.0739332, acc 0.953125\n",
      "2018-05-23T15:05:42.955843: step 16187, loss 0.141636, acc 0.9375\n",
      "2018-05-23T15:05:43.367739: step 16188, loss 0.150287, acc 0.9375\n",
      "2018-05-23T15:05:43.762693: step 16189, loss 0.0875921, acc 0.96875\n",
      "2018-05-23T15:05:44.202050: step 16190, loss 0.106262, acc 0.984375\n",
      "2018-05-23T15:05:44.671310: step 16191, loss 0.131662, acc 0.953125\n",
      "2018-05-23T15:05:45.101175: step 16192, loss 0.0648091, acc 0.96875\n",
      "2018-05-23T15:05:45.546984: step 16193, loss 0.110616, acc 0.9375\n",
      "2018-05-23T15:05:45.955889: step 16194, loss 0.0893895, acc 0.953125\n",
      "2018-05-23T15:05:46.357814: step 16195, loss 0.10055, acc 0.9375\n",
      "2018-05-23T15:05:47.006078: step 16196, loss 0.171226, acc 0.953125\n",
      "2018-05-23T15:05:47.596650: step 16197, loss 0.107205, acc 0.96875\n",
      "2018-05-23T15:05:48.075880: step 16198, loss 0.119193, acc 0.953125\n",
      "2018-05-23T15:05:48.538649: step 16199, loss 0.0971739, acc 0.9375\n",
      "2018-05-23T15:05:48.990440: step 16200, loss 0.114412, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:05:54.979999: step 16200, loss 1.66473, acc 0.714674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16200\n",
      "\n",
      "2018-05-23T15:05:56.828089: step 16201, loss 0.171293, acc 0.96875\n",
      "2018-05-23T15:05:57.426062: step 16202, loss 0.130692, acc 0.921875\n",
      "2018-05-23T15:05:57.908283: step 16203, loss 0.0815658, acc 0.953125\n",
      "2018-05-23T15:05:58.396974: step 16204, loss 0.0563987, acc 0.96875\n",
      "2018-05-23T15:05:58.864127: step 16205, loss 0.0516798, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:05:59.339855: step 16206, loss 0.0618034, acc 0.96875\n",
      "2018-05-23T15:05:59.784665: step 16207, loss 0.0703539, acc 0.96875\n",
      "2018-05-23T15:06:00.286324: step 16208, loss 0.162853, acc 0.90625\n",
      "2018-05-23T15:06:00.937613: step 16209, loss 0.0693075, acc 0.984375\n",
      "2018-05-23T15:06:01.454229: step 16210, loss 0.074505, acc 0.984375\n",
      "2018-05-23T15:06:02.017722: step 16211, loss 0.0874938, acc 0.9375\n",
      "2018-05-23T15:06:02.487984: step 16212, loss 0.164812, acc 0.921875\n",
      "2018-05-23T15:06:02.933791: step 16213, loss 0.145674, acc 0.953125\n",
      "2018-05-23T15:06:03.519224: step 16214, loss 0.0495912, acc 0.984375\n",
      "2018-05-23T15:06:03.984979: step 16215, loss 0.0827999, acc 0.96875\n",
      "2018-05-23T15:06:04.426822: step 16216, loss 0.0470321, acc 0.984375\n",
      "2018-05-23T15:06:04.881117: step 16217, loss 0.141509, acc 0.9375\n",
      "2018-05-23T15:06:05.340401: step 16218, loss 0.293766, acc 0.90625\n",
      "2018-05-23T15:06:06.001158: step 16219, loss 0.0520405, acc 0.984375\n",
      "2018-05-23T15:06:06.475888: step 16220, loss 0.0884053, acc 0.953125\n",
      "2018-05-23T15:06:06.922204: step 16221, loss 0.0850841, acc 0.953125\n",
      "2018-05-23T15:06:07.371005: step 16222, loss 0.0567522, acc 0.984375\n",
      "2018-05-23T15:06:07.808841: step 16223, loss 0.103644, acc 0.96875\n",
      "2018-05-23T15:06:08.383304: step 16224, loss 0.148561, acc 0.9375\n",
      "2018-05-23T15:06:08.842076: step 16225, loss 0.104744, acc 0.9375\n",
      "2018-05-23T15:06:09.306582: step 16226, loss 0.211653, acc 0.90625\n",
      "2018-05-23T15:06:09.763358: step 16227, loss 0.122929, acc 0.953125\n",
      "2018-05-23T15:06:10.220163: step 16228, loss 0.035365, acc 0.984375\n",
      "2018-05-23T15:06:10.784164: step 16229, loss 0.156511, acc 0.9375\n",
      "2018-05-23T15:06:11.251926: step 16230, loss 0.247539, acc 0.953125\n",
      "2018-05-23T15:06:11.719189: step 16231, loss 0.119094, acc 0.953125\n",
      "2018-05-23T15:06:12.193929: step 16232, loss 0.120489, acc 0.921875\n",
      "2018-05-23T15:06:12.639761: step 16233, loss 0.0844277, acc 0.96875\n",
      "2018-05-23T15:06:13.199772: step 16234, loss 0.0640944, acc 0.96875\n",
      "2018-05-23T15:06:13.698438: step 16235, loss 0.173498, acc 0.921875\n",
      "2018-05-23T15:06:14.160227: step 16236, loss 0.0861996, acc 0.96875\n",
      "2018-05-23T15:06:14.607030: step 16237, loss 0.177367, acc 0.90625\n",
      "2018-05-23T15:06:15.079765: step 16238, loss 0.185723, acc 0.96875\n",
      "2018-05-23T15:06:15.627300: step 16239, loss 0.155579, acc 0.953125\n",
      "2018-05-23T15:06:16.155023: step 16240, loss 0.102296, acc 0.953125\n",
      "2018-05-23T15:06:16.657362: step 16241, loss 0.160283, acc 0.9375\n",
      "2018-05-23T15:06:17.135613: step 16242, loss 0.132475, acc 0.953125\n",
      "2018-05-23T15:06:17.644604: step 16243, loss 0.0997076, acc 0.953125\n",
      "2018-05-23T15:06:18.202623: step 16244, loss 0.113245, acc 0.9375\n",
      "2018-05-23T15:06:18.666428: step 16245, loss 0.0866248, acc 1\n",
      "2018-05-23T15:06:19.113241: step 16246, loss 0.127154, acc 0.953125\n",
      "2018-05-23T15:06:19.577069: step 16247, loss 0.149833, acc 0.96875\n",
      "2018-05-23T15:06:20.108668: step 16248, loss 0.104747, acc 0.96875\n",
      "2018-05-23T15:06:20.655213: step 16249, loss 0.0795466, acc 0.96875\n",
      "2018-05-23T15:06:21.167841: step 16250, loss 0.0994048, acc 0.953125\n",
      "2018-05-23T15:06:21.643596: step 16251, loss 0.133218, acc 0.953125\n",
      "2018-05-23T15:06:22.116899: step 16252, loss 0.14378, acc 0.921875\n",
      "2018-05-23T15:06:22.575264: step 16253, loss 0.207438, acc 0.90625\n",
      "2018-05-23T15:06:23.053031: step 16254, loss 0.14142, acc 0.953125\n",
      "2018-05-23T15:06:23.675368: step 16255, loss 0.188533, acc 0.921875\n",
      "2018-05-23T15:06:24.599436: step 16256, loss 0.0966117, acc 0.984375\n",
      "2018-05-23T15:06:25.568557: step 16257, loss 0.0503867, acc 0.984375\n",
      "2018-05-23T15:06:26.038301: step 16258, loss 0.128454, acc 0.953125\n",
      "2018-05-23T15:06:26.483113: step 16259, loss 0.0836458, acc 0.96875\n",
      "2018-05-23T15:06:26.946871: step 16260, loss 0.13384, acc 0.921875\n",
      "2018-05-23T15:06:27.483436: step 16261, loss 0.128956, acc 0.921875\n",
      "2018-05-23T15:06:27.956170: step 16262, loss 0.206949, acc 0.9375\n",
      "2018-05-23T15:06:28.475718: step 16263, loss 0.051386, acc 1\n",
      "2018-05-23T15:06:29.587953: step 16264, loss 0.0558382, acc 1\n",
      "2018-05-23T15:06:30.315585: step 16265, loss 0.183825, acc 0.953125\n",
      "2018-05-23T15:06:30.995862: step 16266, loss 0.133667, acc 0.90625\n",
      "2018-05-23T15:06:31.726034: step 16267, loss 0.0869223, acc 0.953125\n",
      "2018-05-23T15:06:32.243697: step 16268, loss 0.155095, acc 0.9375\n",
      "2018-05-23T15:06:32.750386: step 16269, loss 0.105261, acc 0.96875\n",
      "2018-05-23T15:06:33.227742: step 16270, loss 0.0473722, acc 1\n",
      "2018-05-23T15:06:33.752440: step 16271, loss 0.0798206, acc 0.96875\n",
      "2018-05-23T15:06:34.214698: step 16272, loss 0.0152725, acc 1\n",
      "2018-05-23T15:06:34.683043: step 16273, loss 0.274182, acc 0.890625\n",
      "2018-05-23T15:06:35.140414: step 16274, loss 0.0602511, acc 0.984375\n",
      "2018-05-23T15:06:35.581277: step 16275, loss 0.074038, acc 0.96875\n",
      "2018-05-23T15:06:36.067484: step 16276, loss 0.203708, acc 0.90625\n",
      "2018-05-23T15:06:36.564286: step 16277, loss 0.116346, acc 0.921875\n",
      "2018-05-23T15:06:37.036574: step 16278, loss 0.082631, acc 0.9375\n",
      "2018-05-23T15:06:37.512037: step 16279, loss 0.149471, acc 0.9375\n",
      "2018-05-23T15:06:37.983479: step 16280, loss 0.0538205, acc 0.984375\n",
      "2018-05-23T15:06:38.478124: step 16281, loss 0.0719903, acc 0.984375\n",
      "2018-05-23T15:06:39.031773: step 16282, loss 0.0838436, acc 0.953125\n",
      "2018-05-23T15:06:39.481115: step 16283, loss 0.166469, acc 0.953125\n",
      "2018-05-23T15:06:39.953130: step 16284, loss 0.0881475, acc 0.96875\n",
      "2018-05-23T15:06:40.614145: step 16285, loss 0.0606457, acc 0.984375\n",
      "2018-05-23T15:06:41.320848: step 16286, loss 0.115922, acc 0.953125\n",
      "2018-05-23T15:06:41.968030: step 16287, loss 0.11168, acc 0.921875\n",
      "2018-05-23T15:06:42.693400: step 16288, loss 0.0478812, acc 1\n",
      "2018-05-23T15:06:43.513825: step 16289, loss 0.237357, acc 0.921875\n",
      "2018-05-23T15:06:43.981165: step 16290, loss 0.0805211, acc 0.96875\n",
      "2018-05-23T15:06:44.458483: step 16291, loss 0.0489771, acc 0.96875\n",
      "2018-05-23T15:06:45.013095: step 16292, loss 0.106023, acc 0.96875\n",
      "2018-05-23T15:06:45.758247: step 16293, loss 0.109491, acc 0.953125\n",
      "2018-05-23T15:06:46.454502: step 16294, loss 0.111585, acc 0.953125\n",
      "2018-05-23T15:06:47.311549: step 16295, loss 0.158588, acc 0.9375\n",
      "2018-05-23T15:06:48.070155: step 16296, loss 0.042511, acc 1\n",
      "2018-05-23T15:06:48.924726: step 16297, loss 0.0661106, acc 0.96875\n",
      "2018-05-23T15:06:49.585002: step 16298, loss 0.211583, acc 0.921875\n",
      "2018-05-23T15:06:50.426751: step 16299, loss 0.0927452, acc 0.984375\n",
      "2018-05-23T15:06:51.219142: step 16300, loss 0.0794859, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:06:57.696403: step 16300, loss 1.69062, acc 0.714674\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16300\n",
      "\n",
      "2018-05-23T15:07:00.265093: step 16301, loss 0.0906932, acc 0.953125\n",
      "2018-05-23T15:07:00.718879: step 16302, loss 0.0712949, acc 0.984375\n",
      "2018-05-23T15:07:01.306820: step 16303, loss 0.0809579, acc 0.96875\n",
      "2018-05-23T15:07:01.803041: step 16304, loss 0.180584, acc 0.90625\n",
      "2018-05-23T15:07:02.345376: step 16305, loss 0.0839997, acc 0.953125\n",
      "2018-05-23T15:07:02.853528: step 16306, loss 0.0684274, acc 0.96875\n",
      "2018-05-23T15:07:03.407600: step 16307, loss 0.133061, acc 0.953125\n",
      "2018-05-23T15:07:03.882402: step 16308, loss 0.0861617, acc 0.96875\n",
      "2018-05-23T15:07:04.353070: step 16309, loss 0.0524811, acc 0.96875\n",
      "2018-05-23T15:07:04.813860: step 16310, loss 0.107356, acc 0.953125\n",
      "2018-05-23T15:07:05.287634: step 16311, loss 0.111835, acc 0.953125\n",
      "2018-05-23T15:07:05.891533: step 16312, loss 0.102525, acc 0.96875\n",
      "2018-05-23T15:07:06.341329: step 16313, loss 0.0619953, acc 0.984375\n",
      "2018-05-23T15:07:06.793120: step 16314, loss 0.144357, acc 0.9375\n",
      "2018-05-23T15:07:07.250440: step 16315, loss 0.137318, acc 0.921875\n",
      "2018-05-23T15:07:07.688267: step 16316, loss 0.089065, acc 0.96875\n",
      "2018-05-23T15:07:08.265724: step 16317, loss 0.108094, acc 0.953125\n",
      "2018-05-23T15:07:08.758404: step 16318, loss 0.18228, acc 0.9375\n",
      "2018-05-23T15:07:09.248213: step 16319, loss 0.0897914, acc 0.96875\n",
      "2018-05-23T15:07:09.735980: step 16320, loss 0.0325198, acc 0.984375\n",
      "2018-05-23T15:07:10.206520: step 16321, loss 0.103274, acc 0.96875\n",
      "2018-05-23T15:07:10.810025: step 16322, loss 0.0680055, acc 1\n",
      "2018-05-23T15:07:11.279423: step 16323, loss 0.141042, acc 0.921875\n",
      "2018-05-23T15:07:11.743848: step 16324, loss 0.0943135, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:07:12.241068: step 16325, loss 0.066734, acc 0.984375\n",
      "2018-05-23T15:07:12.691483: step 16326, loss 0.209525, acc 0.953125\n",
      "2018-05-23T15:07:13.189704: step 16327, loss 0.0476504, acc 0.984375\n",
      "2018-05-23T15:07:13.597961: step 16328, loss 0.0603962, acc 0.984375\n",
      "2018-05-23T15:07:14.000936: step 16329, loss 0.159079, acc 0.953125\n",
      "2018-05-23T15:07:14.402866: step 16330, loss 0.0649939, acc 0.984375\n",
      "2018-05-23T15:07:14.792969: step 16331, loss 0.0863312, acc 0.953125\n",
      "2018-05-23T15:07:15.248771: step 16332, loss 0.065621, acc 0.96875\n",
      "2018-05-23T15:07:15.699150: step 16333, loss 0.0806051, acc 0.953125\n",
      "2018-05-23T15:07:16.101126: step 16334, loss 0.10259, acc 0.96875\n",
      "2018-05-23T15:07:16.499578: step 16335, loss 0.0536539, acc 0.984375\n",
      "2018-05-23T15:07:16.900092: step 16336, loss 0.0717981, acc 0.96875\n",
      "2018-05-23T15:07:17.317091: step 16337, loss 0.14585, acc 0.96875\n",
      "2018-05-23T15:07:17.884239: step 16338, loss 0.100419, acc 0.9375\n",
      "2018-05-23T15:07:18.281749: step 16339, loss 0.0781593, acc 0.953125\n",
      "2018-05-23T15:07:18.671148: step 16340, loss 0.0873533, acc 0.96875\n",
      "2018-05-23T15:07:19.053697: step 16341, loss 0.087665, acc 0.984375\n",
      "2018-05-23T15:07:19.452084: step 16342, loss 0.102598, acc 0.921875\n",
      "2018-05-23T15:07:19.857008: step 16343, loss 0.0961968, acc 0.953125\n",
      "2018-05-23T15:07:20.247026: step 16344, loss 0.153829, acc 0.9375\n",
      "2018-05-23T15:07:20.658865: step 16345, loss 0.0680266, acc 0.96875\n",
      "2018-05-23T15:07:21.203416: step 16346, loss 0.0675544, acc 0.96875\n",
      "2018-05-23T15:07:21.613387: step 16347, loss 0.133087, acc 0.953125\n",
      "2018-05-23T15:07:22.019245: step 16348, loss 0.06735, acc 0.96875\n",
      "2018-05-23T15:07:22.413309: step 16349, loss 0.0235531, acc 1\n",
      "2018-05-23T15:07:22.817162: step 16350, loss 0.0893675, acc 0.96875\n",
      "2018-05-23T15:07:23.240963: step 16351, loss 0.0547954, acc 0.96875\n",
      "2018-05-23T15:07:23.676827: step 16352, loss 0.059175, acc 0.96875\n",
      "2018-05-23T15:07:24.090745: step 16353, loss 0.0969169, acc 0.953125\n",
      "2018-05-23T15:07:24.761020: step 16354, loss 0.0699153, acc 0.984375\n",
      "2018-05-23T15:07:25.348961: step 16355, loss 0.158554, acc 0.90625\n",
      "2018-05-23T15:07:25.897547: step 16356, loss 0.126866, acc 0.96875\n",
      "2018-05-23T15:07:26.434113: step 16357, loss 0.0460041, acc 0.984375\n",
      "2018-05-23T15:07:26.965691: step 16358, loss 0.13013, acc 0.9375\n",
      "2018-05-23T15:07:27.504562: step 16359, loss 0.156091, acc 0.921875\n",
      "2018-05-23T15:07:27.983319: step 16360, loss 0.0512428, acc 0.96875\n",
      "2018-05-23T15:07:28.391454: step 16361, loss 0.054286, acc 0.96875\n",
      "2018-05-23T15:07:28.795902: step 16362, loss 0.139257, acc 0.9375\n",
      "2018-05-23T15:07:29.191356: step 16363, loss 0.0370845, acc 1\n",
      "2018-05-23T15:07:29.581828: step 16364, loss 0.0573105, acc 1\n",
      "2018-05-23T15:07:29.997714: step 16365, loss 0.0299569, acc 1\n",
      "2018-05-23T15:07:30.399642: step 16366, loss 0.156174, acc 0.921875\n",
      "2018-05-23T15:07:30.792588: step 16367, loss 0.137712, acc 0.953125\n",
      "2018-05-23T15:07:31.191569: step 16368, loss 0.093621, acc 0.953125\n",
      "2018-05-23T15:07:31.608452: step 16369, loss 0.0900595, acc 0.96875\n",
      "2018-05-23T15:07:32.014366: step 16370, loss 0.185098, acc 0.953125\n",
      "2018-05-23T15:07:32.426242: step 16371, loss 0.148883, acc 0.96875\n",
      "2018-05-23T15:07:32.893019: step 16372, loss 0.0827168, acc 0.953125\n",
      "2018-05-23T15:07:33.362795: step 16373, loss 0.15191, acc 0.953125\n",
      "2018-05-23T15:07:33.764978: step 16374, loss 0.0574349, acc 0.96875\n",
      "2018-05-23T15:07:34.163912: step 16375, loss 0.0900286, acc 0.921875\n",
      "2018-05-23T15:07:34.575814: step 16376, loss 0.13892, acc 0.9375\n",
      "2018-05-23T15:07:34.976750: step 16377, loss 0.0807295, acc 0.96875\n",
      "2018-05-23T15:07:35.371700: step 16378, loss 0.083444, acc 0.953125\n",
      "2018-05-23T15:07:35.770509: step 16379, loss 0.0759689, acc 0.96875\n",
      "2018-05-23T15:07:36.188900: step 16380, loss 0.0666616, acc 0.953125\n",
      "2018-05-23T15:07:36.614769: step 16381, loss 0.0631774, acc 0.984375\n",
      "2018-05-23T15:07:37.013739: step 16382, loss 0.0694005, acc 0.96875\n",
      "2018-05-23T15:07:37.414653: step 16383, loss 0.0671129, acc 0.96875\n",
      "2018-05-23T15:07:37.809453: step 16384, loss 0.0431365, acc 0.984375\n",
      "2018-05-23T15:07:38.194035: step 16385, loss 0.0545377, acc 1\n",
      "2018-05-23T15:07:38.607760: step 16386, loss 0.142637, acc 0.9375\n",
      "2018-05-23T15:07:39.067055: step 16387, loss 0.168493, acc 0.96875\n",
      "2018-05-23T15:07:39.482941: step 16388, loss 0.0345374, acc 0.984375\n",
      "2018-05-23T15:07:39.885376: step 16389, loss 0.101767, acc 0.953125\n",
      "2018-05-23T15:07:40.281318: step 16390, loss 0.0838599, acc 0.953125\n",
      "2018-05-23T15:07:40.683241: step 16391, loss 0.176436, acc 0.9375\n",
      "2018-05-23T15:07:41.074202: step 16392, loss 0.0958459, acc 0.984375\n",
      "2018-05-23T15:07:41.475136: step 16393, loss 0.0904839, acc 0.96875\n",
      "2018-05-23T15:07:41.862610: step 16394, loss 0.100985, acc 0.96875\n",
      "2018-05-23T15:07:42.278520: step 16395, loss 0.102092, acc 0.953125\n",
      "2018-05-23T15:07:42.723841: step 16396, loss 0.120437, acc 0.953125\n",
      "2018-05-23T15:07:43.132754: step 16397, loss 0.123743, acc 0.9375\n",
      "2018-05-23T15:07:43.539666: step 16398, loss 0.0708709, acc 0.96875\n",
      "2018-05-23T15:07:43.933611: step 16399, loss 0.209308, acc 0.890625\n",
      "2018-05-23T15:07:44.402359: step 16400, loss 0.0755491, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:07:51.742696: step 16400, loss 1.71267, acc 0.716817\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16400\n",
      "\n",
      "2018-05-23T15:07:54.076567: step 16401, loss 0.182825, acc 0.90625\n",
      "2018-05-23T15:07:54.514399: step 16402, loss 0.113141, acc 0.96875\n",
      "2018-05-23T15:07:54.955277: step 16403, loss 0.0900196, acc 0.953125\n",
      "2018-05-23T15:07:55.382164: step 16404, loss 0.100378, acc 0.984375\n",
      "2018-05-23T15:07:55.786632: step 16405, loss 0.0491054, acc 0.984375\n",
      "2018-05-23T15:07:56.202633: step 16406, loss 0.0553214, acc 0.984375\n",
      "2018-05-23T15:07:56.607537: step 16407, loss 0.167331, acc 0.953125\n",
      "2018-05-23T15:07:57.013644: step 16408, loss 0.0881506, acc 0.953125\n",
      "2018-05-23T15:07:57.425544: step 16409, loss 0.161774, acc 0.96875\n",
      "2018-05-23T15:07:57.814600: step 16410, loss 0.155926, acc 0.9375\n",
      "2018-05-23T15:07:58.203275: step 16411, loss 0.0824534, acc 0.984375\n",
      "2018-05-23T15:07:58.687283: step 16412, loss 0.0525407, acc 0.96875\n",
      "2018-05-23T15:07:59.109305: step 16413, loss 0.127581, acc 0.9375\n",
      "2018-05-23T15:07:59.515753: step 16414, loss 0.0685337, acc 0.96875\n",
      "2018-05-23T15:07:59.911701: step 16415, loss 0.170408, acc 0.90625\n",
      "2018-05-23T15:08:00.322298: step 16416, loss 0.0228131, acc 1\n",
      "2018-05-23T15:08:00.723880: step 16417, loss 0.0477261, acc 0.96875\n",
      "2018-05-23T15:08:01.190315: step 16418, loss 0.0342022, acc 1\n",
      "2018-05-23T15:08:01.649218: step 16419, loss 0.222729, acc 0.921875\n",
      "2018-05-23T15:08:02.071222: step 16420, loss 0.0592995, acc 0.984375\n",
      "2018-05-23T15:08:02.558514: step 16421, loss 0.136163, acc 0.953125\n",
      "2018-05-23T15:08:03.159393: step 16422, loss 0.08417, acc 0.96875\n",
      "2018-05-23T15:08:03.626287: step 16423, loss 0.0240465, acc 1\n",
      "2018-05-23T15:08:04.060818: step 16424, loss 0.0674434, acc 0.96875\n",
      "2018-05-23T15:08:04.839065: step 16425, loss 0.0593418, acc 0.984375\n",
      "2018-05-23T15:08:05.328755: step 16426, loss 0.118873, acc 0.9375\n",
      "2018-05-23T15:08:05.804525: step 16427, loss 0.0941256, acc 0.9375\n",
      "2018-05-23T15:08:06.288188: step 16428, loss 0.13897, acc 0.921875\n",
      "2018-05-23T15:08:06.929098: step 16429, loss 0.177309, acc 0.90625\n",
      "2018-05-23T15:08:07.543995: step 16430, loss 0.14374, acc 0.9375\n",
      "2018-05-23T15:08:08.231666: step 16431, loss 0.168746, acc 0.921875\n",
      "2018-05-23T15:08:09.076441: step 16432, loss 0.190927, acc 0.984375\n",
      "2018-05-23T15:08:09.658880: step 16433, loss 0.143548, acc 0.921875\n",
      "2018-05-23T15:08:10.315126: step 16434, loss 0.0213479, acc 0.984375\n",
      "2018-05-23T15:08:10.923498: step 16435, loss 0.0416397, acc 0.96875\n",
      "2018-05-23T15:08:11.643610: step 16436, loss 0.148217, acc 0.953125\n",
      "2018-05-23T15:08:12.269934: step 16437, loss 0.0613422, acc 0.984375\n",
      "2018-05-23T15:08:12.832949: step 16438, loss 0.17503, acc 0.9375\n",
      "2018-05-23T15:08:13.522751: step 16439, loss 0.105622, acc 0.953125\n",
      "2018-05-23T15:08:14.075273: step 16440, loss 0.123132, acc 0.953125\n",
      "2018-05-23T15:08:14.618819: step 16441, loss 0.049214, acc 0.984375\n",
      "2018-05-23T15:08:15.031735: step 16442, loss 0.040351, acc 0.984375\n",
      "2018-05-23T15:08:15.446586: step 16443, loss 0.186727, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:08:15.841554: step 16444, loss 0.0932011, acc 0.953125\n",
      "2018-05-23T15:08:16.240024: step 16445, loss 0.172655, acc 0.921875\n",
      "2018-05-23T15:08:16.647946: step 16446, loss 0.103337, acc 0.96875\n",
      "2018-05-23T15:08:17.029926: step 16447, loss 0.217357, acc 0.9375\n",
      "2018-05-23T15:08:17.427195: step 16448, loss 0.0422828, acc 0.984375\n",
      "2018-05-23T15:08:17.864024: step 16449, loss 0.0737956, acc 0.953125\n",
      "2018-05-23T15:08:18.269945: step 16450, loss 0.170315, acc 0.9375\n",
      "2018-05-23T15:08:18.731713: step 16451, loss 0.0443575, acc 0.984375\n",
      "2018-05-23T15:08:19.300736: step 16452, loss 0.0552087, acc 0.984375\n",
      "2018-05-23T15:08:19.876299: step 16453, loss 0.203642, acc 0.953125\n",
      "2018-05-23T15:08:20.435396: step 16454, loss 0.264954, acc 0.9375\n",
      "2018-05-23T15:08:21.113643: step 16455, loss 0.275507, acc 0.9375\n",
      "2018-05-23T15:08:21.680591: step 16456, loss 0.0646137, acc 0.96875\n",
      "2018-05-23T15:08:22.190689: step 16457, loss 0.131786, acc 0.953125\n",
      "2018-05-23T15:08:22.619570: step 16458, loss 0.0908624, acc 0.96875\n",
      "2018-05-23T15:08:23.056144: step 16459, loss 0.157943, acc 0.953125\n",
      "2018-05-23T15:08:23.479786: step 16460, loss 0.118849, acc 0.96875\n",
      "2018-05-23T15:08:23.889786: step 16461, loss 0.124425, acc 0.96875\n",
      "2018-05-23T15:08:24.278843: step 16462, loss 0.0523926, acc 0.96875\n",
      "2018-05-23T15:08:24.676327: step 16463, loss 0.0392543, acc 0.96875\n",
      "2018-05-23T15:08:25.077769: step 16464, loss 0.112513, acc 0.921875\n",
      "2018-05-23T15:08:25.468997: step 16465, loss 0.0685879, acc 0.96875\n",
      "2018-05-23T15:08:26.006101: step 16466, loss 0.101857, acc 0.953125\n",
      "2018-05-23T15:08:26.634932: step 16467, loss 0.0448538, acc 0.984375\n",
      "2018-05-23T15:08:27.180471: step 16468, loss 0.176129, acc 0.921875\n",
      "2018-05-23T15:08:27.724526: step 16469, loss 0.168628, acc 0.9375\n",
      "2018-05-23T15:08:28.412685: step 16470, loss 0.0493618, acc 0.984375\n",
      "2018-05-23T15:08:28.930299: step 16471, loss 0.121754, acc 0.9375\n",
      "2018-05-23T15:08:29.362144: step 16472, loss 0.122204, acc 0.9375\n",
      "2018-05-23T15:08:29.760082: step 16473, loss 0.0617878, acc 0.96875\n",
      "2018-05-23T15:08:30.163999: step 16474, loss 0.20009, acc 0.953125\n",
      "2018-05-23T15:08:30.554965: step 16475, loss 0.0892718, acc 0.953125\n",
      "2018-05-23T15:08:30.951122: step 16476, loss 0.181165, acc 0.953125\n",
      "2018-05-23T15:08:31.348556: step 16477, loss 0.0524776, acc 1\n",
      "2018-05-23T15:08:31.744506: step 16478, loss 0.122431, acc 0.953125\n",
      "2018-05-23T15:08:32.146516: step 16479, loss 0.255315, acc 0.90625\n",
      "2018-05-23T15:08:32.543452: step 16480, loss 0.171746, acc 0.90625\n",
      "2018-05-23T15:08:32.938435: step 16481, loss 0.0747413, acc 0.984375\n",
      "2018-05-23T15:08:33.379765: step 16482, loss 0.0960706, acc 0.984375\n",
      "2018-05-23T15:08:33.787676: step 16483, loss 0.129307, acc 0.953125\n",
      "2018-05-23T15:08:34.199572: step 16484, loss 0.131103, acc 0.9375\n",
      "2018-05-23T15:08:34.601497: step 16485, loss 0.0594641, acc 0.96875\n",
      "2018-05-23T15:08:35.001434: step 16486, loss 0.183637, acc 0.9375\n",
      "2018-05-23T15:08:35.396399: step 16487, loss 0.13819, acc 0.953125\n",
      "2018-05-23T15:08:35.790345: step 16488, loss 0.158898, acc 0.953125\n",
      "2018-05-23T15:08:36.268116: step 16489, loss 0.0679761, acc 0.984375\n",
      "2018-05-23T15:08:36.676025: step 16490, loss 0.119871, acc 0.953125\n",
      "2018-05-23T15:08:37.074955: step 16491, loss 0.116849, acc 0.9375\n",
      "2018-05-23T15:08:37.575648: step 16492, loss 0.0914443, acc 0.9375\n",
      "2018-05-23T15:08:37.989579: step 16493, loss 0.180028, acc 0.9375\n",
      "2018-05-23T15:08:38.392501: step 16494, loss 0.0407135, acc 1\n",
      "2018-05-23T15:08:38.798427: step 16495, loss 0.181467, acc 0.921875\n",
      "2018-05-23T15:08:39.194387: step 16496, loss 0.149377, acc 0.953125\n",
      "2018-05-23T15:08:39.594316: step 16497, loss 0.148788, acc 0.9375\n",
      "2018-05-23T15:08:40.002693: step 16498, loss 0.071557, acc 0.96875\n",
      "2018-05-23T15:08:40.389089: step 16499, loss 0.0468692, acc 0.984375\n",
      "2018-05-23T15:08:40.831417: step 16500, loss 0.0600636, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:08:46.902737: step 16500, loss 1.72965, acc 0.716531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16500\n",
      "\n",
      "2018-05-23T15:08:49.068456: step 16501, loss 0.075716, acc 0.96875\n",
      "2018-05-23T15:08:49.553166: step 16502, loss 0.0928217, acc 0.96875\n",
      "2018-05-23T15:08:49.997180: step 16503, loss 0.0721413, acc 0.96875\n",
      "2018-05-23T15:08:50.459978: step 16504, loss 0.134256, acc 0.984375\n",
      "2018-05-23T15:08:50.869889: step 16505, loss 0.0936506, acc 0.96875\n",
      "2018-05-23T15:08:51.303241: step 16506, loss 0.0480105, acc 0.984375\n",
      "2018-05-23T15:08:51.702175: step 16507, loss 0.0922457, acc 0.953125\n",
      "2018-05-23T15:08:52.101181: step 16508, loss 0.123187, acc 0.9375\n",
      "2018-05-23T15:08:52.493072: step 16509, loss 0.105386, acc 0.953125\n",
      "2018-05-23T15:08:52.892219: step 16510, loss 0.198268, acc 0.9375\n",
      "2018-05-23T15:08:53.291667: step 16511, loss 0.0819267, acc 0.984375\n",
      "2018-05-23T15:08:53.689467: step 16512, loss 0.0832545, acc 0.9375\n",
      "2018-05-23T15:08:54.076628: step 16513, loss 0.103945, acc 0.96875\n",
      "2018-05-23T15:08:54.489526: step 16514, loss 0.12604, acc 0.9375\n",
      "2018-05-23T15:08:54.884685: step 16515, loss 0.166149, acc 0.9375\n",
      "2018-05-23T15:08:55.314535: step 16516, loss 0.0936468, acc 0.953125\n",
      "2018-05-23T15:08:55.745416: step 16517, loss 0.0560401, acc 0.984375\n",
      "2018-05-23T15:08:56.165329: step 16518, loss 0.0678934, acc 0.984375\n",
      "2018-05-23T15:08:56.567767: step 16519, loss 0.0817735, acc 0.984375\n",
      "2018-05-23T15:08:56.967696: step 16520, loss 0.137045, acc 0.953125\n",
      "2018-05-23T15:08:57.367636: step 16521, loss 0.107304, acc 0.953125\n",
      "2018-05-23T15:08:57.771064: step 16522, loss 0.0564844, acc 0.984375\n",
      "2018-05-23T15:08:58.172415: step 16523, loss 0.0958303, acc 0.953125\n",
      "2018-05-23T15:08:58.576349: step 16524, loss 0.121873, acc 0.9375\n",
      "2018-05-23T15:08:58.972303: step 16525, loss 0.0467002, acc 0.96875\n",
      "2018-05-23T15:08:59.367293: step 16526, loss 0.155815, acc 0.953125\n",
      "2018-05-23T15:08:59.762649: step 16527, loss 0.241697, acc 0.875\n",
      "2018-05-23T15:09:00.169561: step 16528, loss 0.066359, acc 0.984375\n",
      "2018-05-23T15:09:00.604425: step 16529, loss 0.0593326, acc 0.984375\n",
      "2018-05-23T15:09:01.087806: step 16530, loss 0.0930655, acc 0.953125\n",
      "2018-05-23T15:09:01.489239: step 16531, loss 0.125067, acc 0.953125\n",
      "2018-05-23T15:09:01.904171: step 16532, loss 0.125889, acc 0.984375\n",
      "2018-05-23T15:09:02.311090: step 16533, loss 0.0598846, acc 0.96875\n",
      "2018-05-23T15:09:02.746044: step 16534, loss 0.112442, acc 0.921875\n",
      "2018-05-23T15:09:03.200334: step 16535, loss 0.102791, acc 0.96875\n",
      "2018-05-23T15:09:03.622278: step 16536, loss 0.0426018, acc 1\n",
      "2018-05-23T15:09:04.080051: step 16537, loss 0.0785762, acc 0.984375\n",
      "2018-05-23T15:09:04.773199: step 16538, loss 0.0939108, acc 0.96875\n",
      "2018-05-23T15:09:05.495493: step 16539, loss 0.114976, acc 0.953125\n",
      "2018-05-23T15:09:06.083092: step 16540, loss 0.0989926, acc 0.953125\n",
      "2018-05-23T15:09:06.786532: step 16541, loss 0.0567319, acc 0.96875\n",
      "2018-05-23T15:09:07.338058: step 16542, loss 0.0909106, acc 0.984375\n",
      "2018-05-23T15:09:07.785894: step 16543, loss 0.269218, acc 0.984375\n",
      "2018-05-23T15:09:08.205737: step 16544, loss 0.193286, acc 0.90625\n",
      "2018-05-23T15:09:09.006594: step 16545, loss 0.083767, acc 0.953125\n",
      "2018-05-23T15:09:09.548145: step 16546, loss 0.0825436, acc 0.96875\n",
      "2018-05-23T15:09:09.984980: step 16547, loss 0.138039, acc 0.9375\n",
      "2018-05-23T15:09:10.374932: step 16548, loss 0.0930718, acc 0.953125\n",
      "2018-05-23T15:09:10.806777: step 16549, loss 0.151096, acc 0.890625\n",
      "2018-05-23T15:09:11.376257: step 16550, loss 0.0967882, acc 0.9375\n",
      "2018-05-23T15:09:11.776186: step 16551, loss 0.101074, acc 0.953125\n",
      "2018-05-23T15:09:12.204041: step 16552, loss 0.083157, acc 0.953125\n",
      "2018-05-23T15:09:12.604967: step 16553, loss 0.101088, acc 0.953125\n",
      "2018-05-23T15:09:13.015870: step 16554, loss 0.184149, acc 0.953125\n",
      "2018-05-23T15:09:13.576369: step 16555, loss 0.130938, acc 0.953125\n",
      "2018-05-23T15:09:13.970315: step 16556, loss 0.0879178, acc 0.984375\n",
      "2018-05-23T15:09:14.367254: step 16557, loss 0.0595812, acc 0.984375\n",
      "2018-05-23T15:09:14.741251: step 16558, loss 0.0811746, acc 0.953125\n",
      "2018-05-23T15:09:15.128219: step 16559, loss 0.109097, acc 0.953125\n",
      "2018-05-23T15:09:15.525157: step 16560, loss 0.0890873, acc 0.9375\n",
      "2018-05-23T15:09:15.988917: step 16561, loss 0.0686804, acc 0.953125\n",
      "2018-05-23T15:09:16.546425: step 16562, loss 0.143138, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:09:17.376203: step 16563, loss 0.162491, acc 0.921875\n",
      "2018-05-23T15:09:18.151129: step 16564, loss 0.238623, acc 0.96875\n",
      "2018-05-23T15:09:18.807375: step 16565, loss 0.0620794, acc 0.96875\n",
      "2018-05-23T15:09:19.250191: step 16566, loss 0.214619, acc 0.9375\n",
      "2018-05-23T15:09:19.673059: step 16567, loss 0.0479543, acc 0.984375\n",
      "2018-05-23T15:09:20.097925: step 16568, loss 0.142023, acc 0.953125\n",
      "2018-05-23T15:09:20.569661: step 16569, loss 0.0974576, acc 0.96875\n",
      "2018-05-23T15:09:21.326269: step 16570, loss 0.132501, acc 0.9375\n",
      "2018-05-23T15:09:21.847874: step 16571, loss 0.0984406, acc 0.96875\n",
      "2018-05-23T15:09:22.256780: step 16572, loss 0.0495695, acc 1\n",
      "2018-05-23T15:09:22.669676: step 16573, loss 0.0610606, acc 0.984375\n",
      "2018-05-23T15:09:23.258103: step 16574, loss 0.0712788, acc 0.96875\n",
      "2018-05-23T15:09:24.002709: step 16575, loss 0.0965752, acc 0.953125\n",
      "2018-05-23T15:09:24.692861: step 16576, loss 0.150239, acc 0.921875\n",
      "2018-05-23T15:09:25.342148: step 16577, loss 0.0626107, acc 0.96875\n",
      "2018-05-23T15:09:25.898719: step 16578, loss 0.0461745, acc 0.984375\n",
      "2018-05-23T15:09:26.500747: step 16579, loss 0.118477, acc 0.953125\n",
      "2018-05-23T15:09:26.934136: step 16580, loss 0.0547816, acc 1\n",
      "2018-05-23T15:09:27.342123: step 16581, loss 0.328763, acc 0.90625\n",
      "2018-05-23T15:09:27.750142: step 16582, loss 0.13012, acc 0.9375\n",
      "2018-05-23T15:09:28.147657: step 16583, loss 0.134772, acc 0.90625\n",
      "2018-05-23T15:09:28.535158: step 16584, loss 0.0817385, acc 0.953125\n",
      "2018-05-23T15:09:28.940245: step 16585, loss 0.206319, acc 0.921875\n",
      "2018-05-23T15:09:29.337676: step 16586, loss 0.124684, acc 0.890625\n",
      "2018-05-23T15:09:29.748203: step 16587, loss 0.0865576, acc 0.953125\n",
      "2018-05-23T15:09:30.133713: step 16588, loss 0.102464, acc 0.921875\n",
      "2018-05-23T15:09:30.598604: step 16589, loss 0.212851, acc 0.96875\n",
      "2018-05-23T15:09:31.009068: step 16590, loss 0.119814, acc 0.953125\n",
      "2018-05-23T15:09:31.555255: step 16591, loss 0.37557, acc 0.921875\n",
      "2018-05-23T15:09:32.109326: step 16592, loss 0.138048, acc 0.96875\n",
      "2018-05-23T15:09:32.745968: step 16593, loss 0.0839659, acc 0.96875\n",
      "2018-05-23T15:09:33.306004: step 16594, loss 0.0725865, acc 0.953125\n",
      "2018-05-23T15:09:33.868243: step 16595, loss 0.074297, acc 0.953125\n",
      "2018-05-23T15:09:34.435487: step 16596, loss 0.0451111, acc 0.984375\n",
      "2018-05-23T15:09:34.866436: step 16597, loss 0.270723, acc 0.9375\n",
      "2018-05-23T15:09:35.328713: step 16598, loss 0.134278, acc 0.953125\n",
      "2018-05-23T15:09:35.737639: step 16599, loss 0.0855899, acc 0.96875\n",
      "2018-05-23T15:09:36.220354: step 16600, loss 0.094365, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:09:41.962531: step 16600, loss 1.73778, acc 0.717531\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16600\n",
      "\n",
      "2018-05-23T15:09:43.827803: step 16601, loss 0.0611125, acc 0.96875\n",
      "2018-05-23T15:09:44.253609: step 16602, loss 0.162467, acc 0.890625\n",
      "2018-05-23T15:09:44.676042: step 16603, loss 0.15401, acc 0.921875\n",
      "2018-05-23T15:09:45.072028: step 16604, loss 0.0959394, acc 0.953125\n",
      "2018-05-23T15:09:45.539806: step 16605, loss 0.0763483, acc 0.96875\n",
      "2018-05-23T15:09:45.967173: step 16606, loss 0.0472848, acc 0.984375\n",
      "2018-05-23T15:09:46.377250: step 16607, loss 0.128784, acc 0.953125\n",
      "2018-05-23T15:09:46.810622: step 16608, loss 0.344956, acc 0.9375\n",
      "2018-05-23T15:09:47.207042: step 16609, loss 0.0587771, acc 0.984375\n",
      "2018-05-23T15:09:47.638404: step 16610, loss 0.14891, acc 0.890625\n",
      "2018-05-23T15:09:48.039438: step 16611, loss 0.123701, acc 0.984375\n",
      "2018-05-23T15:09:48.437915: step 16612, loss 0.199093, acc 0.953125\n",
      "2018-05-23T15:09:48.846821: step 16613, loss 0.0873224, acc 0.953125\n",
      "2018-05-23T15:09:49.339501: step 16614, loss 0.0980595, acc 0.953125\n",
      "2018-05-23T15:09:49.746413: step 16615, loss 0.246453, acc 0.9375\n",
      "2018-05-23T15:09:50.139362: step 16616, loss 0.0561177, acc 0.984375\n",
      "2018-05-23T15:09:50.536302: step 16617, loss 0.105751, acc 0.9375\n",
      "2018-05-23T15:09:50.933239: step 16618, loss 0.0552009, acc 0.96875\n",
      "2018-05-23T15:09:51.380045: step 16619, loss 0.145729, acc 0.9375\n",
      "2018-05-23T15:09:51.801916: step 16620, loss 0.115539, acc 0.96875\n",
      "2018-05-23T15:09:52.284622: step 16621, loss 0.0630909, acc 0.96875\n",
      "2018-05-23T15:09:52.722451: step 16622, loss 0.0852292, acc 0.984375\n",
      "2018-05-23T15:09:53.168258: step 16623, loss 0.182723, acc 0.921875\n",
      "2018-05-23T15:09:53.582151: step 16624, loss 0.117336, acc 0.9375\n",
      "2018-05-23T15:09:53.986073: step 16625, loss 0.0981923, acc 0.96875\n",
      "2018-05-23T15:09:54.386003: step 16626, loss 0.0806759, acc 0.953125\n",
      "2018-05-23T15:09:54.879725: step 16627, loss 0.0899148, acc 0.96875\n",
      "2018-05-23T15:09:55.269682: step 16628, loss 0.0477809, acc 1\n",
      "2018-05-23T15:09:55.676595: step 16629, loss 0.0806138, acc 0.96875\n",
      "2018-05-23T15:09:56.072600: step 16630, loss 0.146262, acc 0.9375\n",
      "2018-05-23T15:09:56.456573: step 16631, loss 0.117314, acc 0.9375\n",
      "2018-05-23T15:09:56.870465: step 16632, loss 0.116434, acc 0.953125\n",
      "2018-05-23T15:09:57.361153: step 16633, loss 0.404994, acc 0.921875\n",
      "2018-05-23T15:09:57.756098: step 16634, loss 0.0825898, acc 0.96875\n",
      "2018-05-23T15:09:58.161012: step 16635, loss 0.169002, acc 0.890625\n",
      "2018-05-23T15:09:58.554958: step 16636, loss 0.062332, acc 0.953125\n",
      "2018-05-23T15:09:58.972376: step 16637, loss 0.151486, acc 0.9375\n",
      "2018-05-23T15:09:59.381282: step 16638, loss 0.0753732, acc 0.984375\n",
      "2018-05-23T15:09:59.899896: step 16639, loss 0.10344, acc 0.9375\n",
      "2018-05-23T15:10:00.318777: step 16640, loss 0.178219, acc 0.96875\n",
      "2018-05-23T15:10:00.727681: step 16641, loss 0.119716, acc 0.9375\n",
      "2018-05-23T15:10:01.127610: step 16642, loss 0.100606, acc 0.96875\n",
      "2018-05-23T15:10:01.686117: step 16643, loss 0.155285, acc 0.953125\n",
      "2018-05-23T15:10:02.159851: step 16644, loss 0.161795, acc 0.921875\n",
      "2018-05-23T15:10:02.611643: step 16645, loss 0.192586, acc 0.90625\n",
      "2018-05-23T15:10:03.021544: step 16646, loss 0.0380151, acc 0.984375\n",
      "2018-05-23T15:10:03.428456: step 16647, loss 0.0593244, acc 0.984375\n",
      "2018-05-23T15:10:03.829382: step 16648, loss 0.105199, acc 0.96875\n",
      "2018-05-23T15:10:04.328051: step 16649, loss 0.145056, acc 0.90625\n",
      "2018-05-23T15:10:04.771863: step 16650, loss 0.0955897, acc 0.96875\n",
      "2018-05-23T15:10:05.462016: step 16651, loss 0.0941472, acc 0.953125\n",
      "2018-05-23T15:10:06.199044: step 16652, loss 0.149934, acc 0.9375\n",
      "2018-05-23T15:10:06.770516: step 16653, loss 0.0657363, acc 1\n",
      "2018-05-23T15:10:07.358942: step 16654, loss 0.0759028, acc 0.984375\n",
      "2018-05-23T15:10:07.932406: step 16655, loss 0.115693, acc 0.9375\n",
      "2018-05-23T15:10:08.359966: step 16656, loss 0.0833255, acc 0.9375\n",
      "2018-05-23T15:10:08.804774: step 16657, loss 0.153957, acc 0.921875\n",
      "2018-05-23T15:10:09.219666: step 16658, loss 0.19746, acc 0.9375\n",
      "2018-05-23T15:10:09.612613: step 16659, loss 0.129972, acc 0.953125\n",
      "2018-05-23T15:10:10.017531: step 16660, loss 0.0649928, acc 0.984375\n",
      "2018-05-23T15:10:10.406490: step 16661, loss 0.146992, acc 0.9375\n",
      "2018-05-23T15:10:10.806419: step 16662, loss 0.232153, acc 0.921875\n",
      "2018-05-23T15:10:11.215360: step 16663, loss 0.0833527, acc 0.96875\n",
      "2018-05-23T15:10:11.601294: step 16664, loss 0.0439367, acc 0.984375\n",
      "2018-05-23T15:10:12.009202: step 16665, loss 0.0414556, acc 0.984375\n",
      "2018-05-23T15:10:12.402151: step 16666, loss 0.16658, acc 0.9375\n",
      "2018-05-23T15:10:12.810062: step 16667, loss 0.0669311, acc 0.96875\n",
      "2018-05-23T15:10:13.213980: step 16668, loss 0.1273, acc 0.90625\n",
      "2018-05-23T15:10:13.635850: step 16669, loss 0.207229, acc 0.9375\n",
      "2018-05-23T15:10:14.075673: step 16670, loss 0.17181, acc 0.9375\n",
      "2018-05-23T15:10:14.470619: step 16671, loss 0.245885, acc 0.921875\n",
      "2018-05-23T15:10:14.874537: step 16672, loss 0.0748893, acc 0.984375\n",
      "2018-05-23T15:10:15.295410: step 16673, loss 0.0595966, acc 0.984375\n",
      "2018-05-23T15:10:15.687056: step 16674, loss 0.162797, acc 0.9375\n",
      "2018-05-23T15:10:16.081999: step 16675, loss 0.163486, acc 0.9375\n",
      "2018-05-23T15:10:16.482927: step 16676, loss 0.0623308, acc 0.984375\n",
      "2018-05-23T15:10:16.899814: step 16677, loss 0.074598, acc 0.984375\n",
      "2018-05-23T15:10:17.299741: step 16678, loss 0.170813, acc 0.96875\n",
      "2018-05-23T15:10:17.693690: step 16679, loss 0.105319, acc 0.90625\n",
      "2018-05-23T15:10:18.104588: step 16680, loss 0.287832, acc 0.890625\n",
      "2018-05-23T15:10:18.502523: step 16681, loss 0.156112, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:10:18.917419: step 16682, loss 0.0587665, acc 0.96875\n",
      "2018-05-23T15:10:19.375189: step 16683, loss 0.10554, acc 0.96875\n",
      "2018-05-23T15:10:19.789084: step 16684, loss 0.133507, acc 0.9375\n",
      "2018-05-23T15:10:20.192006: step 16685, loss 0.131115, acc 0.96875\n",
      "2018-05-23T15:10:20.610883: step 16686, loss 0.12859, acc 0.9375\n",
      "2018-05-23T15:10:21.011814: step 16687, loss 0.0546607, acc 0.96875\n",
      "2018-05-23T15:10:21.446647: step 16688, loss 0.129414, acc 0.90625\n",
      "2018-05-23T15:10:21.840594: step 16689, loss 0.167543, acc 0.90625\n",
      "2018-05-23T15:10:22.242344: step 16690, loss 0.0948081, acc 0.9375\n",
      "2018-05-23T15:10:22.634297: step 16691, loss 0.136567, acc 0.9375\n",
      "2018-05-23T15:10:23.035223: step 16692, loss 0.0557814, acc 0.96875\n",
      "2018-05-23T15:10:23.442136: step 16693, loss 0.140481, acc 0.953125\n",
      "2018-05-23T15:10:23.835084: step 16694, loss 0.0639716, acc 0.984375\n",
      "2018-05-23T15:10:24.244986: step 16695, loss 0.119557, acc 0.96875\n",
      "2018-05-23T15:10:24.707750: step 16696, loss 0.142008, acc 0.953125\n",
      "2018-05-23T15:10:25.119647: step 16697, loss 0.0395235, acc 0.984375\n",
      "2018-05-23T15:10:25.556479: step 16698, loss 0.127799, acc 0.90625\n",
      "2018-05-23T15:10:25.952421: step 16699, loss 0.0444448, acc 1\n",
      "2018-05-23T15:10:26.362740: step 16700, loss 0.0701743, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:10:32.065485: step 16700, loss 1.72968, acc 0.712816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16700\n",
      "\n",
      "2018-05-23T15:10:33.473747: step 16701, loss 0.10887, acc 0.953125\n",
      "2018-05-23T15:10:34.022280: step 16702, loss 0.474748, acc 0.828125\n",
      "2018-05-23T15:10:34.433178: step 16703, loss 0.140511, acc 0.9375\n",
      "2018-05-23T15:10:34.860037: step 16704, loss 0.137619, acc 0.90625\n",
      "2018-05-23T15:10:35.261962: step 16705, loss 0.101876, acc 0.96875\n",
      "2018-05-23T15:10:35.660896: step 16706, loss 0.0815174, acc 0.984375\n",
      "2018-05-23T15:10:36.149959: step 16707, loss 0.0974264, acc 0.953125\n",
      "2018-05-23T15:10:36.581804: step 16708, loss 0.0961338, acc 0.9375\n",
      "2018-05-23T15:10:37.031602: step 16709, loss 0.103661, acc 0.9375\n",
      "2018-05-23T15:10:37.461453: step 16710, loss 0.0589975, acc 1\n",
      "2018-05-23T15:10:37.913243: step 16711, loss 0.046507, acc 0.984375\n",
      "2018-05-23T15:10:38.323146: step 16712, loss 0.159421, acc 0.9375\n",
      "2018-05-23T15:10:38.806853: step 16713, loss 0.206168, acc 0.921875\n",
      "2018-05-23T15:10:39.228726: step 16714, loss 0.0686987, acc 0.984375\n",
      "2018-05-23T15:10:39.626658: step 16715, loss 0.0712742, acc 0.96875\n",
      "2018-05-23T15:10:40.022639: step 16716, loss 0.155329, acc 0.9375\n",
      "2018-05-23T15:10:40.420576: step 16717, loss 0.133602, acc 0.953125\n",
      "2018-05-23T15:10:40.827485: step 16718, loss 0.15082, acc 0.9375\n",
      "2018-05-23T15:10:41.328145: step 16719, loss 0.0886028, acc 0.96875\n",
      "2018-05-23T15:10:41.737055: step 16720, loss 0.143424, acc 0.953125\n",
      "2018-05-23T15:10:42.149947: step 16721, loss 0.0805935, acc 0.96875\n",
      "2018-05-23T15:10:42.543893: step 16722, loss 0.126699, acc 0.953125\n",
      "2018-05-23T15:10:42.943823: step 16723, loss 0.13943, acc 0.953125\n",
      "2018-05-23T15:10:43.454457: step 16724, loss 0.148107, acc 0.9375\n",
      "2018-05-23T15:10:43.870346: step 16725, loss 0.125842, acc 0.9375\n",
      "2018-05-23T15:10:44.273267: step 16726, loss 0.115461, acc 0.921875\n",
      "2018-05-23T15:10:44.664220: step 16727, loss 0.0676769, acc 0.984375\n",
      "2018-05-23T15:10:45.062156: step 16728, loss 0.0789083, acc 0.96875\n",
      "2018-05-23T15:10:45.464115: step 16729, loss 0.253595, acc 0.890625\n",
      "2018-05-23T15:10:45.859028: step 16730, loss 0.0913147, acc 0.96875\n",
      "2018-05-23T15:10:46.253971: step 16731, loss 0.0983006, acc 0.953125\n",
      "2018-05-23T15:10:46.713739: step 16732, loss 0.0725654, acc 0.984375\n",
      "2018-05-23T15:10:47.139618: step 16733, loss 0.242671, acc 0.953125\n",
      "2018-05-23T15:10:47.542540: step 16734, loss 0.151017, acc 0.9375\n",
      "2018-05-23T15:10:47.970398: step 16735, loss 0.0663431, acc 1\n",
      "2018-05-23T15:10:48.369330: step 16736, loss 0.15051, acc 0.953125\n",
      "2018-05-23T15:10:48.855029: step 16737, loss 0.2293, acc 0.921875\n",
      "2018-05-23T15:10:49.266928: step 16738, loss 0.126522, acc 0.953125\n",
      "2018-05-23T15:10:49.674869: step 16739, loss 0.072395, acc 0.96875\n",
      "2018-05-23T15:10:50.066808: step 16740, loss 0.053966, acc 0.984375\n",
      "2018-05-23T15:10:50.534557: step 16741, loss 0.109756, acc 0.921875\n",
      "2018-05-23T15:10:51.185400: step 16742, loss 0.0882941, acc 0.984375\n",
      "2018-05-23T15:10:51.874515: step 16743, loss 0.116256, acc 0.9375\n",
      "2018-05-23T15:10:52.430029: step 16744, loss 0.0625899, acc 1\n",
      "2018-05-23T15:10:52.852906: step 16745, loss 0.161338, acc 1\n",
      "2018-05-23T15:10:53.483220: step 16746, loss 0.120565, acc 0.921875\n",
      "2018-05-23T15:10:54.121023: step 16747, loss 0.0478395, acc 0.984375\n",
      "2018-05-23T15:10:54.774323: step 16748, loss 0.0259229, acc 0.984375\n",
      "2018-05-23T15:10:55.326844: step 16749, loss 0.0473172, acc 0.96875\n",
      "2018-05-23T15:10:55.925245: step 16750, loss 0.0628163, acc 0.96875\n",
      "2018-05-23T15:10:56.545586: step 16751, loss 0.0966854, acc 0.96875\n",
      "2018-05-23T15:10:56.969442: step 16752, loss 0.0634043, acc 0.96875\n",
      "2018-05-23T15:10:57.362392: step 16753, loss 0.0434907, acc 0.984375\n",
      "2018-05-23T15:10:57.778278: step 16754, loss 0.0794178, acc 0.953125\n",
      "2018-05-23T15:10:58.189690: step 16755, loss 0.15572, acc 0.953125\n",
      "2018-05-23T15:10:58.585641: step 16756, loss 0.0330662, acc 1\n",
      "2018-05-23T15:10:59.045942: step 16757, loss 0.166165, acc 0.953125\n",
      "2018-05-23T15:10:59.625430: step 16758, loss 0.122524, acc 0.9375\n",
      "2018-05-23T15:11:00.192910: step 16759, loss 0.143532, acc 0.921875\n",
      "2018-05-23T15:11:00.844203: step 16760, loss 0.071271, acc 0.96875\n",
      "2018-05-23T15:11:01.426645: step 16761, loss 0.0717765, acc 1\n",
      "2018-05-23T15:11:02.024605: step 16762, loss 0.0713933, acc 0.96875\n",
      "2018-05-23T15:11:02.513823: step 16763, loss 0.152681, acc 0.9375\n",
      "2018-05-23T15:11:03.347336: step 16764, loss 0.0821871, acc 0.96875\n",
      "2018-05-23T15:11:03.982635: step 16765, loss 0.0701144, acc 0.96875\n",
      "2018-05-23T15:11:04.596993: step 16766, loss 0.10676, acc 0.953125\n",
      "2018-05-23T15:11:05.213363: step 16767, loss 0.0662248, acc 0.96875\n",
      "2018-05-23T15:11:05.886082: step 16768, loss 0.072405, acc 0.96875\n",
      "2018-05-23T15:11:06.503009: step 16769, loss 0.0496717, acc 0.984375\n",
      "2018-05-23T15:11:06.927858: step 16770, loss 0.0816642, acc 0.96875\n",
      "2018-05-23T15:11:07.337764: step 16771, loss 0.128979, acc 0.9375\n",
      "2018-05-23T15:11:07.743737: step 16772, loss 0.0409801, acc 0.984375\n",
      "2018-05-23T15:11:08.133692: step 16773, loss 0.0404311, acc 0.984375\n",
      "2018-05-23T15:11:08.534669: step 16774, loss 0.0650718, acc 0.96875\n",
      "2018-05-23T15:11:08.938539: step 16775, loss 0.102425, acc 0.96875\n",
      "2018-05-23T15:11:09.335482: step 16776, loss 0.14291, acc 0.96875\n",
      "2018-05-23T15:11:09.733639: step 16777, loss 0.0409253, acc 0.96875\n",
      "2018-05-23T15:11:10.127957: step 16778, loss 0.0707324, acc 0.984375\n",
      "2018-05-23T15:11:10.557825: step 16779, loss 0.124984, acc 0.921875\n",
      "2018-05-23T15:11:11.036058: step 16780, loss 0.0462394, acc 0.96875\n",
      "2018-05-23T15:11:11.442995: step 16781, loss 0.0827151, acc 0.96875\n",
      "2018-05-23T15:11:11.867370: step 16782, loss 0.125456, acc 0.90625\n",
      "2018-05-23T15:11:12.272294: step 16783, loss 0.102456, acc 0.9375\n",
      "2018-05-23T15:11:12.692680: step 16784, loss 0.0689202, acc 0.96875\n",
      "2018-05-23T15:11:13.181374: step 16785, loss 0.154044, acc 0.953125\n",
      "2018-05-23T15:11:13.863556: step 16786, loss 0.101342, acc 0.96875\n",
      "2018-05-23T15:11:14.400715: step 16787, loss 0.0715338, acc 0.96875\n",
      "2018-05-23T15:11:14.948802: step 16788, loss 0.10764, acc 0.953125\n",
      "2018-05-23T15:11:15.591749: step 16789, loss 0.107238, acc 0.953125\n",
      "2018-05-23T15:11:16.172087: step 16790, loss 0.053445, acc 0.96875\n",
      "2018-05-23T15:11:16.649896: step 16791, loss 0.0754772, acc 0.953125\n",
      "2018-05-23T15:11:17.244188: step 16792, loss 0.0531227, acc 0.984375\n",
      "2018-05-23T15:11:18.027840: step 16793, loss 0.151435, acc 0.90625\n",
      "2018-05-23T15:11:18.551438: step 16794, loss 0.0839695, acc 0.984375\n",
      "2018-05-23T15:11:19.180755: step 16795, loss 0.033678, acc 0.984375\n",
      "2018-05-23T15:11:20.059406: step 16796, loss 0.120857, acc 0.9375\n",
      "2018-05-23T15:11:20.741580: step 16797, loss 0.107525, acc 0.9375\n",
      "2018-05-23T15:11:21.439713: step 16798, loss 0.104556, acc 0.953125\n",
      "2018-05-23T15:11:22.048085: step 16799, loss 0.0917758, acc 0.953125\n",
      "2018-05-23T15:11:22.783118: step 16800, loss 0.032177, acc 0.984375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:11:28.637513: step 16800, loss 1.72505, acc 0.712816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16800\n",
      "\n",
      "2018-05-23T15:11:30.409773: step 16801, loss 0.146324, acc 0.953125\n",
      "2018-05-23T15:11:30.838626: step 16802, loss 0.0533345, acc 0.96875\n",
      "2018-05-23T15:11:31.247752: step 16803, loss 0.0656631, acc 0.984375\n",
      "2018-05-23T15:11:31.655659: step 16804, loss 0.0748896, acc 0.96875\n",
      "2018-05-23T15:11:32.201200: step 16805, loss 0.112023, acc 0.96875\n",
      "2018-05-23T15:11:32.622074: step 16806, loss 0.0586215, acc 1\n",
      "2018-05-23T15:11:33.049929: step 16807, loss 0.0521048, acc 1\n",
      "2018-05-23T15:11:33.453848: step 16808, loss 0.0623792, acc 1\n",
      "2018-05-23T15:11:33.860793: step 16809, loss 0.158115, acc 0.9375\n",
      "2018-05-23T15:11:34.349453: step 16810, loss 0.0596725, acc 0.96875\n",
      "2018-05-23T15:11:34.759356: step 16811, loss 0.0759776, acc 0.953125\n",
      "2018-05-23T15:11:35.171254: step 16812, loss 0.117236, acc 0.96875\n",
      "2018-05-23T15:11:35.569189: step 16813, loss 0.0383291, acc 0.984375\n",
      "2018-05-23T15:11:35.983084: step 16814, loss 0.182197, acc 0.96875\n",
      "2018-05-23T15:11:36.492720: step 16815, loss 0.198717, acc 0.9375\n",
      "2018-05-23T15:11:36.897681: step 16816, loss 0.085121, acc 0.953125\n",
      "2018-05-23T15:11:37.319554: step 16817, loss 0.0923115, acc 0.96875\n",
      "2018-05-23T15:11:37.712501: step 16818, loss 0.0592276, acc 0.984375\n",
      "2018-05-23T15:11:38.105452: step 16819, loss 0.11444, acc 0.921875\n",
      "2018-05-23T15:11:38.524333: step 16820, loss 0.104655, acc 0.953125\n",
      "2018-05-23T15:11:39.013024: step 16821, loss 0.0853711, acc 0.96875\n",
      "2018-05-23T15:11:39.423925: step 16822, loss 0.0516177, acc 1\n",
      "2018-05-23T15:11:39.823852: step 16823, loss 0.112316, acc 0.953125\n",
      "2018-05-23T15:11:40.216804: step 16824, loss 0.0737189, acc 0.984375\n",
      "2018-05-23T15:11:40.612139: step 16825, loss 0.0768439, acc 0.96875\n",
      "2018-05-23T15:11:41.084874: step 16826, loss 0.0759475, acc 0.96875\n",
      "2018-05-23T15:11:41.515722: step 16827, loss 0.117841, acc 0.9375\n",
      "2018-05-23T15:11:41.919641: step 16828, loss 0.0625872, acc 0.984375\n",
      "2018-05-23T15:11:42.375421: step 16829, loss 0.0637173, acc 0.96875\n",
      "2018-05-23T15:11:42.772359: step 16830, loss 0.0682865, acc 0.96875\n",
      "2018-05-23T15:11:43.165309: step 16831, loss 0.0598701, acc 0.984375\n",
      "2018-05-23T15:11:43.674752: step 16832, loss 0.0520072, acc 0.96875\n",
      "2018-05-23T15:11:44.077675: step 16833, loss 0.0976098, acc 0.96875\n",
      "2018-05-23T15:11:44.484586: step 16834, loss 0.0808728, acc 0.953125\n",
      "2018-05-23T15:11:44.903466: step 16835, loss 0.0553747, acc 0.984375\n",
      "2018-05-23T15:11:45.302399: step 16836, loss 0.0772178, acc 0.984375\n",
      "2018-05-23T15:11:45.697344: step 16837, loss 0.0682267, acc 0.96875\n",
      "2018-05-23T15:11:46.093282: step 16838, loss 0.0875869, acc 0.96875\n",
      "2018-05-23T15:11:46.549103: step 16839, loss 0.0741367, acc 0.96875\n",
      "2018-05-23T15:11:47.024833: step 16840, loss 0.0994256, acc 0.953125\n",
      "2018-05-23T15:11:47.431743: step 16841, loss 0.0553355, acc 0.96875\n",
      "2018-05-23T15:11:47.847630: step 16842, loss 0.106434, acc 0.9375\n",
      "2018-05-23T15:11:48.235590: step 16843, loss 0.117181, acc 0.96875\n",
      "2018-05-23T15:11:48.632530: step 16844, loss 0.0820486, acc 0.96875\n",
      "2018-05-23T15:11:49.115237: step 16845, loss 0.0489091, acc 0.984375\n",
      "2018-05-23T15:11:49.548081: step 16846, loss 0.127764, acc 0.96875\n",
      "2018-05-23T15:11:49.953968: step 16847, loss 0.0837604, acc 0.96875\n",
      "2018-05-23T15:11:50.401770: step 16848, loss 0.121621, acc 0.9375\n",
      "2018-05-23T15:11:50.809681: step 16849, loss 0.103118, acc 0.953125\n",
      "2018-05-23T15:11:51.578707: step 16850, loss 0.123865, acc 0.953125\n",
      "2018-05-23T15:11:52.157161: step 16851, loss 0.11251, acc 0.953125\n",
      "2018-05-23T15:11:52.739118: step 16852, loss 0.117981, acc 0.9375\n",
      "2018-05-23T15:11:53.322937: step 16853, loss 0.0668499, acc 1\n",
      "2018-05-23T15:11:53.960418: step 16854, loss 0.160116, acc 0.9375\n",
      "2018-05-23T15:11:54.532622: step 16855, loss 0.248814, acc 0.9375\n",
      "2018-05-23T15:11:55.136058: step 16856, loss 0.0564196, acc 0.984375\n",
      "2018-05-23T15:11:55.769399: step 16857, loss 0.0898324, acc 0.96875\n",
      "2018-05-23T15:11:56.369980: step 16858, loss 0.0670054, acc 0.96875\n",
      "2018-05-23T15:11:56.964671: step 16859, loss 0.129975, acc 0.9375\n",
      "2018-05-23T15:11:57.382772: step 16860, loss 0.0977854, acc 0.9375\n",
      "2018-05-23T15:11:57.791981: step 16861, loss 0.0879621, acc 0.96875\n",
      "2018-05-23T15:11:58.263343: step 16862, loss 0.101864, acc 0.9375\n",
      "2018-05-23T15:11:58.676478: step 16863, loss 0.0505617, acc 0.984375\n",
      "2018-05-23T15:11:59.078962: step 16864, loss 0.056085, acc 0.984375\n",
      "2018-05-23T15:11:59.469998: step 16865, loss 0.126598, acc 0.9375\n",
      "2018-05-23T15:11:59.883761: step 16866, loss 0.069357, acc 0.953125\n",
      "2018-05-23T15:12:00.312734: step 16867, loss 0.120687, acc 0.921875\n",
      "2018-05-23T15:12:00.714664: step 16868, loss 0.0546783, acc 0.984375\n",
      "2018-05-23T15:12:01.107631: step 16869, loss 0.138402, acc 0.9375\n",
      "2018-05-23T15:12:01.555434: step 16870, loss 0.0725079, acc 0.984375\n",
      "2018-05-23T15:12:02.004232: step 16871, loss 0.0741602, acc 0.984375\n",
      "2018-05-23T15:12:02.403711: step 16872, loss 0.0535774, acc 0.984375\n",
      "2018-05-23T15:12:03.145648: step 16873, loss 0.08045, acc 0.96875\n",
      "2018-05-23T15:12:03.607459: step 16874, loss 0.139883, acc 0.96875\n",
      "2018-05-23T15:12:04.239924: step 16875, loss 0.126912, acc 0.921875\n",
      "2018-05-23T15:12:04.737592: step 16876, loss 0.0605315, acc 0.953125\n",
      "2018-05-23T15:12:05.120567: step 16877, loss 0.151724, acc 0.9375\n",
      "2018-05-23T15:12:05.502545: step 16878, loss 0.180089, acc 0.921875\n",
      "2018-05-23T15:12:05.904357: step 16879, loss 0.0651437, acc 0.96875\n",
      "2018-05-23T15:12:06.571572: step 16880, loss 0.0368027, acc 0.984375\n",
      "2018-05-23T15:12:07.404343: step 16881, loss 0.0903235, acc 0.953125\n",
      "2018-05-23T15:12:08.010722: step 16882, loss 0.20377, acc 0.953125\n",
      "2018-05-23T15:12:08.672950: step 16883, loss 0.0425157, acc 0.984375\n",
      "2018-05-23T15:12:09.351137: step 16884, loss 0.221506, acc 0.9375\n",
      "2018-05-23T15:12:10.071210: step 16885, loss 0.12511, acc 0.9375\n",
      "2018-05-23T15:12:10.918986: step 16886, loss 0.0854949, acc 0.96875\n",
      "2018-05-23T15:12:11.516393: step 16887, loss 0.0838063, acc 0.953125\n",
      "2018-05-23T15:12:12.186594: step 16888, loss 0.136424, acc 0.9375\n",
      "2018-05-23T15:12:12.811921: step 16889, loss 0.0546497, acc 0.984375\n",
      "2018-05-23T15:12:13.433962: step 16890, loss 0.0688269, acc 0.953125\n",
      "2018-05-23T15:12:13.989103: step 16891, loss 0.0611416, acc 0.96875\n",
      "2018-05-23T15:12:14.513808: step 16892, loss 0.0865393, acc 0.953125\n",
      "2018-05-23T15:12:14.941664: step 16893, loss 0.0983424, acc 0.921875\n",
      "2018-05-23T15:12:15.344113: step 16894, loss 0.0813671, acc 0.953125\n",
      "2018-05-23T15:12:15.842962: step 16895, loss 0.0622461, acc 0.96875\n",
      "2018-05-23T15:12:16.249408: step 16896, loss 0.0737334, acc 0.953125\n",
      "2018-05-23T15:12:16.675343: step 16897, loss 0.0753982, acc 0.96875\n",
      "2018-05-23T15:12:17.068346: step 16898, loss 0.0905476, acc 0.96875\n",
      "2018-05-23T15:12:17.474272: step 16899, loss 0.0646076, acc 0.96875\n",
      "2018-05-23T15:12:17.937756: step 16900, loss 0.0784235, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:12:24.533065: step 16900, loss 1.761, acc 0.713245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-16900\n",
      "\n",
      "2018-05-23T15:12:26.194857: step 16901, loss 0.0692835, acc 0.96875\n",
      "2018-05-23T15:12:26.778293: step 16902, loss 0.0481605, acc 0.96875\n",
      "2018-05-23T15:12:27.214128: step 16903, loss 0.0625662, acc 0.984375\n",
      "2018-05-23T15:12:27.931209: step 16904, loss 0.119205, acc 0.984375\n",
      "2018-05-23T15:12:28.660259: step 16905, loss 0.114046, acc 0.9375\n",
      "2018-05-23T15:12:29.332462: step 16906, loss 0.0763245, acc 0.984375\n",
      "2018-05-23T15:12:29.938840: step 16907, loss 0.0705288, acc 0.96875\n",
      "2018-05-23T15:12:30.690827: step 16908, loss 0.174533, acc 0.96875\n",
      "2018-05-23T15:12:31.275267: step 16909, loss 0.0880318, acc 0.953125\n",
      "2018-05-23T15:12:31.841749: step 16910, loss 0.0416426, acc 0.984375\n",
      "2018-05-23T15:12:32.308499: step 16911, loss 0.038225, acc 1\n",
      "2018-05-23T15:12:32.824121: step 16912, loss 0.0608427, acc 0.96875\n",
      "2018-05-23T15:12:33.234024: step 16913, loss 0.111128, acc 0.953125\n",
      "2018-05-23T15:12:33.632955: step 16914, loss 0.0907919, acc 0.96875\n",
      "2018-05-23T15:12:34.037875: step 16915, loss 0.0896808, acc 0.9375\n",
      "2018-05-23T15:12:34.460741: step 16916, loss 0.0508154, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:12:34.858712: step 16917, loss 0.0823106, acc 0.953125\n",
      "2018-05-23T15:12:35.384864: step 16918, loss 0.11437, acc 0.9375\n",
      "2018-05-23T15:12:35.976006: step 16919, loss 0.0325561, acc 1\n",
      "2018-05-23T15:12:36.613898: step 16920, loss 0.0926254, acc 0.984375\n",
      "2018-05-23T15:12:37.193541: step 16921, loss 0.101237, acc 0.96875\n",
      "2018-05-23T15:12:37.921175: step 16922, loss 0.0362215, acc 1\n",
      "2018-05-23T15:12:38.426752: step 16923, loss 0.0425284, acc 1\n",
      "2018-05-23T15:12:38.852612: step 16924, loss 0.159314, acc 0.9375\n",
      "2018-05-23T15:12:39.260549: step 16925, loss 0.0499968, acc 1\n",
      "2018-05-23T15:12:39.675441: step 16926, loss 0.0467993, acc 0.984375\n",
      "2018-05-23T15:12:40.071404: step 16927, loss 0.116331, acc 0.9375\n",
      "2018-05-23T15:12:40.479319: step 16928, loss 0.0578972, acc 0.984375\n",
      "2018-05-23T15:12:40.892222: step 16929, loss 0.0569045, acc 0.96875\n",
      "2018-05-23T15:12:41.299166: step 16930, loss 0.197433, acc 0.9375\n",
      "2018-05-23T15:12:41.711029: step 16931, loss 0.0340149, acc 1\n",
      "2018-05-23T15:12:42.172802: step 16932, loss 0.0730963, acc 0.96875\n",
      "2018-05-23T15:12:42.738287: step 16933, loss 0.0421746, acc 1\n",
      "2018-05-23T15:12:43.175118: step 16934, loss 0.0726561, acc 0.984375\n",
      "2018-05-23T15:12:43.589522: step 16935, loss 0.0508389, acc 0.984375\n",
      "2018-05-23T15:12:44.228367: step 16936, loss 0.0788254, acc 0.96875\n",
      "2018-05-23T15:12:44.643796: step 16937, loss 0.0358325, acc 0.96875\n",
      "2018-05-23T15:12:45.097142: step 16938, loss 0.0931221, acc 0.96875\n",
      "2018-05-23T15:12:45.513042: step 16939, loss 0.142417, acc 0.9375\n",
      "2018-05-23T15:12:45.934205: step 16940, loss 0.189519, acc 0.953125\n",
      "2018-05-23T15:12:46.334367: step 16941, loss 0.0625808, acc 0.96875\n",
      "2018-05-23T15:12:46.738820: step 16942, loss 0.0580479, acc 0.984375\n",
      "2018-05-23T15:12:47.161236: step 16943, loss 0.0473467, acc 1\n",
      "2018-05-23T15:12:47.555201: step 16944, loss 0.130256, acc 0.921875\n",
      "2018-05-23T15:12:47.975619: step 16945, loss 0.050164, acc 1\n",
      "2018-05-23T15:12:48.379536: step 16946, loss 0.0671268, acc 0.984375\n",
      "2018-05-23T15:12:48.785452: step 16947, loss 0.162127, acc 0.90625\n",
      "2018-05-23T15:12:49.208579: step 16948, loss 0.0234692, acc 1\n",
      "2018-05-23T15:12:49.726211: step 16949, loss 0.0506116, acc 0.984375\n",
      "2018-05-23T15:12:50.128645: step 16950, loss 0.0443057, acc 0.984375\n",
      "2018-05-23T15:12:50.532564: step 16951, loss 0.07259, acc 0.96875\n",
      "2018-05-23T15:12:50.936580: step 16952, loss 0.134071, acc 0.9375\n",
      "2018-05-23T15:12:51.357453: step 16953, loss 0.0412967, acc 0.984375\n",
      "2018-05-23T15:12:51.782316: step 16954, loss 0.090602, acc 0.96875\n",
      "2018-05-23T15:12:52.427591: step 16955, loss 0.0684946, acc 0.984375\n",
      "2018-05-23T15:12:52.955182: step 16956, loss 0.0873698, acc 0.96875\n",
      "2018-05-23T15:12:53.448858: step 16957, loss 0.0706936, acc 0.96875\n",
      "2018-05-23T15:12:54.114080: step 16958, loss 0.0877551, acc 0.96875\n",
      "2018-05-23T15:12:54.604769: step 16959, loss 0.0705336, acc 0.984375\n",
      "2018-05-23T15:12:55.197182: step 16960, loss 0.0475109, acc 0.984375\n",
      "2018-05-23T15:12:55.770648: step 16961, loss 0.164061, acc 0.9375\n",
      "2018-05-23T15:12:56.231930: step 16962, loss 0.14388, acc 0.921875\n",
      "2018-05-23T15:12:56.932057: step 16963, loss 0.0369904, acc 1\n",
      "2018-05-23T15:12:57.700999: step 16964, loss 0.0874367, acc 0.96875\n",
      "2018-05-23T15:12:58.363228: step 16965, loss 0.126555, acc 0.953125\n",
      "2018-05-23T15:12:59.081308: step 16966, loss 0.135083, acc 0.921875\n",
      "2018-05-23T15:12:59.932598: step 16967, loss 0.128979, acc 0.9375\n",
      "2018-05-23T15:13:00.910109: step 16968, loss 0.0485229, acc 0.984375\n",
      "2018-05-23T15:13:01.968822: step 16969, loss 0.120657, acc 0.953125\n",
      "2018-05-23T15:13:02.654545: step 16970, loss 0.124823, acc 0.984375\n",
      "2018-05-23T15:13:03.686524: step 16971, loss 0.128201, acc 0.9375\n",
      "2018-05-23T15:13:04.324390: step 16972, loss 0.163114, acc 0.9375\n",
      "2018-05-23T15:13:04.956208: step 16973, loss 0.161084, acc 0.953125\n",
      "2018-05-23T15:13:05.583367: step 16974, loss 0.0741964, acc 0.984375\n",
      "2018-05-23T15:13:06.201226: step 16975, loss 0.0516306, acc 0.96875\n",
      "2018-05-23T15:13:06.791830: step 16976, loss 0.0291111, acc 0.96875\n",
      "2018-05-23T15:13:07.212975: step 16977, loss 0.23059, acc 0.890625\n",
      "2018-05-23T15:13:07.626384: step 16978, loss 0.065199, acc 0.96875\n",
      "2018-05-23T15:13:08.024381: step 16979, loss 0.0491269, acc 1\n",
      "2018-05-23T15:13:08.524216: step 16980, loss 0.224722, acc 0.9375\n",
      "2018-05-23T15:13:08.940112: step 16981, loss 0.0753529, acc 0.953125\n",
      "2018-05-23T15:13:09.502316: step 16982, loss 0.179322, acc 0.90625\n",
      "2018-05-23T15:13:10.146617: step 16983, loss 0.123749, acc 0.9375\n",
      "2018-05-23T15:13:10.711697: step 16984, loss 0.114047, acc 0.953125\n",
      "2018-05-23T15:13:11.251255: step 16985, loss 0.0413449, acc 0.96875\n",
      "2018-05-23T15:13:11.847658: step 16986, loss 0.0645665, acc 0.953125\n",
      "2018-05-23T15:13:12.411151: step 16987, loss 0.146335, acc 0.921875\n",
      "2018-05-23T15:13:12.844990: step 16988, loss 0.132997, acc 0.9375\n",
      "2018-05-23T15:13:13.271861: step 16989, loss 0.0911905, acc 0.96875\n",
      "2018-05-23T15:13:13.723662: step 16990, loss 0.0915188, acc 0.953125\n",
      "2018-05-23T15:13:14.126094: step 16991, loss 0.122236, acc 0.953125\n",
      "2018-05-23T15:13:14.532790: step 16992, loss 0.0398848, acc 0.984375\n",
      "2018-05-23T15:13:14.939666: step 16993, loss 0.100748, acc 0.9375\n",
      "2018-05-23T15:13:15.353559: step 16994, loss 0.151438, acc 0.96875\n",
      "2018-05-23T15:13:15.845754: step 16995, loss 0.0858524, acc 0.96875\n",
      "2018-05-23T15:13:16.275604: step 16996, loss 0.0582397, acc 0.96875\n",
      "2018-05-23T15:13:16.699471: step 16997, loss 0.159177, acc 0.953125\n",
      "2018-05-23T15:13:17.103390: step 16998, loss 0.21035, acc 0.953125\n",
      "2018-05-23T15:13:17.517284: step 16999, loss 0.0810148, acc 0.9375\n",
      "2018-05-23T15:13:18.007482: step 17000, loss 0.099589, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:13:24.400922: step 17000, loss 1.78123, acc 0.711673\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17000\n",
      "\n",
      "2018-05-23T15:13:26.119327: step 17001, loss 0.0915512, acc 0.953125\n",
      "2018-05-23T15:13:26.554161: step 17002, loss 0.0854073, acc 0.96875\n",
      "2018-05-23T15:13:27.045847: step 17003, loss 0.115093, acc 0.953125\n",
      "2018-05-23T15:13:27.559474: step 17004, loss 0.054454, acc 0.984375\n",
      "2018-05-23T15:13:28.227196: step 17005, loss 0.069671, acc 0.984375\n",
      "2018-05-23T15:13:28.936300: step 17006, loss 0.139608, acc 0.953125\n",
      "2018-05-23T15:13:29.594538: step 17007, loss 0.168956, acc 0.9375\n",
      "2018-05-23T15:13:30.202911: step 17008, loss 0.0850846, acc 0.96875\n",
      "2018-05-23T15:13:30.973363: step 17009, loss 0.173631, acc 0.9375\n",
      "2018-05-23T15:13:31.640577: step 17010, loss 0.120784, acc 0.984375\n",
      "2018-05-23T15:13:32.166172: step 17011, loss 0.042692, acc 1\n",
      "2018-05-23T15:13:32.666833: step 17012, loss 0.0836145, acc 0.96875\n",
      "2018-05-23T15:13:33.169487: step 17013, loss 0.0905165, acc 0.953125\n",
      "2018-05-23T15:13:33.588367: step 17014, loss 0.152638, acc 0.96875\n",
      "2018-05-23T15:13:34.027195: step 17015, loss 0.057548, acc 0.984375\n",
      "2018-05-23T15:13:34.437095: step 17016, loss 0.0870613, acc 0.96875\n",
      "2018-05-23T15:13:34.851986: step 17017, loss 0.12392, acc 0.953125\n",
      "2018-05-23T15:13:35.251915: step 17018, loss 0.119087, acc 0.96875\n",
      "2018-05-23T15:13:35.663816: step 17019, loss 0.0325589, acc 1\n",
      "2018-05-23T15:13:36.057760: step 17020, loss 0.104008, acc 0.96875\n",
      "2018-05-23T15:13:36.582358: step 17021, loss 0.108509, acc 0.96875\n",
      "2018-05-23T15:13:37.011209: step 17022, loss 0.0598215, acc 0.984375\n",
      "2018-05-23T15:13:37.420119: step 17023, loss 0.0888105, acc 0.953125\n",
      "2018-05-23T15:13:37.838995: step 17024, loss 0.0660117, acc 0.984375\n",
      "2018-05-23T15:13:38.239924: step 17025, loss 0.118087, acc 0.9375\n",
      "2018-05-23T15:13:38.632872: step 17026, loss 0.165665, acc 0.9375\n",
      "2018-05-23T15:13:39.030828: step 17027, loss 0.162426, acc 0.921875\n",
      "2018-05-23T15:13:39.453697: step 17028, loss 0.0669151, acc 0.984375\n",
      "2018-05-23T15:13:39.962336: step 17029, loss 0.103347, acc 0.921875\n",
      "2018-05-23T15:13:40.358276: step 17030, loss 0.0735228, acc 0.953125\n",
      "2018-05-23T15:13:40.782141: step 17031, loss 0.0867469, acc 0.953125\n",
      "2018-05-23T15:13:41.185063: step 17032, loss 0.0410033, acc 1\n",
      "2018-05-23T15:13:41.619900: step 17033, loss 0.0696424, acc 0.96875\n",
      "2018-05-23T15:13:42.124571: step 17034, loss 0.0939131, acc 0.96875\n",
      "2018-05-23T15:13:42.649213: step 17035, loss 0.0198128, acc 1\n",
      "2018-05-23T15:13:43.284605: step 17036, loss 0.178822, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:13:43.856076: step 17037, loss 0.0876975, acc 0.96875\n",
      "2018-05-23T15:13:44.399134: step 17038, loss 0.224583, acc 0.953125\n",
      "2018-05-23T15:13:44.991549: step 17039, loss 0.257595, acc 0.921875\n",
      "2018-05-23T15:13:45.542251: step 17040, loss 0.109621, acc 0.953125\n",
      "2018-05-23T15:13:45.972615: step 17041, loss 0.0770792, acc 0.96875\n",
      "2018-05-23T15:13:46.439368: step 17042, loss 0.0707531, acc 0.984375\n",
      "2018-05-23T15:13:46.856509: step 17043, loss 0.0594972, acc 0.984375\n",
      "2018-05-23T15:13:47.266446: step 17044, loss 0.0590985, acc 0.984375\n",
      "2018-05-23T15:13:47.762089: step 17045, loss 0.0738572, acc 0.953125\n",
      "2018-05-23T15:13:48.225360: step 17046, loss 0.0960471, acc 0.96875\n",
      "2018-05-23T15:13:48.682141: step 17047, loss 0.0540367, acc 0.984375\n",
      "2018-05-23T15:13:49.295498: step 17048, loss 0.122984, acc 0.9375\n",
      "2018-05-23T15:13:49.732330: step 17049, loss 0.0565861, acc 1\n",
      "2018-05-23T15:13:50.155232: step 17050, loss 0.0743412, acc 0.953125\n",
      "2018-05-23T15:13:50.552139: step 17051, loss 0.143238, acc 0.9375\n",
      "2018-05-23T15:13:50.993955: step 17052, loss 0.084756, acc 0.953125\n",
      "2018-05-23T15:13:51.453725: step 17053, loss 0.156982, acc 0.953125\n",
      "2018-05-23T15:13:51.850665: step 17054, loss 0.0676027, acc 0.984375\n",
      "2018-05-23T15:13:52.389031: step 17055, loss 0.204621, acc 0.84375\n",
      "2018-05-23T15:13:52.944545: step 17056, loss 0.055822, acc 0.984375\n",
      "2018-05-23T15:13:53.426255: step 17057, loss 0.0591943, acc 0.984375\n",
      "2018-05-23T15:13:53.991742: step 17058, loss 0.0911248, acc 0.96875\n",
      "2018-05-23T15:13:54.460489: step 17059, loss 0.0476082, acc 0.984375\n",
      "2018-05-23T15:13:54.862416: step 17060, loss 0.147161, acc 0.9375\n",
      "2018-05-23T15:13:55.294261: step 17061, loss 0.100851, acc 0.9375\n",
      "2018-05-23T15:13:55.718124: step 17062, loss 0.0524906, acc 0.953125\n",
      "2018-05-23T15:13:56.138000: step 17063, loss 0.0788921, acc 0.96875\n",
      "2018-05-23T15:13:56.581814: step 17064, loss 0.0774429, acc 0.984375\n",
      "2018-05-23T15:13:57.014658: step 17065, loss 0.0603937, acc 1\n",
      "2018-05-23T15:13:57.621539: step 17066, loss 0.0642523, acc 0.96875\n",
      "2018-05-23T15:13:58.311695: step 17067, loss 0.0547997, acc 0.96875\n",
      "2018-05-23T15:13:58.925053: step 17068, loss 0.0731444, acc 0.984375\n",
      "2018-05-23T15:13:59.549382: step 17069, loss 0.111952, acc 0.953125\n",
      "2018-05-23T15:14:00.188673: step 17070, loss 0.125997, acc 0.984375\n",
      "2018-05-23T15:14:00.827973: step 17071, loss 0.11044, acc 0.953125\n",
      "2018-05-23T15:14:01.423013: step 17072, loss 0.101684, acc 0.984375\n",
      "2018-05-23T15:14:02.016138: step 17073, loss 0.0485626, acc 0.984375\n",
      "2018-05-23T15:14:02.466486: step 17074, loss 0.0792856, acc 0.96875\n",
      "2018-05-23T15:14:02.893863: step 17075, loss 0.0970856, acc 0.953125\n",
      "2018-05-23T15:14:03.364376: step 17076, loss 0.0707213, acc 0.96875\n",
      "2018-05-23T15:14:03.822785: step 17077, loss 0.0725968, acc 0.953125\n",
      "2018-05-23T15:14:04.261451: step 17078, loss 0.0183659, acc 1\n",
      "2018-05-23T15:14:04.753264: step 17079, loss 0.140325, acc 0.9375\n",
      "2018-05-23T15:14:05.359310: step 17080, loss 0.0892954, acc 0.96875\n",
      "2018-05-23T15:14:05.922242: step 17081, loss 0.0825555, acc 0.984375\n",
      "2018-05-23T15:14:06.513181: step 17082, loss 0.0871552, acc 0.96875\n",
      "2018-05-23T15:14:07.073715: step 17083, loss 0.059582, acc 0.96875\n",
      "2018-05-23T15:14:07.672116: step 17084, loss 0.156382, acc 0.921875\n",
      "2018-05-23T15:14:08.173772: step 17085, loss 0.0620646, acc 0.96875\n",
      "2018-05-23T15:14:08.587671: step 17086, loss 0.0688867, acc 0.984375\n",
      "2018-05-23T15:14:08.986603: step 17087, loss 0.0596449, acc 0.96875\n",
      "2018-05-23T15:14:09.393516: step 17088, loss 0.0792677, acc 0.953125\n",
      "2018-05-23T15:14:09.799430: step 17089, loss 0.0969472, acc 0.9375\n",
      "2018-05-23T15:14:10.191449: step 17090, loss 0.0765925, acc 0.953125\n",
      "2018-05-23T15:14:10.591645: step 17091, loss 0.170989, acc 0.9375\n",
      "2018-05-23T15:14:11.010967: step 17092, loss 0.0615209, acc 0.984375\n",
      "2018-05-23T15:14:11.413384: step 17093, loss 0.0583202, acc 0.984375\n",
      "2018-05-23T15:14:11.807332: step 17094, loss 0.0639272, acc 0.953125\n",
      "2018-05-23T15:14:12.260119: step 17095, loss 0.112289, acc 0.9375\n",
      "2018-05-23T15:14:12.936544: step 17096, loss 0.147314, acc 0.890625\n",
      "2018-05-23T15:14:13.913443: step 17097, loss 0.058143, acc 0.953125\n",
      "2018-05-23T15:14:14.550737: step 17098, loss 0.172783, acc 0.921875\n",
      "2018-05-23T15:14:15.120213: step 17099, loss 0.0548303, acc 0.984375\n",
      "2018-05-23T15:14:15.743546: step 17100, loss 0.187465, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:14:21.769938: step 17100, loss 1.80221, acc 0.712102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17100\n",
      "\n",
      "2018-05-23T15:14:23.799490: step 17101, loss 0.0940853, acc 0.953125\n",
      "2018-05-23T15:14:24.245300: step 17102, loss 0.127726, acc 0.96875\n",
      "2018-05-23T15:14:24.659213: step 17103, loss 0.179884, acc 0.875\n",
      "2018-05-23T15:14:25.057151: step 17104, loss 0.0444733, acc 1\n",
      "2018-05-23T15:14:25.525946: step 17105, loss 0.103029, acc 0.9375\n",
      "2018-05-23T15:14:25.950897: step 17106, loss 0.0750205, acc 0.984375\n",
      "2018-05-23T15:14:26.348344: step 17107, loss 0.0941981, acc 0.9375\n",
      "2018-05-23T15:14:26.752786: step 17108, loss 0.118884, acc 0.96875\n",
      "2018-05-23T15:14:27.160693: step 17109, loss 0.161386, acc 0.890625\n",
      "2018-05-23T15:14:27.620473: step 17110, loss 0.1169, acc 0.90625\n",
      "2018-05-23T15:14:28.065281: step 17111, loss 0.105345, acc 0.921875\n",
      "2018-05-23T15:14:28.471195: step 17112, loss 0.0783876, acc 0.96875\n",
      "2018-05-23T15:14:28.917999: step 17113, loss 0.0981673, acc 0.9375\n",
      "2018-05-23T15:14:29.402061: step 17114, loss 0.029558, acc 1\n",
      "2018-05-23T15:14:29.814454: step 17115, loss 0.0431149, acc 0.984375\n",
      "2018-05-23T15:14:30.216378: step 17116, loss 0.0809089, acc 0.96875\n",
      "2018-05-23T15:14:30.609335: step 17117, loss 0.0823298, acc 0.96875\n",
      "2018-05-23T15:14:31.009268: step 17118, loss 0.128862, acc 0.96875\n",
      "2018-05-23T15:14:31.411214: step 17119, loss 0.0307491, acc 1\n",
      "2018-05-23T15:14:31.849061: step 17120, loss 0.0902405, acc 0.953125\n",
      "2018-05-23T15:14:32.331775: step 17121, loss 0.00911152, acc 1\n",
      "2018-05-23T15:14:32.733285: step 17122, loss 0.056011, acc 0.96875\n",
      "2018-05-23T15:14:33.140198: step 17123, loss 0.048026, acc 0.984375\n",
      "2018-05-23T15:14:33.553093: step 17124, loss 0.0816186, acc 0.953125\n",
      "2018-05-23T15:14:34.053262: step 17125, loss 0.110592, acc 0.953125\n",
      "2018-05-23T15:14:34.476130: step 17126, loss 0.0510538, acc 0.96875\n",
      "2018-05-23T15:14:35.016685: step 17127, loss 0.0480306, acc 1\n",
      "2018-05-23T15:14:35.620071: step 17128, loss 0.113306, acc 0.96875\n",
      "2018-05-23T15:14:36.207025: step 17129, loss 0.0597244, acc 0.984375\n",
      "2018-05-23T15:14:36.983957: step 17130, loss 0.0598222, acc 0.984375\n",
      "2018-05-23T15:14:37.688104: step 17131, loss 0.079281, acc 0.9375\n",
      "2018-05-23T15:14:38.302461: step 17132, loss 0.0551762, acc 0.984375\n",
      "2018-05-23T15:14:39.033504: step 17133, loss 0.0756207, acc 0.9375\n",
      "2018-05-23T15:14:39.680352: step 17134, loss 0.0456093, acc 1\n",
      "2018-05-23T15:14:40.208936: step 17135, loss 0.141646, acc 0.9375\n",
      "2018-05-23T15:14:40.639785: step 17136, loss 0.0237444, acc 1\n",
      "2018-05-23T15:14:41.055695: step 17137, loss 0.0528726, acc 0.984375\n",
      "2018-05-23T15:14:41.481555: step 17138, loss 0.12854, acc 0.921875\n",
      "2018-05-23T15:14:42.022558: step 17139, loss 0.13367, acc 0.9375\n",
      "2018-05-23T15:14:42.457381: step 17140, loss 0.0521908, acc 0.984375\n",
      "2018-05-23T15:14:42.889246: step 17141, loss 0.0834881, acc 0.984375\n",
      "2018-05-23T15:14:43.299156: step 17142, loss 0.0981601, acc 0.9375\n",
      "2018-05-23T15:14:43.717086: step 17143, loss 0.024883, acc 1\n",
      "2018-05-23T15:14:44.189822: step 17144, loss 0.0447265, acc 0.984375\n",
      "2018-05-23T15:14:44.661567: step 17145, loss 0.0958964, acc 0.921875\n",
      "2018-05-23T15:14:45.082949: step 17146, loss 0.0617186, acc 0.96875\n",
      "2018-05-23T15:14:45.498269: step 17147, loss 0.190753, acc 0.90625\n",
      "2018-05-23T15:14:45.913700: step 17148, loss 0.0850634, acc 0.953125\n",
      "2018-05-23T15:14:46.416869: step 17149, loss 0.0364209, acc 0.984375\n",
      "2018-05-23T15:14:46.839899: step 17150, loss 0.0961251, acc 0.953125\n",
      "2018-05-23T15:14:47.290692: step 17151, loss 0.0913836, acc 0.9375\n",
      "2018-05-23T15:14:47.767422: step 17152, loss 0.0870166, acc 0.953125\n",
      "2018-05-23T15:14:48.179317: step 17153, loss 0.0538965, acc 0.96875\n",
      "2018-05-23T15:14:48.632105: step 17154, loss 0.0885635, acc 0.953125\n",
      "2018-05-23T15:14:49.112819: step 17155, loss 0.107647, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:14:49.528708: step 17156, loss 0.211342, acc 0.9375\n",
      "2018-05-23T15:14:49.934620: step 17157, loss 0.0861773, acc 0.96875\n",
      "2018-05-23T15:14:50.386923: step 17158, loss 0.197675, acc 0.9375\n",
      "2018-05-23T15:14:50.808793: step 17159, loss 0.0207815, acc 1\n",
      "2018-05-23T15:14:51.306748: step 17160, loss 0.151986, acc 0.96875\n",
      "2018-05-23T15:14:51.764553: step 17161, loss 0.0657036, acc 0.96875\n",
      "2018-05-23T15:14:52.187978: step 17162, loss 0.0714431, acc 0.96875\n",
      "2018-05-23T15:14:52.599901: step 17163, loss 0.0810493, acc 0.9375\n",
      "2018-05-23T15:14:53.070154: step 17164, loss 0.057104, acc 0.953125\n",
      "2018-05-23T15:14:53.697986: step 17165, loss 0.035844, acc 0.984375\n",
      "2018-05-23T15:14:54.346252: step 17166, loss 0.110633, acc 0.984375\n",
      "2018-05-23T15:14:54.959630: step 17167, loss 0.0421633, acc 1\n",
      "2018-05-23T15:14:55.419749: step 17168, loss 0.0549958, acc 0.984375\n",
      "2018-05-23T15:14:56.009172: step 17169, loss 0.172396, acc 0.90625\n",
      "2018-05-23T15:14:56.664419: step 17170, loss 0.0251857, acc 1\n",
      "2018-05-23T15:14:57.266807: step 17171, loss 0.0863229, acc 0.984375\n",
      "2018-05-23T15:14:57.814378: step 17172, loss 0.0735407, acc 0.953125\n",
      "2018-05-23T15:14:58.358885: step 17173, loss 0.0553078, acc 0.984375\n",
      "2018-05-23T15:14:58.806710: step 17174, loss 0.111452, acc 0.953125\n",
      "2018-05-23T15:14:59.282796: step 17175, loss 0.0820202, acc 0.953125\n",
      "2018-05-23T15:14:59.715638: step 17176, loss 0.10709, acc 0.953125\n",
      "2018-05-23T15:15:00.151473: step 17177, loss 0.0835, acc 0.96875\n",
      "2018-05-23T15:15:00.570351: step 17178, loss 0.194886, acc 0.921875\n",
      "2018-05-23T15:15:01.241557: step 17179, loss 0.0488543, acc 0.984375\n",
      "2018-05-23T15:15:02.027099: step 17180, loss 0.0749959, acc 0.96875\n",
      "2018-05-23T15:15:02.644810: step 17181, loss 0.0951953, acc 0.96875\n",
      "2018-05-23T15:15:03.623216: step 17182, loss 0.222411, acc 0.921875\n",
      "2018-05-23T15:15:04.183212: step 17183, loss 0.0528794, acc 0.984375\n",
      "2018-05-23T15:15:04.729876: step 17184, loss 0.108493, acc 0.9375\n",
      "2018-05-23T15:15:05.342314: step 17185, loss 0.135743, acc 0.90625\n",
      "2018-05-23T15:15:05.898175: step 17186, loss 0.0894403, acc 0.953125\n",
      "2018-05-23T15:15:06.351342: step 17187, loss 0.104327, acc 0.96875\n",
      "2018-05-23T15:15:06.818184: step 17188, loss 0.0783345, acc 1\n",
      "2018-05-23T15:15:07.246713: step 17189, loss 0.0651032, acc 0.96875\n",
      "2018-05-23T15:15:07.738432: step 17190, loss 0.178699, acc 0.90625\n",
      "2018-05-23T15:15:08.192739: step 17191, loss 0.17226, acc 0.953125\n",
      "2018-05-23T15:15:08.648146: step 17192, loss 0.0508907, acc 0.984375\n",
      "2018-05-23T15:15:09.060565: step 17193, loss 0.117968, acc 0.953125\n",
      "2018-05-23T15:15:09.464074: step 17194, loss 0.15161, acc 0.9375\n",
      "2018-05-23T15:15:09.871983: step 17195, loss 0.196828, acc 0.90625\n",
      "2018-05-23T15:15:10.272482: step 17196, loss 0.0756444, acc 0.953125\n",
      "2018-05-23T15:15:10.678575: step 17197, loss 0.0225775, acc 1\n",
      "2018-05-23T15:15:11.097962: step 17198, loss 0.137861, acc 0.9375\n",
      "2018-05-23T15:15:11.499527: step 17199, loss 0.093047, acc 0.9375\n",
      "2018-05-23T15:15:11.917878: step 17200, loss 0.132697, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:15:18.878434: step 17200, loss 1.83877, acc 0.710959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17200\n",
      "\n",
      "2018-05-23T15:15:20.710906: step 17201, loss 0.236433, acc 0.9375\n",
      "2018-05-23T15:15:21.163248: step 17202, loss 0.0766902, acc 0.984375\n",
      "2018-05-23T15:15:21.616029: step 17203, loss 0.0500737, acc 0.984375\n",
      "2018-05-23T15:15:22.029475: step 17204, loss 0.149418, acc 0.90625\n",
      "2018-05-23T15:15:22.543132: step 17205, loss 0.109156, acc 0.9375\n",
      "2018-05-23T15:15:22.969591: step 17206, loss 0.0993307, acc 0.9375\n",
      "2018-05-23T15:15:23.370524: step 17207, loss 0.0641709, acc 0.984375\n",
      "2018-05-23T15:15:23.769025: step 17208, loss 0.0694719, acc 0.9375\n",
      "2018-05-23T15:15:24.168500: step 17209, loss 0.127055, acc 0.90625\n",
      "2018-05-23T15:15:24.595870: step 17210, loss 0.0615309, acc 0.953125\n",
      "2018-05-23T15:15:25.006284: step 17211, loss 0.0885033, acc 0.96875\n",
      "2018-05-23T15:15:25.412259: step 17212, loss 0.0328767, acc 1\n",
      "2018-05-23T15:15:25.936612: step 17213, loss 0.170882, acc 0.953125\n",
      "2018-05-23T15:15:26.351003: step 17214, loss 0.0750527, acc 0.953125\n",
      "2018-05-23T15:15:26.759069: step 17215, loss 0.111689, acc 0.9375\n",
      "2018-05-23T15:15:27.210736: step 17216, loss 0.117927, acc 0.9375\n",
      "2018-05-23T15:15:27.609671: step 17217, loss 0.0783684, acc 0.984375\n",
      "2018-05-23T15:15:28.017707: step 17218, loss 0.0583766, acc 1\n",
      "2018-05-23T15:15:28.418331: step 17219, loss 0.0789801, acc 0.96875\n",
      "2018-05-23T15:15:28.820656: step 17220, loss 0.0440045, acc 0.96875\n",
      "2018-05-23T15:15:29.238115: step 17221, loss 0.0465292, acc 0.984375\n",
      "2018-05-23T15:15:29.640735: step 17222, loss 0.0782715, acc 0.96875\n",
      "2018-05-23T15:15:30.042756: step 17223, loss 0.110033, acc 0.921875\n",
      "2018-05-23T15:15:30.573564: step 17224, loss 0.0858529, acc 0.984375\n",
      "2018-05-23T15:15:31.016908: step 17225, loss 0.0446559, acc 0.984375\n",
      "2018-05-23T15:15:31.428317: step 17226, loss 0.0755017, acc 0.984375\n",
      "2018-05-23T15:15:31.860671: step 17227, loss 0.0522277, acc 0.984375\n",
      "2018-05-23T15:15:32.349398: step 17228, loss 0.118343, acc 0.921875\n",
      "2018-05-23T15:15:32.759431: step 17229, loss 0.0396482, acc 0.984375\n",
      "2018-05-23T15:15:33.207840: step 17230, loss 0.0876368, acc 0.9375\n",
      "2018-05-23T15:15:33.613292: step 17231, loss 0.056934, acc 0.953125\n",
      "2018-05-23T15:15:34.188535: step 17232, loss 0.175877, acc 0.9375\n",
      "2018-05-23T15:15:34.921091: step 17233, loss 0.0633214, acc 0.96875\n",
      "2018-05-23T15:15:35.500050: step 17234, loss 0.164597, acc 0.921875\n",
      "2018-05-23T15:15:36.057075: step 17235, loss 0.0939639, acc 0.953125\n",
      "2018-05-23T15:15:36.832518: step 17236, loss 0.075857, acc 0.96875\n",
      "2018-05-23T15:15:37.386036: step 17237, loss 0.100204, acc 0.953125\n",
      "2018-05-23T15:15:37.799931: step 17238, loss 0.0411747, acc 0.984375\n",
      "2018-05-23T15:15:38.218810: step 17239, loss 0.100373, acc 0.96875\n",
      "2018-05-23T15:15:38.657634: step 17240, loss 0.149824, acc 0.96875\n",
      "2018-05-23T15:15:39.099055: step 17241, loss 0.0360358, acc 0.984375\n",
      "2018-05-23T15:15:39.544930: step 17242, loss 0.0883997, acc 0.953125\n",
      "2018-05-23T15:15:39.990251: step 17243, loss 0.0525265, acc 0.984375\n",
      "2018-05-23T15:15:40.415117: step 17244, loss 0.0815642, acc 0.9375\n",
      "2018-05-23T15:15:40.814048: step 17245, loss 0.0958988, acc 0.9375\n",
      "2018-05-23T15:15:41.224948: step 17246, loss 0.189509, acc 0.953125\n",
      "2018-05-23T15:15:41.623883: step 17247, loss 0.0580671, acc 0.96875\n",
      "2018-05-23T15:15:42.113572: step 17248, loss 0.114874, acc 0.9375\n",
      "2018-05-23T15:15:42.534447: step 17249, loss 0.150149, acc 0.921875\n",
      "2018-05-23T15:15:42.940720: step 17250, loss 0.097323, acc 0.953125\n",
      "2018-05-23T15:15:43.348632: step 17251, loss 0.0616435, acc 0.984375\n",
      "2018-05-23T15:15:43.753547: step 17252, loss 0.118515, acc 0.9375\n",
      "2018-05-23T15:15:44.151483: step 17253, loss 0.0590118, acc 0.96875\n",
      "2018-05-23T15:15:44.627209: step 17254, loss 0.116452, acc 0.953125\n",
      "2018-05-23T15:15:45.148812: step 17255, loss 0.21722, acc 0.921875\n",
      "2018-05-23T15:15:45.621061: step 17256, loss 0.22421, acc 0.96875\n",
      "2018-05-23T15:15:46.032959: step 17257, loss 0.0386984, acc 0.984375\n",
      "2018-05-23T15:15:46.459233: step 17258, loss 0.054497, acc 0.984375\n",
      "2018-05-23T15:15:46.863155: step 17259, loss 0.0898202, acc 0.953125\n",
      "2018-05-23T15:15:47.268973: step 17260, loss 0.0965381, acc 0.984375\n",
      "2018-05-23T15:15:47.678941: step 17261, loss 0.133013, acc 0.96875\n",
      "2018-05-23T15:15:48.089287: step 17262, loss 0.100601, acc 0.96875\n",
      "2018-05-23T15:15:48.535095: step 17263, loss 0.127887, acc 0.921875\n",
      "2018-05-23T15:15:48.947989: step 17264, loss 0.120285, acc 0.984375\n",
      "2018-05-23T15:15:49.374849: step 17265, loss 0.0664817, acc 0.984375\n",
      "2018-05-23T15:15:49.835615: step 17266, loss 0.113169, acc 0.953125\n",
      "2018-05-23T15:15:50.247513: step 17267, loss 0.133376, acc 0.921875\n",
      "2018-05-23T15:15:50.654434: step 17268, loss 0.105216, acc 0.96875\n",
      "2018-05-23T15:15:51.049762: step 17269, loss 0.0904986, acc 0.953125\n",
      "2018-05-23T15:15:51.464652: step 17270, loss 0.0538847, acc 1\n",
      "2018-05-23T15:15:51.918954: step 17271, loss 0.0569931, acc 0.984375\n",
      "2018-05-23T15:15:52.335349: step 17272, loss 0.10151, acc 0.96875\n",
      "2018-05-23T15:15:52.840994: step 17273, loss 0.0912519, acc 0.9375\n",
      "2018-05-23T15:15:53.264861: step 17274, loss 0.0731412, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:15:53.671774: step 17275, loss 0.0490759, acc 0.984375\n",
      "2018-05-23T15:15:54.078267: step 17276, loss 0.0845269, acc 0.9375\n",
      "2018-05-23T15:15:54.490093: step 17277, loss 0.0765098, acc 0.96875\n",
      "2018-05-23T15:15:54.902029: step 17278, loss 0.071919, acc 0.96875\n",
      "2018-05-23T15:15:55.399210: step 17279, loss 0.10384, acc 0.953125\n",
      "2018-05-23T15:15:55.811679: step 17280, loss 0.134908, acc 0.96875\n",
      "2018-05-23T15:15:56.211613: step 17281, loss 0.0806136, acc 0.953125\n",
      "2018-05-23T15:15:56.618523: step 17282, loss 0.102304, acc 0.953125\n",
      "2018-05-23T15:15:57.019455: step 17283, loss 0.10451, acc 0.96875\n",
      "2018-05-23T15:15:57.426086: step 17284, loss 0.0946099, acc 0.984375\n",
      "2018-05-23T15:15:57.936420: step 17285, loss 0.0945625, acc 0.921875\n",
      "2018-05-23T15:15:58.486090: step 17286, loss 0.0163039, acc 1\n",
      "2018-05-23T15:15:59.079926: step 17287, loss 0.0528147, acc 0.984375\n",
      "2018-05-23T15:15:59.692288: step 17288, loss 0.0472655, acc 1\n",
      "2018-05-23T15:16:00.359315: step 17289, loss 0.112417, acc 0.96875\n",
      "2018-05-23T15:16:00.986399: step 17290, loss 0.347515, acc 0.90625\n",
      "2018-05-23T15:16:01.628703: step 17291, loss 0.194215, acc 0.90625\n",
      "2018-05-23T15:16:02.418825: step 17292, loss 0.0903659, acc 0.953125\n",
      "2018-05-23T15:16:03.001432: step 17293, loss 0.0893666, acc 0.96875\n",
      "2018-05-23T15:16:03.424304: step 17294, loss 0.0249209, acc 1\n",
      "2018-05-23T15:16:03.841715: step 17295, loss 0.128013, acc 0.953125\n",
      "2018-05-23T15:16:04.234403: step 17296, loss 0.114668, acc 0.96875\n",
      "2018-05-23T15:16:04.650504: step 17297, loss 0.0367417, acc 0.984375\n",
      "2018-05-23T15:16:05.043960: step 17298, loss 0.0919158, acc 0.96875\n",
      "2018-05-23T15:16:05.462852: step 17299, loss 0.114408, acc 0.96875\n",
      "2018-05-23T15:16:05.880778: step 17300, loss 0.120373, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:16:11.537434: step 17300, loss 1.84913, acc 0.70753\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17300\n",
      "\n",
      "2018-05-23T15:16:13.303567: step 17301, loss 0.0277306, acc 1\n",
      "2018-05-23T15:16:13.840376: step 17302, loss 0.187307, acc 0.953125\n",
      "2018-05-23T15:16:14.264341: step 17303, loss 0.090074, acc 0.96875\n",
      "2018-05-23T15:16:14.693211: step 17304, loss 0.0842743, acc 0.953125\n",
      "2018-05-23T15:16:15.093140: step 17305, loss 0.121989, acc 0.9375\n",
      "2018-05-23T15:16:15.492633: step 17306, loss 0.114202, acc 0.9375\n",
      "2018-05-23T15:16:15.931713: step 17307, loss 0.0907413, acc 0.953125\n",
      "2018-05-23T15:16:16.337398: step 17308, loss 0.249396, acc 0.921875\n",
      "2018-05-23T15:16:16.738375: step 17309, loss 0.128323, acc 0.96875\n",
      "2018-05-23T15:16:17.148338: step 17310, loss 0.130167, acc 0.953125\n",
      "2018-05-23T15:16:17.553279: step 17311, loss 0.0556883, acc 0.984375\n",
      "2018-05-23T15:16:18.061578: step 17312, loss 0.140913, acc 0.9375\n",
      "2018-05-23T15:16:18.464067: step 17313, loss 0.0838088, acc 0.953125\n",
      "2018-05-23T15:16:18.922455: step 17314, loss 0.174374, acc 0.921875\n",
      "2018-05-23T15:16:19.332367: step 17315, loss 0.0831547, acc 0.953125\n",
      "2018-05-23T15:16:19.760284: step 17316, loss 0.0422073, acc 0.96875\n",
      "2018-05-23T15:16:20.169071: step 17317, loss 0.0953721, acc 0.953125\n",
      "2018-05-23T15:16:20.567017: step 17318, loss 0.0630545, acc 0.96875\n",
      "2018-05-23T15:16:20.967546: step 17319, loss 0.0520003, acc 0.96875\n",
      "2018-05-23T15:16:21.362608: step 17320, loss 0.208602, acc 0.90625\n",
      "2018-05-23T15:16:21.799016: step 17321, loss 0.141499, acc 0.921875\n",
      "2018-05-23T15:16:22.217511: step 17322, loss 0.112115, acc 0.96875\n",
      "2018-05-23T15:16:22.621938: step 17323, loss 0.0718399, acc 0.953125\n",
      "2018-05-23T15:16:23.028603: step 17324, loss 0.0541068, acc 0.96875\n",
      "2018-05-23T15:16:23.508452: step 17325, loss 0.0884396, acc 0.953125\n",
      "2018-05-23T15:16:23.925348: step 17326, loss 0.121107, acc 0.96875\n",
      "2018-05-23T15:16:24.325899: step 17327, loss 0.0858907, acc 0.9375\n",
      "2018-05-23T15:16:24.731458: step 17328, loss 0.0884498, acc 0.953125\n",
      "2018-05-23T15:16:25.128957: step 17329, loss 0.0469816, acc 1\n",
      "2018-05-23T15:16:25.535011: step 17330, loss 0.0642464, acc 0.96875\n",
      "2018-05-23T15:16:25.949901: step 17331, loss 0.254007, acc 0.90625\n",
      "2018-05-23T15:16:26.343537: step 17332, loss 0.0590815, acc 0.984375\n",
      "2018-05-23T15:16:26.749043: step 17333, loss 0.083728, acc 0.984375\n",
      "2018-05-23T15:16:27.150093: step 17334, loss 0.0827706, acc 0.96875\n",
      "2018-05-23T15:16:27.554058: step 17335, loss 0.0545278, acc 0.96875\n",
      "2018-05-23T15:16:27.971491: step 17336, loss 0.156667, acc 0.9375\n",
      "2018-05-23T15:16:28.378558: step 17337, loss 0.09867, acc 0.953125\n",
      "2018-05-23T15:16:28.804522: step 17338, loss 0.0767007, acc 0.984375\n",
      "2018-05-23T15:16:29.238781: step 17339, loss 0.0614775, acc 0.984375\n",
      "2018-05-23T15:16:29.644242: step 17340, loss 0.071894, acc 0.953125\n",
      "2018-05-23T15:16:30.050316: step 17341, loss 0.057014, acc 0.96875\n",
      "2018-05-23T15:16:30.457299: step 17342, loss 0.0368236, acc 1\n",
      "2018-05-23T15:16:30.868964: step 17343, loss 0.289641, acc 0.90625\n",
      "2018-05-23T15:16:31.268949: step 17344, loss 0.0368965, acc 0.984375\n",
      "2018-05-23T15:16:31.669391: step 17345, loss 0.0519546, acc 0.96875\n",
      "2018-05-23T15:16:32.340814: step 17346, loss 0.0589333, acc 0.984375\n",
      "2018-05-23T15:16:32.844466: step 17347, loss 0.107048, acc 0.953125\n",
      "2018-05-23T15:16:33.302242: step 17348, loss 0.0913206, acc 0.953125\n",
      "2018-05-23T15:16:33.911612: step 17349, loss 0.03982, acc 1\n",
      "2018-05-23T15:16:34.444188: step 17350, loss 0.0892083, acc 0.96875\n",
      "2018-05-23T15:16:34.913932: step 17351, loss 0.0229464, acc 1\n",
      "2018-05-23T15:16:35.344777: step 17352, loss 0.0934195, acc 0.953125\n",
      "2018-05-23T15:16:36.108815: step 17353, loss 0.0712744, acc 0.96875\n",
      "2018-05-23T15:16:37.190435: step 17354, loss 0.0695056, acc 0.953125\n",
      "2018-05-23T15:16:37.925469: step 17355, loss 0.0398458, acc 1\n",
      "2018-05-23T15:16:38.584221: step 17356, loss 0.155848, acc 0.96875\n",
      "2018-05-23T15:16:39.151768: step 17357, loss 0.0780706, acc 0.96875\n",
      "2018-05-23T15:16:39.697307: step 17358, loss 0.086992, acc 0.96875\n",
      "2018-05-23T15:16:40.116696: step 17359, loss 0.0667255, acc 0.984375\n",
      "2018-05-23T15:16:40.613414: step 17360, loss 0.107273, acc 0.96875\n",
      "2018-05-23T15:16:41.026309: step 17361, loss 0.0687625, acc 0.96875\n",
      "2018-05-23T15:16:41.423802: step 17362, loss 0.062964, acc 0.96875\n",
      "2018-05-23T15:16:41.822253: step 17363, loss 0.0328044, acc 0.984375\n",
      "2018-05-23T15:16:42.224206: step 17364, loss 0.0745553, acc 0.96875\n",
      "2018-05-23T15:16:42.630123: step 17365, loss 0.0929066, acc 0.953125\n",
      "2018-05-23T15:16:43.031048: step 17366, loss 0.118204, acc 0.9375\n",
      "2018-05-23T15:16:43.454915: step 17367, loss 0.154128, acc 0.9375\n",
      "2018-05-23T15:16:43.869806: step 17368, loss 0.0719221, acc 0.96875\n",
      "2018-05-23T15:16:44.270251: step 17369, loss 0.0473906, acc 0.984375\n",
      "2018-05-23T15:16:44.764440: step 17370, loss 0.187903, acc 0.953125\n",
      "2018-05-23T15:16:45.850583: step 17371, loss 0.174116, acc 0.9375\n",
      "2018-05-23T15:16:47.042980: step 17372, loss 0.128465, acc 0.953125\n",
      "2018-05-23T15:16:47.981937: step 17373, loss 0.0886262, acc 0.953125\n",
      "2018-05-23T15:16:48.406799: step 17374, loss 0.103372, acc 0.9375\n",
      "2018-05-23T15:16:48.806882: step 17375, loss 0.0761111, acc 0.96875\n",
      "2018-05-23T15:16:49.188880: step 17376, loss 0.0413264, acc 1\n",
      "2018-05-23T15:16:49.581868: step 17377, loss 0.152646, acc 0.953125\n",
      "2018-05-23T15:16:50.004789: step 17378, loss 0.0295041, acc 1\n",
      "2018-05-23T15:16:50.403789: step 17379, loss 0.0437059, acc 0.984375\n",
      "2018-05-23T15:16:50.792689: step 17380, loss 0.0603454, acc 0.96875\n",
      "2018-05-23T15:16:51.192623: step 17381, loss 0.0514777, acc 1\n",
      "2018-05-23T15:16:51.646921: step 17382, loss 0.154105, acc 0.953125\n",
      "2018-05-23T15:16:52.042868: step 17383, loss 0.0466775, acc 1\n",
      "2018-05-23T15:16:52.514275: step 17384, loss 0.0638211, acc 0.953125\n",
      "2018-05-23T15:16:52.985566: step 17385, loss 0.0737374, acc 0.953125\n",
      "2018-05-23T15:16:53.378514: step 17386, loss 0.138725, acc 0.953125\n",
      "2018-05-23T15:16:53.770653: step 17387, loss 0.0486982, acc 0.984375\n",
      "2018-05-23T15:16:54.162601: step 17388, loss 0.036783, acc 1\n",
      "2018-05-23T15:16:54.677735: step 17389, loss 0.0811196, acc 0.953125\n",
      "2018-05-23T15:16:55.068636: step 17390, loss 0.0751681, acc 0.953125\n",
      "2018-05-23T15:16:55.455708: step 17391, loss 0.04882, acc 0.96875\n",
      "2018-05-23T15:16:55.892024: step 17392, loss 0.0653351, acc 0.96875\n",
      "2018-05-23T15:16:56.383709: step 17393, loss 0.118074, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:16:56.873724: step 17394, loss 0.185361, acc 0.9375\n",
      "2018-05-23T15:16:57.371391: step 17395, loss 0.140749, acc 0.953125\n",
      "2018-05-23T15:16:58.178790: step 17396, loss 0.0842304, acc 0.96875\n",
      "2018-05-23T15:16:58.845021: step 17397, loss 0.0797727, acc 0.953125\n",
      "2018-05-23T15:16:59.437441: step 17398, loss 0.134684, acc 0.9375\n",
      "2018-05-23T15:17:00.139084: step 17399, loss 0.109043, acc 0.9375\n",
      "2018-05-23T15:17:00.569957: step 17400, loss 0.0875962, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:17:07.761786: step 17400, loss 1.87022, acc 0.708816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17400\n",
      "\n",
      "2018-05-23T15:17:10.588226: step 17401, loss 0.154722, acc 0.9375\n",
      "2018-05-23T15:17:11.001122: step 17402, loss 0.152546, acc 0.921875\n",
      "2018-05-23T15:17:11.420000: step 17403, loss 0.136884, acc 0.9375\n",
      "2018-05-23T15:17:11.828905: step 17404, loss 0.0669855, acc 0.96875\n",
      "2018-05-23T15:17:12.269726: step 17405, loss 0.0543719, acc 0.984375\n",
      "2018-05-23T15:17:12.724510: step 17406, loss 0.0556854, acc 0.984375\n",
      "2018-05-23T15:17:13.139399: step 17407, loss 0.102631, acc 0.984375\n",
      "2018-05-23T15:17:13.560274: step 17408, loss 0.0822864, acc 0.96875\n",
      "2018-05-23T15:17:14.192583: step 17409, loss 0.0236184, acc 1\n",
      "2018-05-23T15:17:14.889721: step 17410, loss 0.122323, acc 0.96875\n",
      "2018-05-23T15:17:15.583862: step 17411, loss 0.18802, acc 0.9375\n",
      "2018-05-23T15:17:16.226142: step 17412, loss 0.0638809, acc 0.953125\n",
      "2018-05-23T15:17:16.684918: step 17413, loss 0.105421, acc 0.953125\n",
      "2018-05-23T15:17:17.110777: step 17414, loss 0.0865006, acc 0.96875\n",
      "2018-05-23T15:17:17.496744: step 17415, loss 0.0384208, acc 1\n",
      "2018-05-23T15:17:18.023334: step 17416, loss 0.143798, acc 0.9375\n",
      "2018-05-23T15:17:18.393345: step 17417, loss 0.139973, acc 0.921875\n",
      "2018-05-23T15:17:18.805243: step 17418, loss 0.189323, acc 0.921875\n",
      "2018-05-23T15:17:19.170268: step 17419, loss 0.040982, acc 0.984375\n",
      "2018-05-23T15:17:19.521328: step 17420, loss 0.157785, acc 0.953125\n",
      "2018-05-23T15:17:19.881363: step 17421, loss 0.0499847, acc 0.96875\n",
      "2018-05-23T15:17:20.291268: step 17422, loss 0.103907, acc 0.953125\n",
      "2018-05-23T15:17:20.759015: step 17423, loss 0.0916308, acc 0.953125\n",
      "2018-05-23T15:17:21.214799: step 17424, loss 0.105695, acc 0.953125\n",
      "2018-05-23T15:17:21.600766: step 17425, loss 0.0809666, acc 0.96875\n",
      "2018-05-23T15:17:21.978752: step 17426, loss 0.127234, acc 0.90625\n",
      "2018-05-23T15:17:22.388656: step 17427, loss 0.0842298, acc 0.953125\n",
      "2018-05-23T15:17:22.777616: step 17428, loss 0.120859, acc 0.953125\n",
      "2018-05-23T15:17:23.290244: step 17429, loss 0.162896, acc 0.921875\n",
      "2018-05-23T15:17:23.726079: step 17430, loss 0.115817, acc 0.9375\n",
      "2018-05-23T15:17:24.117032: step 17431, loss 0.127989, acc 0.9375\n",
      "2018-05-23T15:17:24.481061: step 17432, loss 0.0628358, acc 0.96875\n",
      "2018-05-23T15:17:24.872012: step 17433, loss 0.118706, acc 0.96875\n",
      "2018-05-23T15:17:25.258978: step 17434, loss 0.0514245, acc 0.96875\n",
      "2018-05-23T15:17:25.626995: step 17435, loss 0.104548, acc 0.96875\n",
      "2018-05-23T15:17:25.997005: step 17436, loss 0.0492403, acc 0.984375\n",
      "2018-05-23T15:17:26.355047: step 17437, loss 0.121552, acc 0.9375\n",
      "2018-05-23T15:17:26.708507: step 17438, loss 0.0832998, acc 0.953125\n",
      "2018-05-23T15:17:27.070539: step 17439, loss 0.0654806, acc 0.96875\n",
      "2018-05-23T15:17:27.432572: step 17440, loss 0.171083, acc 0.90625\n",
      "2018-05-23T15:17:27.787654: step 17441, loss 0.091949, acc 0.953125\n",
      "2018-05-23T15:17:28.138682: step 17442, loss 0.163138, acc 0.921875\n",
      "2018-05-23T15:17:28.531633: step 17443, loss 0.0813372, acc 0.96875\n",
      "2018-05-23T15:17:29.008358: step 17444, loss 0.0705482, acc 0.953125\n",
      "2018-05-23T15:17:29.448179: step 17445, loss 0.117085, acc 0.96875\n",
      "2018-05-23T15:17:29.823179: step 17446, loss 0.0644644, acc 0.984375\n",
      "2018-05-23T15:17:30.189197: step 17447, loss 0.0599738, acc 0.96875\n",
      "2018-05-23T15:17:30.570177: step 17448, loss 0.0820229, acc 0.96875\n",
      "2018-05-23T15:17:30.932208: step 17449, loss 0.0963194, acc 0.953125\n",
      "2018-05-23T15:17:31.289254: step 17450, loss 0.0875237, acc 0.953125\n",
      "2018-05-23T15:17:31.657272: step 17451, loss 0.13805, acc 0.921875\n",
      "2018-05-23T15:17:32.011197: step 17452, loss 0.0767064, acc 0.984375\n",
      "2018-05-23T15:17:32.362261: step 17453, loss 0.101083, acc 0.953125\n",
      "2018-05-23T15:17:32.760196: step 17454, loss 0.0529114, acc 0.96875\n",
      "2018-05-23T15:17:33.244898: step 17455, loss 0.0587953, acc 0.984375\n",
      "2018-05-23T15:17:33.974945: step 17456, loss 0.170586, acc 0.921875\n",
      "2018-05-23T15:17:34.600273: step 17457, loss 0.0833901, acc 0.96875\n",
      "2018-05-23T15:17:35.059044: step 17458, loss 0.078213, acc 0.953125\n",
      "2018-05-23T15:17:35.873865: step 17459, loss 0.154587, acc 0.9375\n",
      "2018-05-23T15:17:36.729576: step 17460, loss 0.106153, acc 0.921875\n",
      "2018-05-23T15:17:37.282101: step 17461, loss 0.0372941, acc 1\n",
      "2018-05-23T15:17:38.010151: step 17462, loss 0.0998596, acc 0.96875\n",
      "2018-05-23T15:17:38.694320: step 17463, loss 0.0569316, acc 0.96875\n",
      "2018-05-23T15:17:39.416387: step 17464, loss 0.145066, acc 0.921875\n",
      "2018-05-23T15:17:39.963923: step 17465, loss 0.0777015, acc 0.96875\n",
      "2018-05-23T15:17:40.736854: step 17466, loss 0.0881093, acc 0.984375\n",
      "2018-05-23T15:17:41.340243: step 17467, loss 0.0720769, acc 0.96875\n",
      "2018-05-23T15:17:42.055328: step 17468, loss 0.0629548, acc 0.96875\n",
      "2018-05-23T15:17:42.773408: step 17469, loss 0.0801436, acc 0.953125\n",
      "2018-05-23T15:17:43.625129: step 17470, loss 0.104545, acc 0.9375\n",
      "2018-05-23T15:17:44.259431: step 17471, loss 0.280742, acc 0.9375\n",
      "2018-05-23T15:17:45.070265: step 17472, loss 0.16025, acc 0.9375\n",
      "2018-05-23T15:17:45.485155: step 17473, loss 0.124009, acc 0.9375\n",
      "2018-05-23T15:17:45.868128: step 17474, loss 0.156149, acc 0.890625\n",
      "2018-05-23T15:17:46.212207: step 17475, loss 0.0804452, acc 0.96875\n",
      "2018-05-23T15:17:46.553294: step 17476, loss 0.186485, acc 0.90625\n",
      "2018-05-23T15:17:46.915329: step 17477, loss 0.109829, acc 0.921875\n",
      "2018-05-23T15:17:47.252427: step 17478, loss 0.0830024, acc 0.96875\n",
      "2018-05-23T15:17:47.593514: step 17479, loss 0.0831543, acc 0.953125\n",
      "2018-05-23T15:17:47.962525: step 17480, loss 0.0569497, acc 0.984375\n",
      "2018-05-23T15:17:48.316578: step 17481, loss 0.109736, acc 0.9375\n",
      "2018-05-23T15:17:48.663953: step 17482, loss 0.156012, acc 0.953125\n",
      "2018-05-23T15:17:49.016014: step 17483, loss 0.120485, acc 0.9375\n",
      "2018-05-23T15:17:49.355105: step 17484, loss 0.193474, acc 0.9375\n",
      "2018-05-23T15:17:49.705168: step 17485, loss 0.0840416, acc 0.96875\n",
      "2018-05-23T15:17:50.056427: step 17486, loss 0.105666, acc 0.984375\n",
      "2018-05-23T15:17:50.394522: step 17487, loss 0.122754, acc 0.96875\n",
      "2018-05-23T15:17:50.752564: step 17488, loss 0.11001, acc 0.9375\n",
      "2018-05-23T15:17:51.096644: step 17489, loss 0.0581526, acc 0.984375\n",
      "2018-05-23T15:17:51.435736: step 17490, loss 0.0574207, acc 0.984375\n",
      "2018-05-23T15:17:51.820706: step 17491, loss 0.1264, acc 0.96875\n",
      "2018-05-23T15:17:52.169772: step 17492, loss 0.246191, acc 0.921875\n",
      "2018-05-23T15:17:52.508897: step 17493, loss 0.128708, acc 0.921875\n",
      "2018-05-23T15:17:52.855935: step 17494, loss 0.0716977, acc 0.96875\n",
      "2018-05-23T15:17:53.205001: step 17495, loss 0.126255, acc 0.921875\n",
      "2018-05-23T15:17:53.550113: step 17496, loss 0.317363, acc 0.90625\n",
      "2018-05-23T15:17:53.901138: step 17497, loss 0.0634989, acc 0.984375\n",
      "2018-05-23T15:17:54.247213: step 17498, loss 0.100459, acc 0.953125\n",
      "2018-05-23T15:17:54.589299: step 17499, loss 0.130178, acc 0.90625\n",
      "2018-05-23T15:17:54.950334: step 17500, loss 0.0353558, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:17:59.513125: step 17500, loss 1.82715, acc 0.711387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17500\n",
      "\n",
      "2018-05-23T15:18:00.761787: step 17501, loss 0.0421004, acc 1\n",
      "2018-05-23T15:18:01.167699: step 17502, loss 0.0724119, acc 0.984375\n",
      "2018-05-23T15:18:01.576607: step 17503, loss 0.273122, acc 0.9375\n",
      "2018-05-23T15:18:01.961577: step 17504, loss 0.0396898, acc 1\n",
      "2018-05-23T15:18:02.316625: step 17505, loss 0.132399, acc 0.9375\n",
      "2018-05-23T15:18:02.675668: step 17506, loss 0.0317067, acc 0.984375\n",
      "2018-05-23T15:18:03.033707: step 17507, loss 0.12099, acc 0.9375\n",
      "2018-05-23T15:18:03.382776: step 17508, loss 0.194576, acc 0.96875\n",
      "2018-05-23T15:18:03.721869: step 17509, loss 0.085933, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:18:04.065948: step 17510, loss 0.0391825, acc 1\n",
      "2018-05-23T15:18:04.412022: step 17511, loss 0.165509, acc 0.921875\n",
      "2018-05-23T15:18:04.750117: step 17512, loss 0.0980671, acc 0.953125\n",
      "2018-05-23T15:18:05.087214: step 17513, loss 0.065384, acc 0.984375\n",
      "2018-05-23T15:18:05.419326: step 17514, loss 0.0821488, acc 0.96875\n",
      "2018-05-23T15:18:05.765402: step 17515, loss 0.0878916, acc 0.953125\n",
      "2018-05-23T15:18:06.111476: step 17516, loss 0.0599845, acc 0.984375\n",
      "2018-05-23T15:18:06.463534: step 17517, loss 0.0686714, acc 0.984375\n",
      "2018-05-23T15:18:06.855486: step 17518, loss 0.083941, acc 0.96875\n",
      "2018-05-23T15:18:07.199564: step 17519, loss 0.0621983, acc 0.953125\n",
      "2018-05-23T15:18:07.552620: step 17520, loss 0.0660706, acc 1\n",
      "2018-05-23T15:18:07.898694: step 17521, loss 0.195525, acc 0.96875\n",
      "2018-05-23T15:18:08.233796: step 17522, loss 0.081052, acc 0.96875\n",
      "2018-05-23T15:18:08.578873: step 17523, loss 0.0501089, acc 0.984375\n",
      "2018-05-23T15:18:08.930934: step 17524, loss 0.0463207, acc 0.984375\n",
      "2018-05-23T15:18:09.265037: step 17525, loss 0.0666762, acc 0.984375\n",
      "2018-05-23T15:18:09.610156: step 17526, loss 0.148667, acc 0.921875\n",
      "2018-05-23T15:18:09.951240: step 17527, loss 0.109687, acc 0.96875\n",
      "2018-05-23T15:18:10.295320: step 17528, loss 0.083818, acc 0.9375\n",
      "2018-05-23T15:18:10.656354: step 17529, loss 0.037607, acc 0.984375\n",
      "2018-05-23T15:18:10.996444: step 17530, loss 0.113348, acc 0.953125\n",
      "2018-05-23T15:18:11.333542: step 17531, loss 0.0906214, acc 0.953125\n",
      "2018-05-23T15:18:11.681612: step 17532, loss 0.114045, acc 0.953125\n",
      "2018-05-23T15:18:12.018709: step 17533, loss 0.0742578, acc 0.96875\n",
      "2018-05-23T15:18:12.351818: step 17534, loss 0.0578507, acc 0.984375\n",
      "2018-05-23T15:18:12.697892: step 17535, loss 0.0659197, acc 0.96875\n",
      "2018-05-23T15:18:13.034990: step 17536, loss 0.0543711, acc 0.96875\n",
      "2018-05-23T15:18:13.377078: step 17537, loss 0.149498, acc 0.9375\n",
      "2018-05-23T15:18:13.724147: step 17538, loss 0.104544, acc 0.921875\n",
      "2018-05-23T15:18:14.064240: step 17539, loss 0.0540087, acc 0.953125\n",
      "2018-05-23T15:18:14.395351: step 17540, loss 0.116296, acc 0.9375\n",
      "2018-05-23T15:18:14.757383: step 17541, loss 0.113541, acc 0.96875\n",
      "2018-05-23T15:18:15.101465: step 17542, loss 0.105521, acc 0.96875\n",
      "2018-05-23T15:18:15.436565: step 17543, loss 0.0380342, acc 0.984375\n",
      "2018-05-23T15:18:15.800592: step 17544, loss 0.138709, acc 0.921875\n",
      "2018-05-23T15:18:16.139686: step 17545, loss 0.110408, acc 0.9375\n",
      "2018-05-23T15:18:16.470799: step 17546, loss 0.146779, acc 0.921875\n",
      "2018-05-23T15:18:16.808896: step 17547, loss 0.0509886, acc 0.984375\n",
      "2018-05-23T15:18:17.146990: step 17548, loss 0.102167, acc 0.953125\n",
      "2018-05-23T15:18:17.490073: step 17549, loss 0.0326304, acc 1\n",
      "2018-05-23T15:18:17.845122: step 17550, loss 0.212358, acc 0.9375\n",
      "2018-05-23T15:18:18.194190: step 17551, loss 0.0266427, acc 0.984375\n",
      "2018-05-23T15:18:18.529292: step 17552, loss 0.0897804, acc 0.953125\n",
      "2018-05-23T15:18:18.876913: step 17553, loss 0.0608657, acc 0.984375\n",
      "2018-05-23T15:18:19.210020: step 17554, loss 0.0982066, acc 0.96875\n",
      "2018-05-23T15:18:19.556094: step 17555, loss 0.106089, acc 0.9375\n",
      "2018-05-23T15:18:19.910149: step 17556, loss 0.0233145, acc 1\n",
      "2018-05-23T15:18:20.246588: step 17557, loss 0.100161, acc 0.96875\n",
      "2018-05-23T15:18:20.600641: step 17558, loss 0.0298299, acc 1\n",
      "2018-05-23T15:18:20.941729: step 17559, loss 0.0660012, acc 0.984375\n",
      "2018-05-23T15:18:21.281821: step 17560, loss 0.302423, acc 0.9375\n",
      "2018-05-23T15:18:21.615927: step 17561, loss 0.106868, acc 0.9375\n",
      "2018-05-23T15:18:22.023834: step 17562, loss 0.0852005, acc 0.953125\n",
      "2018-05-23T15:18:22.368910: step 17563, loss 0.0908348, acc 0.953125\n",
      "2018-05-23T15:18:22.716154: step 17564, loss 0.275554, acc 0.90625\n",
      "2018-05-23T15:18:23.091151: step 17565, loss 0.0686039, acc 0.953125\n",
      "2018-05-23T15:18:23.447197: step 17566, loss 0.0378537, acc 0.984375\n",
      "2018-05-23T15:18:23.807237: step 17567, loss 0.214473, acc 0.921875\n",
      "2018-05-23T15:18:24.152313: step 17568, loss 0.0696297, acc 0.953125\n",
      "2018-05-23T15:18:24.507362: step 17569, loss 0.0360535, acc 1\n",
      "2018-05-23T15:18:24.911280: step 17570, loss 0.120759, acc 0.9375\n",
      "2018-05-23T15:18:25.442861: step 17571, loss 0.101464, acc 0.953125\n",
      "2018-05-23T15:18:26.107081: step 17572, loss 0.0983231, acc 0.953125\n",
      "2018-05-23T15:18:26.535934: step 17573, loss 0.0532748, acc 0.984375\n",
      "2018-05-23T15:18:26.917915: step 17574, loss 0.0229649, acc 0.984375\n",
      "2018-05-23T15:18:27.289917: step 17575, loss 0.0884308, acc 0.96875\n",
      "2018-05-23T15:18:27.629013: step 17576, loss 0.0814877, acc 0.953125\n",
      "2018-05-23T15:18:27.981712: step 17577, loss 0.0742289, acc 0.984375\n",
      "2018-05-23T15:18:28.322799: step 17578, loss 0.0834628, acc 0.96875\n",
      "2018-05-23T15:18:28.662888: step 17579, loss 0.105399, acc 0.96875\n",
      "2018-05-23T15:18:28.996995: step 17580, loss 0.107588, acc 0.953125\n",
      "2018-05-23T15:18:29.339081: step 17581, loss 0.227785, acc 0.921875\n",
      "2018-05-23T15:18:29.676178: step 17582, loss 0.0973805, acc 0.953125\n",
      "2018-05-23T15:18:30.012278: step 17583, loss 0.091421, acc 0.9375\n",
      "2018-05-23T15:18:30.368328: step 17584, loss 0.083611, acc 0.953125\n",
      "2018-05-23T15:18:30.724376: step 17585, loss 0.0445347, acc 0.984375\n",
      "2018-05-23T15:18:31.079426: step 17586, loss 0.169089, acc 0.921875\n",
      "2018-05-23T15:18:31.426508: step 17587, loss 0.0957064, acc 0.96875\n",
      "2018-05-23T15:18:31.906212: step 17588, loss 0.0979796, acc 0.96875\n",
      "2018-05-23T15:18:32.376953: step 17589, loss 0.0400871, acc 1\n",
      "2018-05-23T15:18:32.862653: step 17590, loss 0.0561478, acc 0.96875\n",
      "2018-05-23T15:18:33.249620: step 17591, loss 0.110107, acc 0.953125\n",
      "2018-05-23T15:18:33.598684: step 17592, loss 0.0576697, acc 0.96875\n",
      "2018-05-23T15:18:33.956726: step 17593, loss 0.0653904, acc 0.96875\n",
      "2018-05-23T15:18:34.416497: step 17594, loss 0.0280789, acc 0.984375\n",
      "2018-05-23T15:18:34.782519: step 17595, loss 0.26156, acc 0.875\n",
      "2018-05-23T15:18:35.132580: step 17596, loss 0.0784673, acc 0.984375\n",
      "2018-05-23T15:18:35.477657: step 17597, loss 0.0863777, acc 0.953125\n",
      "2018-05-23T15:18:35.887562: step 17598, loss 0.047744, acc 0.984375\n",
      "2018-05-23T15:18:36.335363: step 17599, loss 0.0162443, acc 1\n",
      "2018-05-23T15:18:37.044474: step 17600, loss 0.121429, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:18:44.640147: step 17600, loss 1.83957, acc 0.714959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17600\n",
      "\n",
      "2018-05-23T15:18:46.031114: step 17601, loss 0.0585504, acc 0.96875\n",
      "2018-05-23T15:18:46.428052: step 17602, loss 0.123355, acc 0.921875\n",
      "2018-05-23T15:18:46.840529: step 17603, loss 0.25154, acc 0.890625\n",
      "2018-05-23T15:18:47.222504: step 17604, loss 0.206531, acc 0.921875\n",
      "2018-05-23T15:18:47.638393: step 17605, loss 0.148632, acc 0.953125\n",
      "2018-05-23T15:18:48.719506: step 17606, loss 0.0809392, acc 0.96875\n",
      "2018-05-23T15:18:49.911313: step 17607, loss 0.0664001, acc 0.96875\n",
      "2018-05-23T15:18:50.434911: step 17608, loss 0.123559, acc 0.9375\n",
      "2018-05-23T15:18:51.034308: step 17609, loss 0.0500126, acc 0.984375\n",
      "2018-05-23T15:18:52.265016: step 17610, loss 0.0926194, acc 0.96875\n",
      "2018-05-23T15:18:52.980103: step 17611, loss 0.0591324, acc 0.984375\n",
      "2018-05-23T15:18:53.387015: step 17612, loss 0.00817197, acc 1\n",
      "2018-05-23T15:18:53.742065: step 17613, loss 0.106378, acc 0.921875\n",
      "2018-05-23T15:18:54.092128: step 17614, loss 0.101242, acc 0.9375\n",
      "2018-05-23T15:18:54.437203: step 17615, loss 0.0537814, acc 0.984375\n",
      "2018-05-23T15:18:54.770313: step 17616, loss 0.129926, acc 0.953125\n",
      "2018-05-23T15:18:55.113396: step 17617, loss 0.0762564, acc 0.96875\n",
      "2018-05-23T15:18:55.447502: step 17618, loss 0.0818196, acc 0.984375\n",
      "2018-05-23T15:18:55.786594: step 17619, loss 0.0939062, acc 0.9375\n",
      "2018-05-23T15:18:56.126686: step 17620, loss 0.144339, acc 0.953125\n",
      "2018-05-23T15:18:56.467771: step 17621, loss 0.239621, acc 0.953125\n",
      "2018-05-23T15:18:56.812848: step 17622, loss 0.128058, acc 0.96875\n",
      "2018-05-23T15:18:57.164908: step 17623, loss 0.0937979, acc 0.96875\n",
      "2018-05-23T15:18:57.558858: step 17624, loss 0.0891591, acc 0.96875\n",
      "2018-05-23T15:18:58.236042: step 17625, loss 0.0809071, acc 0.953125\n",
      "2018-05-23T15:18:58.702793: step 17626, loss 0.0839215, acc 0.9375\n",
      "2018-05-23T15:18:59.127658: step 17627, loss 0.164912, acc 0.9375\n",
      "2018-05-23T15:18:59.524600: step 17628, loss 0.274586, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:18:59.917542: step 17629, loss 0.0837721, acc 0.953125\n",
      "2018-05-23T15:19:00.293537: step 17630, loss 0.0740513, acc 0.953125\n",
      "2018-05-23T15:19:00.640608: step 17631, loss 0.132842, acc 0.953125\n",
      "2018-05-23T15:19:01.013611: step 17632, loss 0.11889, acc 0.96875\n",
      "2018-05-23T15:19:01.475378: step 17633, loss 0.0571961, acc 0.984375\n",
      "2018-05-23T15:19:01.856355: step 17634, loss 0.0535881, acc 1\n",
      "2018-05-23T15:19:02.219384: step 17635, loss 0.0864849, acc 0.96875\n",
      "2018-05-23T15:19:02.583411: step 17636, loss 0.170026, acc 0.90625\n",
      "2018-05-23T15:19:03.286531: step 17637, loss 0.0708403, acc 0.96875\n",
      "2018-05-23T15:19:03.958734: step 17638, loss 0.20796, acc 0.984375\n",
      "2018-05-23T15:19:04.514248: step 17639, loss 0.0462085, acc 0.96875\n",
      "2018-05-23T15:19:04.982992: step 17640, loss 0.0369264, acc 0.984375\n",
      "2018-05-23T15:19:05.587377: step 17641, loss 0.0716024, acc 0.984375\n",
      "2018-05-23T15:19:05.994287: step 17642, loss 0.173842, acc 0.890625\n",
      "2018-05-23T15:19:06.349574: step 17643, loss 0.0795056, acc 0.953125\n",
      "2018-05-23T15:19:06.741523: step 17644, loss 0.123178, acc 0.984375\n",
      "2018-05-23T15:19:07.080616: step 17645, loss 0.0992587, acc 0.96875\n",
      "2018-05-23T15:19:07.446637: step 17646, loss 0.0482852, acc 0.984375\n",
      "2018-05-23T15:19:07.832604: step 17647, loss 0.143358, acc 0.9375\n",
      "2018-05-23T15:19:08.197628: step 17648, loss 0.129662, acc 0.921875\n",
      "2018-05-23T15:19:08.552677: step 17649, loss 0.0479103, acc 1\n",
      "2018-05-23T15:19:08.906730: step 17650, loss 0.0877057, acc 0.953125\n",
      "2018-05-23T15:19:09.244826: step 17651, loss 0.168703, acc 0.921875\n",
      "2018-05-23T15:19:09.590587: step 17652, loss 0.0769709, acc 0.96875\n",
      "2018-05-23T15:19:09.934668: step 17653, loss 0.170845, acc 0.9375\n",
      "2018-05-23T15:19:10.277750: step 17654, loss 0.0841118, acc 0.984375\n",
      "2018-05-23T15:19:10.616844: step 17655, loss 0.109822, acc 0.96875\n",
      "2018-05-23T15:19:10.963914: step 17656, loss 0.0744723, acc 0.984375\n",
      "2018-05-23T15:19:11.297022: step 17657, loss 0.0705186, acc 0.953125\n",
      "2018-05-23T15:19:11.670026: step 17658, loss 0.118099, acc 0.953125\n",
      "2018-05-23T15:19:12.010117: step 17659, loss 0.0478747, acc 1\n",
      "2018-05-23T15:19:12.351202: step 17660, loss 0.123033, acc 0.9375\n",
      "2018-05-23T15:19:12.695281: step 17661, loss 0.0488391, acc 1\n",
      "2018-05-23T15:19:13.039361: step 17662, loss 0.101466, acc 0.9375\n",
      "2018-05-23T15:19:13.377456: step 17663, loss 0.133931, acc 0.96875\n",
      "2018-05-23T15:19:14.023728: step 17664, loss 0.0983082, acc 0.9375\n",
      "2018-05-23T15:19:14.429677: step 17665, loss 0.133494, acc 0.9375\n",
      "2018-05-23T15:19:15.050983: step 17666, loss 0.0479628, acc 0.984375\n",
      "2018-05-23T15:19:15.555629: step 17667, loss 0.136544, acc 0.953125\n",
      "2018-05-23T15:19:15.949576: step 17668, loss 0.0615749, acc 0.984375\n",
      "2018-05-23T15:19:16.596846: step 17669, loss 0.100082, acc 0.953125\n",
      "2018-05-23T15:19:17.091520: step 17670, loss 0.0987084, acc 0.96875\n",
      "2018-05-23T15:19:17.579216: step 17671, loss 0.2066, acc 0.890625\n",
      "2018-05-23T15:19:18.009066: step 17672, loss 0.089836, acc 0.984375\n",
      "2018-05-23T15:19:18.565579: step 17673, loss 0.0835015, acc 0.9375\n",
      "2018-05-23T15:19:19.331530: step 17674, loss 0.0539973, acc 0.984375\n",
      "2018-05-23T15:19:20.044623: step 17675, loss 0.0750903, acc 0.953125\n",
      "2018-05-23T15:19:20.847474: step 17676, loss 0.0676568, acc 0.96875\n",
      "2018-05-23T15:19:21.371116: step 17677, loss 0.2, acc 0.921875\n",
      "2018-05-23T15:19:21.929578: step 17678, loss 0.13091, acc 0.953125\n",
      "2018-05-23T15:19:22.294601: step 17679, loss 0.086819, acc 0.96875\n",
      "2018-05-23T15:19:22.653641: step 17680, loss 0.0839057, acc 0.953125\n",
      "2018-05-23T15:19:23.002709: step 17681, loss 0.0791311, acc 0.96875\n",
      "2018-05-23T15:19:23.345789: step 17682, loss 0.114434, acc 0.96875\n",
      "2018-05-23T15:19:23.695855: step 17683, loss 0.115703, acc 0.9375\n",
      "2018-05-23T15:19:24.043924: step 17684, loss 0.0810704, acc 0.96875\n",
      "2018-05-23T15:19:24.392990: step 17685, loss 0.133322, acc 0.953125\n",
      "2018-05-23T15:19:24.735075: step 17686, loss 0.0606667, acc 0.984375\n",
      "2018-05-23T15:19:25.074198: step 17687, loss 0.0966806, acc 0.953125\n",
      "2018-05-23T15:19:25.415253: step 17688, loss 0.132764, acc 0.953125\n",
      "2018-05-23T15:19:25.762327: step 17689, loss 0.353613, acc 0.921875\n",
      "2018-05-23T15:19:26.103414: step 17690, loss 0.0605862, acc 0.96875\n",
      "2018-05-23T15:19:26.441510: step 17691, loss 0.113115, acc 0.953125\n",
      "2018-05-23T15:19:26.785589: step 17692, loss 0.18066, acc 0.9375\n",
      "2018-05-23T15:19:27.129669: step 17693, loss 0.0628152, acc 0.96875\n",
      "2018-05-23T15:19:27.472749: step 17694, loss 0.116752, acc 0.921875\n",
      "2018-05-23T15:19:27.828796: step 17695, loss 0.0409091, acc 1\n",
      "2018-05-23T15:19:28.171881: step 17696, loss 0.125288, acc 0.9375\n",
      "2018-05-23T15:19:28.519947: step 17697, loss 0.168864, acc 0.90625\n",
      "2018-05-23T15:19:28.875997: step 17698, loss 0.0831786, acc 0.953125\n",
      "2018-05-23T15:19:29.224066: step 17699, loss 0.127034, acc 0.96875\n",
      "2018-05-23T15:19:29.581111: step 17700, loss 0.042273, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:19:36.141558: step 17700, loss 1.8247, acc 0.715245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17700\n",
      "\n",
      "2018-05-23T15:19:37.477983: step 17701, loss 0.127042, acc 0.953125\n",
      "2018-05-23T15:19:37.862955: step 17702, loss 0.0773009, acc 0.953125\n",
      "2018-05-23T15:19:38.223989: step 17703, loss 0.1452, acc 0.921875\n",
      "2018-05-23T15:19:38.567069: step 17704, loss 0.0517708, acc 1\n",
      "2018-05-23T15:19:38.920125: step 17705, loss 0.0584302, acc 0.96875\n",
      "2018-05-23T15:19:39.263208: step 17706, loss 0.181243, acc 0.9375\n",
      "2018-05-23T15:19:39.601302: step 17707, loss 0.102905, acc 0.953125\n",
      "2018-05-23T15:19:39.938403: step 17708, loss 0.0866844, acc 0.953125\n",
      "2018-05-23T15:19:40.284477: step 17709, loss 0.0942254, acc 0.953125\n",
      "2018-05-23T15:19:40.626560: step 17710, loss 0.158478, acc 0.953125\n",
      "2018-05-23T15:19:40.969645: step 17711, loss 0.0326471, acc 0.984375\n",
      "2018-05-23T15:19:41.314754: step 17712, loss 0.0653197, acc 0.953125\n",
      "2018-05-23T15:19:41.657801: step 17713, loss 0.0922804, acc 0.96875\n",
      "2018-05-23T15:19:41.999886: step 17714, loss 0.068864, acc 0.953125\n",
      "2018-05-23T15:19:42.340974: step 17715, loss 0.119282, acc 0.921875\n",
      "2018-05-23T15:19:42.678071: step 17716, loss 0.112959, acc 0.90625\n",
      "2018-05-23T15:19:43.022153: step 17717, loss 0.192389, acc 0.9375\n",
      "2018-05-23T15:19:43.369223: step 17718, loss 0.0605721, acc 0.984375\n",
      "2018-05-23T15:19:43.701337: step 17719, loss 0.0662899, acc 0.96875\n",
      "2018-05-23T15:19:44.042421: step 17720, loss 0.109747, acc 0.953125\n",
      "2018-05-23T15:19:44.384964: step 17721, loss 0.167315, acc 0.953125\n",
      "2018-05-23T15:19:44.740017: step 17722, loss 0.0807519, acc 0.984375\n",
      "2018-05-23T15:19:45.135955: step 17723, loss 0.172007, acc 0.921875\n",
      "2018-05-23T15:19:45.494994: step 17724, loss 0.069171, acc 0.984375\n",
      "2018-05-23T15:19:45.907637: step 17725, loss 0.195087, acc 0.984375\n",
      "2018-05-23T15:19:46.380370: step 17726, loss 0.16172, acc 0.96875\n",
      "2018-05-23T15:19:46.794263: step 17727, loss 0.162357, acc 0.921875\n",
      "2018-05-23T15:19:47.155297: step 17728, loss 0.127637, acc 0.953125\n",
      "2018-05-23T15:19:47.511346: step 17729, loss 0.0705157, acc 0.96875\n",
      "2018-05-23T15:19:47.782622: step 17730, loss 0.309703, acc 0.882353\n",
      "2018-05-23T15:19:48.157618: step 17731, loss 0.0756689, acc 0.953125\n",
      "2018-05-23T15:19:48.558543: step 17732, loss 0.0729598, acc 0.953125\n",
      "2018-05-23T15:19:49.118571: step 17733, loss 0.127288, acc 0.9375\n",
      "2018-05-23T15:19:49.712980: step 17734, loss 0.0793835, acc 0.984375\n",
      "2018-05-23T15:19:50.097952: step 17735, loss 0.0421916, acc 0.96875\n",
      "2018-05-23T15:19:50.471952: step 17736, loss 0.0987225, acc 0.984375\n",
      "2018-05-23T15:19:50.877897: step 17737, loss 0.06518, acc 0.984375\n",
      "2018-05-23T15:19:51.388498: step 17738, loss 0.131739, acc 0.9375\n",
      "2018-05-23T15:19:51.795409: step 17739, loss 0.0906608, acc 0.96875\n",
      "2018-05-23T15:19:52.151457: step 17740, loss 0.120566, acc 0.96875\n",
      "2018-05-23T15:19:52.498529: step 17741, loss 0.0733105, acc 0.96875\n",
      "2018-05-23T15:19:52.865546: step 17742, loss 0.100922, acc 0.953125\n",
      "2018-05-23T15:19:53.241427: step 17743, loss 0.0182423, acc 1\n",
      "2018-05-23T15:19:53.591491: step 17744, loss 0.116907, acc 0.9375\n",
      "2018-05-23T15:19:53.948534: step 17745, loss 0.0441062, acc 1\n",
      "2018-05-23T15:19:54.430248: step 17746, loss 0.0955205, acc 0.953125\n",
      "2018-05-23T15:19:54.870071: step 17747, loss 0.0376176, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:19:55.252049: step 17748, loss 0.146997, acc 0.921875\n",
      "2018-05-23T15:19:55.637018: step 17749, loss 0.01298, acc 1\n",
      "2018-05-23T15:19:56.006030: step 17750, loss 0.0612557, acc 0.984375\n",
      "2018-05-23T15:19:56.381027: step 17751, loss 0.10009, acc 0.953125\n",
      "2018-05-23T15:19:56.744058: step 17752, loss 0.0937361, acc 0.953125\n",
      "2018-05-23T15:19:57.099108: step 17753, loss 0.119245, acc 0.96875\n",
      "2018-05-23T15:19:57.448174: step 17754, loss 0.0388856, acc 0.984375\n",
      "2018-05-23T15:19:57.791615: step 17755, loss 0.0534084, acc 0.984375\n",
      "2018-05-23T15:19:58.135697: step 17756, loss 0.138167, acc 0.96875\n",
      "2018-05-23T15:19:58.471795: step 17757, loss 0.0649718, acc 0.953125\n",
      "2018-05-23T15:19:58.813881: step 17758, loss 0.152301, acc 0.953125\n",
      "2018-05-23T15:19:59.160952: step 17759, loss 0.0926432, acc 0.96875\n",
      "2018-05-23T15:19:59.516999: step 17760, loss 0.0640233, acc 0.984375\n",
      "2018-05-23T15:19:59.866037: step 17761, loss 0.19748, acc 0.953125\n",
      "2018-05-23T15:20:00.234051: step 17762, loss 0.100184, acc 0.9375\n",
      "2018-05-23T15:20:00.581122: step 17763, loss 0.198256, acc 0.9375\n",
      "2018-05-23T15:20:00.936174: step 17764, loss 0.0926649, acc 0.953125\n",
      "2018-05-23T15:20:01.301233: step 17765, loss 0.0401095, acc 0.984375\n",
      "2018-05-23T15:20:01.990389: step 17766, loss 0.0880718, acc 0.953125\n",
      "2018-05-23T15:20:02.481077: step 17767, loss 0.109217, acc 0.9375\n",
      "2018-05-23T15:20:02.902948: step 17768, loss 0.0927219, acc 0.9375\n",
      "2018-05-23T15:20:03.275952: step 17769, loss 0.0397299, acc 1\n",
      "2018-05-23T15:20:03.647957: step 17770, loss 0.0442864, acc 0.984375\n",
      "2018-05-23T15:20:03.999016: step 17771, loss 0.0692006, acc 0.953125\n",
      "2018-05-23T15:20:04.350077: step 17772, loss 0.0616307, acc 0.96875\n",
      "2018-05-23T15:20:04.695154: step 17773, loss 0.0501942, acc 0.984375\n",
      "2018-05-23T15:20:05.051203: step 17774, loss 0.0790541, acc 0.9375\n",
      "2018-05-23T15:20:05.425202: step 17775, loss 0.121336, acc 0.953125\n",
      "2018-05-23T15:20:05.782248: step 17776, loss 0.0313979, acc 0.984375\n",
      "2018-05-23T15:20:06.125868: step 17777, loss 0.173282, acc 0.90625\n",
      "2018-05-23T15:20:06.534781: step 17778, loss 0.0951562, acc 0.9375\n",
      "2018-05-23T15:20:07.113229: step 17779, loss 0.150679, acc 0.953125\n",
      "2018-05-23T15:20:07.636827: step 17780, loss 0.0641427, acc 0.96875\n",
      "2018-05-23T15:20:08.181369: step 17781, loss 0.0637725, acc 1\n",
      "2018-05-23T15:20:08.567336: step 17782, loss 0.102585, acc 0.96875\n",
      "2018-05-23T15:20:08.928373: step 17783, loss 0.101663, acc 0.96875\n",
      "2018-05-23T15:20:09.267463: step 17784, loss 0.105243, acc 0.984375\n",
      "2018-05-23T15:20:09.605811: step 17785, loss 0.0911135, acc 0.984375\n",
      "2018-05-23T15:20:09.966844: step 17786, loss 0.144155, acc 0.921875\n",
      "2018-05-23T15:20:10.309926: step 17787, loss 0.12876, acc 0.921875\n",
      "2018-05-23T15:20:10.649019: step 17788, loss 0.0740422, acc 0.984375\n",
      "2018-05-23T15:20:10.991105: step 17789, loss 0.107446, acc 0.9375\n",
      "2018-05-23T15:20:11.374078: step 17790, loss 0.041166, acc 1\n",
      "2018-05-23T15:20:11.804928: step 17791, loss 0.156339, acc 0.890625\n",
      "2018-05-23T15:20:12.167956: step 17792, loss 0.100816, acc 0.953125\n",
      "2018-05-23T15:20:12.524153: step 17793, loss 0.141623, acc 0.9375\n",
      "2018-05-23T15:20:12.867236: step 17794, loss 0.0799211, acc 0.984375\n",
      "2018-05-23T15:20:13.206331: step 17795, loss 0.0475646, acc 0.984375\n",
      "2018-05-23T15:20:13.561381: step 17796, loss 0.0419275, acc 0.984375\n",
      "2018-05-23T15:20:14.136873: step 17797, loss 0.0614583, acc 0.984375\n",
      "2018-05-23T15:20:14.653457: step 17798, loss 0.100157, acc 0.96875\n",
      "2018-05-23T15:20:15.068347: step 17799, loss 0.0623607, acc 0.953125\n",
      "2018-05-23T15:20:15.436363: step 17800, loss 0.11895, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:20:20.377147: step 17800, loss 1.85908, acc 0.709959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17800\n",
      "\n",
      "2018-05-23T15:20:21.673677: step 17801, loss 0.0475832, acc 0.984375\n",
      "2018-05-23T15:20:22.117492: step 17802, loss 0.196145, acc 0.9375\n",
      "2018-05-23T15:20:22.522407: step 17803, loss 0.0783301, acc 0.96875\n",
      "2018-05-23T15:20:22.876459: step 17804, loss 0.0446385, acc 0.984375\n",
      "2018-05-23T15:20:23.218544: step 17805, loss 0.0848977, acc 0.984375\n",
      "2018-05-23T15:20:23.570606: step 17806, loss 0.0803843, acc 0.96875\n",
      "2018-05-23T15:20:24.223854: step 17807, loss 0.0734043, acc 0.96875\n",
      "2018-05-23T15:20:24.602841: step 17808, loss 0.100587, acc 0.953125\n",
      "2018-05-23T15:20:24.959919: step 17809, loss 0.0511281, acc 0.984375\n",
      "2018-05-23T15:20:25.325906: step 17810, loss 0.0683491, acc 0.96875\n",
      "2018-05-23T15:20:25.685943: step 17811, loss 0.0377203, acc 0.984375\n",
      "2018-05-23T15:20:26.123807: step 17812, loss 0.0763635, acc 0.984375\n",
      "2018-05-23T15:20:26.481814: step 17813, loss 0.109463, acc 0.9375\n",
      "2018-05-23T15:20:26.842848: step 17814, loss 0.0720577, acc 0.96875\n",
      "2018-05-23T15:20:27.187926: step 17815, loss 0.0860121, acc 0.953125\n",
      "2018-05-23T15:20:27.533999: step 17816, loss 0.0465577, acc 0.96875\n",
      "2018-05-23T15:20:27.882068: step 17817, loss 0.0518206, acc 0.96875\n",
      "2018-05-23T15:20:28.227145: step 17818, loss 0.0256295, acc 1\n",
      "2018-05-23T15:20:28.576212: step 17819, loss 0.0987889, acc 0.96875\n",
      "2018-05-23T15:20:28.920294: step 17820, loss 0.0608345, acc 0.96875\n",
      "2018-05-23T15:20:29.276338: step 17821, loss 0.131437, acc 0.96875\n",
      "2018-05-23T15:20:29.641364: step 17822, loss 0.106145, acc 0.921875\n",
      "2018-05-23T15:20:30.022342: step 17823, loss 0.0836511, acc 0.96875\n",
      "2018-05-23T15:20:30.375399: step 17824, loss 0.0287109, acc 1\n",
      "2018-05-23T15:20:30.716485: step 17825, loss 0.0896392, acc 0.96875\n",
      "2018-05-23T15:20:31.059568: step 17826, loss 0.0799151, acc 0.953125\n",
      "2018-05-23T15:20:31.402652: step 17827, loss 0.0887296, acc 0.96875\n",
      "2018-05-23T15:20:31.929244: step 17828, loss 0.0843166, acc 0.953125\n",
      "2018-05-23T15:20:32.355103: step 17829, loss 0.0926233, acc 0.953125\n",
      "2018-05-23T15:20:32.958488: step 17830, loss 0.0459995, acc 1\n",
      "2018-05-23T15:20:33.421253: step 17831, loss 0.0789751, acc 0.96875\n",
      "2018-05-23T15:20:33.786273: step 17832, loss 0.0708192, acc 0.953125\n",
      "2018-05-23T15:20:34.124369: step 17833, loss 0.0337639, acc 1\n",
      "2018-05-23T15:20:34.470445: step 17834, loss 0.0784749, acc 0.96875\n",
      "2018-05-23T15:20:34.814522: step 17835, loss 0.12066, acc 0.953125\n",
      "2018-05-23T15:20:35.148628: step 17836, loss 0.0874004, acc 0.953125\n",
      "2018-05-23T15:20:35.495700: step 17837, loss 0.0630716, acc 0.96875\n",
      "2018-05-23T15:20:35.846763: step 17838, loss 0.0388133, acc 1\n",
      "2018-05-23T15:20:36.189843: step 17839, loss 0.141965, acc 0.953125\n",
      "2018-05-23T15:20:36.544895: step 17840, loss 0.0978695, acc 0.953125\n",
      "2018-05-23T15:20:37.020621: step 17841, loss 0.0650652, acc 0.953125\n",
      "2018-05-23T15:20:37.352068: step 17842, loss 0.0654338, acc 0.96875\n",
      "2018-05-23T15:20:37.694514: step 17843, loss 0.0818802, acc 0.953125\n",
      "2018-05-23T15:20:38.032609: step 17844, loss 0.0504145, acc 0.96875\n",
      "2018-05-23T15:20:38.380680: step 17845, loss 0.0896827, acc 0.953125\n",
      "2018-05-23T15:20:38.732737: step 17846, loss 0.118671, acc 0.96875\n",
      "2018-05-23T15:20:39.075819: step 17847, loss 0.0774707, acc 0.953125\n",
      "2018-05-23T15:20:39.417906: step 17848, loss 0.0368608, acc 0.984375\n",
      "2018-05-23T15:20:39.793900: step 17849, loss 0.0332503, acc 0.984375\n",
      "2018-05-23T15:20:40.172847: step 17850, loss 0.0587357, acc 0.96875\n",
      "2018-05-23T15:20:40.541860: step 17851, loss 0.178396, acc 0.9375\n",
      "2018-05-23T15:20:40.929821: step 17852, loss 0.0664679, acc 0.96875\n",
      "2018-05-23T15:20:41.314794: step 17853, loss 0.139909, acc 0.96875\n",
      "2018-05-23T15:20:41.707743: step 17854, loss 0.0428619, acc 0.984375\n",
      "2018-05-23T15:20:42.281208: step 17855, loss 0.0459482, acc 0.984375\n",
      "2018-05-23T15:20:43.004273: step 17856, loss 0.0339294, acc 0.984375\n",
      "2018-05-23T15:20:43.675477: step 17857, loss 0.0569685, acc 0.984375\n",
      "2018-05-23T15:20:44.099344: step 17858, loss 0.0300197, acc 1\n",
      "2018-05-23T15:20:44.470393: step 17859, loss 0.0320088, acc 1\n",
      "2018-05-23T15:20:44.834419: step 17860, loss 0.262477, acc 0.921875\n",
      "2018-05-23T15:20:45.191463: step 17861, loss 0.103994, acc 0.96875\n",
      "2018-05-23T15:20:45.532552: step 17862, loss 0.071875, acc 0.984375\n",
      "2018-05-23T15:20:45.877626: step 17863, loss 0.200478, acc 0.921875\n",
      "2018-05-23T15:20:46.215722: step 17864, loss 0.0853691, acc 0.96875\n",
      "2018-05-23T15:20:46.559801: step 17865, loss 0.15238, acc 0.953125\n",
      "2018-05-23T15:20:46.906872: step 17866, loss 0.150475, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:20:47.245967: step 17867, loss 0.0580691, acc 0.984375\n",
      "2018-05-23T15:20:47.591043: step 17868, loss 0.0865973, acc 0.953125\n",
      "2018-05-23T15:20:47.940109: step 17869, loss 0.112018, acc 0.953125\n",
      "2018-05-23T15:20:48.302142: step 17870, loss 0.0515349, acc 0.96875\n",
      "2018-05-23T15:20:48.659185: step 17871, loss 0.0800583, acc 0.96875\n",
      "2018-05-23T15:20:49.011246: step 17872, loss 0.0850491, acc 0.953125\n",
      "2018-05-23T15:20:49.460060: step 17873, loss 0.108159, acc 0.953125\n",
      "2018-05-23T15:20:49.894882: step 17874, loss 0.0879108, acc 0.953125\n",
      "2018-05-23T15:20:50.256912: step 17875, loss 0.0809546, acc 0.96875\n",
      "2018-05-23T15:20:50.613956: step 17876, loss 0.0400908, acc 0.984375\n",
      "2018-05-23T15:20:50.969006: step 17877, loss 0.122433, acc 0.96875\n",
      "2018-05-23T15:20:51.313085: step 17878, loss 0.141193, acc 0.96875\n",
      "2018-05-23T15:20:51.655170: step 17879, loss 0.0988978, acc 0.9375\n",
      "2018-05-23T15:20:52.201708: step 17880, loss 0.0333155, acc 1\n",
      "2018-05-23T15:20:52.582971: step 17881, loss 0.0951551, acc 0.953125\n",
      "2018-05-23T15:20:52.937022: step 17882, loss 0.0557026, acc 0.96875\n",
      "2018-05-23T15:20:53.294067: step 17883, loss 0.0574506, acc 0.96875\n",
      "2018-05-23T15:20:53.633159: step 17884, loss 0.127199, acc 0.9375\n",
      "2018-05-23T15:20:53.976244: step 17885, loss 0.136313, acc 0.921875\n",
      "2018-05-23T15:20:54.327304: step 17886, loss 0.157298, acc 0.921875\n",
      "2018-05-23T15:20:54.701301: step 17887, loss 0.0803751, acc 0.953125\n",
      "2018-05-23T15:20:55.039399: step 17888, loss 0.100748, acc 0.953125\n",
      "2018-05-23T15:20:55.393400: step 17889, loss 0.0470323, acc 1\n",
      "2018-05-23T15:20:55.727509: step 17890, loss 0.0827864, acc 0.96875\n",
      "2018-05-23T15:20:56.069289: step 17891, loss 0.0411012, acc 0.984375\n",
      "2018-05-23T15:20:56.641610: step 17892, loss 0.0661086, acc 0.96875\n",
      "2018-05-23T15:20:57.025586: step 17893, loss 0.0231032, acc 1\n",
      "2018-05-23T15:20:57.389613: step 17894, loss 0.107349, acc 0.921875\n",
      "2018-05-23T15:20:57.819461: step 17895, loss 0.0581086, acc 0.984375\n",
      "2018-05-23T15:20:58.300176: step 17896, loss 0.0349533, acc 0.984375\n",
      "2018-05-23T15:20:58.685144: step 17897, loss 0.112874, acc 0.953125\n",
      "2018-05-23T15:20:59.046178: step 17898, loss 0.110387, acc 0.96875\n",
      "2018-05-23T15:20:59.394247: step 17899, loss 0.0176674, acc 1\n",
      "2018-05-23T15:20:59.882941: step 17900, loss 0.0486274, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:21:04.888550: step 17900, loss 1.89368, acc 0.714102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-17900\n",
      "\n",
      "2018-05-23T15:21:06.995622: step 17901, loss 0.0717158, acc 0.96875\n",
      "2018-05-23T15:21:07.439418: step 17902, loss 0.0611415, acc 0.96875\n",
      "2018-05-23T15:21:07.921133: step 17903, loss 0.0997779, acc 0.953125\n",
      "2018-05-23T15:21:08.481633: step 17904, loss 0.123864, acc 0.921875\n",
      "2018-05-23T15:21:08.918461: step 17905, loss 0.0872679, acc 0.953125\n",
      "2018-05-23T15:21:09.300439: step 17906, loss 0.0809928, acc 0.984375\n",
      "2018-05-23T15:21:09.650503: step 17907, loss 0.131391, acc 0.953125\n",
      "2018-05-23T15:21:10.000566: step 17908, loss 0.169013, acc 0.9375\n",
      "2018-05-23T15:21:10.361285: step 17909, loss 0.215292, acc 0.875\n",
      "2018-05-23T15:21:10.712347: step 17910, loss 0.0726071, acc 0.96875\n",
      "2018-05-23T15:21:11.062411: step 17911, loss 0.029299, acc 0.984375\n",
      "2018-05-23T15:21:11.408482: step 17912, loss 0.102247, acc 0.921875\n",
      "2018-05-23T15:21:11.763533: step 17913, loss 0.0748919, acc 0.96875\n",
      "2018-05-23T15:21:12.110645: step 17914, loss 0.14845, acc 0.96875\n",
      "2018-05-23T15:21:12.450736: step 17915, loss 0.0409346, acc 0.984375\n",
      "2018-05-23T15:21:12.796809: step 17916, loss 0.115015, acc 0.96875\n",
      "2018-05-23T15:21:13.365291: step 17917, loss 0.0746637, acc 0.953125\n",
      "2018-05-23T15:21:14.174125: step 17918, loss 0.0363135, acc 0.984375\n",
      "2018-05-23T15:21:14.759559: step 17919, loss 0.012546, acc 1\n",
      "2018-05-23T15:21:15.182428: step 17920, loss 0.0718085, acc 0.96875\n",
      "2018-05-23T15:21:15.574381: step 17921, loss 0.0377284, acc 0.984375\n",
      "2018-05-23T15:21:15.937409: step 17922, loss 0.0865816, acc 0.953125\n",
      "2018-05-23T15:21:16.280491: step 17923, loss 0.173906, acc 0.9375\n",
      "2018-05-23T15:21:16.648506: step 17924, loss 0.0967335, acc 0.953125\n",
      "2018-05-23T15:21:16.994580: step 17925, loss 0.112001, acc 0.96875\n",
      "2018-05-23T15:21:17.353620: step 17926, loss 0.0559976, acc 0.984375\n",
      "2018-05-23T15:21:17.695704: step 17927, loss 0.156701, acc 0.90625\n",
      "2018-05-23T15:21:18.045803: step 17928, loss 0.107485, acc 0.921875\n",
      "2018-05-23T15:21:18.437720: step 17929, loss 0.128452, acc 0.96875\n",
      "2018-05-23T15:21:18.789777: step 17930, loss 0.0543856, acc 0.984375\n",
      "2018-05-23T15:21:19.144829: step 17931, loss 0.0926556, acc 0.9375\n",
      "2018-05-23T15:21:19.495888: step 17932, loss 0.0650162, acc 0.984375\n",
      "2018-05-23T15:21:19.831989: step 17933, loss 0.0662734, acc 0.953125\n",
      "2018-05-23T15:21:20.514165: step 17934, loss 0.0425357, acc 1\n",
      "2018-05-23T15:21:21.061735: step 17935, loss 0.0644457, acc 0.984375\n",
      "2018-05-23T15:21:21.601246: step 17936, loss 0.191553, acc 0.953125\n",
      "2018-05-23T15:21:22.165740: step 17937, loss 0.0515181, acc 0.984375\n",
      "2018-05-23T15:21:22.619522: step 17938, loss 0.118635, acc 0.96875\n",
      "2018-05-23T15:21:23.133146: step 17939, loss 0.0835189, acc 0.9375\n",
      "2018-05-23T15:21:24.026759: step 17940, loss 0.0275708, acc 0.984375\n",
      "2018-05-23T15:21:24.515448: step 17941, loss 0.02372, acc 1\n",
      "2018-05-23T15:21:25.213583: step 17942, loss 0.0582462, acc 0.984375\n",
      "2018-05-23T15:21:26.062311: step 17943, loss 0.0678048, acc 0.984375\n",
      "2018-05-23T15:21:26.859183: step 17944, loss 0.139884, acc 0.921875\n",
      "2018-05-23T15:21:27.708918: step 17945, loss 0.0796961, acc 0.953125\n",
      "2018-05-23T15:21:28.237494: step 17946, loss 0.0986309, acc 0.96875\n",
      "2018-05-23T15:21:28.628446: step 17947, loss 0.117744, acc 0.9375\n",
      "2018-05-23T15:21:28.986488: step 17948, loss 0.117209, acc 0.96875\n",
      "2018-05-23T15:21:29.332562: step 17949, loss 0.0784856, acc 0.96875\n",
      "2018-05-23T15:21:29.726510: step 17950, loss 0.0623494, acc 0.984375\n",
      "2018-05-23T15:21:30.395718: step 17951, loss 0.0841015, acc 0.921875\n",
      "2018-05-23T15:21:31.177626: step 17952, loss 0.0925884, acc 0.9375\n",
      "2018-05-23T15:21:32.163991: step 17953, loss 0.0962545, acc 0.9375\n",
      "2018-05-23T15:21:32.814247: step 17954, loss 0.0840262, acc 0.953125\n",
      "2018-05-23T15:21:33.247090: step 17955, loss 0.0368822, acc 1\n",
      "2018-05-23T15:21:33.614108: step 17956, loss 0.163691, acc 0.921875\n",
      "2018-05-23T15:21:33.966168: step 17957, loss 0.12975, acc 0.9375\n",
      "2018-05-23T15:21:34.346149: step 17958, loss 0.0971665, acc 0.953125\n",
      "2018-05-23T15:21:34.844819: step 17959, loss 0.0675843, acc 0.984375\n",
      "2018-05-23T15:21:35.378390: step 17960, loss 0.0464733, acc 1\n",
      "2018-05-23T15:21:35.771339: step 17961, loss 0.118098, acc 0.9375\n",
      "2018-05-23T15:21:36.129381: step 17962, loss 0.107247, acc 0.984375\n",
      "2018-05-23T15:21:36.469472: step 17963, loss 0.0976223, acc 0.921875\n",
      "2018-05-23T15:21:36.823525: step 17964, loss 0.0719478, acc 0.96875\n",
      "2018-05-23T15:21:37.161464: step 17965, loss 0.0944826, acc 0.953125\n",
      "2018-05-23T15:21:37.510529: step 17966, loss 0.0817338, acc 0.9375\n",
      "2018-05-23T15:21:37.867577: step 17967, loss 0.0382314, acc 1\n",
      "2018-05-23T15:21:38.201681: step 17968, loss 0.114846, acc 0.953125\n",
      "2018-05-23T15:21:38.533795: step 17969, loss 0.209163, acc 0.90625\n",
      "2018-05-23T15:21:38.876874: step 17970, loss 0.0789985, acc 0.96875\n",
      "2018-05-23T15:21:39.216967: step 17971, loss 0.0400459, acc 0.96875\n",
      "2018-05-23T15:21:39.560046: step 17972, loss 0.0650341, acc 0.984375\n",
      "2018-05-23T15:21:39.904129: step 17973, loss 0.0510082, acc 0.953125\n",
      "2018-05-23T15:21:40.241226: step 17974, loss 0.0332623, acc 0.984375\n",
      "2018-05-23T15:21:40.579320: step 17975, loss 0.0491555, acc 1\n",
      "2018-05-23T15:21:40.925394: step 17976, loss 0.134213, acc 0.9375\n",
      "2018-05-23T15:21:41.262493: step 17977, loss 0.0979948, acc 0.9375\n",
      "2018-05-23T15:21:41.736229: step 17978, loss 0.0592106, acc 0.96875\n",
      "2018-05-23T15:21:42.143139: step 17979, loss 0.0915001, acc 0.9375\n",
      "2018-05-23T15:21:42.508163: step 17980, loss 0.0790916, acc 0.96875\n",
      "2018-05-23T15:21:42.855231: step 17981, loss 0.0426863, acc 1\n",
      "2018-05-23T15:21:43.206295: step 17982, loss 0.115228, acc 0.9375\n",
      "2018-05-23T15:21:43.555361: step 17983, loss 0.0644012, acc 0.96875\n",
      "2018-05-23T15:21:43.921380: step 17984, loss 0.0801648, acc 0.953125\n",
      "2018-05-23T15:21:44.272441: step 17985, loss 0.0357638, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:21:44.627490: step 17986, loss 0.11311, acc 0.953125\n",
      "2018-05-23T15:21:44.967580: step 17987, loss 0.0725372, acc 0.96875\n",
      "2018-05-23T15:21:45.314652: step 17988, loss 0.0989221, acc 0.953125\n",
      "2018-05-23T15:21:45.661725: step 17989, loss 0.177776, acc 0.9375\n",
      "2018-05-23T15:21:46.011787: step 17990, loss 0.227758, acc 0.9375\n",
      "2018-05-23T15:21:46.384790: step 17991, loss 0.0813308, acc 0.984375\n",
      "2018-05-23T15:21:46.790706: step 17992, loss 0.118906, acc 0.953125\n",
      "2018-05-23T15:21:47.250473: step 17993, loss 0.0883857, acc 0.96875\n",
      "2018-05-23T15:21:47.677367: step 17994, loss 0.166781, acc 0.953125\n",
      "2018-05-23T15:21:48.074269: step 17995, loss 0.0559971, acc 0.984375\n",
      "2018-05-23T15:21:48.454253: step 17996, loss 0.0851328, acc 0.96875\n",
      "2018-05-23T15:21:48.820791: step 17997, loss 0.137247, acc 0.984375\n",
      "2018-05-23T15:21:49.171850: step 17998, loss 0.0479388, acc 0.96875\n",
      "2018-05-23T15:21:49.529893: step 17999, loss 0.0827013, acc 0.953125\n",
      "2018-05-23T15:21:49.869982: step 18000, loss 0.0472276, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:21:55.655506: step 18000, loss 1.92591, acc 0.711959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18000\n",
      "\n",
      "2018-05-23T15:21:57.045787: step 18001, loss 0.088707, acc 0.96875\n",
      "2018-05-23T15:21:57.429760: step 18002, loss 0.0772582, acc 0.96875\n",
      "2018-05-23T15:21:57.818721: step 18003, loss 0.0875221, acc 0.96875\n",
      "2018-05-23T15:21:58.163795: step 18004, loss 0.0738021, acc 0.953125\n",
      "2018-05-23T15:21:58.504882: step 18005, loss 0.0748983, acc 0.953125\n",
      "2018-05-23T15:21:58.875892: step 18006, loss 0.0404158, acc 0.984375\n",
      "2018-05-23T15:21:59.216977: step 18007, loss 0.0813276, acc 0.96875\n",
      "2018-05-23T15:21:59.556070: step 18008, loss 0.0658598, acc 0.96875\n",
      "2018-05-23T15:21:59.906136: step 18009, loss 0.0238596, acc 1\n",
      "2018-05-23T15:22:00.261184: step 18010, loss 0.0762233, acc 0.96875\n",
      "2018-05-23T15:22:00.601274: step 18011, loss 0.0748972, acc 0.96875\n",
      "2018-05-23T15:22:01.118914: step 18012, loss 0.125783, acc 0.9375\n",
      "2018-05-23T15:22:01.845650: step 18013, loss 0.103524, acc 0.96875\n",
      "2018-05-23T15:22:02.633541: step 18014, loss 0.110747, acc 0.984375\n",
      "2018-05-23T15:22:03.072367: step 18015, loss 0.0856025, acc 0.96875\n",
      "2018-05-23T15:22:03.539120: step 18016, loss 0.0709, acc 0.953125\n",
      "2018-05-23T15:22:04.149492: step 18017, loss 0.0150096, acc 1\n",
      "2018-05-23T15:22:04.758855: step 18018, loss 0.0524509, acc 0.984375\n",
      "2018-05-23T15:22:05.397160: step 18019, loss 0.163745, acc 0.9375\n",
      "2018-05-23T15:22:06.105254: step 18020, loss 0.0486386, acc 0.984375\n",
      "2018-05-23T15:22:06.781445: step 18021, loss 0.142323, acc 0.953125\n",
      "2018-05-23T15:22:07.203316: step 18022, loss 0.145941, acc 0.9375\n",
      "2018-05-23T15:22:07.572329: step 18023, loss 0.0895754, acc 0.96875\n",
      "2018-05-23T15:22:07.919402: step 18024, loss 0.133381, acc 0.921875\n",
      "2018-05-23T15:22:08.261929: step 18025, loss 0.135471, acc 0.953125\n",
      "2018-05-23T15:22:08.655872: step 18026, loss 0.0873759, acc 0.9375\n",
      "2018-05-23T15:22:09.017904: step 18027, loss 0.0735494, acc 0.96875\n",
      "2018-05-23T15:22:09.367967: step 18028, loss 0.0614868, acc 0.96875\n",
      "2018-05-23T15:22:09.721022: step 18029, loss 0.151626, acc 0.953125\n",
      "2018-05-23T15:22:10.055129: step 18030, loss 0.0474524, acc 0.984375\n",
      "2018-05-23T15:22:10.395222: step 18031, loss 0.108277, acc 0.9375\n",
      "2018-05-23T15:22:10.746280: step 18032, loss 0.12203, acc 0.96875\n",
      "2018-05-23T15:22:11.085373: step 18033, loss 0.0998543, acc 0.9375\n",
      "2018-05-23T15:22:11.424468: step 18034, loss 0.0848481, acc 0.96875\n",
      "2018-05-23T15:22:11.760569: step 18035, loss 0.0266254, acc 1\n",
      "2018-05-23T15:22:12.098662: step 18036, loss 0.0731393, acc 0.96875\n",
      "2018-05-23T15:22:12.447729: step 18037, loss 0.132746, acc 0.984375\n",
      "2018-05-23T15:22:12.814746: step 18038, loss 0.120977, acc 0.96875\n",
      "2018-05-23T15:22:13.153839: step 18039, loss 0.0401331, acc 0.96875\n",
      "2018-05-23T15:22:13.497920: step 18040, loss 0.0996389, acc 0.921875\n",
      "2018-05-23T15:22:13.857955: step 18041, loss 0.044835, acc 0.984375\n",
      "2018-05-23T15:22:14.199044: step 18042, loss 0.157498, acc 0.953125\n",
      "2018-05-23T15:22:14.538139: step 18043, loss 0.0241272, acc 1\n",
      "2018-05-23T15:22:14.898173: step 18044, loss 0.0335833, acc 1\n",
      "2018-05-23T15:22:15.238263: step 18045, loss 0.0741183, acc 0.953125\n",
      "2018-05-23T15:22:15.582342: step 18046, loss 0.0491569, acc 0.984375\n",
      "2018-05-23T15:22:15.927422: step 18047, loss 0.0929924, acc 0.953125\n",
      "2018-05-23T15:22:16.266512: step 18048, loss 0.0778401, acc 0.984375\n",
      "2018-05-23T15:22:16.628544: step 18049, loss 0.0800027, acc 0.953125\n",
      "2018-05-23T15:22:16.984594: step 18050, loss 0.0831839, acc 0.96875\n",
      "2018-05-23T15:22:17.328671: step 18051, loss 0.0704905, acc 0.984375\n",
      "2018-05-23T15:22:17.670755: step 18052, loss 0.0575686, acc 0.984375\n",
      "2018-05-23T15:22:18.023812: step 18053, loss 0.0524834, acc 1\n",
      "2018-05-23T15:22:18.378794: step 18054, loss 0.0339975, acc 0.984375\n",
      "2018-05-23T15:22:18.720881: step 18055, loss 0.0701964, acc 0.953125\n",
      "2018-05-23T15:22:19.071942: step 18056, loss 0.0955916, acc 0.953125\n",
      "2018-05-23T15:22:19.411067: step 18057, loss 0.0492096, acc 0.984375\n",
      "2018-05-23T15:22:19.755115: step 18058, loss 0.0994394, acc 0.953125\n",
      "2018-05-23T15:22:20.102184: step 18059, loss 0.0328945, acc 0.984375\n",
      "2018-05-23T15:22:20.447262: step 18060, loss 0.0786276, acc 0.96875\n",
      "2018-05-23T15:22:20.797791: step 18061, loss 0.0570539, acc 0.984375\n",
      "2018-05-23T15:22:21.147854: step 18062, loss 0.0691032, acc 0.96875\n",
      "2018-05-23T15:22:21.490940: step 18063, loss 0.120636, acc 0.96875\n",
      "2018-05-23T15:22:21.861944: step 18064, loss 0.0655733, acc 0.984375\n",
      "2018-05-23T15:22:22.216994: step 18065, loss 0.158941, acc 0.953125\n",
      "2018-05-23T15:22:22.563068: step 18066, loss 0.0366451, acc 0.984375\n",
      "2018-05-23T15:22:22.912134: step 18067, loss 0.0606358, acc 0.984375\n",
      "2018-05-23T15:22:23.266187: step 18068, loss 0.0462118, acc 0.984375\n",
      "2018-05-23T15:22:23.607275: step 18069, loss 0.0535196, acc 0.984375\n",
      "2018-05-23T15:22:23.969307: step 18070, loss 0.0701342, acc 0.96875\n",
      "2018-05-23T15:22:24.328347: step 18071, loss 0.279135, acc 0.875\n",
      "2018-05-23T15:22:24.676417: step 18072, loss 0.0910594, acc 0.96875\n",
      "2018-05-23T15:22:25.025481: step 18073, loss 0.0646477, acc 0.96875\n",
      "2018-05-23T15:22:25.372552: step 18074, loss 0.192223, acc 0.953125\n",
      "2018-05-23T15:22:25.718626: step 18075, loss 0.166364, acc 0.921875\n",
      "2018-05-23T15:22:26.072679: step 18076, loss 0.0577008, acc 0.96875\n",
      "2018-05-23T15:22:26.433714: step 18077, loss 0.06127, acc 0.96875\n",
      "2018-05-23T15:22:26.781782: step 18078, loss 0.0728737, acc 0.96875\n",
      "2018-05-23T15:22:27.133840: step 18079, loss 0.0878422, acc 0.953125\n",
      "2018-05-23T15:22:27.485899: step 18080, loss 0.0485331, acc 0.96875\n",
      "2018-05-23T15:22:27.841220: step 18081, loss 0.185072, acc 0.9375\n",
      "2018-05-23T15:22:28.192862: step 18082, loss 0.109199, acc 0.96875\n",
      "2018-05-23T15:22:28.550905: step 18083, loss 0.057307, acc 0.984375\n",
      "2018-05-23T15:22:28.911940: step 18084, loss 0.0396107, acc 1\n",
      "2018-05-23T15:22:29.261003: step 18085, loss 0.127847, acc 0.921875\n",
      "2018-05-23T15:22:29.618048: step 18086, loss 0.060831, acc 0.96875\n",
      "2018-05-23T15:22:29.979082: step 18087, loss 0.0370838, acc 1\n",
      "2018-05-23T15:22:30.335132: step 18088, loss 0.0485407, acc 0.96875\n",
      "2018-05-23T15:22:30.710126: step 18089, loss 0.0791425, acc 0.96875\n",
      "2018-05-23T15:22:31.083131: step 18090, loss 0.026728, acc 0.984375\n",
      "2018-05-23T15:22:31.442171: step 18091, loss 0.0636533, acc 0.96875\n",
      "2018-05-23T15:22:31.824148: step 18092, loss 0.135698, acc 0.9375\n",
      "2018-05-23T15:22:32.315831: step 18093, loss 0.0335335, acc 0.984375\n",
      "2018-05-23T15:22:32.880320: step 18094, loss 0.0695925, acc 0.953125\n",
      "2018-05-23T15:22:33.470742: step 18095, loss 0.186254, acc 0.96875\n",
      "2018-05-23T15:22:33.891616: step 18096, loss 0.23496, acc 0.953125\n",
      "2018-05-23T15:22:34.270602: step 18097, loss 0.118277, acc 0.96875\n",
      "2018-05-23T15:22:34.626649: step 18098, loss 0.0873217, acc 0.96875\n",
      "2018-05-23T15:22:35.128885: step 18099, loss 0.0861913, acc 0.953125\n",
      "2018-05-23T15:22:35.528567: step 18100, loss 0.0638706, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:22:40.253926: step 18100, loss 1.92452, acc 0.713673\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18100\n",
      "\n",
      "2018-05-23T15:22:42.746629: step 18101, loss 0.0945701, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:22:43.067770: step 18102, loss 0.0437237, acc 0.96875\n",
      "2018-05-23T15:22:43.403870: step 18103, loss 0.120371, acc 0.96875\n",
      "2018-05-23T15:22:44.008254: step 18104, loss 0.151745, acc 0.953125\n",
      "2018-05-23T15:22:44.466031: step 18105, loss 0.172375, acc 0.90625\n",
      "2018-05-23T15:22:45.212036: step 18106, loss 0.0948952, acc 0.953125\n",
      "2018-05-23T15:22:45.645872: step 18107, loss 0.131221, acc 0.921875\n",
      "2018-05-23T15:22:46.041814: step 18108, loss 0.105253, acc 0.953125\n",
      "2018-05-23T15:22:46.401851: step 18109, loss 0.0400015, acc 1\n",
      "2018-05-23T15:22:46.752912: step 18110, loss 0.0528807, acc 0.984375\n",
      "2018-05-23T15:22:47.128908: step 18111, loss 0.0665348, acc 0.96875\n",
      "2018-05-23T15:22:47.483959: step 18112, loss 0.0375047, acc 1\n",
      "2018-05-23T15:22:47.832024: step 18113, loss 0.127877, acc 0.921875\n",
      "2018-05-23T15:22:48.205028: step 18114, loss 0.0690408, acc 0.96875\n",
      "2018-05-23T15:22:48.558082: step 18115, loss 0.180149, acc 0.953125\n",
      "2018-05-23T15:22:48.913132: step 18116, loss 0.0329801, acc 1\n",
      "2018-05-23T15:22:49.287132: step 18117, loss 0.0672919, acc 0.984375\n",
      "2018-05-23T15:22:49.643181: step 18118, loss 0.200375, acc 0.90625\n",
      "2018-05-23T15:22:49.991248: step 18119, loss 0.0584799, acc 0.96875\n",
      "2018-05-23T15:22:50.342310: step 18120, loss 0.0408946, acc 0.984375\n",
      "2018-05-23T15:22:50.685391: step 18121, loss 0.164746, acc 0.921875\n",
      "2018-05-23T15:22:51.031468: step 18122, loss 0.0194303, acc 1\n",
      "2018-05-23T15:22:51.385518: step 18123, loss 0.107871, acc 0.953125\n",
      "2018-05-23T15:22:51.729600: step 18124, loss 0.087656, acc 0.953125\n",
      "2018-05-23T15:22:52.086644: step 18125, loss 0.0483227, acc 0.984375\n",
      "2018-05-23T15:22:52.436708: step 18126, loss 0.0513509, acc 0.984375\n",
      "2018-05-23T15:22:52.787767: step 18127, loss 0.0383607, acc 1\n",
      "2018-05-23T15:22:53.144812: step 18128, loss 0.150262, acc 0.921875\n",
      "2018-05-23T15:22:53.493880: step 18129, loss 0.053455, acc 0.96875\n",
      "2018-05-23T15:22:53.850922: step 18130, loss 0.118649, acc 0.953125\n",
      "2018-05-23T15:22:54.220933: step 18131, loss 0.0338134, acc 0.984375\n",
      "2018-05-23T15:22:54.585956: step 18132, loss 0.0613892, acc 0.984375\n",
      "2018-05-23T15:22:54.939048: step 18133, loss 0.0403556, acc 0.96875\n",
      "2018-05-23T15:22:55.291070: step 18134, loss 0.110174, acc 0.984375\n",
      "2018-05-23T15:22:55.640139: step 18135, loss 0.1353, acc 0.9375\n",
      "2018-05-23T15:22:55.991196: step 18136, loss 0.120156, acc 0.953125\n",
      "2018-05-23T15:22:56.338270: step 18137, loss 0.0604574, acc 0.984375\n",
      "2018-05-23T15:22:56.689331: step 18138, loss 0.150162, acc 0.953125\n",
      "2018-05-23T15:22:57.037398: step 18139, loss 0.0900499, acc 0.96875\n",
      "2018-05-23T15:22:57.385626: step 18140, loss 0.117655, acc 0.953125\n",
      "2018-05-23T15:22:57.741283: step 18141, loss 0.0437201, acc 0.984375\n",
      "2018-05-23T15:22:58.085363: step 18142, loss 0.106707, acc 0.9375\n",
      "2018-05-23T15:22:58.440413: step 18143, loss 0.0446628, acc 1\n",
      "2018-05-23T15:22:58.790478: step 18144, loss 0.0613824, acc 0.984375\n",
      "2018-05-23T15:22:59.136551: step 18145, loss 0.104742, acc 0.9375\n",
      "2018-05-23T15:22:59.519527: step 18146, loss 0.0487159, acc 0.984375\n",
      "2018-05-23T15:22:59.877568: step 18147, loss 0.0755705, acc 0.953125\n",
      "2018-05-23T15:23:00.237605: step 18148, loss 0.152539, acc 0.921875\n",
      "2018-05-23T15:23:00.592655: step 18149, loss 0.0678309, acc 0.96875\n",
      "2018-05-23T15:23:00.958677: step 18150, loss 0.0829108, acc 0.9375\n",
      "2018-05-23T15:23:01.308078: step 18151, loss 0.0551779, acc 0.984375\n",
      "2018-05-23T15:23:01.651153: step 18152, loss 0.104231, acc 0.953125\n",
      "2018-05-23T15:23:02.005181: step 18153, loss 0.0243244, acc 1\n",
      "2018-05-23T15:23:02.350152: step 18154, loss 0.096621, acc 0.953125\n",
      "2018-05-23T15:23:02.690241: step 18155, loss 0.0818839, acc 0.9375\n",
      "2018-05-23T15:23:03.038308: step 18156, loss 0.0641851, acc 0.984375\n",
      "2018-05-23T15:23:03.412308: step 18157, loss 0.0141993, acc 1\n",
      "2018-05-23T15:23:03.774340: step 18158, loss 0.0561151, acc 0.953125\n",
      "2018-05-23T15:23:04.139364: step 18159, loss 0.0567697, acc 0.96875\n",
      "2018-05-23T15:23:04.490427: step 18160, loss 0.0950696, acc 0.96875\n",
      "2018-05-23T15:23:04.855447: step 18161, loss 0.019615, acc 1\n",
      "2018-05-23T15:23:05.223464: step 18162, loss 0.0844123, acc 0.9375\n",
      "2018-05-23T15:23:05.571532: step 18163, loss 0.0800051, acc 0.953125\n",
      "2018-05-23T15:23:05.916612: step 18164, loss 0.0668142, acc 0.96875\n",
      "2018-05-23T15:23:06.273656: step 18165, loss 0.0540481, acc 0.96875\n",
      "2018-05-23T15:23:06.629700: step 18166, loss 0.0567329, acc 0.96875\n",
      "2018-05-23T15:23:07.129367: step 18167, loss 0.121696, acc 0.9375\n",
      "2018-05-23T15:23:07.486409: step 18168, loss 0.10338, acc 0.953125\n",
      "2018-05-23T15:23:07.838468: step 18169, loss 0.0894085, acc 0.9375\n",
      "2018-05-23T15:23:08.184541: step 18170, loss 0.131976, acc 0.953125\n",
      "2018-05-23T15:23:08.539591: step 18171, loss 0.0931936, acc 0.96875\n",
      "2018-05-23T15:23:08.878684: step 18172, loss 0.0610225, acc 0.96875\n",
      "2018-05-23T15:23:09.221767: step 18173, loss 0.0724725, acc 0.984375\n",
      "2018-05-23T15:23:09.572829: step 18174, loss 0.114234, acc 0.953125\n",
      "2018-05-23T15:23:09.917904: step 18175, loss 0.0366766, acc 0.984375\n",
      "2018-05-23T15:23:10.267968: step 18176, loss 0.0783857, acc 0.9375\n",
      "2018-05-23T15:23:10.617035: step 18177, loss 0.0890928, acc 0.9375\n",
      "2018-05-23T15:23:10.978068: step 18178, loss 0.0571927, acc 0.984375\n",
      "2018-05-23T15:23:11.326140: step 18179, loss 0.0989387, acc 0.9375\n",
      "2018-05-23T15:23:11.669220: step 18180, loss 0.0814374, acc 0.984375\n",
      "2018-05-23T15:23:12.021280: step 18181, loss 0.128409, acc 0.9375\n",
      "2018-05-23T15:23:12.375334: step 18182, loss 0.109929, acc 0.953125\n",
      "2018-05-23T15:23:12.723401: step 18183, loss 0.0909401, acc 0.921875\n",
      "2018-05-23T15:23:13.075457: step 18184, loss 0.0700048, acc 0.96875\n",
      "2018-05-23T15:23:13.422963: step 18185, loss 0.0546707, acc 0.984375\n",
      "2018-05-23T15:23:13.764054: step 18186, loss 0.16637, acc 0.953125\n",
      "2018-05-23T15:23:14.126617: step 18187, loss 0.0991111, acc 0.96875\n",
      "2018-05-23T15:23:14.478674: step 18188, loss 0.0776375, acc 0.953125\n",
      "2018-05-23T15:23:14.848686: step 18189, loss 0.119996, acc 0.9375\n",
      "2018-05-23T15:23:15.227671: step 18190, loss 0.135562, acc 0.9375\n",
      "2018-05-23T15:23:15.578732: step 18191, loss 0.0871841, acc 0.96875\n",
      "2018-05-23T15:23:15.928795: step 18192, loss 0.0970349, acc 0.96875\n",
      "2018-05-23T15:23:16.283845: step 18193, loss 0.147218, acc 0.953125\n",
      "2018-05-23T15:23:16.631914: step 18194, loss 0.0522781, acc 0.96875\n",
      "2018-05-23T15:23:16.990953: step 18195, loss 0.0784544, acc 0.96875\n",
      "2018-05-23T15:23:17.347999: step 18196, loss 0.0321439, acc 1\n",
      "2018-05-23T15:23:17.695070: step 18197, loss 0.0846999, acc 0.96875\n",
      "2018-05-23T15:23:18.045137: step 18198, loss 0.0436807, acc 0.984375\n",
      "2018-05-23T15:23:18.454040: step 18199, loss 0.0535736, acc 0.96875\n",
      "2018-05-23T15:23:18.808094: step 18200, loss 0.123811, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:23:23.467072: step 18200, loss 1.91869, acc 0.713388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18200\n",
      "\n",
      "2018-05-23T15:23:24.746650: step 18201, loss 0.0561771, acc 0.953125\n",
      "2018-05-23T15:23:25.198441: step 18202, loss 0.142132, acc 0.9375\n",
      "2018-05-23T15:23:25.585409: step 18203, loss 0.0531171, acc 0.96875\n",
      "2018-05-23T15:23:25.969381: step 18204, loss 0.116354, acc 0.9375\n",
      "2018-05-23T15:23:26.335402: step 18205, loss 0.0841119, acc 0.96875\n",
      "2018-05-23T15:23:26.698430: step 18206, loss 0.063977, acc 0.953125\n",
      "2018-05-23T15:23:27.059464: step 18207, loss 0.120163, acc 0.9375\n",
      "2018-05-23T15:23:27.404541: step 18208, loss 0.0857711, acc 0.921875\n",
      "2018-05-23T15:23:27.758592: step 18209, loss 0.107563, acc 0.953125\n",
      "2018-05-23T15:23:28.109655: step 18210, loss 0.0424506, acc 0.984375\n",
      "2018-05-23T15:23:28.457724: step 18211, loss 0.0616131, acc 0.984375\n",
      "2018-05-23T15:23:28.814769: step 18212, loss 0.136679, acc 0.921875\n",
      "2018-05-23T15:23:29.164830: step 18213, loss 0.0570988, acc 0.96875\n",
      "2018-05-23T15:23:29.523872: step 18214, loss 0.0666581, acc 0.953125\n",
      "2018-05-23T15:23:29.880914: step 18215, loss 0.200954, acc 0.9375\n",
      "2018-05-23T15:23:30.222004: step 18216, loss 0.046834, acc 0.984375\n",
      "2018-05-23T15:23:30.571068: step 18217, loss 0.0313981, acc 1\n",
      "2018-05-23T15:23:30.927156: step 18218, loss 0.0657442, acc 0.984375\n",
      "2018-05-23T15:23:31.271235: step 18219, loss 0.057685, acc 1\n",
      "2018-05-23T15:23:31.613321: step 18220, loss 0.0880525, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:23:31.967372: step 18221, loss 0.12155, acc 0.9375\n",
      "2018-05-23T15:23:32.313446: step 18222, loss 0.0674623, acc 0.953125\n",
      "2018-05-23T15:23:32.657173: step 18223, loss 0.112913, acc 0.96875\n",
      "2018-05-23T15:23:33.012224: step 18224, loss 0.108516, acc 0.921875\n",
      "2018-05-23T15:23:33.385228: step 18225, loss 0.105393, acc 0.96875\n",
      "2018-05-23T15:23:33.728309: step 18226, loss 0.197706, acc 0.9375\n",
      "2018-05-23T15:23:34.087348: step 18227, loss 0.128591, acc 0.96875\n",
      "2018-05-23T15:23:34.436415: step 18228, loss 0.113029, acc 0.9375\n",
      "2018-05-23T15:23:34.781492: step 18229, loss 0.199717, acc 0.921875\n",
      "2018-05-23T15:23:35.136541: step 18230, loss 0.0782425, acc 0.96875\n",
      "2018-05-23T15:23:35.476632: step 18231, loss 0.062069, acc 0.984375\n",
      "2018-05-23T15:23:35.819714: step 18232, loss 0.0237955, acc 1\n",
      "2018-05-23T15:23:36.173765: step 18233, loss 0.105551, acc 0.953125\n",
      "2018-05-23T15:23:36.514852: step 18234, loss 0.121174, acc 0.9375\n",
      "2018-05-23T15:23:36.871896: step 18235, loss 0.0667687, acc 0.984375\n",
      "2018-05-23T15:23:37.216974: step 18236, loss 0.0960119, acc 0.953125\n",
      "2018-05-23T15:23:37.573021: step 18237, loss 0.0546827, acc 0.984375\n",
      "2018-05-23T15:23:37.919130: step 18238, loss 0.142492, acc 0.9375\n",
      "2018-05-23T15:23:38.265169: step 18239, loss 0.0621385, acc 0.984375\n",
      "2018-05-23T15:23:38.607764: step 18240, loss 0.0958015, acc 0.96875\n",
      "2018-05-23T15:23:38.945862: step 18241, loss 0.1169, acc 0.96875\n",
      "2018-05-23T15:23:39.294927: step 18242, loss 0.0315021, acc 1\n",
      "2018-05-23T15:23:39.638009: step 18243, loss 0.125131, acc 0.921875\n",
      "2018-05-23T15:23:39.973114: step 18244, loss 0.0926001, acc 0.9375\n",
      "2018-05-23T15:23:40.316196: step 18245, loss 0.0733034, acc 0.984375\n",
      "2018-05-23T15:23:40.649305: step 18246, loss 0.125581, acc 0.9375\n",
      "2018-05-23T15:23:40.991388: step 18247, loss 0.119703, acc 0.9375\n",
      "2018-05-23T15:23:41.335469: step 18248, loss 0.155143, acc 0.921875\n",
      "2018-05-23T15:23:41.673563: step 18249, loss 0.0884023, acc 0.96875\n",
      "2018-05-23T15:23:42.007669: step 18250, loss 0.0341195, acc 1\n",
      "2018-05-23T15:23:42.351751: step 18251, loss 0.164714, acc 0.953125\n",
      "2018-05-23T15:23:42.699818: step 18252, loss 0.0596836, acc 0.96875\n",
      "2018-05-23T15:23:43.046890: step 18253, loss 0.056881, acc 0.96875\n",
      "2018-05-23T15:23:43.389973: step 18254, loss 0.0776221, acc 0.96875\n",
      "2018-05-23T15:23:43.728076: step 18255, loss 0.168867, acc 0.921875\n",
      "2018-05-23T15:23:44.072146: step 18256, loss 0.187948, acc 0.9375\n",
      "2018-05-23T15:23:44.421212: step 18257, loss 0.0815634, acc 0.9375\n",
      "2018-05-23T15:23:44.763299: step 18258, loss 0.147562, acc 0.96875\n",
      "2018-05-23T15:23:45.107379: step 18259, loss 0.152605, acc 0.921875\n",
      "2018-05-23T15:23:45.445474: step 18260, loss 0.0657956, acc 0.984375\n",
      "2018-05-23T15:23:45.783568: step 18261, loss 0.0495622, acc 0.96875\n",
      "2018-05-23T15:23:46.127647: step 18262, loss 0.0590124, acc 0.96875\n",
      "2018-05-23T15:23:46.465776: step 18263, loss 0.0900528, acc 0.96875\n",
      "2018-05-23T15:23:46.825780: step 18264, loss 0.0573687, acc 0.984375\n",
      "2018-05-23T15:23:47.169860: step 18265, loss 0.0861723, acc 0.953125\n",
      "2018-05-23T15:23:47.520922: step 18266, loss 0.152717, acc 0.921875\n",
      "2018-05-23T15:23:47.865999: step 18267, loss 0.155695, acc 0.96875\n",
      "2018-05-23T15:23:48.229027: step 18268, loss 0.103902, acc 0.953125\n",
      "2018-05-23T15:23:48.572107: step 18269, loss 0.190584, acc 0.953125\n",
      "2018-05-23T15:23:48.916189: step 18270, loss 0.234962, acc 0.953125\n",
      "2018-05-23T15:23:49.261266: step 18271, loss 0.20087, acc 0.953125\n",
      "2018-05-23T15:23:49.602353: step 18272, loss 0.0532241, acc 0.984375\n",
      "2018-05-23T15:23:49.967378: step 18273, loss 0.113384, acc 0.9375\n",
      "2018-05-23T15:23:50.307467: step 18274, loss 0.0260402, acc 1\n",
      "2018-05-23T15:23:50.649551: step 18275, loss 0.132321, acc 0.921875\n",
      "2018-05-23T15:23:50.991637: step 18276, loss 0.0520092, acc 0.96875\n",
      "2018-05-23T15:23:51.333753: step 18277, loss 0.0503594, acc 0.984375\n",
      "2018-05-23T15:23:51.683817: step 18278, loss 0.11757, acc 0.953125\n",
      "2018-05-23T15:23:52.102696: step 18279, loss 0.0700961, acc 0.96875\n",
      "2018-05-23T15:23:52.462734: step 18280, loss 0.0666063, acc 0.96875\n",
      "2018-05-23T15:23:52.808807: step 18281, loss 0.0500672, acc 0.984375\n",
      "2018-05-23T15:23:53.148898: step 18282, loss 0.161787, acc 0.9375\n",
      "2018-05-23T15:23:53.498962: step 18283, loss 0.0681503, acc 0.984375\n",
      "2018-05-23T15:23:53.849025: step 18284, loss 0.0638289, acc 0.96875\n",
      "2018-05-23T15:23:54.189115: step 18285, loss 0.187753, acc 0.9375\n",
      "2018-05-23T15:23:54.535189: step 18286, loss 0.0907457, acc 0.9375\n",
      "2018-05-23T15:23:54.883970: step 18287, loss 0.0109634, acc 1\n",
      "2018-05-23T15:23:55.217079: step 18288, loss 0.101804, acc 0.9375\n",
      "2018-05-23T15:23:55.554176: step 18289, loss 0.0652352, acc 0.96875\n",
      "2018-05-23T15:23:55.898255: step 18290, loss 0.0428271, acc 0.984375\n",
      "2018-05-23T15:23:56.239345: step 18291, loss 0.13145, acc 0.9375\n",
      "2018-05-23T15:23:56.578437: step 18292, loss 0.0331221, acc 0.96875\n",
      "2018-05-23T15:23:56.915533: step 18293, loss 0.102177, acc 0.953125\n",
      "2018-05-23T15:23:57.254659: step 18294, loss 0.23263, acc 0.921875\n",
      "2018-05-23T15:23:57.815163: step 18295, loss 0.0503306, acc 0.984375\n",
      "2018-05-23T15:23:58.234042: step 18296, loss 0.0438365, acc 0.984375\n",
      "2018-05-23T15:23:58.729718: step 18297, loss 0.02551, acc 1\n",
      "2018-05-23T15:23:59.299193: step 18298, loss 0.100601, acc 0.96875\n",
      "2018-05-23T15:23:59.684166: step 18299, loss 0.0825846, acc 0.953125\n",
      "2018-05-23T15:24:00.051181: step 18300, loss 0.181914, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:24:05.105662: step 18300, loss 1.96886, acc 0.711244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18300\n",
      "\n",
      "2018-05-23T15:24:06.941748: step 18301, loss 0.0737851, acc 0.96875\n",
      "2018-05-23T15:24:07.304492: step 18302, loss 0.0768861, acc 1\n",
      "2018-05-23T15:24:07.649571: step 18303, loss 0.139436, acc 0.953125\n",
      "2018-05-23T15:24:07.997077: step 18304, loss 0.22835, acc 0.9375\n",
      "2018-05-23T15:24:08.342155: step 18305, loss 0.0496449, acc 0.984375\n",
      "2018-05-23T15:24:08.685237: step 18306, loss 0.116745, acc 0.96875\n",
      "2018-05-23T15:24:09.019345: step 18307, loss 0.0531631, acc 0.953125\n",
      "2018-05-23T15:24:09.378383: step 18308, loss 0.0725752, acc 0.984375\n",
      "2018-05-23T15:24:09.713488: step 18309, loss 0.0926238, acc 0.9375\n",
      "2018-05-23T15:24:10.048590: step 18310, loss 0.0815541, acc 0.96875\n",
      "2018-05-23T15:24:10.386687: step 18311, loss 0.139673, acc 0.9375\n",
      "2018-05-23T15:24:10.732798: step 18312, loss 0.299501, acc 0.875\n",
      "2018-05-23T15:24:11.075880: step 18313, loss 0.0589503, acc 1\n",
      "2018-05-23T15:24:11.433922: step 18314, loss 0.064156, acc 0.96875\n",
      "2018-05-23T15:24:11.769028: step 18315, loss 0.0945829, acc 0.953125\n",
      "2018-05-23T15:24:12.113108: step 18316, loss 0.0878045, acc 0.9375\n",
      "2018-05-23T15:24:12.461177: step 18317, loss 0.0861083, acc 0.953125\n",
      "2018-05-23T15:24:12.802701: step 18318, loss 0.132603, acc 0.890625\n",
      "2018-05-23T15:24:13.132816: step 18319, loss 0.0664224, acc 0.96875\n",
      "2018-05-23T15:24:13.471911: step 18320, loss 0.107384, acc 0.921875\n",
      "2018-05-23T15:24:13.803026: step 18321, loss 0.0483634, acc 1\n",
      "2018-05-23T15:24:14.147103: step 18322, loss 0.119998, acc 0.953125\n",
      "2018-05-23T15:24:14.483206: step 18323, loss 0.0739991, acc 0.96875\n",
      "2018-05-23T15:24:14.819566: step 18324, loss 0.0785969, acc 0.96875\n",
      "2018-05-23T15:24:15.166669: step 18325, loss 0.141586, acc 0.9375\n",
      "2018-05-23T15:24:15.519692: step 18326, loss 0.172106, acc 0.921875\n",
      "2018-05-23T15:24:15.866763: step 18327, loss 0.0634034, acc 0.96875\n",
      "2018-05-23T15:24:16.212836: step 18328, loss 0.0336476, acc 1\n",
      "2018-05-23T15:24:16.567886: step 18329, loss 0.11506, acc 0.9375\n",
      "2018-05-23T15:24:16.904984: step 18330, loss 0.0786731, acc 0.96875\n",
      "2018-05-23T15:24:17.320619: step 18331, loss 0.0483239, acc 0.984375\n",
      "2018-05-23T15:24:17.726532: step 18332, loss 0.0848757, acc 0.984375\n",
      "2018-05-23T15:24:18.175331: step 18333, loss 0.106434, acc 0.953125\n",
      "2018-05-23T15:24:18.583242: step 18334, loss 0.0503917, acc 1\n",
      "2018-05-23T15:24:18.924328: step 18335, loss 0.0456038, acc 0.96875\n",
      "2018-05-23T15:24:19.269406: step 18336, loss 0.194444, acc 0.9375\n",
      "2018-05-23T15:24:19.610492: step 18337, loss 0.0476495, acc 0.984375\n",
      "2018-05-23T15:24:19.961553: step 18338, loss 0.0642408, acc 0.984375\n",
      "2018-05-23T15:24:20.303639: step 18339, loss 0.0728516, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:24:20.637743: step 18340, loss 0.0589452, acc 0.984375\n",
      "2018-05-23T15:24:20.979862: step 18341, loss 0.0748335, acc 0.953125\n",
      "2018-05-23T15:24:21.317923: step 18342, loss 0.175477, acc 0.9375\n",
      "2018-05-23T15:24:21.653025: step 18343, loss 0.159727, acc 0.9375\n",
      "2018-05-23T15:24:22.006080: step 18344, loss 0.0337213, acc 1\n",
      "2018-05-23T15:24:22.340186: step 18345, loss 0.061314, acc 0.984375\n",
      "2018-05-23T15:24:22.690248: step 18346, loss 0.106838, acc 0.953125\n",
      "2018-05-23T15:24:23.034327: step 18347, loss 0.0763094, acc 0.953125\n",
      "2018-05-23T15:24:23.374419: step 18348, loss 0.096707, acc 0.953125\n",
      "2018-05-23T15:24:23.716501: step 18349, loss 0.0406394, acc 1\n",
      "2018-05-23T15:24:24.063573: step 18350, loss 0.0916124, acc 0.96875\n",
      "2018-05-23T15:24:24.406656: step 18351, loss 0.157787, acc 0.921875\n",
      "2018-05-23T15:24:24.746746: step 18352, loss 0.125758, acc 0.953125\n",
      "2018-05-23T15:24:25.093818: step 18353, loss 0.0763827, acc 0.953125\n",
      "2018-05-23T15:24:25.431914: step 18354, loss 0.0337997, acc 0.984375\n",
      "2018-05-23T15:24:25.776990: step 18355, loss 0.08117, acc 0.953125\n",
      "2018-05-23T15:24:26.119074: step 18356, loss 0.103279, acc 0.96875\n",
      "2018-05-23T15:24:26.469139: step 18357, loss 0.0811842, acc 0.953125\n",
      "2018-05-23T15:24:26.817207: step 18358, loss 0.0937258, acc 0.96875\n",
      "2018-05-23T15:24:27.171260: step 18359, loss 0.0757513, acc 0.984375\n",
      "2018-05-23T15:24:27.617068: step 18360, loss 0.0585503, acc 0.984375\n",
      "2018-05-23T15:24:28.036626: step 18361, loss 0.0953877, acc 0.96875\n",
      "2018-05-23T15:24:28.403643: step 18362, loss 0.145756, acc 0.96875\n",
      "2018-05-23T15:24:28.743735: step 18363, loss 0.0856091, acc 0.953125\n",
      "2018-05-23T15:24:29.111750: step 18364, loss 0.109686, acc 0.984375\n",
      "2018-05-23T15:24:29.456866: step 18365, loss 0.13707, acc 0.9375\n",
      "2018-05-23T15:24:29.791968: step 18366, loss 0.044538, acc 1\n",
      "2018-05-23T15:24:30.143029: step 18367, loss 0.109653, acc 0.9375\n",
      "2018-05-23T15:24:30.481125: step 18368, loss 0.0480292, acc 0.984375\n",
      "2018-05-23T15:24:30.815232: step 18369, loss 0.0821215, acc 0.953125\n",
      "2018-05-23T15:24:31.218154: step 18370, loss 0.0944566, acc 0.953125\n",
      "2018-05-23T15:24:31.999079: step 18371, loss 0.0296139, acc 0.984375\n",
      "2018-05-23T15:24:32.587492: step 18372, loss 0.0690246, acc 0.953125\n",
      "2018-05-23T15:24:33.398324: step 18373, loss 0.0785609, acc 0.984375\n",
      "2018-05-23T15:24:34.121386: step 18374, loss 0.103124, acc 0.921875\n",
      "2018-05-23T15:24:34.758681: step 18375, loss 0.0331717, acc 1\n",
      "2018-05-23T15:24:35.159608: step 18376, loss 0.119453, acc 0.921875\n",
      "2018-05-23T15:24:35.528623: step 18377, loss 0.0593426, acc 0.96875\n",
      "2018-05-23T15:24:35.874696: step 18378, loss 0.0742922, acc 0.9375\n",
      "2018-05-23T15:24:36.221768: step 18379, loss 0.042098, acc 0.984375\n",
      "2018-05-23T15:24:36.746399: step 18380, loss 0.0775959, acc 0.953125\n",
      "2018-05-23T15:24:37.361720: step 18381, loss 0.0442274, acc 0.984375\n",
      "2018-05-23T15:24:37.759653: step 18382, loss 0.137282, acc 0.921875\n",
      "2018-05-23T15:24:38.378999: step 18383, loss 0.0905722, acc 0.953125\n",
      "2018-05-23T15:24:38.758979: step 18384, loss 0.0508089, acc 0.984375\n",
      "2018-05-23T15:24:39.122786: step 18385, loss 0.102632, acc 0.96875\n",
      "2018-05-23T15:24:39.488810: step 18386, loss 0.0568984, acc 0.984375\n",
      "2018-05-23T15:24:40.053300: step 18387, loss 0.0901628, acc 0.953125\n",
      "2018-05-23T15:24:40.439268: step 18388, loss 0.0778764, acc 0.96875\n",
      "2018-05-23T15:24:40.885073: step 18389, loss 0.149466, acc 0.921875\n",
      "2018-05-23T15:24:41.360800: step 18390, loss 0.0854214, acc 0.9375\n",
      "2018-05-23T15:24:41.749761: step 18391, loss 0.0782941, acc 0.953125\n",
      "2018-05-23T15:24:42.114785: step 18392, loss 0.100219, acc 0.96875\n",
      "2018-05-23T15:24:42.466056: step 18393, loss 0.151182, acc 0.890625\n",
      "2018-05-23T15:24:43.027553: step 18394, loss 0.183107, acc 0.96875\n",
      "2018-05-23T15:24:43.420502: step 18395, loss 0.124434, acc 0.921875\n",
      "2018-05-23T15:24:43.770566: step 18396, loss 0.0759273, acc 0.953125\n",
      "2018-05-23T15:24:44.128609: step 18397, loss 0.0792522, acc 0.96875\n",
      "2018-05-23T15:24:44.462714: step 18398, loss 0.196472, acc 0.921875\n",
      "2018-05-23T15:24:44.815769: step 18399, loss 0.0593457, acc 0.984375\n",
      "2018-05-23T15:24:45.182788: step 18400, loss 0.221958, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:24:49.678760: step 18400, loss 1.94015, acc 0.712245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18400\n",
      "\n",
      "2018-05-23T15:24:50.962329: step 18401, loss 0.0632742, acc 1\n",
      "2018-05-23T15:24:51.369238: step 18402, loss 0.0303508, acc 1\n",
      "2018-05-23T15:24:51.735296: step 18403, loss 0.0244261, acc 1\n",
      "2018-05-23T15:24:52.089312: step 18404, loss 0.0694395, acc 0.953125\n",
      "2018-05-23T15:24:52.429404: step 18405, loss 0.0928594, acc 0.953125\n",
      "2018-05-23T15:24:52.766502: step 18406, loss 0.136022, acc 0.96875\n",
      "2018-05-23T15:24:53.109582: step 18407, loss 0.181808, acc 0.921875\n",
      "2018-05-23T15:24:53.477598: step 18408, loss 0.062791, acc 0.96875\n",
      "2018-05-23T15:24:53.818686: step 18409, loss 0.0815877, acc 0.953125\n",
      "2018-05-23T15:24:54.161768: step 18410, loss 0.0911362, acc 0.9375\n",
      "2018-05-23T15:24:54.515820: step 18411, loss 0.0352341, acc 0.984375\n",
      "2018-05-23T15:24:54.849926: step 18412, loss 0.12194, acc 0.953125\n",
      "2018-05-23T15:24:55.184032: step 18413, loss 0.043347, acc 0.984375\n",
      "2018-05-23T15:24:55.521659: step 18414, loss 0.0964046, acc 0.9375\n",
      "2018-05-23T15:24:55.858758: step 18415, loss 0.0366095, acc 1\n",
      "2018-05-23T15:24:56.198846: step 18416, loss 0.0803442, acc 0.96875\n",
      "2018-05-23T15:24:56.543923: step 18417, loss 0.010328, acc 1\n",
      "2018-05-23T15:24:56.876034: step 18418, loss 0.116869, acc 0.9375\n",
      "2018-05-23T15:24:57.223108: step 18419, loss 0.148064, acc 0.96875\n",
      "2018-05-23T15:24:57.558212: step 18420, loss 0.0958046, acc 0.953125\n",
      "2018-05-23T15:24:57.890220: step 18421, loss 0.0691266, acc 0.96875\n",
      "2018-05-23T15:24:58.233302: step 18422, loss 0.186035, acc 0.9375\n",
      "2018-05-23T15:24:58.581372: step 18423, loss 0.0973894, acc 0.96875\n",
      "2018-05-23T15:24:58.918469: step 18424, loss 0.100467, acc 0.96875\n",
      "2018-05-23T15:24:59.257561: step 18425, loss 0.126763, acc 0.90625\n",
      "2018-05-23T15:24:59.601641: step 18426, loss 0.0982487, acc 0.953125\n",
      "2018-05-23T15:24:59.934752: step 18427, loss 0.0426535, acc 1\n",
      "2018-05-23T15:25:00.294246: step 18428, loss 0.0953065, acc 0.96875\n",
      "2018-05-23T15:25:00.648301: step 18429, loss 0.128193, acc 0.90625\n",
      "2018-05-23T15:25:00.988389: step 18430, loss 0.184818, acc 0.90625\n",
      "2018-05-23T15:25:01.325487: step 18431, loss 0.0716342, acc 0.96875\n",
      "2018-05-23T15:25:01.671561: step 18432, loss 0.108984, acc 0.953125\n",
      "2018-05-23T15:25:02.010654: step 18433, loss 0.124291, acc 0.953125\n",
      "2018-05-23T15:25:02.347369: step 18434, loss 0.0458701, acc 0.96875\n",
      "2018-05-23T15:25:02.703419: step 18435, loss 0.0384228, acc 0.984375\n",
      "2018-05-23T15:25:03.038521: step 18436, loss 0.0506662, acc 0.984375\n",
      "2018-05-23T15:25:03.412520: step 18437, loss 0.111898, acc 0.9375\n",
      "2018-05-23T15:25:03.749618: step 18438, loss 0.124896, acc 0.921875\n",
      "2018-05-23T15:25:04.088712: step 18439, loss 0.0771364, acc 0.96875\n",
      "2018-05-23T15:25:04.423817: step 18440, loss 0.0923273, acc 0.96875\n",
      "2018-05-23T15:25:04.762910: step 18441, loss 0.0690017, acc 0.984375\n",
      "2018-05-23T15:25:05.097013: step 18442, loss 0.101915, acc 0.96875\n",
      "2018-05-23T15:25:05.445083: step 18443, loss 0.0679394, acc 0.96875\n",
      "2018-05-23T15:25:05.784175: step 18444, loss 0.094544, acc 0.953125\n",
      "2018-05-23T15:25:06.126078: step 18445, loss 0.0327324, acc 1\n",
      "2018-05-23T15:25:06.463176: step 18446, loss 0.0822491, acc 0.96875\n",
      "2018-05-23T15:25:06.813240: step 18447, loss 0.0729634, acc 0.984375\n",
      "2018-05-23T15:25:07.164300: step 18448, loss 0.0428039, acc 0.984375\n",
      "2018-05-23T15:25:07.502396: step 18449, loss 0.0971247, acc 0.921875\n",
      "2018-05-23T15:25:07.844481: step 18450, loss 0.104275, acc 0.9375\n",
      "2018-05-23T15:25:08.181577: step 18451, loss 0.07438, acc 0.96875\n",
      "2018-05-23T15:25:08.523664: step 18452, loss 0.12865, acc 0.9375\n",
      "2018-05-23T15:25:08.856771: step 18453, loss 0.116532, acc 0.953125\n",
      "2018-05-23T15:25:09.196862: step 18454, loss 0.218769, acc 0.953125\n",
      "2018-05-23T15:25:09.536951: step 18455, loss 0.0717217, acc 0.953125\n",
      "2018-05-23T15:25:09.881031: step 18456, loss 0.0836114, acc 0.953125\n",
      "2018-05-23T15:25:10.215136: step 18457, loss 0.0427239, acc 1\n",
      "2018-05-23T15:25:10.555227: step 18458, loss 0.171035, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:25:10.941194: step 18459, loss 0.137426, acc 0.921875\n",
      "2018-05-23T15:25:11.271313: step 18460, loss 0.0594837, acc 0.96875\n",
      "2018-05-23T15:25:11.614396: step 18461, loss 0.0878225, acc 0.953125\n",
      "2018-05-23T15:25:11.959472: step 18462, loss 0.0678795, acc 0.984375\n",
      "2018-05-23T15:25:12.298563: step 18463, loss 0.0514871, acc 0.984375\n",
      "2018-05-23T15:25:12.629680: step 18464, loss 0.158866, acc 0.921875\n",
      "2018-05-23T15:25:12.982735: step 18465, loss 0.104714, acc 0.9375\n",
      "2018-05-23T15:25:13.319831: step 18466, loss 0.15682, acc 0.96875\n",
      "2018-05-23T15:25:13.652940: step 18467, loss 0.0883709, acc 0.96875\n",
      "2018-05-23T15:25:13.991253: step 18468, loss 0.0851098, acc 0.953125\n",
      "2018-05-23T15:25:14.336293: step 18469, loss 0.209049, acc 0.921875\n",
      "2018-05-23T15:25:14.673392: step 18470, loss 0.107646, acc 0.953125\n",
      "2018-05-23T15:25:15.013485: step 18471, loss 0.0724146, acc 0.984375\n",
      "2018-05-23T15:25:15.353572: step 18472, loss 0.123475, acc 0.9375\n",
      "2018-05-23T15:25:15.697652: step 18473, loss 0.0453056, acc 1\n",
      "2018-05-23T15:25:16.041731: step 18474, loss 0.179375, acc 0.96875\n",
      "2018-05-23T15:25:16.384813: step 18475, loss 0.0860252, acc 0.96875\n",
      "2018-05-23T15:25:16.720914: step 18476, loss 0.0778089, acc 0.9375\n",
      "2018-05-23T15:25:17.067142: step 18477, loss 0.0975189, acc 0.9375\n",
      "2018-05-23T15:25:17.402244: step 18478, loss 0.0489013, acc 0.96875\n",
      "2018-05-23T15:25:17.740339: step 18479, loss 0.0928957, acc 0.9375\n",
      "2018-05-23T15:25:18.094392: step 18480, loss 0.265435, acc 0.90625\n",
      "2018-05-23T15:25:18.501303: step 18481, loss 0.10454, acc 0.96875\n",
      "2018-05-23T15:25:18.840397: step 18482, loss 0.0702165, acc 0.96875\n",
      "2018-05-23T15:25:19.217388: step 18483, loss 0.0427873, acc 0.984375\n",
      "2018-05-23T15:25:19.647241: step 18484, loss 0.112787, acc 0.96875\n",
      "2018-05-23T15:25:20.170430: step 18485, loss 0.0569557, acc 0.984375\n",
      "2018-05-23T15:25:20.549416: step 18486, loss 0.0524384, acc 0.96875\n",
      "2018-05-23T15:25:20.964307: step 18487, loss 0.082129, acc 0.984375\n",
      "2018-05-23T15:25:21.560222: step 18488, loss 0.0721506, acc 0.96875\n",
      "2018-05-23T15:25:22.091798: step 18489, loss 0.192264, acc 0.953125\n",
      "2018-05-23T15:25:22.537606: step 18490, loss 0.197277, acc 0.953125\n",
      "2018-05-23T15:25:22.923575: step 18491, loss 0.0761159, acc 0.9375\n",
      "2018-05-23T15:25:23.308544: step 18492, loss 0.131008, acc 0.9375\n",
      "2018-05-23T15:25:23.829153: step 18493, loss 0.121986, acc 0.953125\n",
      "2018-05-23T15:25:24.209167: step 18494, loss 0.0915434, acc 0.96875\n",
      "2018-05-23T15:25:24.630008: step 18495, loss 0.0794566, acc 0.96875\n",
      "2018-05-23T15:25:25.008995: step 18496, loss 0.0869007, acc 0.96875\n",
      "2018-05-23T15:25:25.448819: step 18497, loss 0.0782188, acc 0.96875\n",
      "2018-05-23T15:25:25.808812: step 18498, loss 0.0723732, acc 0.96875\n",
      "2018-05-23T15:25:26.175830: step 18499, loss 0.0719768, acc 0.953125\n",
      "2018-05-23T15:25:26.530878: step 18500, loss 0.0710728, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:25:31.601314: step 18500, loss 1.9472, acc 0.713102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18500\n",
      "\n",
      "2018-05-23T15:25:33.039466: step 18501, loss 0.159098, acc 0.890625\n",
      "2018-05-23T15:25:33.422445: step 18502, loss 0.0345326, acc 1\n",
      "2018-05-23T15:25:33.785474: step 18503, loss 0.0793702, acc 0.953125\n",
      "2018-05-23T15:25:34.173434: step 18504, loss 0.105113, acc 0.953125\n",
      "2018-05-23T15:25:34.547435: step 18505, loss 0.076482, acc 0.953125\n",
      "2018-05-23T15:25:34.916446: step 18506, loss 0.124363, acc 0.9375\n",
      "2018-05-23T15:25:35.384408: step 18507, loss 0.0216515, acc 1\n",
      "2018-05-23T15:25:35.757410: step 18508, loss 0.158704, acc 0.90625\n",
      "2018-05-23T15:25:36.218179: step 18509, loss 0.106784, acc 0.90625\n",
      "2018-05-23T15:25:36.580208: step 18510, loss 0.0770981, acc 0.96875\n",
      "2018-05-23T15:25:37.067939: step 18511, loss 0.0511607, acc 0.96875\n",
      "2018-05-23T15:25:37.432927: step 18512, loss 0.0773471, acc 0.96875\n",
      "2018-05-23T15:25:37.822885: step 18513, loss 0.125026, acc 0.953125\n",
      "2018-05-23T15:25:38.267697: step 18514, loss 0.135477, acc 0.9375\n",
      "2018-05-23T15:25:38.742424: step 18515, loss 0.162369, acc 0.921875\n",
      "2018-05-23T15:25:39.256051: step 18516, loss 0.0492448, acc 0.96875\n",
      "2018-05-23T15:25:39.739758: step 18517, loss 0.179532, acc 0.9375\n",
      "2018-05-23T15:25:40.137697: step 18518, loss 0.113044, acc 0.96875\n",
      "2018-05-23T15:25:40.683234: step 18519, loss 0.086565, acc 0.953125\n",
      "2018-05-23T15:25:41.075184: step 18520, loss 0.120031, acc 0.90625\n",
      "2018-05-23T15:25:41.461151: step 18521, loss 0.17804, acc 0.953125\n",
      "2018-05-23T15:25:41.830163: step 18522, loss 0.0606564, acc 0.984375\n",
      "2018-05-23T15:25:42.212142: step 18523, loss 0.087035, acc 0.953125\n",
      "2018-05-23T15:25:42.692856: step 18524, loss 0.133728, acc 0.953125\n",
      "2018-05-23T15:25:43.247372: step 18525, loss 0.0916757, acc 0.96875\n",
      "2018-05-23T15:25:43.701159: step 18526, loss 0.0340433, acc 1\n",
      "2018-05-23T15:25:44.139985: step 18527, loss 0.198938, acc 0.921875\n",
      "2018-05-23T15:25:44.624688: step 18528, loss 0.0528011, acc 0.984375\n",
      "2018-05-23T15:25:45.070501: step 18529, loss 0.157818, acc 0.9375\n",
      "2018-05-23T15:25:45.529269: step 18530, loss 0.128546, acc 0.9375\n",
      "2018-05-23T15:25:45.991033: step 18531, loss 0.054509, acc 0.96875\n",
      "2018-05-23T15:25:46.383982: step 18532, loss 0.0801054, acc 0.953125\n",
      "2018-05-23T15:25:46.754879: step 18533, loss 0.0903625, acc 0.96875\n",
      "2018-05-23T15:25:47.103943: step 18534, loss 0.157454, acc 0.953125\n",
      "2018-05-23T15:25:47.452012: step 18535, loss 0.193824, acc 0.921875\n",
      "2018-05-23T15:25:47.810053: step 18536, loss 0.195529, acc 0.921875\n",
      "2018-05-23T15:25:48.262842: step 18537, loss 0.0652294, acc 0.984375\n",
      "2018-05-23T15:25:48.636841: step 18538, loss 0.118302, acc 0.953125\n",
      "2018-05-23T15:25:49.001868: step 18539, loss 0.130189, acc 0.953125\n",
      "2018-05-23T15:25:49.353924: step 18540, loss 0.122138, acc 0.9375\n",
      "2018-05-23T15:25:49.699001: step 18541, loss 0.140331, acc 0.953125\n",
      "2018-05-23T15:25:50.173932: step 18542, loss 0.0334664, acc 0.984375\n",
      "2018-05-23T15:25:50.759398: step 18543, loss 0.100922, acc 0.953125\n",
      "2018-05-23T15:25:51.217141: step 18544, loss 0.108763, acc 0.953125\n",
      "2018-05-23T15:25:51.605101: step 18545, loss 0.0592457, acc 0.984375\n",
      "2018-05-23T15:25:51.956164: step 18546, loss 0.0793502, acc 0.96875\n",
      "2018-05-23T15:25:52.318196: step 18547, loss 0.149629, acc 0.953125\n",
      "2018-05-23T15:25:52.665265: step 18548, loss 0.175715, acc 0.9375\n",
      "2018-05-23T15:25:53.003818: step 18549, loss 0.0558612, acc 0.984375\n",
      "2018-05-23T15:25:53.354878: step 18550, loss 0.133176, acc 0.921875\n",
      "2018-05-23T15:25:53.711922: step 18551, loss 0.0849927, acc 0.984375\n",
      "2018-05-23T15:25:54.061987: step 18552, loss 0.171986, acc 0.9375\n",
      "2018-05-23T15:25:54.416084: step 18553, loss 0.0461129, acc 0.96875\n",
      "2018-05-23T15:25:54.762188: step 18554, loss 0.0579709, acc 0.984375\n",
      "2018-05-23T15:25:55.110226: step 18555, loss 0.0483021, acc 1\n",
      "2018-05-23T15:25:55.451311: step 18556, loss 0.0581878, acc 0.984375\n",
      "2018-05-23T15:25:55.793396: step 18557, loss 0.163324, acc 0.921875\n",
      "2018-05-23T15:25:56.145410: step 18558, loss 0.16092, acc 0.921875\n",
      "2018-05-23T15:25:56.484505: step 18559, loss 0.0983132, acc 0.96875\n",
      "2018-05-23T15:25:56.832574: step 18560, loss 0.0524302, acc 0.96875\n",
      "2018-05-23T15:25:57.181641: step 18561, loss 0.0958044, acc 0.96875\n",
      "2018-05-23T15:25:57.524409: step 18562, loss 0.111522, acc 0.953125\n",
      "2018-05-23T15:25:57.876468: step 18563, loss 0.0651438, acc 0.96875\n",
      "2018-05-23T15:25:58.219550: step 18564, loss 0.0982327, acc 0.9375\n",
      "2018-05-23T15:25:58.597541: step 18565, loss 0.147358, acc 0.9375\n",
      "2018-05-23T15:25:58.981513: step 18566, loss 0.0547511, acc 0.984375\n",
      "2018-05-23T15:25:59.323597: step 18567, loss 0.110509, acc 0.984375\n",
      "2018-05-23T15:25:59.664684: step 18568, loss 0.0517363, acc 0.984375\n",
      "2018-05-23T15:26:00.026715: step 18569, loss 0.145648, acc 0.96875\n",
      "2018-05-23T15:26:00.368800: step 18570, loss 0.0621135, acc 0.96875\n",
      "2018-05-23T15:26:00.781696: step 18571, loss 0.0965696, acc 0.953125\n",
      "2018-05-23T15:26:01.178636: step 18572, loss 0.0674144, acc 0.96875\n",
      "2018-05-23T15:26:01.543658: step 18573, loss 0.115919, acc 0.953125\n",
      "2018-05-23T15:26:01.890729: step 18574, loss 0.188932, acc 0.921875\n",
      "2018-05-23T15:26:02.241790: step 18575, loss 0.0782442, acc 0.9375\n",
      "2018-05-23T15:26:02.583908: step 18576, loss 0.0515444, acc 1\n",
      "2018-05-23T15:26:02.942960: step 18577, loss 0.0713289, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:26:03.299961: step 18578, loss 0.204343, acc 0.9375\n",
      "2018-05-23T15:26:03.639054: step 18579, loss 0.148437, acc 0.9375\n",
      "2018-05-23T15:26:03.986123: step 18580, loss 0.0507325, acc 0.96875\n",
      "2018-05-23T15:26:04.335192: step 18581, loss 0.125182, acc 0.9375\n",
      "2018-05-23T15:26:04.677276: step 18582, loss 0.0529936, acc 0.984375\n",
      "2018-05-23T15:26:05.056260: step 18583, loss 0.0560204, acc 0.953125\n",
      "2018-05-23T15:26:05.459182: step 18584, loss 0.0320831, acc 1\n",
      "2018-05-23T15:26:05.868089: step 18585, loss 0.055982, acc 0.984375\n",
      "2018-05-23T15:26:06.864425: step 18586, loss 0.058856, acc 0.984375\n",
      "2018-05-23T15:26:08.401314: step 18587, loss 0.0470232, acc 0.984375\n",
      "2018-05-23T15:26:09.522314: step 18588, loss 0.121611, acc 0.953125\n",
      "2018-05-23T15:26:10.462797: step 18589, loss 0.0682608, acc 0.96875\n",
      "2018-05-23T15:26:11.255680: step 18590, loss 0.229155, acc 0.90625\n",
      "2018-05-23T15:26:11.960790: step 18591, loss 0.211379, acc 0.953125\n",
      "2018-05-23T15:26:12.834454: step 18592, loss 0.121028, acc 0.9375\n",
      "2018-05-23T15:26:13.636308: step 18593, loss 0.0531461, acc 0.984375\n",
      "2018-05-23T15:26:14.261636: step 18594, loss 0.0489598, acc 0.96875\n",
      "2018-05-23T15:26:15.129314: step 18595, loss 0.128929, acc 0.953125\n",
      "2018-05-23T15:26:15.924189: step 18596, loss 0.206465, acc 0.890625\n",
      "2018-05-23T15:26:16.587414: step 18597, loss 0.119029, acc 0.953125\n",
      "2018-05-23T15:26:17.205760: step 18598, loss 0.159896, acc 0.921875\n",
      "2018-05-23T15:26:17.765264: step 18599, loss 0.0516862, acc 0.96875\n",
      "2018-05-23T15:26:18.318781: step 18600, loss 0.191051, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:26:28.273153: step 18600, loss 1.92832, acc 0.713102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18600\n",
      "\n",
      "2018-05-23T15:26:31.047729: step 18601, loss 0.0917082, acc 0.953125\n",
      "2018-05-23T15:26:31.714945: step 18602, loss 0.0527741, acc 0.96875\n",
      "2018-05-23T15:26:32.404102: step 18603, loss 0.0622907, acc 0.96875\n",
      "2018-05-23T15:26:32.939669: step 18604, loss 0.10685, acc 0.96875\n",
      "2018-05-23T15:26:33.591923: step 18605, loss 0.049091, acc 0.96875\n",
      "2018-05-23T15:26:34.185336: step 18606, loss 0.0728336, acc 0.984375\n",
      "2018-05-23T15:26:34.701953: step 18607, loss 0.282341, acc 0.9375\n",
      "2018-05-23T15:26:35.274422: step 18608, loss 0.0990866, acc 0.96875\n",
      "2018-05-23T15:26:35.908726: step 18609, loss 0.17568, acc 0.9375\n",
      "2018-05-23T15:26:36.507126: step 18610, loss 0.0805951, acc 0.953125\n",
      "2018-05-23T15:26:37.204259: step 18611, loss 0.158525, acc 0.96875\n",
      "2018-05-23T15:26:37.867486: step 18612, loss 0.0511254, acc 0.96875\n",
      "2018-05-23T15:26:38.491815: step 18613, loss 0.064262, acc 0.953125\n",
      "2018-05-23T15:26:39.248791: step 18614, loss 0.0282206, acc 1\n",
      "2018-05-23T15:26:39.805300: step 18615, loss 0.106924, acc 0.984375\n",
      "2018-05-23T15:26:40.340868: step 18616, loss 0.213571, acc 0.90625\n",
      "2018-05-23T15:26:40.816595: step 18617, loss 0.0522198, acc 0.984375\n",
      "2018-05-23T15:26:41.588531: step 18618, loss 0.10727, acc 0.9375\n",
      "2018-05-23T15:26:42.116157: step 18619, loss 0.0540284, acc 0.96875\n",
      "2018-05-23T15:26:42.588887: step 18620, loss 0.100785, acc 0.96875\n",
      "2018-05-23T15:26:42.979808: step 18621, loss 0.203331, acc 0.875\n",
      "2018-05-23T15:26:43.434593: step 18622, loss 0.0464631, acc 0.984375\n",
      "2018-05-23T15:26:43.802606: step 18623, loss 0.109047, acc 0.9375\n",
      "2018-05-23T15:26:44.212512: step 18624, loss 0.115164, acc 0.96875\n",
      "2018-05-23T15:26:44.577533: step 18625, loss 0.0481235, acc 1\n",
      "2018-05-23T15:26:44.974473: step 18626, loss 0.0676625, acc 0.96875\n",
      "2018-05-23T15:26:45.403324: step 18627, loss 0.0988492, acc 0.96875\n",
      "2018-05-23T15:26:45.861099: step 18628, loss 0.0369584, acc 1\n",
      "2018-05-23T15:26:46.295939: step 18629, loss 0.0470517, acc 0.984375\n",
      "2018-05-23T15:26:46.772662: step 18630, loss 0.0803312, acc 0.96875\n",
      "2018-05-23T15:26:47.144666: step 18631, loss 0.205291, acc 0.90625\n",
      "2018-05-23T15:26:47.556565: step 18632, loss 0.051902, acc 1\n",
      "2018-05-23T15:26:48.038278: step 18633, loss 0.0582885, acc 0.96875\n",
      "2018-05-23T15:26:48.572847: step 18634, loss 0.0871142, acc 0.96875\n",
      "2018-05-23T15:26:49.164263: step 18635, loss 0.100273, acc 0.953125\n",
      "2018-05-23T15:26:49.937270: step 18636, loss 0.183359, acc 0.9375\n",
      "2018-05-23T15:26:50.452402: step 18637, loss 0.0401838, acc 0.984375\n",
      "2018-05-23T15:26:50.962042: step 18638, loss 0.147443, acc 0.921875\n",
      "2018-05-23T15:26:51.838695: step 18639, loss 0.0738048, acc 0.96875\n",
      "2018-05-23T15:26:52.497936: step 18640, loss 0.0607726, acc 0.984375\n",
      "2018-05-23T15:26:53.087355: step 18641, loss 0.141267, acc 0.953125\n",
      "2018-05-23T15:26:53.618933: step 18642, loss 0.194246, acc 0.875\n",
      "2018-05-23T15:26:54.184418: step 18643, loss 0.0910928, acc 0.953125\n",
      "2018-05-23T15:26:54.798776: step 18644, loss 0.159511, acc 0.953125\n",
      "2018-05-23T15:26:55.345315: step 18645, loss 0.0775964, acc 0.984375\n",
      "2018-05-23T15:26:55.866917: step 18646, loss 0.0897165, acc 0.96875\n",
      "2018-05-23T15:26:56.449359: step 18647, loss 0.123904, acc 0.953125\n",
      "2018-05-23T15:26:57.095632: step 18648, loss 0.0978174, acc 0.953125\n",
      "2018-05-23T15:26:57.679071: step 18649, loss 0.117399, acc 0.953125\n",
      "2018-05-23T15:26:58.232595: step 18650, loss 0.0758453, acc 0.96875\n",
      "2018-05-23T15:26:58.839966: step 18651, loss 0.158305, acc 0.890625\n",
      "2018-05-23T15:26:59.389494: step 18652, loss 0.0860052, acc 0.953125\n",
      "2018-05-23T15:26:59.947002: step 18653, loss 0.181829, acc 0.921875\n",
      "2018-05-23T15:27:00.516489: step 18654, loss 0.0995917, acc 0.9375\n",
      "2018-05-23T15:27:01.165757: step 18655, loss 0.141553, acc 0.9375\n",
      "2018-05-23T15:27:01.754168: step 18656, loss 0.0815604, acc 0.9375\n",
      "2018-05-23T15:27:02.281763: step 18657, loss 0.106444, acc 0.953125\n",
      "2018-05-23T15:27:02.829292: step 18658, loss 0.181505, acc 0.9375\n",
      "2018-05-23T15:27:03.502491: step 18659, loss 0.0651762, acc 0.96875\n",
      "2018-05-23T15:27:04.088923: step 18660, loss 0.0524433, acc 0.96875\n",
      "2018-05-23T15:27:04.633469: step 18661, loss 0.147448, acc 0.953125\n",
      "2018-05-23T15:27:05.164052: step 18662, loss 0.131494, acc 0.953125\n",
      "2018-05-23T15:27:05.703604: step 18663, loss 0.116005, acc 0.921875\n",
      "2018-05-23T15:27:06.240169: step 18664, loss 0.0704321, acc 0.96875\n",
      "2018-05-23T15:27:06.777729: step 18665, loss 0.132753, acc 0.953125\n",
      "2018-05-23T15:27:07.356183: step 18666, loss 0.069896, acc 0.953125\n",
      "2018-05-23T15:27:08.048331: step 18667, loss 0.115551, acc 0.953125\n",
      "2018-05-23T15:27:08.625786: step 18668, loss 0.0677944, acc 0.96875\n",
      "2018-05-23T15:27:09.194265: step 18669, loss 0.137286, acc 0.921875\n",
      "2018-05-23T15:27:09.818594: step 18670, loss 0.127774, acc 0.9375\n",
      "2018-05-23T15:27:10.351169: step 18671, loss 0.0421571, acc 0.96875\n",
      "2018-05-23T15:27:10.898710: step 18672, loss 0.0890128, acc 0.96875\n",
      "2018-05-23T15:27:11.452225: step 18673, loss 0.245808, acc 0.90625\n",
      "2018-05-23T15:27:12.077551: step 18674, loss 0.0998164, acc 0.96875\n",
      "2018-05-23T15:27:12.627083: step 18675, loss 0.058791, acc 0.984375\n",
      "2018-05-23T15:27:13.193565: step 18676, loss 0.162345, acc 0.921875\n",
      "2018-05-23T15:27:13.747087: step 18677, loss 0.249892, acc 0.96875\n",
      "2018-05-23T15:27:14.383382: step 18678, loss 0.0536531, acc 0.984375\n",
      "2018-05-23T15:27:14.915957: step 18679, loss 0.0746526, acc 0.953125\n",
      "2018-05-23T15:27:15.482442: step 18680, loss 0.0678776, acc 0.96875\n",
      "2018-05-23T15:27:16.126718: step 18681, loss 0.0793448, acc 0.96875\n",
      "2018-05-23T15:27:16.698196: step 18682, loss 0.125611, acc 0.921875\n",
      "2018-05-23T15:27:17.365512: step 18683, loss 0.232093, acc 0.96875\n",
      "2018-05-23T15:27:17.965904: step 18684, loss 0.114985, acc 0.953125\n",
      "2018-05-23T15:27:18.884447: step 18685, loss 0.167103, acc 0.953125\n",
      "2018-05-23T15:27:19.734417: step 18686, loss 0.0596957, acc 1\n",
      "2018-05-23T15:27:20.332809: step 18687, loss 0.0354937, acc 1\n",
      "2018-05-23T15:27:20.984068: step 18688, loss 0.419571, acc 0.953125\n",
      "2018-05-23T15:27:21.631335: step 18689, loss 0.156466, acc 0.96875\n",
      "2018-05-23T15:27:22.298550: step 18690, loss 0.125735, acc 0.9375\n",
      "2018-05-23T15:27:23.072481: step 18691, loss 0.0638223, acc 0.984375\n",
      "2018-05-23T15:27:23.683848: step 18692, loss 0.0658722, acc 0.96875\n",
      "2018-05-23T15:27:24.340340: step 18693, loss 0.218358, acc 0.921875\n",
      "2018-05-23T15:27:25.053428: step 18694, loss 0.0693943, acc 0.984375\n",
      "2018-05-23T15:27:25.734605: step 18695, loss 0.0841859, acc 0.953125\n",
      "2018-05-23T15:27:26.516633: step 18696, loss 0.0756506, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:27:27.201799: step 18697, loss 0.0805366, acc 0.953125\n",
      "2018-05-23T15:27:27.883973: step 18698, loss 0.116553, acc 0.9375\n",
      "2018-05-23T15:27:28.713754: step 18699, loss 0.0336147, acc 0.984375\n",
      "2018-05-23T15:27:29.558497: step 18700, loss 0.0917292, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:27:38.719991: step 18700, loss 1.9343, acc 0.711816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18700\n",
      "\n",
      "2018-05-23T15:27:41.550414: step 18701, loss 0.195768, acc 0.953125\n",
      "2018-05-23T15:27:42.396786: step 18702, loss 0.052228, acc 0.984375\n",
      "2018-05-23T15:27:43.128830: step 18703, loss 0.0947133, acc 0.953125\n",
      "2018-05-23T15:27:43.815988: step 18704, loss 0.170406, acc 0.921875\n",
      "2018-05-23T15:27:44.657736: step 18705, loss 0.0685025, acc 1\n",
      "2018-05-23T15:27:45.377811: step 18706, loss 0.0602659, acc 0.96875\n",
      "2018-05-23T15:27:46.084919: step 18707, loss 0.0749943, acc 0.96875\n",
      "2018-05-23T15:27:46.802001: step 18708, loss 0.0877529, acc 0.96875\n",
      "2018-05-23T15:27:47.535039: step 18709, loss 0.0793056, acc 0.953125\n",
      "2018-05-23T15:27:48.275059: step 18710, loss 0.169733, acc 0.9375\n",
      "2018-05-23T15:27:48.937294: step 18711, loss 0.222002, acc 0.90625\n",
      "2018-05-23T15:27:49.670326: step 18712, loss 0.175917, acc 0.921875\n",
      "2018-05-23T15:27:50.556954: step 18713, loss 0.202427, acc 0.953125\n",
      "2018-05-23T15:27:51.273045: step 18714, loss 0.0319128, acc 0.984375\n",
      "2018-05-23T15:27:51.927290: step 18715, loss 0.0225661, acc 1\n",
      "2018-05-23T15:27:52.890273: step 18716, loss 0.0967614, acc 0.984375\n",
      "2018-05-23T15:27:53.589403: step 18717, loss 0.0770604, acc 0.96875\n",
      "2018-05-23T15:27:54.212736: step 18718, loss 0.0777879, acc 0.953125\n",
      "2018-05-23T15:27:54.785205: step 18719, loss 0.140895, acc 0.9375\n",
      "2018-05-23T15:27:55.302820: step 18720, loss 0.0705363, acc 0.96875\n",
      "2018-05-23T15:27:55.747629: step 18721, loss 0.0567832, acc 0.984375\n",
      "2018-05-23T15:27:56.221361: step 18722, loss 0.0722451, acc 0.96875\n",
      "2018-05-23T15:27:56.764909: step 18723, loss 0.0741437, acc 0.984375\n",
      "2018-05-23T15:27:57.308454: step 18724, loss 0.105773, acc 0.96875\n",
      "2018-05-23T15:27:57.902864: step 18725, loss 0.0812216, acc 0.953125\n",
      "2018-05-23T15:27:58.470346: step 18726, loss 0.0454373, acc 0.953125\n",
      "2018-05-23T15:27:59.139556: step 18727, loss 0.0882154, acc 0.9375\n",
      "2018-05-23T15:27:59.817741: step 18728, loss 0.124513, acc 0.953125\n",
      "2018-05-23T15:28:01.051444: step 18729, loss 0.100177, acc 0.9375\n",
      "2018-05-23T15:28:01.962005: step 18730, loss 0.0744131, acc 0.953125\n",
      "2018-05-23T15:28:02.987261: step 18731, loss 0.0582448, acc 0.984375\n",
      "2018-05-23T15:28:03.777148: step 18732, loss 0.108047, acc 0.9375\n",
      "2018-05-23T15:28:04.536119: step 18733, loss 0.0832077, acc 0.953125\n",
      "2018-05-23T15:28:05.176406: step 18734, loss 0.0437635, acc 0.984375\n",
      "2018-05-23T15:28:06.275466: step 18735, loss 0.11633, acc 0.953125\n",
      "2018-05-23T15:28:06.981577: step 18736, loss 0.125003, acc 0.921875\n",
      "2018-05-23T15:28:08.049735: step 18737, loss 0.0569636, acc 1\n",
      "2018-05-23T15:28:08.827639: step 18738, loss 0.0469306, acc 0.984375\n",
      "2018-05-23T15:28:09.782085: step 18739, loss 0.148961, acc 0.984375\n",
      "2018-05-23T15:28:10.553022: step 18740, loss 0.105787, acc 0.984375\n",
      "2018-05-23T15:28:11.325954: step 18741, loss 0.0832392, acc 0.953125\n",
      "2018-05-23T15:28:12.012118: step 18742, loss 0.0557293, acc 0.953125\n",
      "2018-05-23T15:28:12.649415: step 18743, loss 0.0638915, acc 0.96875\n",
      "2018-05-23T15:28:13.349540: step 18744, loss 0.0739492, acc 0.984375\n",
      "2018-05-23T15:28:13.870148: step 18745, loss 0.0321477, acc 0.984375\n",
      "2018-05-23T15:28:14.315955: step 18746, loss 0.16179, acc 0.921875\n",
      "2018-05-23T15:28:14.734835: step 18747, loss 0.102489, acc 0.953125\n",
      "2018-05-23T15:28:15.243474: step 18748, loss 0.0518518, acc 0.984375\n",
      "2018-05-23T15:28:15.854838: step 18749, loss 0.100925, acc 0.953125\n",
      "2018-05-23T15:28:16.404369: step 18750, loss 0.15406, acc 0.953125\n",
      "2018-05-23T15:28:16.881092: step 18751, loss 0.179482, acc 0.875\n",
      "2018-05-23T15:28:17.310942: step 18752, loss 0.0629437, acc 0.96875\n",
      "2018-05-23T15:28:17.796644: step 18753, loss 0.0705473, acc 0.984375\n",
      "2018-05-23T15:28:18.214525: step 18754, loss 0.0969192, acc 0.96875\n",
      "2018-05-23T15:28:18.673299: step 18755, loss 0.0807327, acc 0.953125\n",
      "2018-05-23T15:28:19.469171: step 18756, loss 0.115921, acc 0.9375\n",
      "2018-05-23T15:28:20.101477: step 18757, loss 0.0416079, acc 0.984375\n",
      "2018-05-23T15:28:20.670954: step 18758, loss 0.093144, acc 0.953125\n",
      "2018-05-23T15:28:21.337171: step 18759, loss 0.055139, acc 0.96875\n",
      "2018-05-23T15:28:21.905659: step 18760, loss 0.105848, acc 0.96875\n",
      "2018-05-23T15:28:22.540951: step 18761, loss 0.0602084, acc 0.953125\n",
      "2018-05-23T15:28:22.993740: step 18762, loss 0.0609253, acc 0.96875\n",
      "2018-05-23T15:28:23.506370: step 18763, loss 0.173214, acc 0.9375\n",
      "2018-05-23T15:28:24.033959: step 18764, loss 0.050102, acc 0.984375\n",
      "2018-05-23T15:28:24.627371: step 18765, loss 0.12159, acc 0.921875\n",
      "2018-05-23T15:28:25.316528: step 18766, loss 0.0561625, acc 0.96875\n",
      "2018-05-23T15:28:25.968783: step 18767, loss 0.0720344, acc 0.96875\n",
      "2018-05-23T15:28:26.660931: step 18768, loss 0.13968, acc 0.9375\n",
      "2018-05-23T15:28:27.380007: step 18769, loss 0.0403785, acc 1\n",
      "2018-05-23T15:28:28.175878: step 18770, loss 0.0839395, acc 0.96875\n",
      "2018-05-23T15:28:28.970751: step 18771, loss 0.099341, acc 0.984375\n",
      "2018-05-23T15:28:29.868349: step 18772, loss 0.259544, acc 0.890625\n",
      "2018-05-23T15:28:30.507641: step 18773, loss 0.0486233, acc 0.96875\n",
      "2018-05-23T15:28:31.102050: step 18774, loss 0.0687524, acc 0.984375\n",
      "2018-05-23T15:28:31.927841: step 18775, loss 0.23007, acc 0.921875\n",
      "2018-05-23T15:28:32.682821: step 18776, loss 0.140394, acc 0.921875\n",
      "2018-05-23T15:28:33.696110: step 18777, loss 0.107365, acc 0.96875\n",
      "2018-05-23T15:28:34.445109: step 18778, loss 0.111184, acc 0.9375\n",
      "2018-05-23T15:28:34.964716: step 18779, loss 0.188036, acc 0.953125\n",
      "2018-05-23T15:28:35.550149: step 18780, loss 0.0487219, acc 0.984375\n",
      "2018-05-23T15:28:36.064773: step 18781, loss 0.0443245, acc 0.96875\n",
      "2018-05-23T15:28:36.528532: step 18782, loss 0.0536432, acc 0.96875\n",
      "2018-05-23T15:28:36.968355: step 18783, loss 0.107238, acc 0.9375\n",
      "2018-05-23T15:28:37.437102: step 18784, loss 0.0493628, acc 0.984375\n",
      "2018-05-23T15:28:37.838030: step 18785, loss 0.0913312, acc 0.953125\n",
      "2018-05-23T15:28:38.239957: step 18786, loss 0.0548981, acc 0.984375\n",
      "2018-05-23T15:28:38.630907: step 18787, loss 0.15947, acc 0.9375\n",
      "2018-05-23T15:28:38.982965: step 18788, loss 0.0387056, acc 1\n",
      "2018-05-23T15:28:39.327045: step 18789, loss 0.104668, acc 0.9375\n",
      "2018-05-23T15:28:39.665755: step 18790, loss 0.0479096, acc 0.984375\n",
      "2018-05-23T15:28:40.015816: step 18791, loss 0.0380086, acc 0.984375\n",
      "2018-05-23T15:28:40.355908: step 18792, loss 0.0467822, acc 1\n",
      "2018-05-23T15:28:40.687020: step 18793, loss 0.109783, acc 0.9375\n",
      "2018-05-23T15:28:41.063016: step 18794, loss 0.0875505, acc 0.984375\n",
      "2018-05-23T15:28:41.463942: step 18795, loss 0.0394831, acc 0.984375\n",
      "2018-05-23T15:28:41.859885: step 18796, loss 0.0600011, acc 0.96875\n",
      "2018-05-23T15:28:42.663733: step 18797, loss 0.0379953, acc 1\n",
      "2018-05-23T15:28:43.455614: step 18798, loss 0.0191899, acc 1\n",
      "2018-05-23T15:28:44.494834: step 18799, loss 0.0520724, acc 0.96875\n",
      "2018-05-23T15:28:45.091239: step 18800, loss 0.10756, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:28:50.320251: step 18800, loss 1.97066, acc 0.71353\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18800\n",
      "\n",
      "2018-05-23T15:28:51.833215: step 18801, loss 0.0781226, acc 0.953125\n",
      "2018-05-23T15:28:52.312920: step 18802, loss 0.125217, acc 0.953125\n",
      "2018-05-23T15:28:52.788647: step 18803, loss 0.0477994, acc 0.984375\n",
      "2018-05-23T15:28:53.243429: step 18804, loss 0.0608996, acc 0.96875\n",
      "2018-05-23T15:28:53.721153: step 18805, loss 0.0584445, acc 0.953125\n",
      "2018-05-23T15:28:54.110115: step 18806, loss 0.0405105, acc 1\n",
      "2018-05-23T15:28:54.492090: step 18807, loss 0.0332445, acc 1\n",
      "2018-05-23T15:28:54.864094: step 18808, loss 0.0375928, acc 0.984375\n",
      "2018-05-23T15:28:55.228121: step 18809, loss 0.0448306, acc 0.984375\n",
      "2018-05-23T15:28:55.615073: step 18810, loss 0.0203735, acc 1\n",
      "2018-05-23T15:28:55.976108: step 18811, loss 0.25698, acc 0.9375\n",
      "2018-05-23T15:28:56.334152: step 18812, loss 0.047815, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:28:56.714134: step 18813, loss 0.0426181, acc 0.984375\n",
      "2018-05-23T15:28:57.071385: step 18814, loss 0.10223, acc 0.9375\n",
      "2018-05-23T15:28:57.426435: step 18815, loss 0.0835983, acc 0.953125\n",
      "2018-05-23T15:28:57.773505: step 18816, loss 0.166369, acc 0.921875\n",
      "2018-05-23T15:28:58.131549: step 18817, loss 0.111487, acc 0.9375\n",
      "2018-05-23T15:28:58.471641: step 18818, loss 0.0939572, acc 0.953125\n",
      "2018-05-23T15:28:58.827688: step 18819, loss 0.0577604, acc 0.96875\n",
      "2018-05-23T15:28:59.187724: step 18820, loss 0.207526, acc 0.921875\n",
      "2018-05-23T15:28:59.542821: step 18821, loss 0.0301774, acc 1\n",
      "2018-05-23T15:28:59.942745: step 18822, loss 0.071857, acc 0.96875\n",
      "2018-05-23T15:29:00.328713: step 18823, loss 0.101407, acc 0.96875\n",
      "2018-05-23T15:29:00.729640: step 18824, loss 0.0971675, acc 0.9375\n",
      "2018-05-23T15:29:01.147522: step 18825, loss 0.0727623, acc 0.984375\n",
      "2018-05-23T15:29:01.498583: step 18826, loss 0.06433, acc 0.984375\n",
      "2018-05-23T15:29:01.848646: step 18827, loss 0.0876602, acc 0.96875\n",
      "2018-05-23T15:29:02.412141: step 18828, loss 0.0758528, acc 0.96875\n",
      "2018-05-23T15:29:02.936739: step 18829, loss 0.041998, acc 0.984375\n",
      "2018-05-23T15:29:03.428424: step 18830, loss 0.0381634, acc 1\n",
      "2018-05-23T15:29:04.024827: step 18831, loss 0.0305081, acc 0.984375\n",
      "2018-05-23T15:29:05.124884: step 18832, loss 0.0997116, acc 0.953125\n",
      "2018-05-23T15:29:05.879863: step 18833, loss 0.0295965, acc 1\n",
      "2018-05-23T15:29:06.658780: step 18834, loss 0.0610608, acc 0.984375\n",
      "2018-05-23T15:29:07.528453: step 18835, loss 0.0899612, acc 0.9375\n",
      "2018-05-23T15:29:08.772128: step 18836, loss 0.0690863, acc 0.953125\n",
      "2018-05-23T15:29:09.609885: step 18837, loss 0.0939084, acc 0.953125\n",
      "2018-05-23T15:29:10.305027: step 18838, loss 0.0429605, acc 0.984375\n",
      "2018-05-23T15:29:11.193649: step 18839, loss 0.0391319, acc 1\n",
      "2018-05-23T15:29:12.264783: step 18840, loss 0.0524029, acc 0.984375\n",
      "2018-05-23T15:29:13.118498: step 18841, loss 0.0695198, acc 0.984375\n",
      "2018-05-23T15:29:14.054995: step 18842, loss 0.0585691, acc 0.984375\n",
      "2018-05-23T15:29:14.781053: step 18843, loss 0.080665, acc 0.96875\n",
      "2018-05-23T15:29:15.550991: step 18844, loss 0.158501, acc 0.96875\n",
      "2018-05-23T15:29:16.143407: step 18845, loss 0.10191, acc 0.953125\n",
      "2018-05-23T15:29:16.861485: step 18846, loss 0.016538, acc 1\n",
      "2018-05-23T15:29:17.677303: step 18847, loss 0.0750921, acc 0.984375\n",
      "2018-05-23T15:29:18.432284: step 18848, loss 0.0783122, acc 0.96875\n",
      "2018-05-23T15:29:19.350826: step 18849, loss 0.0746882, acc 0.953125\n",
      "2018-05-23T15:29:20.231470: step 18850, loss 0.0436806, acc 0.984375\n",
      "2018-05-23T15:29:21.259720: step 18851, loss 0.0869773, acc 0.96875\n",
      "2018-05-23T15:29:22.060577: step 18852, loss 0.0657185, acc 0.96875\n",
      "2018-05-23T15:29:22.717818: step 18853, loss 0.178151, acc 0.90625\n",
      "2018-05-23T15:29:23.281313: step 18854, loss 0.138962, acc 0.9375\n",
      "2018-05-23T15:29:23.985428: step 18855, loss 0.246234, acc 0.953125\n",
      "2018-05-23T15:29:24.473122: step 18856, loss 0.0894065, acc 0.953125\n",
      "2018-05-23T15:29:24.922920: step 18857, loss 0.0571791, acc 0.96875\n",
      "2018-05-23T15:29:25.374711: step 18858, loss 0.0570384, acc 0.984375\n",
      "2018-05-23T15:29:25.859414: step 18859, loss 0.163806, acc 0.953125\n",
      "2018-05-23T15:29:26.295248: step 18860, loss 0.0460743, acc 0.96875\n",
      "2018-05-23T15:29:26.705151: step 18861, loss 0.138129, acc 0.96875\n",
      "2018-05-23T15:29:27.123034: step 18862, loss 0.0563677, acc 0.984375\n",
      "2018-05-23T15:29:27.513990: step 18863, loss 0.048551, acc 0.953125\n",
      "2018-05-23T15:29:27.913918: step 18864, loss 0.0375094, acc 0.984375\n",
      "2018-05-23T15:29:28.331799: step 18865, loss 0.195978, acc 0.90625\n",
      "2018-05-23T15:29:28.723752: step 18866, loss 0.0794865, acc 0.953125\n",
      "2018-05-23T15:29:29.139639: step 18867, loss 0.0379959, acc 0.984375\n",
      "2018-05-23T15:29:29.533585: step 18868, loss 0.0314126, acc 1\n",
      "2018-05-23T15:29:29.948475: step 18869, loss 0.0771325, acc 0.984375\n",
      "2018-05-23T15:29:30.338432: step 18870, loss 0.0883954, acc 0.953125\n",
      "2018-05-23T15:29:30.740355: step 18871, loss 0.113065, acc 0.921875\n",
      "2018-05-23T15:29:31.143278: step 18872, loss 0.0608837, acc 0.96875\n",
      "2018-05-23T15:29:31.534236: step 18873, loss 0.39726, acc 0.9375\n",
      "2018-05-23T15:29:31.931173: step 18874, loss 0.175753, acc 0.953125\n",
      "2018-05-23T15:29:32.338085: step 18875, loss 0.135068, acc 0.984375\n",
      "2018-05-23T15:29:32.742002: step 18876, loss 0.0652869, acc 0.96875\n",
      "2018-05-23T15:29:33.131959: step 18877, loss 0.140005, acc 0.921875\n",
      "2018-05-23T15:29:33.604694: step 18878, loss 0.0957555, acc 0.921875\n",
      "2018-05-23T15:29:33.984677: step 18879, loss 0.154904, acc 0.9375\n",
      "2018-05-23T15:29:34.391590: step 18880, loss 0.133161, acc 0.953125\n",
      "2018-05-23T15:29:34.809471: step 18881, loss 0.0969838, acc 0.953125\n",
      "2018-05-23T15:29:35.203416: step 18882, loss 0.133774, acc 0.953125\n",
      "2018-05-23T15:29:35.601353: step 18883, loss 0.059815, acc 0.984375\n",
      "2018-05-23T15:29:36.011255: step 18884, loss 0.0450023, acc 0.984375\n",
      "2018-05-23T15:29:36.416173: step 18885, loss 0.094505, acc 0.953125\n",
      "2018-05-23T15:29:36.807129: step 18886, loss 0.0621638, acc 0.96875\n",
      "2018-05-23T15:29:37.203067: step 18887, loss 0.100726, acc 0.96875\n",
      "2018-05-23T15:29:37.667824: step 18888, loss 0.118748, acc 0.9375\n",
      "2018-05-23T15:29:38.086707: step 18889, loss 0.112074, acc 0.953125\n",
      "2018-05-23T15:29:38.479653: step 18890, loss 0.0742332, acc 0.96875\n",
      "2018-05-23T15:29:38.878584: step 18891, loss 0.0636236, acc 0.9375\n",
      "2018-05-23T15:29:39.270536: step 18892, loss 0.143243, acc 0.96875\n",
      "2018-05-23T15:29:39.665480: step 18893, loss 0.0618442, acc 0.984375\n",
      "2018-05-23T15:29:40.051451: step 18894, loss 0.197117, acc 0.96875\n",
      "2018-05-23T15:29:40.463346: step 18895, loss 0.0654838, acc 0.984375\n",
      "2018-05-23T15:29:40.860283: step 18896, loss 0.155584, acc 0.9375\n",
      "2018-05-23T15:29:41.264202: step 18897, loss 0.0792243, acc 0.96875\n",
      "2018-05-23T15:29:41.652166: step 18898, loss 0.0314599, acc 0.984375\n",
      "2018-05-23T15:29:42.068053: step 18899, loss 0.0693242, acc 0.96875\n",
      "2018-05-23T15:29:42.466986: step 18900, loss 0.0348876, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:29:47.313022: step 18900, loss 1.98079, acc 0.713959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-18900\n",
      "\n",
      "2018-05-23T15:29:49.176038: step 18901, loss 0.0951802, acc 0.9375\n",
      "2018-05-23T15:29:49.633813: step 18902, loss 0.0395711, acc 0.984375\n",
      "2018-05-23T15:29:50.061668: step 18903, loss 0.0641831, acc 0.953125\n",
      "2018-05-23T15:29:50.477557: step 18904, loss 0.0564422, acc 0.984375\n",
      "2018-05-23T15:29:50.891449: step 18905, loss 0.099398, acc 0.96875\n",
      "2018-05-23T15:29:51.274424: step 18906, loss 0.05382, acc 0.984375\n",
      "2018-05-23T15:29:51.691309: step 18907, loss 0.0602256, acc 0.953125\n",
      "2018-05-23T15:29:52.104204: step 18908, loss 0.141027, acc 0.953125\n",
      "2018-05-23T15:29:52.521088: step 18909, loss 0.112134, acc 0.921875\n",
      "2018-05-23T15:29:52.933985: step 18910, loss 0.0743448, acc 0.96875\n",
      "2018-05-23T15:29:53.366826: step 18911, loss 0.0212686, acc 0.984375\n",
      "2018-05-23T15:29:53.773738: step 18912, loss 0.0945012, acc 0.96875\n",
      "2018-05-23T15:29:54.186635: step 18913, loss 0.14453, acc 0.953125\n",
      "2018-05-23T15:29:54.602520: step 18914, loss 0.0565153, acc 0.984375\n",
      "2018-05-23T15:29:55.029378: step 18915, loss 0.0431621, acc 0.96875\n",
      "2018-05-23T15:29:55.417341: step 18916, loss 0.0625214, acc 0.96875\n",
      "2018-05-23T15:29:55.819266: step 18917, loss 0.128086, acc 0.9375\n",
      "2018-05-23T15:29:56.216205: step 18918, loss 0.191916, acc 0.921875\n",
      "2018-05-23T15:29:56.623117: step 18919, loss 0.0357531, acc 0.984375\n",
      "2018-05-23T15:29:57.026037: step 18920, loss 0.0752881, acc 0.96875\n",
      "2018-05-23T15:29:57.421978: step 18921, loss 0.0275302, acc 1\n",
      "2018-05-23T15:29:57.810937: step 18922, loss 0.0298823, acc 1\n",
      "2018-05-23T15:29:58.202890: step 18923, loss 0.0845383, acc 0.9375\n",
      "2018-05-23T15:29:58.581876: step 18924, loss 0.049903, acc 0.984375\n",
      "2018-05-23T15:29:58.986791: step 18925, loss 0.0493506, acc 0.984375\n",
      "2018-05-23T15:29:59.414648: step 18926, loss 0.0591242, acc 0.96875\n",
      "2018-05-23T15:29:59.809593: step 18927, loss 0.0605943, acc 0.96875\n",
      "2018-05-23T15:30:00.230465: step 18928, loss 0.110964, acc 0.9375\n",
      "2018-05-23T15:30:00.620421: step 18929, loss 0.210847, acc 0.953125\n",
      "2018-05-23T15:30:01.020353: step 18930, loss 0.0394314, acc 0.984375\n",
      "2018-05-23T15:30:01.469151: step 18931, loss 0.060132, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:30:01.910969: step 18932, loss 0.181716, acc 0.921875\n",
      "2018-05-23T15:30:02.315886: step 18933, loss 0.0905669, acc 0.9375\n",
      "2018-05-23T15:30:02.727784: step 18934, loss 0.152639, acc 0.9375\n",
      "2018-05-23T15:30:03.126719: step 18935, loss 0.0615784, acc 0.984375\n",
      "2018-05-23T15:30:03.537620: step 18936, loss 0.0430434, acc 0.984375\n",
      "2018-05-23T15:30:03.929569: step 18937, loss 0.152244, acc 0.875\n",
      "2018-05-23T15:30:04.320522: step 18938, loss 0.118147, acc 0.953125\n",
      "2018-05-23T15:30:04.704497: step 18939, loss 0.059464, acc 0.984375\n",
      "2018-05-23T15:30:05.101436: step 18940, loss 0.0177014, acc 1\n",
      "2018-05-23T15:30:05.504356: step 18941, loss 0.0461852, acc 0.984375\n",
      "2018-05-23T15:30:05.897305: step 18942, loss 0.0630001, acc 0.984375\n",
      "2018-05-23T15:30:06.306213: step 18943, loss 0.0108349, acc 1\n",
      "2018-05-23T15:30:06.702152: step 18944, loss 0.0616625, acc 0.96875\n",
      "2018-05-23T15:30:07.090114: step 18945, loss 0.138358, acc 0.953125\n",
      "2018-05-23T15:30:07.565842: step 18946, loss 0.0495552, acc 0.984375\n",
      "2018-05-23T15:30:07.984723: step 18947, loss 0.113236, acc 0.96875\n",
      "2018-05-23T15:30:08.385651: step 18948, loss 0.237859, acc 0.90625\n",
      "2018-05-23T15:30:08.789567: step 18949, loss 0.0818881, acc 0.984375\n",
      "2018-05-23T15:30:09.182517: step 18950, loss 0.128326, acc 0.984375\n",
      "2018-05-23T15:30:09.597406: step 18951, loss 0.0285944, acc 1\n",
      "2018-05-23T15:30:09.995342: step 18952, loss 0.031005, acc 0.96875\n",
      "2018-05-23T15:30:10.373330: step 18953, loss 0.0337832, acc 1\n",
      "2018-05-23T15:30:10.764284: step 18954, loss 0.039428, acc 1\n",
      "2018-05-23T15:30:11.174191: step 18955, loss 0.152734, acc 0.953125\n",
      "2018-05-23T15:30:11.559158: step 18956, loss 0.113476, acc 0.953125\n",
      "2018-05-23T15:30:11.993995: step 18957, loss 0.0573438, acc 0.984375\n",
      "2018-05-23T15:30:12.417861: step 18958, loss 0.0769342, acc 0.984375\n",
      "2018-05-23T15:30:12.824774: step 18959, loss 0.0579249, acc 0.96875\n",
      "2018-05-23T15:30:13.225699: step 18960, loss 0.204256, acc 0.90625\n",
      "2018-05-23T15:30:13.642610: step 18961, loss 0.0628875, acc 0.96875\n",
      "2018-05-23T15:30:14.065475: step 18962, loss 0.0693474, acc 0.96875\n",
      "2018-05-23T15:30:14.452441: step 18963, loss 0.0768279, acc 0.96875\n",
      "2018-05-23T15:30:14.869325: step 18964, loss 0.040905, acc 0.984375\n",
      "2018-05-23T15:30:15.401900: step 18965, loss 0.0750568, acc 0.953125\n",
      "2018-05-23T15:30:15.921512: step 18966, loss 0.0661149, acc 0.96875\n",
      "2018-05-23T15:30:16.354352: step 18967, loss 0.078215, acc 0.984375\n",
      "2018-05-23T15:30:16.774228: step 18968, loss 0.0609609, acc 0.96875\n",
      "2018-05-23T15:30:17.186128: step 18969, loss 0.100859, acc 0.921875\n",
      "2018-05-23T15:30:17.600022: step 18970, loss 0.119074, acc 0.921875\n",
      "2018-05-23T15:30:17.990973: step 18971, loss 0.0374612, acc 0.984375\n",
      "2018-05-23T15:30:18.527538: step 18972, loss 0.220158, acc 0.90625\n",
      "2018-05-23T15:30:18.986311: step 18973, loss 0.05302, acc 0.984375\n",
      "2018-05-23T15:30:19.371283: step 18974, loss 0.0692849, acc 0.984375\n",
      "2018-05-23T15:30:19.781185: step 18975, loss 0.0948302, acc 0.953125\n",
      "2018-05-23T15:30:20.191088: step 18976, loss 0.158793, acc 0.953125\n",
      "2018-05-23T15:30:20.601988: step 18977, loss 0.0787918, acc 0.953125\n",
      "2018-05-23T15:30:20.952052: step 18978, loss 0.113222, acc 0.921875\n",
      "2018-05-23T15:30:21.304146: step 18979, loss 0.136309, acc 0.953125\n",
      "2018-05-23T15:30:21.647192: step 18980, loss 0.0559226, acc 0.96875\n",
      "2018-05-23T15:30:21.993268: step 18981, loss 0.11531, acc 0.9375\n",
      "2018-05-23T15:30:22.354300: step 18982, loss 0.0242084, acc 0.984375\n",
      "2018-05-23T15:30:22.722316: step 18983, loss 0.0331993, acc 0.984375\n",
      "2018-05-23T15:30:23.071382: step 18984, loss 0.082245, acc 0.96875\n",
      "2018-05-23T15:30:23.420448: step 18985, loss 0.0313156, acc 1\n",
      "2018-05-23T15:30:23.763531: step 18986, loss 0.0648819, acc 0.96875\n",
      "2018-05-23T15:30:24.120576: step 18987, loss 0.13731, acc 0.9375\n",
      "2018-05-23T15:30:24.467647: step 18988, loss 0.0368784, acc 1\n",
      "2018-05-23T15:30:24.802753: step 18989, loss 0.0597642, acc 0.96875\n",
      "2018-05-23T15:30:25.144838: step 18990, loss 0.0535087, acc 0.96875\n",
      "2018-05-23T15:30:25.498888: step 18991, loss 0.121168, acc 0.9375\n",
      "2018-05-23T15:30:25.840974: step 18992, loss 0.140721, acc 0.953125\n",
      "2018-05-23T15:30:26.188044: step 18993, loss 0.0247173, acc 1\n",
      "2018-05-23T15:30:26.541100: step 18994, loss 0.0692532, acc 0.96875\n",
      "2018-05-23T15:30:26.886178: step 18995, loss 0.0587674, acc 0.96875\n",
      "2018-05-23T15:30:27.224273: step 18996, loss 0.0871105, acc 0.96875\n",
      "2018-05-23T15:30:27.577330: step 18997, loss 0.138613, acc 0.90625\n",
      "2018-05-23T15:30:27.924403: step 18998, loss 0.052522, acc 0.96875\n",
      "2018-05-23T15:30:28.272468: step 18999, loss 0.171, acc 0.96875\n",
      "2018-05-23T15:30:28.621534: step 19000, loss 0.0497998, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:30:33.233197: step 19000, loss 2.02382, acc 0.713102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19000\n",
      "\n",
      "2018-05-23T15:30:35.274969: step 19001, loss 0.0573909, acc 1\n",
      "2018-05-23T15:30:36.061867: step 19002, loss 0.0544902, acc 0.984375\n",
      "2018-05-23T15:30:36.715122: step 19003, loss 0.0850518, acc 0.96875\n",
      "2018-05-23T15:30:37.219767: step 19004, loss 0.130099, acc 0.921875\n",
      "2018-05-23T15:30:37.822156: step 19005, loss 0.0646226, acc 0.96875\n",
      "2018-05-23T15:30:38.214106: step 19006, loss 0.0270387, acc 1\n",
      "2018-05-23T15:30:38.573146: step 19007, loss 0.0589937, acc 0.984375\n",
      "2018-05-23T15:30:38.920218: step 19008, loss 0.139644, acc 0.9375\n",
      "2018-05-23T15:30:39.273273: step 19009, loss 0.0411124, acc 1\n",
      "2018-05-23T15:30:39.611371: step 19010, loss 0.0459313, acc 0.984375\n",
      "2018-05-23T15:30:39.950464: step 19011, loss 0.0439052, acc 0.984375\n",
      "2018-05-23T15:30:40.298530: step 19012, loss 0.0659687, acc 0.953125\n",
      "2018-05-23T15:30:40.637623: step 19013, loss 0.0851849, acc 0.96875\n",
      "2018-05-23T15:30:40.984696: step 19014, loss 0.186231, acc 0.921875\n",
      "2018-05-23T15:30:41.336753: step 19015, loss 0.0519284, acc 0.984375\n",
      "2018-05-23T15:30:41.675848: step 19016, loss 0.0641306, acc 0.96875\n",
      "2018-05-23T15:30:42.019928: step 19017, loss 0.0342191, acc 0.96875\n",
      "2018-05-23T15:30:42.362010: step 19018, loss 0.0386598, acc 0.984375\n",
      "2018-05-23T15:30:42.703098: step 19019, loss 0.0450468, acc 0.984375\n",
      "2018-05-23T15:30:43.048174: step 19020, loss 0.0334234, acc 0.984375\n",
      "2018-05-23T15:30:43.403225: step 19021, loss 0.0913247, acc 0.9375\n",
      "2018-05-23T15:30:43.745310: step 19022, loss 0.191805, acc 0.953125\n",
      "2018-05-23T15:30:44.090390: step 19023, loss 0.221826, acc 0.90625\n",
      "2018-05-23T15:30:44.439453: step 19024, loss 0.115538, acc 0.9375\n",
      "2018-05-23T15:30:44.783532: step 19025, loss 0.158114, acc 0.953125\n",
      "2018-05-23T15:30:45.124620: step 19026, loss 0.0894834, acc 0.953125\n",
      "2018-05-23T15:30:45.472962: step 19027, loss 0.084752, acc 0.984375\n",
      "2018-05-23T15:30:45.813050: step 19028, loss 0.080412, acc 0.96875\n",
      "2018-05-23T15:30:46.158128: step 19029, loss 0.0948348, acc 0.96875\n",
      "2018-05-23T15:30:46.500214: step 19030, loss 0.040905, acc 0.984375\n",
      "2018-05-23T15:30:46.842297: step 19031, loss 0.0395022, acc 0.984375\n",
      "2018-05-23T15:30:47.190366: step 19032, loss 0.0701209, acc 0.96875\n",
      "2018-05-23T15:30:47.534446: step 19033, loss 0.0833686, acc 0.984375\n",
      "2018-05-23T15:30:47.869551: step 19034, loss 0.0994786, acc 0.953125\n",
      "2018-05-23T15:30:48.216623: step 19035, loss 0.171286, acc 0.984375\n",
      "2018-05-23T15:30:48.594611: step 19036, loss 0.0956272, acc 0.96875\n",
      "2018-05-23T15:30:48.933701: step 19037, loss 0.0651474, acc 0.96875\n",
      "2018-05-23T15:30:49.271800: step 19038, loss 0.151305, acc 0.9375\n",
      "2018-05-23T15:30:49.621862: step 19039, loss 0.13966, acc 0.953125\n",
      "2018-05-23T15:30:49.962949: step 19040, loss 0.1691, acc 0.921875\n",
      "2018-05-23T15:30:50.304037: step 19041, loss 0.0654187, acc 0.984375\n",
      "2018-05-23T15:30:50.646123: step 19042, loss 0.101881, acc 0.984375\n",
      "2018-05-23T15:30:50.989205: step 19043, loss 0.0730465, acc 0.953125\n",
      "2018-05-23T15:30:51.331287: step 19044, loss 0.113062, acc 0.9375\n",
      "2018-05-23T15:30:51.679359: step 19045, loss 0.0396975, acc 1\n",
      "2018-05-23T15:30:52.035406: step 19046, loss 0.0627913, acc 0.96875\n",
      "2018-05-23T15:30:52.393448: step 19047, loss 0.0665279, acc 0.953125\n",
      "2018-05-23T15:30:52.743509: step 19048, loss 0.141482, acc 0.953125\n",
      "2018-05-23T15:30:53.088589: step 19049, loss 0.124246, acc 0.921875\n",
      "2018-05-23T15:30:53.427682: step 19050, loss 0.245059, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:30:53.768769: step 19051, loss 0.0601811, acc 0.953125\n",
      "2018-05-23T15:30:54.125815: step 19052, loss 0.145295, acc 0.953125\n",
      "2018-05-23T15:30:54.468895: step 19053, loss 0.108942, acc 0.9375\n",
      "2018-05-23T15:30:54.808985: step 19054, loss 0.114258, acc 0.9375\n",
      "2018-05-23T15:30:55.152069: step 19055, loss 0.170613, acc 0.953125\n",
      "2018-05-23T15:30:55.497145: step 19056, loss 0.046554, acc 0.96875\n",
      "2018-05-23T15:30:55.845215: step 19057, loss 0.132901, acc 0.921875\n",
      "2018-05-23T15:30:56.185304: step 19058, loss 0.0688016, acc 0.953125\n",
      "2018-05-23T15:30:56.528386: step 19059, loss 0.0256816, acc 0.984375\n",
      "2018-05-23T15:30:56.867477: step 19060, loss 0.114523, acc 0.9375\n",
      "2018-05-23T15:30:57.212554: step 19061, loss 0.0914203, acc 0.96875\n",
      "2018-05-23T15:30:57.555639: step 19062, loss 0.191735, acc 0.953125\n",
      "2018-05-23T15:30:57.913680: step 19063, loss 0.16833, acc 0.9375\n",
      "2018-05-23T15:30:58.252774: step 19064, loss 0.184389, acc 0.953125\n",
      "2018-05-23T15:30:58.595856: step 19065, loss 0.0665832, acc 0.96875\n",
      "2018-05-23T15:30:58.941930: step 19066, loss 0.048155, acc 0.984375\n",
      "2018-05-23T15:30:59.292988: step 19067, loss 0.176402, acc 0.921875\n",
      "2018-05-23T15:30:59.636072: step 19068, loss 0.0430437, acc 0.984375\n",
      "2018-05-23T15:30:59.986137: step 19069, loss 0.0536625, acc 0.96875\n",
      "2018-05-23T15:31:00.337197: step 19070, loss 0.0674992, acc 0.96875\n",
      "2018-05-23T15:31:00.674296: step 19071, loss 0.138431, acc 0.953125\n",
      "2018-05-23T15:31:01.029344: step 19072, loss 0.0654524, acc 0.953125\n",
      "2018-05-23T15:31:01.367441: step 19073, loss 0.0522031, acc 0.984375\n",
      "2018-05-23T15:31:01.706533: step 19074, loss 0.0838828, acc 0.9375\n",
      "2018-05-23T15:31:02.052609: step 19075, loss 0.0312528, acc 1\n",
      "2018-05-23T15:31:02.394691: step 19076, loss 0.0796602, acc 0.953125\n",
      "2018-05-23T15:31:02.728872: step 19077, loss 0.15883, acc 0.953125\n",
      "2018-05-23T15:31:03.075909: step 19078, loss 0.0457583, acc 0.96875\n",
      "2018-05-23T15:31:03.451903: step 19079, loss 0.237985, acc 0.90625\n",
      "2018-05-23T15:31:03.793990: step 19080, loss 0.14856, acc 0.953125\n",
      "2018-05-23T15:31:04.142090: step 19081, loss 0.0777927, acc 0.953125\n",
      "2018-05-23T15:31:04.484143: step 19082, loss 0.0874346, acc 0.953125\n",
      "2018-05-23T15:31:04.825233: step 19083, loss 0.0782979, acc 0.9375\n",
      "2018-05-23T15:31:05.195242: step 19084, loss 0.0730534, acc 0.96875\n",
      "2018-05-23T15:31:05.539829: step 19085, loss 0.128279, acc 0.953125\n",
      "2018-05-23T15:31:05.879920: step 19086, loss 0.0931185, acc 0.96875\n",
      "2018-05-23T15:31:06.224995: step 19087, loss 0.109351, acc 0.96875\n",
      "2018-05-23T15:31:06.565085: step 19088, loss 0.0606385, acc 0.96875\n",
      "2018-05-23T15:31:06.949059: step 19089, loss 0.0736415, acc 0.96875\n",
      "2018-05-23T15:31:07.607299: step 19090, loss 0.0858486, acc 0.96875\n",
      "2018-05-23T15:31:08.185752: step 19091, loss 0.10717, acc 0.984375\n",
      "2018-05-23T15:31:08.576706: step 19092, loss 0.200699, acc 0.9375\n",
      "2018-05-23T15:31:08.930759: step 19093, loss 0.0458048, acc 0.984375\n",
      "2018-05-23T15:31:09.326701: step 19094, loss 0.152684, acc 0.96875\n",
      "2018-05-23T15:31:09.942053: step 19095, loss 0.0565398, acc 0.984375\n",
      "2018-05-23T15:31:10.413793: step 19096, loss 0.0555884, acc 0.984375\n",
      "2018-05-23T15:31:11.018175: step 19097, loss 0.112847, acc 0.921875\n",
      "2018-05-23T15:31:11.519830: step 19098, loss 0.0422501, acc 0.984375\n",
      "2018-05-23T15:31:12.017501: step 19099, loss 0.254805, acc 0.890625\n",
      "2018-05-23T15:31:12.397482: step 19100, loss 0.069229, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:31:16.979264: step 19100, loss 2.03269, acc 0.714102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19100\n",
      "\n",
      "2018-05-23T15:31:18.321674: step 19101, loss 0.0365225, acc 0.96875\n",
      "2018-05-23T15:31:18.731575: step 19102, loss 0.059882, acc 0.96875\n",
      "2018-05-23T15:31:19.091649: step 19103, loss 0.0746723, acc 0.96875\n",
      "2018-05-23T15:31:19.700984: step 19104, loss 0.0318397, acc 1\n",
      "2018-05-23T15:31:20.135821: step 19105, loss 0.076639, acc 0.953125\n",
      "2018-05-23T15:31:20.504831: step 19106, loss 0.0279252, acc 1\n",
      "2018-05-23T15:31:20.901713: step 19107, loss 0.0235375, acc 0.984375\n",
      "2018-05-23T15:31:21.465205: step 19108, loss 0.0926657, acc 0.96875\n",
      "2018-05-23T15:31:21.955892: step 19109, loss 0.0399247, acc 0.984375\n",
      "2018-05-23T15:31:22.327896: step 19110, loss 0.0423488, acc 0.984375\n",
      "2018-05-23T15:31:22.742788: step 19111, loss 0.113791, acc 0.96875\n",
      "2018-05-23T15:31:23.097838: step 19112, loss 0.035791, acc 1\n",
      "2018-05-23T15:31:23.439922: step 19113, loss 0.123349, acc 0.921875\n",
      "2018-05-23T15:31:23.790983: step 19114, loss 0.0902923, acc 0.984375\n",
      "2018-05-23T15:31:24.140049: step 19115, loss 0.114641, acc 0.96875\n",
      "2018-05-23T15:31:24.478143: step 19116, loss 0.060395, acc 0.96875\n",
      "2018-05-23T15:31:24.930933: step 19117, loss 0.0627027, acc 0.96875\n",
      "2018-05-23T15:31:25.349816: step 19118, loss 0.0658992, acc 0.96875\n",
      "2018-05-23T15:31:26.046950: step 19119, loss 0.0968591, acc 0.953125\n",
      "2018-05-23T15:31:26.439898: step 19120, loss 0.0498895, acc 0.984375\n",
      "2018-05-23T15:31:26.923607: step 19121, loss 0.208397, acc 0.90625\n",
      "2018-05-23T15:31:27.533969: step 19122, loss 0.0597443, acc 0.96875\n",
      "2018-05-23T15:31:28.069537: step 19123, loss 0.0834302, acc 0.953125\n",
      "2018-05-23T15:31:28.606109: step 19124, loss 0.0776084, acc 0.953125\n",
      "2018-05-23T15:31:29.098784: step 19125, loss 0.0691748, acc 0.9375\n",
      "2018-05-23T15:31:29.477771: step 19126, loss 0.105254, acc 0.96875\n",
      "2018-05-23T15:31:29.862739: step 19127, loss 0.0638923, acc 0.96875\n",
      "2018-05-23T15:31:30.482085: step 19128, loss 0.0825855, acc 0.953125\n",
      "2018-05-23T15:31:30.918915: step 19129, loss 0.0917508, acc 0.9375\n",
      "2018-05-23T15:31:31.291918: step 19130, loss 0.0470131, acc 0.984375\n",
      "2018-05-23T15:31:31.644974: step 19131, loss 0.0869181, acc 0.96875\n",
      "2018-05-23T15:31:31.993043: step 19132, loss 0.124845, acc 0.953125\n",
      "2018-05-23T15:31:32.348090: step 19133, loss 0.0392211, acc 1\n",
      "2018-05-23T15:31:32.691174: step 19134, loss 0.249267, acc 0.96875\n",
      "2018-05-23T15:31:33.047222: step 19135, loss 0.208383, acc 0.9375\n",
      "2018-05-23T15:31:33.425211: step 19136, loss 0.0234394, acc 1\n",
      "2018-05-23T15:31:33.765302: step 19137, loss 0.0606041, acc 0.96875\n",
      "2018-05-23T15:31:34.115362: step 19138, loss 0.0386908, acc 0.984375\n",
      "2018-05-23T15:31:34.470412: step 19139, loss 0.0906515, acc 0.953125\n",
      "2018-05-23T15:31:34.807513: step 19140, loss 0.0345124, acc 0.984375\n",
      "2018-05-23T15:31:35.157574: step 19141, loss 0.121217, acc 0.9375\n",
      "2018-05-23T15:31:35.508637: step 19142, loss 0.117444, acc 0.953125\n",
      "2018-05-23T15:31:35.877649: step 19143, loss 0.0738763, acc 0.984375\n",
      "2018-05-23T15:31:36.548855: step 19144, loss 0.114548, acc 0.953125\n",
      "2018-05-23T15:31:37.109359: step 19145, loss 0.149901, acc 0.921875\n",
      "2018-05-23T15:31:37.634948: step 19146, loss 0.0499377, acc 1\n",
      "2018-05-23T15:31:38.155556: step 19147, loss 0.103743, acc 0.90625\n",
      "2018-05-23T15:31:38.683144: step 19148, loss 0.123445, acc 0.9375\n",
      "2018-05-23T15:31:39.069114: step 19149, loss 0.135471, acc 0.953125\n",
      "2018-05-23T15:31:39.437126: step 19150, loss 0.0488315, acc 1\n",
      "2018-05-23T15:31:39.789186: step 19151, loss 0.0663398, acc 0.984375\n",
      "2018-05-23T15:31:40.149223: step 19152, loss 0.0442389, acc 1\n",
      "2018-05-23T15:31:40.497292: step 19153, loss 0.0865087, acc 0.953125\n",
      "2018-05-23T15:31:40.851344: step 19154, loss 0.0541927, acc 0.953125\n",
      "2018-05-23T15:31:41.196422: step 19155, loss 0.103927, acc 0.953125\n",
      "2018-05-23T15:31:41.548503: step 19156, loss 0.0573861, acc 0.984375\n",
      "2018-05-23T15:31:41.918488: step 19157, loss 0.106026, acc 0.984375\n",
      "2018-05-23T15:31:42.318419: step 19158, loss 0.0590951, acc 0.96875\n",
      "2018-05-23T15:31:43.003586: step 19159, loss 0.0610906, acc 0.96875\n",
      "2018-05-23T15:31:43.488289: step 19160, loss 0.0879773, acc 0.953125\n",
      "2018-05-23T15:31:43.939082: step 19161, loss 0.0263997, acc 1\n",
      "2018-05-23T15:31:44.364943: step 19162, loss 0.073014, acc 0.953125\n",
      "2018-05-23T15:31:44.938410: step 19163, loss 0.0497431, acc 1\n",
      "2018-05-23T15:31:45.459017: step 19164, loss 0.106363, acc 0.953125\n",
      "2018-05-23T15:31:45.918787: step 19165, loss 0.0335859, acc 0.984375\n",
      "2018-05-23T15:31:46.410472: step 19166, loss 0.150504, acc 0.953125\n",
      "2018-05-23T15:31:46.925094: step 19167, loss 0.0574461, acc 0.96875\n",
      "2018-05-23T15:31:47.516513: step 19168, loss 0.114784, acc 0.953125\n",
      "2018-05-23T15:31:47.934395: step 19169, loss 0.0393871, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:31:48.301933: step 19170, loss 0.0875754, acc 0.984375\n",
      "2018-05-23T15:31:48.750733: step 19171, loss 0.0888784, acc 0.9375\n",
      "2018-05-23T15:31:49.159638: step 19172, loss 0.275157, acc 0.875\n",
      "2018-05-23T15:31:49.607443: step 19173, loss 0.120373, acc 0.96875\n",
      "2018-05-23T15:31:50.152981: step 19174, loss 0.109411, acc 0.96875\n",
      "2018-05-23T15:31:50.591808: step 19175, loss 0.0648831, acc 0.96875\n",
      "2018-05-23T15:31:51.004702: step 19176, loss 0.176856, acc 0.953125\n",
      "2018-05-23T15:31:51.406628: step 19177, loss 0.129804, acc 0.96875\n",
      "2018-05-23T15:31:51.775641: step 19178, loss 0.0463986, acc 0.984375\n",
      "2018-05-23T15:31:52.235409: step 19179, loss 0.148976, acc 0.921875\n",
      "2018-05-23T15:31:52.680219: step 19180, loss 0.0603549, acc 0.96875\n",
      "2018-05-23T15:31:53.113062: step 19181, loss 0.146338, acc 0.96875\n",
      "2018-05-23T15:31:53.525957: step 19182, loss 0.125358, acc 0.921875\n",
      "2018-05-23T15:31:53.899959: step 19183, loss 0.0817384, acc 0.984375\n",
      "2018-05-23T15:31:54.243039: step 19184, loss 0.0427376, acc 0.984375\n",
      "2018-05-23T15:31:54.591109: step 19185, loss 0.134965, acc 0.984375\n",
      "2018-05-23T15:31:54.940250: step 19186, loss 0.132258, acc 0.9375\n",
      "2018-05-23T15:31:55.304278: step 19187, loss 0.0430521, acc 0.984375\n",
      "2018-05-23T15:31:55.721194: step 19188, loss 0.0505771, acc 1\n",
      "2018-05-23T15:31:56.104136: step 19189, loss 0.092875, acc 0.96875\n",
      "2018-05-23T15:31:56.529997: step 19190, loss 0.129299, acc 0.953125\n",
      "2018-05-23T15:31:56.964831: step 19191, loss 0.0877163, acc 0.96875\n",
      "2018-05-23T15:31:57.565226: step 19192, loss 0.0526241, acc 0.984375\n",
      "2018-05-23T15:31:58.207507: step 19193, loss 0.0475606, acc 0.96875\n",
      "2018-05-23T15:31:58.903645: step 19194, loss 0.0725413, acc 0.96875\n",
      "2018-05-23T15:31:59.569864: step 19195, loss 0.0592342, acc 0.984375\n",
      "2018-05-23T15:32:00.039606: step 19196, loss 0.155588, acc 0.890625\n",
      "2018-05-23T15:32:00.481425: step 19197, loss 0.0408163, acc 1\n",
      "2018-05-23T15:32:00.897312: step 19198, loss 0.0663263, acc 0.96875\n",
      "2018-05-23T15:32:01.365061: step 19199, loss 0.109181, acc 0.953125\n",
      "2018-05-23T15:32:01.864725: step 19200, loss 0.0366764, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:32:08.062145: step 19200, loss 2.02675, acc 0.711244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19200\n",
      "\n",
      "2018-05-23T15:32:10.057805: step 19201, loss 0.0570079, acc 0.984375\n",
      "2018-05-23T15:32:10.640247: step 19202, loss 0.0226232, acc 1\n",
      "2018-05-23T15:32:11.175814: step 19203, loss 0.136071, acc 0.96875\n",
      "2018-05-23T15:32:11.872951: step 19204, loss 0.0581955, acc 0.953125\n",
      "2018-05-23T15:32:12.619951: step 19205, loss 0.103615, acc 0.96875\n",
      "2018-05-23T15:32:13.082714: step 19206, loss 0.0846954, acc 0.96875\n",
      "2018-05-23T15:32:13.505583: step 19207, loss 0.0676415, acc 0.984375\n",
      "2018-05-23T15:32:13.928451: step 19208, loss 0.0621124, acc 0.984375\n",
      "2018-05-23T15:32:14.392211: step 19209, loss 0.11678, acc 0.96875\n",
      "2018-05-23T15:32:14.811089: step 19210, loss 0.0480308, acc 0.96875\n",
      "2018-05-23T15:32:15.285819: step 19211, loss 0.0594269, acc 0.984375\n",
      "2018-05-23T15:32:16.077702: step 19212, loss 0.187655, acc 0.921875\n",
      "2018-05-23T15:32:16.604294: step 19213, loss 0.0359086, acc 1\n",
      "2018-05-23T15:32:17.028161: step 19214, loss 0.042447, acc 1\n",
      "2018-05-23T15:32:17.422105: step 19215, loss 0.0311808, acc 1\n",
      "2018-05-23T15:32:18.075357: step 19216, loss 0.0746334, acc 0.984375\n",
      "2018-05-23T15:32:18.505207: step 19217, loss 0.164851, acc 0.953125\n",
      "2018-05-23T15:32:18.876559: step 19218, loss 0.072344, acc 0.984375\n",
      "2018-05-23T15:32:19.231609: step 19219, loss 0.0826546, acc 0.953125\n",
      "2018-05-23T15:32:19.592643: step 19220, loss 0.0786998, acc 0.953125\n",
      "2018-05-23T15:32:19.987590: step 19221, loss 0.0229914, acc 0.984375\n",
      "2018-05-23T15:32:20.377546: step 19222, loss 0.08075, acc 0.96875\n",
      "2018-05-23T15:32:20.740574: step 19223, loss 0.294172, acc 0.9375\n",
      "2018-05-23T15:32:21.180400: step 19224, loss 0.0705761, acc 0.96875\n",
      "2018-05-23T15:32:21.643159: step 19225, loss 0.120683, acc 0.953125\n",
      "2018-05-23T15:32:22.238568: step 19226, loss 0.0644238, acc 0.96875\n",
      "2018-05-23T15:32:22.746209: step 19227, loss 0.151666, acc 0.9375\n",
      "2018-05-23T15:32:23.125196: step 19228, loss 0.0520734, acc 0.984375\n",
      "2018-05-23T15:32:23.493211: step 19229, loss 0.144997, acc 0.921875\n",
      "2018-05-23T15:32:23.845268: step 19230, loss 0.0988812, acc 0.984375\n",
      "2018-05-23T15:32:24.226250: step 19231, loss 0.126274, acc 0.9375\n",
      "2018-05-23T15:32:24.612217: step 19232, loss 0.157592, acc 0.9375\n",
      "2018-05-23T15:32:24.999182: step 19233, loss 0.103956, acc 0.9375\n",
      "2018-05-23T15:32:25.466929: step 19234, loss 0.0406702, acc 0.984375\n",
      "2018-05-23T15:32:26.448310: step 19235, loss 0.0975726, acc 0.96875\n",
      "2018-05-23T15:32:27.215253: step 19236, loss 0.0766717, acc 0.984375\n",
      "2018-05-23T15:32:27.813654: step 19237, loss 0.137087, acc 0.953125\n",
      "2018-05-23T15:32:29.103202: step 19238, loss 0.0325886, acc 1\n",
      "2018-05-23T15:32:29.952930: step 19239, loss 0.0998723, acc 0.953125\n",
      "2018-05-23T15:32:30.634107: step 19240, loss 0.12594, acc 0.9375\n",
      "2018-05-23T15:32:31.266416: step 19241, loss 0.0358006, acc 1\n",
      "2018-05-23T15:32:31.856836: step 19242, loss 0.147902, acc 0.96875\n",
      "2018-05-23T15:32:32.411354: step 19243, loss 0.206055, acc 0.9375\n",
      "2018-05-23T15:32:32.958887: step 19244, loss 0.0878441, acc 0.9375\n",
      "2018-05-23T15:32:33.511409: step 19245, loss 0.0661841, acc 0.953125\n",
      "2018-05-23T15:32:34.045981: step 19246, loss 0.103488, acc 0.984375\n",
      "2018-05-23T15:32:34.936597: step 19247, loss 0.0818522, acc 0.96875\n",
      "2018-05-23T15:32:35.813253: step 19248, loss 0.0287945, acc 1\n",
      "2018-05-23T15:32:36.599150: step 19249, loss 0.0633976, acc 0.96875\n",
      "2018-05-23T15:32:37.358121: step 19250, loss 0.11279, acc 0.953125\n",
      "2018-05-23T15:32:38.012369: step 19251, loss 0.0564604, acc 0.96875\n",
      "2018-05-23T15:32:38.649665: step 19252, loss 0.0911421, acc 0.9375\n",
      "2018-05-23T15:32:39.255045: step 19253, loss 0.174412, acc 0.921875\n",
      "2018-05-23T15:32:40.031965: step 19254, loss 0.152335, acc 0.96875\n",
      "2018-05-23T15:32:40.879698: step 19255, loss 0.0300456, acc 1\n",
      "2018-05-23T15:32:42.040675: step 19256, loss 0.130523, acc 0.953125\n",
      "2018-05-23T15:32:42.952235: step 19257, loss 0.0356165, acc 1\n",
      "2018-05-23T15:32:43.654357: step 19258, loss 0.0584713, acc 0.96875\n",
      "2018-05-23T15:32:44.258740: step 19259, loss 0.0590872, acc 0.984375\n",
      "2018-05-23T15:32:44.818245: step 19260, loss 0.0421102, acc 0.984375\n",
      "2018-05-23T15:32:45.384729: step 19261, loss 0.283084, acc 0.9375\n",
      "2018-05-23T15:32:45.943234: step 19262, loss 0.0752633, acc 0.9375\n",
      "2018-05-23T15:32:46.488775: step 19263, loss 0.179793, acc 0.9375\n",
      "2018-05-23T15:32:47.018358: step 19264, loss 0.037581, acc 0.984375\n",
      "2018-05-23T15:32:47.535975: step 19265, loss 0.138665, acc 0.90625\n",
      "2018-05-23T15:32:48.063562: step 19266, loss 0.0301366, acc 0.984375\n",
      "2018-05-23T15:32:48.609104: step 19267, loss 0.0859851, acc 0.953125\n",
      "2018-05-23T15:32:49.135694: step 19268, loss 0.0337377, acc 1\n",
      "2018-05-23T15:32:49.687219: step 19269, loss 0.0323523, acc 0.984375\n",
      "2018-05-23T15:32:50.212814: step 19270, loss 0.0575958, acc 0.96875\n",
      "2018-05-23T15:32:50.817195: step 19271, loss 0.200049, acc 0.953125\n",
      "2018-05-23T15:32:51.365727: step 19272, loss 0.0535109, acc 0.96875\n",
      "2018-05-23T15:32:51.930218: step 19273, loss 0.0896577, acc 0.953125\n",
      "2018-05-23T15:32:52.454815: step 19274, loss 0.0820056, acc 0.96875\n",
      "2018-05-23T15:32:53.067177: step 19275, loss 0.0992249, acc 0.953125\n",
      "2018-05-23T15:32:53.604739: step 19276, loss 0.062591, acc 0.984375\n",
      "2018-05-23T15:32:54.106397: step 19277, loss 0.0564331, acc 0.984375\n",
      "2018-05-23T15:32:54.625009: step 19278, loss 0.177142, acc 0.953125\n",
      "2018-05-23T15:32:55.110710: step 19279, loss 0.0916695, acc 0.953125\n",
      "2018-05-23T15:32:55.601396: step 19280, loss 0.161518, acc 0.9375\n",
      "2018-05-23T15:32:56.094079: step 19281, loss 0.200731, acc 0.9375\n",
      "2018-05-23T15:32:56.601720: step 19282, loss 0.108807, acc 0.9375\n",
      "2018-05-23T15:32:57.097394: step 19283, loss 0.063296, acc 1\n",
      "2018-05-23T15:32:57.575118: step 19284, loss 0.0589649, acc 0.96875\n",
      "2018-05-23T15:32:58.058823: step 19285, loss 0.0889, acc 0.96875\n",
      "2018-05-23T15:32:58.748976: step 19286, loss 0.0981004, acc 0.9375\n",
      "2018-05-23T15:32:59.305488: step 19287, loss 0.0595061, acc 0.96875\n",
      "2018-05-23T15:32:59.827092: step 19288, loss 0.093391, acc 0.96875\n",
      "2018-05-23T15:33:00.371636: step 19289, loss 0.0443203, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:33:00.893239: step 19290, loss 0.118695, acc 0.921875\n",
      "2018-05-23T15:33:01.417836: step 19291, loss 0.0567365, acc 1\n",
      "2018-05-23T15:33:01.922487: step 19292, loss 0.0765371, acc 0.96875\n",
      "2018-05-23T15:33:02.444091: step 19293, loss 0.0611334, acc 0.984375\n",
      "2018-05-23T15:33:02.979658: step 19294, loss 0.0634656, acc 0.96875\n",
      "2018-05-23T15:33:03.520214: step 19295, loss 0.0853912, acc 0.96875\n",
      "2018-05-23T15:33:04.051790: step 19296, loss 0.184233, acc 0.953125\n",
      "2018-05-23T15:33:04.585363: step 19297, loss 0.0722415, acc 0.96875\n",
      "2018-05-23T15:33:05.170797: step 19298, loss 0.0964251, acc 0.953125\n",
      "2018-05-23T15:33:05.742267: step 19299, loss 0.0918923, acc 0.96875\n",
      "2018-05-23T15:33:06.276839: step 19300, loss 0.11523, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:33:12.248861: step 19300, loss 2.03376, acc 0.71253\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19300\n",
      "\n",
      "2018-05-23T15:33:14.472911: step 19301, loss 0.0533073, acc 0.984375\n",
      "2018-05-23T15:33:14.970580: step 19302, loss 0.0244216, acc 1\n",
      "2018-05-23T15:33:15.474233: step 19303, loss 0.0463598, acc 0.96875\n",
      "2018-05-23T15:33:15.987860: step 19304, loss 0.131842, acc 0.953125\n",
      "2018-05-23T15:33:16.558333: step 19305, loss 0.116476, acc 0.96875\n",
      "2018-05-23T15:33:17.106865: step 19306, loss 0.0820687, acc 0.96875\n",
      "2018-05-23T15:33:17.643430: step 19307, loss 0.0694254, acc 0.984375\n",
      "2018-05-23T15:33:18.167030: step 19308, loss 0.234403, acc 0.953125\n",
      "2018-05-23T15:33:18.696612: step 19309, loss 0.0596477, acc 0.984375\n",
      "2018-05-23T15:33:19.209240: step 19310, loss 0.103819, acc 0.953125\n",
      "2018-05-23T15:33:19.704916: step 19311, loss 0.111255, acc 0.96875\n",
      "2018-05-23T15:33:20.221534: step 19312, loss 0.0319355, acc 0.984375\n",
      "2018-05-23T15:33:20.721196: step 19313, loss 0.0753272, acc 0.96875\n",
      "2018-05-23T15:33:21.223853: step 19314, loss 0.121075, acc 0.9375\n",
      "2018-05-23T15:33:21.712544: step 19315, loss 0.0777178, acc 0.953125\n",
      "2018-05-23T15:33:22.201236: step 19316, loss 0.100458, acc 0.921875\n",
      "2018-05-23T15:33:22.732814: step 19317, loss 0.149832, acc 0.9375\n",
      "2018-05-23T15:33:23.221507: step 19318, loss 0.0826892, acc 0.984375\n",
      "2018-05-23T15:33:23.701225: step 19319, loss 0.0939105, acc 0.9375\n",
      "2018-05-23T15:33:24.195900: step 19320, loss 0.0545121, acc 0.984375\n",
      "2018-05-23T15:33:24.671628: step 19321, loss 0.0487307, acc 0.984375\n",
      "2018-05-23T15:33:25.177274: step 19322, loss 0.0972849, acc 0.9375\n",
      "2018-05-23T15:33:25.658986: step 19323, loss 0.0782852, acc 0.953125\n",
      "2018-05-23T15:33:26.136708: step 19324, loss 0.107655, acc 0.9375\n",
      "2018-05-23T15:33:26.646345: step 19325, loss 0.0586302, acc 0.984375\n",
      "2018-05-23T15:33:27.131050: step 19326, loss 0.154092, acc 0.90625\n",
      "2018-05-23T15:33:27.592812: step 19327, loss 0.193635, acc 0.9375\n",
      "2018-05-23T15:33:28.063553: step 19328, loss 0.158774, acc 0.96875\n",
      "2018-05-23T15:33:28.529308: step 19329, loss 0.118472, acc 0.9375\n",
      "2018-05-23T15:33:28.999050: step 19330, loss 0.069682, acc 0.984375\n",
      "2018-05-23T15:33:29.455829: step 19331, loss 0.0534629, acc 0.984375\n",
      "2018-05-23T15:33:29.908618: step 19332, loss 0.0429014, acc 0.984375\n",
      "2018-05-23T15:33:30.361407: step 19333, loss 0.109286, acc 0.921875\n",
      "2018-05-23T15:33:30.825165: step 19334, loss 0.133004, acc 0.921875\n",
      "2018-05-23T15:33:31.312861: step 19335, loss 0.0320532, acc 1\n",
      "2018-05-23T15:33:31.773628: step 19336, loss 0.1053, acc 0.9375\n",
      "2018-05-23T15:33:32.229410: step 19337, loss 0.0540753, acc 0.984375\n",
      "2018-05-23T15:33:32.708129: step 19338, loss 0.116043, acc 0.953125\n",
      "2018-05-23T15:33:33.189840: step 19339, loss 0.0702606, acc 0.984375\n",
      "2018-05-23T15:33:33.695487: step 19340, loss 0.0558861, acc 0.984375\n",
      "2018-05-23T15:33:34.147278: step 19341, loss 0.0986799, acc 0.953125\n",
      "2018-05-23T15:33:34.599070: step 19342, loss 0.15353, acc 0.90625\n",
      "2018-05-23T15:33:35.051858: step 19343, loss 0.0970822, acc 0.96875\n",
      "2018-05-23T15:33:35.518610: step 19344, loss 0.111225, acc 0.953125\n",
      "2018-05-23T15:33:35.960428: step 19345, loss 0.170519, acc 0.953125\n",
      "2018-05-23T15:33:36.414216: step 19346, loss 0.05063, acc 0.984375\n",
      "2018-05-23T15:33:36.870992: step 19347, loss 0.0553531, acc 0.96875\n",
      "2018-05-23T15:33:37.333755: step 19348, loss 0.0323877, acc 0.984375\n",
      "2018-05-23T15:33:37.832420: step 19349, loss 0.0633548, acc 0.96875\n",
      "2018-05-23T15:33:38.293188: step 19350, loss 0.0586381, acc 0.984375\n",
      "2018-05-23T15:33:38.746974: step 19351, loss 0.0333255, acc 1\n",
      "2018-05-23T15:33:39.194776: step 19352, loss 0.154041, acc 0.90625\n",
      "2018-05-23T15:33:39.654546: step 19353, loss 0.0749969, acc 0.96875\n",
      "2018-05-23T15:33:40.095367: step 19354, loss 0.079076, acc 0.953125\n",
      "2018-05-23T15:33:40.541173: step 19355, loss 0.0828203, acc 0.953125\n",
      "2018-05-23T15:33:40.986981: step 19356, loss 0.0300199, acc 1\n",
      "2018-05-23T15:33:41.444757: step 19357, loss 0.044203, acc 1\n",
      "2018-05-23T15:33:41.939433: step 19358, loss 0.0581252, acc 0.984375\n",
      "2018-05-23T15:33:42.397209: step 19359, loss 0.174814, acc 0.921875\n",
      "2018-05-23T15:33:42.854984: step 19360, loss 0.0446738, acc 1\n",
      "2018-05-23T15:33:43.318742: step 19361, loss 0.0751174, acc 0.96875\n",
      "2018-05-23T15:33:43.773527: step 19362, loss 0.0263835, acc 1\n",
      "2018-05-23T15:33:44.248257: step 19363, loss 0.0853022, acc 0.96875\n",
      "2018-05-23T15:33:44.708027: step 19364, loss 0.130028, acc 0.953125\n",
      "2018-05-23T15:33:45.171786: step 19365, loss 0.0644157, acc 0.96875\n",
      "2018-05-23T15:33:45.668459: step 19366, loss 0.063648, acc 0.953125\n",
      "2018-05-23T15:33:46.135209: step 19367, loss 0.202574, acc 0.9375\n",
      "2018-05-23T15:33:46.596975: step 19368, loss 0.0411824, acc 1\n",
      "2018-05-23T15:33:47.056743: step 19369, loss 0.0873822, acc 0.953125\n",
      "2018-05-23T15:33:47.508535: step 19370, loss 0.136525, acc 0.9375\n",
      "2018-05-23T15:33:47.957335: step 19371, loss 0.0693367, acc 0.984375\n",
      "2018-05-23T15:33:48.416107: step 19372, loss 0.100602, acc 0.953125\n",
      "2018-05-23T15:33:48.944692: step 19373, loss 0.34438, acc 0.921875\n",
      "2018-05-23T15:33:49.388506: step 19374, loss 0.0468251, acc 0.96875\n",
      "2018-05-23T15:33:49.842290: step 19375, loss 0.154822, acc 0.9375\n",
      "2018-05-23T15:33:50.308046: step 19376, loss 0.0807813, acc 0.96875\n",
      "2018-05-23T15:33:50.753853: step 19377, loss 0.0952365, acc 0.953125\n",
      "2018-05-23T15:33:51.209634: step 19378, loss 0.0794612, acc 0.96875\n",
      "2018-05-23T15:33:51.666411: step 19379, loss 0.0466802, acc 0.984375\n",
      "2018-05-23T15:33:52.129174: step 19380, loss 0.0902856, acc 0.96875\n",
      "2018-05-23T15:33:52.596923: step 19381, loss 0.0479401, acc 0.96875\n",
      "2018-05-23T15:33:53.112542: step 19382, loss 0.03512, acc 0.984375\n",
      "2018-05-23T15:33:53.564335: step 19383, loss 0.432323, acc 0.890625\n",
      "2018-05-23T15:33:54.035074: step 19384, loss 0.0712188, acc 0.96875\n",
      "2018-05-23T15:33:54.484872: step 19385, loss 0.0187121, acc 1\n",
      "2018-05-23T15:33:54.908738: step 19386, loss 0.056463, acc 0.953125\n",
      "2018-05-23T15:33:55.315648: step 19387, loss 0.0405354, acc 0.96875\n",
      "2018-05-23T15:33:55.702613: step 19388, loss 0.0556155, acc 0.984375\n",
      "2018-05-23T15:33:56.111522: step 19389, loss 0.0519718, acc 0.984375\n",
      "2018-05-23T15:33:56.508460: step 19390, loss 0.0579873, acc 0.96875\n",
      "2018-05-23T15:33:56.907390: step 19391, loss 0.0436893, acc 1\n",
      "2018-05-23T15:33:57.329261: step 19392, loss 0.16112, acc 0.90625\n",
      "2018-05-23T15:33:57.714233: step 19393, loss 0.0775363, acc 0.953125\n",
      "2018-05-23T15:33:58.102195: step 19394, loss 0.276378, acc 0.96875\n",
      "2018-05-23T15:33:58.523069: step 19395, loss 0.0917257, acc 0.9375\n",
      "2018-05-23T15:33:58.922998: step 19396, loss 0.119824, acc 0.984375\n",
      "2018-05-23T15:33:59.378780: step 19397, loss 0.269523, acc 0.953125\n",
      "2018-05-23T15:33:59.766742: step 19398, loss 0.104239, acc 0.9375\n",
      "2018-05-23T15:34:00.181632: step 19399, loss 0.0579784, acc 1\n",
      "2018-05-23T15:34:00.568597: step 19400, loss 0.0532148, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:34:05.528329: step 19400, loss 2.07687, acc 0.704958\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19400\n",
      "\n",
      "2018-05-23T15:34:07.165947: step 19401, loss 0.0820029, acc 0.953125\n",
      "2018-05-23T15:34:07.594800: step 19402, loss 0.080813, acc 0.9375\n",
      "2018-05-23T15:34:07.994729: step 19403, loss 0.0725627, acc 0.96875\n",
      "2018-05-23T15:34:08.397652: step 19404, loss 0.0593364, acc 1\n",
      "2018-05-23T15:34:08.773647: step 19405, loss 0.0872333, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:34:09.177565: step 19406, loss 0.0780049, acc 0.984375\n",
      "2018-05-23T15:34:09.573507: step 19407, loss 0.0715338, acc 0.984375\n",
      "2018-05-23T15:34:09.969447: step 19408, loss 0.0997381, acc 0.9375\n",
      "2018-05-23T15:34:10.353420: step 19409, loss 0.0449501, acc 1\n",
      "2018-05-23T15:34:10.786265: step 19410, loss 0.122964, acc 0.921875\n",
      "2018-05-23T15:34:11.169238: step 19411, loss 0.135689, acc 0.96875\n",
      "2018-05-23T15:34:11.566175: step 19412, loss 0.210685, acc 0.9375\n",
      "2018-05-23T15:34:11.955135: step 19413, loss 0.143546, acc 0.953125\n",
      "2018-05-23T15:34:12.361050: step 19414, loss 0.0614056, acc 0.984375\n",
      "2018-05-23T15:34:12.763972: step 19415, loss 0.0628122, acc 0.984375\n",
      "2018-05-23T15:34:13.159912: step 19416, loss 0.133162, acc 0.90625\n",
      "2018-05-23T15:34:13.569815: step 19417, loss 0.0366151, acc 0.984375\n",
      "2018-05-23T15:34:13.954786: step 19418, loss 0.0428773, acc 0.984375\n",
      "2018-05-23T15:34:14.357707: step 19419, loss 0.0438951, acc 1\n",
      "2018-05-23T15:34:14.725722: step 19420, loss 0.12605, acc 0.953125\n",
      "2018-05-23T15:34:15.088753: step 19421, loss 0.0969719, acc 0.984375\n",
      "2018-05-23T15:34:15.509626: step 19422, loss 0.0893041, acc 0.953125\n",
      "2018-05-23T15:34:15.910553: step 19423, loss 0.158382, acc 0.90625\n",
      "2018-05-23T15:34:16.316467: step 19424, loss 0.0653559, acc 0.984375\n",
      "2018-05-23T15:34:16.744325: step 19425, loss 0.0313215, acc 0.984375\n",
      "2018-05-23T15:34:17.132285: step 19426, loss 0.0644074, acc 0.96875\n",
      "2018-05-23T15:34:17.587071: step 19427, loss 0.0759638, acc 0.984375\n",
      "2018-05-23T15:34:18.023899: step 19428, loss 0.0331918, acc 0.984375\n",
      "2018-05-23T15:34:18.455744: step 19429, loss 0.0687892, acc 0.953125\n",
      "2018-05-23T15:34:18.806805: step 19430, loss 0.22613, acc 0.921875\n",
      "2018-05-23T15:34:19.167845: step 19431, loss 0.0397651, acc 0.984375\n",
      "2018-05-23T15:34:19.509927: step 19432, loss 0.0918514, acc 0.953125\n",
      "2018-05-23T15:34:19.859988: step 19433, loss 0.176247, acc 0.984375\n",
      "2018-05-23T15:34:20.207058: step 19434, loss 0.161456, acc 0.890625\n",
      "2018-05-23T15:34:20.544157: step 19435, loss 0.236694, acc 0.953125\n",
      "2018-05-23T15:34:20.896217: step 19436, loss 0.189288, acc 0.953125\n",
      "2018-05-23T15:34:21.234314: step 19437, loss 0.0714976, acc 0.953125\n",
      "2018-05-23T15:34:21.576398: step 19438, loss 0.0524555, acc 0.984375\n",
      "2018-05-23T15:34:21.918483: step 19439, loss 0.0890469, acc 0.9375\n",
      "2018-05-23T15:34:22.321406: step 19440, loss 0.0966358, acc 0.96875\n",
      "2018-05-23T15:34:22.768207: step 19441, loss 0.0714811, acc 0.96875\n",
      "2018-05-23T15:34:23.134228: step 19442, loss 0.0856458, acc 0.953125\n",
      "2018-05-23T15:34:23.476313: step 19443, loss 0.0488806, acc 0.984375\n",
      "2018-05-23T15:34:23.815406: step 19444, loss 0.111367, acc 0.9375\n",
      "2018-05-23T15:34:24.162479: step 19445, loss 0.101621, acc 0.96875\n",
      "2018-05-23T15:34:24.503565: step 19446, loss 0.123791, acc 0.9375\n",
      "2018-05-23T15:34:24.844653: step 19447, loss 0.162463, acc 0.921875\n",
      "2018-05-23T15:34:25.209677: step 19448, loss 0.0617641, acc 0.96875\n",
      "2018-05-23T15:34:25.547806: step 19449, loss 0.100793, acc 0.984375\n",
      "2018-05-23T15:34:25.885867: step 19450, loss 0.0825218, acc 0.953125\n",
      "2018-05-23T15:34:26.233936: step 19451, loss 0.158892, acc 0.96875\n",
      "2018-05-23T15:34:26.578015: step 19452, loss 0.0373408, acc 0.96875\n",
      "2018-05-23T15:34:26.922095: step 19453, loss 0.0776092, acc 0.96875\n",
      "2018-05-23T15:34:27.281135: step 19454, loss 0.101704, acc 0.953125\n",
      "2018-05-23T15:34:27.623219: step 19455, loss 0.127315, acc 0.9375\n",
      "2018-05-23T15:34:27.971291: step 19456, loss 0.069831, acc 0.96875\n",
      "2018-05-23T15:34:28.318362: step 19457, loss 0.0337464, acc 0.984375\n",
      "2018-05-23T15:34:28.654460: step 19458, loss 0.0435993, acc 0.96875\n",
      "2018-05-23T15:34:29.001535: step 19459, loss 0.0437778, acc 1\n",
      "2018-05-23T15:34:29.349603: step 19460, loss 0.0464495, acc 1\n",
      "2018-05-23T15:34:29.702656: step 19461, loss 0.0468301, acc 0.984375\n",
      "2018-05-23T15:34:30.048733: step 19462, loss 0.0299939, acc 1\n",
      "2018-05-23T15:34:30.416749: step 19463, loss 0.121496, acc 0.9375\n",
      "2018-05-23T15:34:30.763852: step 19464, loss 0.0882917, acc 0.953125\n",
      "2018-05-23T15:34:31.105902: step 19465, loss 0.10242, acc 0.953125\n",
      "2018-05-23T15:34:31.456963: step 19466, loss 0.0611954, acc 0.96875\n",
      "2018-05-23T15:34:31.796056: step 19467, loss 0.110024, acc 0.9375\n",
      "2018-05-23T15:34:32.138142: step 19468, loss 0.0794129, acc 0.953125\n",
      "2018-05-23T15:34:32.488947: step 19469, loss 0.19403, acc 0.921875\n",
      "2018-05-23T15:34:32.826045: step 19470, loss 0.057388, acc 0.953125\n",
      "2018-05-23T15:34:33.170124: step 19471, loss 0.0811438, acc 0.96875\n",
      "2018-05-23T15:34:33.539135: step 19472, loss 0.177024, acc 0.953125\n",
      "2018-05-23T15:34:33.875238: step 19473, loss 0.0867312, acc 0.984375\n",
      "2018-05-23T15:34:34.229289: step 19474, loss 0.132176, acc 0.953125\n",
      "2018-05-23T15:34:34.596307: step 19475, loss 0.0378901, acc 1\n",
      "2018-05-23T15:34:34.935402: step 19476, loss 0.142881, acc 0.953125\n",
      "2018-05-23T15:34:35.281984: step 19477, loss 0.0771186, acc 0.953125\n",
      "2018-05-23T15:34:35.635039: step 19478, loss 0.0889526, acc 0.9375\n",
      "2018-05-23T15:34:35.986102: step 19479, loss 0.0806944, acc 0.953125\n",
      "2018-05-23T15:34:36.394010: step 19480, loss 0.114906, acc 0.953125\n",
      "2018-05-23T15:34:36.864754: step 19481, loss 0.13999, acc 0.96875\n",
      "2018-05-23T15:34:37.406302: step 19482, loss 0.0571104, acc 0.96875\n",
      "2018-05-23T15:34:38.226109: step 19483, loss 0.0356098, acc 1\n",
      "2018-05-23T15:34:39.041927: step 19484, loss 0.0444239, acc 0.984375\n",
      "2018-05-23T15:34:39.522640: step 19485, loss 0.132694, acc 0.921875\n",
      "2018-05-23T15:34:40.044246: step 19486, loss 0.0877535, acc 0.953125\n",
      "2018-05-23T15:34:40.724424: step 19487, loss 0.177923, acc 0.9375\n",
      "2018-05-23T15:34:41.163250: step 19488, loss 0.0572339, acc 0.984375\n",
      "2018-05-23T15:34:41.583127: step 19489, loss 0.10951, acc 0.9375\n",
      "2018-05-23T15:34:42.077805: step 19490, loss 0.0483256, acc 0.96875\n",
      "2018-05-23T15:34:42.604398: step 19491, loss 0.0752922, acc 0.9375\n",
      "2018-05-23T15:34:43.238700: step 19492, loss 0.0992412, acc 0.984375\n",
      "2018-05-23T15:34:43.825129: step 19493, loss 0.0358683, acc 0.984375\n",
      "2018-05-23T15:34:44.541216: step 19494, loss 0.0621635, acc 1\n",
      "2018-05-23T15:34:45.589417: step 19495, loss 0.02671, acc 1\n",
      "2018-05-23T15:34:46.529901: step 19496, loss 0.075014, acc 0.96875\n",
      "2018-05-23T15:34:47.536203: step 19497, loss 0.153025, acc 0.921875\n",
      "2018-05-23T15:34:48.363988: step 19498, loss 0.192736, acc 0.9375\n",
      "2018-05-23T15:34:48.904543: step 19499, loss 0.0402949, acc 1\n",
      "2018-05-23T15:34:49.734322: step 19500, loss 0.0665184, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:34:55.870905: step 19500, loss 2.08314, acc 0.702672\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19500\n",
      "\n",
      "2018-05-23T15:34:58.187708: step 19501, loss 0.0931037, acc 0.96875\n",
      "2018-05-23T15:34:58.767157: step 19502, loss 0.19345, acc 0.96875\n",
      "2018-05-23T15:34:59.233909: step 19503, loss 0.0687794, acc 0.984375\n",
      "2018-05-23T15:34:59.627854: step 19504, loss 0.201216, acc 0.953125\n",
      "2018-05-23T15:35:00.005844: step 19505, loss 0.0632207, acc 0.984375\n",
      "2018-05-23T15:35:00.395802: step 19506, loss 0.0820524, acc 0.96875\n",
      "2018-05-23T15:35:00.794734: step 19507, loss 0.0899677, acc 0.96875\n",
      "2018-05-23T15:35:01.223588: step 19508, loss 0.0800919, acc 0.984375\n",
      "2018-05-23T15:35:01.618529: step 19509, loss 0.0446755, acc 1\n",
      "2018-05-23T15:35:02.009483: step 19510, loss 0.0650511, acc 0.953125\n",
      "2018-05-23T15:35:02.950966: step 19511, loss 0.0527563, acc 0.984375\n",
      "2018-05-23T15:35:04.174692: step 19512, loss 0.216468, acc 0.90625\n",
      "2018-05-23T15:35:04.866840: step 19513, loss 0.115602, acc 0.96875\n",
      "2018-05-23T15:35:05.427341: step 19514, loss 0.0967805, acc 0.953125\n",
      "2018-05-23T15:35:05.908055: step 19515, loss 0.0411507, acc 0.984375\n",
      "2018-05-23T15:35:06.367825: step 19516, loss 0.099408, acc 0.9375\n",
      "2018-05-23T15:35:06.795679: step 19517, loss 0.0728639, acc 0.953125\n",
      "2018-05-23T15:35:07.211567: step 19518, loss 0.125128, acc 0.9375\n",
      "2018-05-23T15:35:07.942612: step 19519, loss 0.0534663, acc 1\n",
      "2018-05-23T15:35:08.709560: step 19520, loss 0.055229, acc 0.984375\n",
      "2018-05-23T15:35:09.774711: step 19521, loss 0.0690389, acc 0.984375\n",
      "2018-05-23T15:35:10.702230: step 19522, loss 0.0527727, acc 0.984375\n",
      "2018-05-23T15:35:11.316586: step 19523, loss 0.100883, acc 0.96875\n",
      "2018-05-23T15:35:12.158334: step 19524, loss 0.0902703, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:35:13.717164: step 19525, loss 0.0564085, acc 1\n",
      "2018-05-23T15:35:14.751397: step 19526, loss 0.15008, acc 0.953125\n",
      "2018-05-23T15:35:15.809567: step 19527, loss 0.107242, acc 0.953125\n",
      "2018-05-23T15:35:17.046257: step 19528, loss 0.135522, acc 0.921875\n",
      "2018-05-23T15:35:18.176235: step 19529, loss 0.179593, acc 0.9375\n",
      "2018-05-23T15:35:19.018981: step 19530, loss 0.110077, acc 0.921875\n",
      "2018-05-23T15:35:19.618377: step 19531, loss 0.0437037, acc 0.984375\n",
      "2018-05-23T15:35:20.141485: step 19532, loss 0.058424, acc 0.953125\n",
      "2018-05-23T15:35:20.671102: step 19533, loss 0.0827838, acc 0.984375\n",
      "2018-05-23T15:35:21.284091: step 19534, loss 0.0539501, acc 0.984375\n",
      "2018-05-23T15:35:22.163263: step 19535, loss 0.0626872, acc 0.953125\n",
      "2018-05-23T15:35:22.984062: step 19536, loss 0.0463228, acc 0.984375\n",
      "2018-05-23T15:35:24.211781: step 19537, loss 0.0854826, acc 0.96875\n",
      "2018-05-23T15:35:25.063500: step 19538, loss 0.204165, acc 0.96875\n",
      "2018-05-23T15:35:25.504321: step 19539, loss 0.0665751, acc 0.96875\n",
      "2018-05-23T15:35:25.916217: step 19540, loss 0.128995, acc 0.96875\n",
      "2018-05-23T15:35:26.309167: step 19541, loss 0.100086, acc 0.953125\n",
      "2018-05-23T15:35:26.675187: step 19542, loss 0.115147, acc 0.921875\n",
      "2018-05-23T15:35:27.051183: step 19543, loss 0.0678671, acc 1\n",
      "2018-05-23T15:35:27.398256: step 19544, loss 0.0587139, acc 0.96875\n",
      "2018-05-23T15:35:27.820126: step 19545, loss 0.0402567, acc 0.984375\n",
      "2018-05-23T15:35:28.316796: step 19546, loss 0.133869, acc 0.953125\n",
      "2018-05-23T15:35:28.737670: step 19547, loss 0.103824, acc 0.9375\n",
      "2018-05-23T15:35:29.106684: step 19548, loss 0.056324, acc 0.984375\n",
      "2018-05-23T15:35:29.449767: step 19549, loss 0.0949933, acc 0.96875\n",
      "2018-05-23T15:35:29.811797: step 19550, loss 0.063045, acc 0.96875\n",
      "2018-05-23T15:35:30.423161: step 19551, loss 0.0314374, acc 1\n",
      "2018-05-23T15:35:30.805141: step 19552, loss 0.0771161, acc 0.984375\n",
      "2018-05-23T15:35:31.150218: step 19553, loss 0.068303, acc 0.953125\n",
      "2018-05-23T15:35:31.505266: step 19554, loss 0.0976278, acc 0.953125\n",
      "2018-05-23T15:35:31.852340: step 19555, loss 0.115241, acc 0.96875\n",
      "2018-05-23T15:35:32.197415: step 19556, loss 0.0563356, acc 0.984375\n",
      "2018-05-23T15:35:32.539501: step 19557, loss 0.0804499, acc 0.96875\n",
      "2018-05-23T15:35:32.885573: step 19558, loss 0.230857, acc 0.9375\n",
      "2018-05-23T15:35:33.225665: step 19559, loss 0.105358, acc 0.953125\n",
      "2018-05-23T15:35:33.568747: step 19560, loss 0.165108, acc 0.90625\n",
      "2018-05-23T15:35:33.913826: step 19561, loss 0.0618011, acc 0.96875\n",
      "2018-05-23T15:35:34.266878: step 19562, loss 0.106576, acc 0.9375\n",
      "2018-05-23T15:35:34.615945: step 19563, loss 0.118052, acc 0.9375\n",
      "2018-05-23T15:35:34.985956: step 19564, loss 0.137409, acc 0.9375\n",
      "2018-05-23T15:35:35.367934: step 19565, loss 0.0872794, acc 0.984375\n",
      "2018-05-23T15:35:35.710020: step 19566, loss 0.0807147, acc 0.96875\n",
      "2018-05-23T15:35:36.057377: step 19567, loss 0.0698906, acc 0.953125\n",
      "2018-05-23T15:35:36.402452: step 19568, loss 0.114101, acc 0.96875\n",
      "2018-05-23T15:35:36.742543: step 19569, loss 0.0837921, acc 0.96875\n",
      "2018-05-23T15:35:37.089614: step 19570, loss 0.118712, acc 0.953125\n",
      "2018-05-23T15:35:37.438218: step 19571, loss 0.0597812, acc 0.984375\n",
      "2018-05-23T15:35:37.783295: step 19572, loss 0.0902376, acc 0.953125\n",
      "2018-05-23T15:35:38.127375: step 19573, loss 0.0748023, acc 0.984375\n",
      "2018-05-23T15:35:38.519327: step 19574, loss 0.0381794, acc 0.984375\n",
      "2018-05-23T15:35:39.099773: step 19575, loss 0.126973, acc 0.921875\n",
      "2018-05-23T15:35:39.534609: step 19576, loss 0.0523854, acc 0.96875\n",
      "2018-05-23T15:35:39.965457: step 19577, loss 0.0817887, acc 0.953125\n",
      "2018-05-23T15:35:40.387330: step 19578, loss 0.13206, acc 0.9375\n",
      "2018-05-23T15:35:40.830146: step 19579, loss 0.074773, acc 0.96875\n",
      "2018-05-23T15:35:41.262986: step 19580, loss 0.0601682, acc 0.96875\n",
      "2018-05-23T15:35:41.726745: step 19581, loss 0.0482377, acc 0.984375\n",
      "2018-05-23T15:35:42.363043: step 19582, loss 0.0368046, acc 0.984375\n",
      "2018-05-23T15:35:43.197811: step 19583, loss 0.0932118, acc 0.953125\n",
      "2018-05-23T15:35:44.091420: step 19584, loss 0.054593, acc 0.96875\n",
      "2018-05-23T15:35:44.881307: step 19585, loss 0.0514704, acc 0.984375\n",
      "2018-05-23T15:35:45.687152: step 19586, loss 0.0550136, acc 0.984375\n",
      "2018-05-23T15:35:46.365336: step 19587, loss 0.0453166, acc 0.984375\n",
      "2018-05-23T15:35:46.883949: step 19588, loss 0.135846, acc 0.90625\n",
      "2018-05-23T15:35:47.516258: step 19589, loss 0.0482948, acc 0.96875\n",
      "2018-05-23T15:35:48.085734: step 19590, loss 0.0635376, acc 0.96875\n",
      "2018-05-23T15:35:48.651222: step 19591, loss 0.0542006, acc 1\n",
      "2018-05-23T15:35:49.135924: step 19592, loss 0.0687379, acc 0.96875\n",
      "2018-05-23T15:35:49.574750: step 19593, loss 0.0885256, acc 0.953125\n",
      "2018-05-23T15:35:50.174148: step 19594, loss 0.0595478, acc 0.984375\n",
      "2018-05-23T15:35:50.975006: step 19595, loss 0.0547157, acc 0.984375\n",
      "2018-05-23T15:35:51.508577: step 19596, loss 0.266547, acc 0.921875\n",
      "2018-05-23T15:35:52.090023: step 19597, loss 0.141191, acc 0.96875\n",
      "2018-05-23T15:35:52.778181: step 19598, loss 0.13526, acc 0.90625\n",
      "2018-05-23T15:35:53.796458: step 19599, loss 0.0505092, acc 0.984375\n",
      "2018-05-23T15:35:54.409818: step 19600, loss 0.113583, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:35:59.925062: step 19600, loss 2.13809, acc 0.703386\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19600\n",
      "\n",
      "2018-05-23T15:36:02.480225: step 19601, loss 0.103566, acc 0.921875\n",
      "2018-05-23T15:36:02.939996: step 19602, loss 0.065722, acc 0.96875\n",
      "2018-05-23T15:36:03.381815: step 19603, loss 0.126096, acc 0.96875\n",
      "2018-05-23T15:36:03.817648: step 19604, loss 0.146963, acc 0.9375\n",
      "2018-05-23T15:36:04.269440: step 19605, loss 0.171125, acc 0.9375\n",
      "2018-05-23T15:36:04.697294: step 19606, loss 0.098379, acc 0.953125\n",
      "2018-05-23T15:36:05.090244: step 19607, loss 0.128599, acc 0.96875\n",
      "2018-05-23T15:36:05.508127: step 19608, loss 0.0895865, acc 0.953125\n",
      "2018-05-23T15:36:05.938973: step 19609, loss 0.0454872, acc 1\n",
      "2018-05-23T15:36:06.335911: step 19610, loss 0.0483282, acc 0.984375\n",
      "2018-05-23T15:36:06.733846: step 19611, loss 0.153023, acc 0.90625\n",
      "2018-05-23T15:36:07.156715: step 19612, loss 0.079894, acc 0.953125\n",
      "2018-05-23T15:36:07.535702: step 19613, loss 0.0782065, acc 0.953125\n",
      "2018-05-23T15:36:08.016418: step 19614, loss 0.0966929, acc 0.96875\n",
      "2018-05-23T15:36:08.417345: step 19615, loss 0.105702, acc 0.953125\n",
      "2018-05-23T15:36:08.818271: step 19616, loss 0.110565, acc 0.90625\n",
      "2018-05-23T15:36:09.231166: step 19617, loss 0.204509, acc 0.9375\n",
      "2018-05-23T15:36:09.626112: step 19618, loss 0.0593382, acc 0.984375\n",
      "2018-05-23T15:36:10.033020: step 19619, loss 0.0578364, acc 0.953125\n",
      "2018-05-23T15:36:10.455892: step 19620, loss 0.13405, acc 0.921875\n",
      "2018-05-23T15:36:10.933610: step 19621, loss 0.0961654, acc 0.921875\n",
      "2018-05-23T15:36:11.349499: step 19622, loss 0.17293, acc 0.953125\n",
      "2018-05-23T15:36:11.755412: step 19623, loss 0.114812, acc 0.9375\n",
      "2018-05-23T15:36:12.176288: step 19624, loss 0.0833925, acc 0.953125\n",
      "2018-05-23T15:36:12.567241: step 19625, loss 0.0692966, acc 0.96875\n",
      "2018-05-23T15:36:12.963181: step 19626, loss 0.0972673, acc 0.96875\n",
      "2018-05-23T15:36:13.357127: step 19627, loss 0.0809627, acc 0.953125\n",
      "2018-05-23T15:36:13.752071: step 19628, loss 0.163704, acc 0.953125\n",
      "2018-05-23T15:36:14.173944: step 19629, loss 0.127624, acc 0.96875\n",
      "2018-05-23T15:36:14.580855: step 19630, loss 0.146059, acc 0.921875\n",
      "2018-05-23T15:36:14.982779: step 19631, loss 0.257767, acc 0.921875\n",
      "2018-05-23T15:36:15.384703: step 19632, loss 0.0418528, acc 0.984375\n",
      "2018-05-23T15:36:15.776655: step 19633, loss 0.104742, acc 0.96875\n",
      "2018-05-23T15:36:16.185561: step 19634, loss 0.0599379, acc 0.96875\n",
      "2018-05-23T15:36:16.593469: step 19635, loss 0.0941238, acc 0.9375\n",
      "2018-05-23T15:36:16.991406: step 19636, loss 0.199993, acc 0.953125\n",
      "2018-05-23T15:36:17.378371: step 19637, loss 0.0378902, acc 0.984375\n",
      "2018-05-23T15:36:17.795256: step 19638, loss 0.0948356, acc 0.984375\n",
      "2018-05-23T15:36:18.188206: step 19639, loss 0.127844, acc 0.9375\n",
      "2018-05-23T15:36:18.606087: step 19640, loss 0.396315, acc 0.921875\n",
      "2018-05-23T15:36:19.007013: step 19641, loss 0.0766318, acc 0.953125\n",
      "2018-05-23T15:36:19.471771: step 19642, loss 0.0859351, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:36:19.947497: step 19643, loss 0.247002, acc 0.953125\n",
      "2018-05-23T15:36:20.376350: step 19644, loss 0.223424, acc 0.921875\n",
      "2018-05-23T15:36:21.004669: step 19645, loss 0.0826782, acc 0.953125\n",
      "2018-05-23T15:36:21.569159: step 19646, loss 0.0471351, acc 0.984375\n",
      "2018-05-23T15:36:22.043889: step 19647, loss 0.163107, acc 0.96875\n",
      "2018-05-23T15:36:22.503660: step 19648, loss 0.133123, acc 0.9375\n",
      "2018-05-23T15:36:23.033243: step 19649, loss 0.217874, acc 0.921875\n",
      "2018-05-23T15:36:23.500991: step 19650, loss 0.0984126, acc 0.953125\n",
      "2018-05-23T15:36:23.960761: step 19651, loss 0.0742188, acc 0.9375\n",
      "2018-05-23T15:36:24.411555: step 19652, loss 0.172248, acc 0.90625\n",
      "2018-05-23T15:36:24.872322: step 19653, loss 0.051365, acc 0.984375\n",
      "2018-05-23T15:36:25.324114: step 19654, loss 0.129383, acc 0.96875\n",
      "2018-05-23T15:36:25.783884: step 19655, loss 0.163984, acc 0.953125\n",
      "2018-05-23T15:36:26.239665: step 19656, loss 0.0407516, acc 1\n",
      "2018-05-23T15:36:26.719383: step 19657, loss 0.396098, acc 0.9375\n",
      "2018-05-23T15:36:27.307807: step 19658, loss 0.0485576, acc 0.984375\n",
      "2018-05-23T15:36:27.821433: step 19659, loss 0.168736, acc 0.96875\n",
      "2018-05-23T15:36:28.307134: step 19660, loss 0.0384155, acc 0.96875\n",
      "2018-05-23T15:36:28.821758: step 19661, loss 0.0830368, acc 0.96875\n",
      "2018-05-23T15:36:29.393229: step 19662, loss 0.0360209, acc 1\n",
      "2018-05-23T15:36:30.094354: step 19663, loss 0.158398, acc 0.953125\n",
      "2018-05-23T15:36:30.703723: step 19664, loss 0.0619795, acc 0.96875\n",
      "2018-05-23T15:36:31.465685: step 19665, loss 0.179239, acc 0.953125\n",
      "2018-05-23T15:36:32.227646: step 19666, loss 0.0785811, acc 0.96875\n",
      "2018-05-23T15:36:33.134221: step 19667, loss 0.110501, acc 0.953125\n",
      "2018-05-23T15:36:33.926103: step 19668, loss 0.0791513, acc 0.953125\n",
      "2018-05-23T15:36:34.719979: step 19669, loss 0.117819, acc 0.953125\n",
      "2018-05-23T15:36:35.376222: step 19670, loss 0.0276626, acc 1\n",
      "2018-05-23T15:36:36.058399: step 19671, loss 0.0761687, acc 0.953125\n",
      "2018-05-23T15:36:36.661784: step 19672, loss 0.0494872, acc 0.984375\n",
      "2018-05-23T15:36:37.207324: step 19673, loss 0.0750111, acc 0.984375\n",
      "2018-05-23T15:36:37.872210: step 19674, loss 0.174774, acc 0.953125\n",
      "2018-05-23T15:36:38.552116: step 19675, loss 0.102191, acc 0.96875\n",
      "2018-05-23T15:36:39.134557: step 19676, loss 0.060931, acc 0.96875\n",
      "2018-05-23T15:36:39.685084: step 19677, loss 0.0991527, acc 0.953125\n",
      "2018-05-23T15:36:40.250573: step 19678, loss 0.07566, acc 0.96875\n",
      "2018-05-23T15:36:40.801100: step 19679, loss 0.0722589, acc 0.953125\n",
      "2018-05-23T15:36:41.401493: step 19680, loss 0.120923, acc 0.9375\n",
      "2018-05-23T15:36:41.941050: step 19681, loss 0.222725, acc 0.90625\n",
      "2018-05-23T15:36:42.484596: step 19682, loss 0.0320863, acc 1\n",
      "2018-05-23T15:36:43.032130: step 19683, loss 0.145177, acc 0.9375\n",
      "2018-05-23T15:36:43.614574: step 19684, loss 0.0518543, acc 0.984375\n",
      "2018-05-23T15:36:44.364567: step 19685, loss 0.0422125, acc 1\n",
      "2018-05-23T15:36:44.939029: step 19686, loss 0.0307715, acc 1\n",
      "2018-05-23T15:36:45.510501: step 19687, loss 0.0434405, acc 0.984375\n",
      "2018-05-23T15:36:46.096932: step 19688, loss 0.0844586, acc 0.9375\n",
      "2018-05-23T15:36:46.674387: step 19689, loss 0.115744, acc 0.9375\n",
      "2018-05-23T15:36:47.218931: step 19690, loss 0.0838466, acc 0.96875\n",
      "2018-05-23T15:36:47.762477: step 19691, loss 0.047343, acc 0.96875\n",
      "2018-05-23T15:36:48.298044: step 19692, loss 0.0272639, acc 1\n",
      "2018-05-23T15:36:48.844582: step 19693, loss 0.0906244, acc 0.96875\n",
      "2018-05-23T15:36:49.391121: step 19694, loss 0.0468352, acc 0.984375\n",
      "2018-05-23T15:36:49.944639: step 19695, loss 0.068441, acc 0.96875\n",
      "2018-05-23T15:36:50.495167: step 19696, loss 0.109833, acc 0.9375\n",
      "2018-05-23T15:36:51.041704: step 19697, loss 0.056419, acc 0.96875\n",
      "2018-05-23T15:36:51.588242: step 19698, loss 0.102196, acc 0.96875\n",
      "2018-05-23T15:36:52.151735: step 19699, loss 0.0786645, acc 0.96875\n",
      "2018-05-23T15:36:52.585573: step 19700, loss 0.0130632, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:36:58.491774: step 19700, loss 2.07275, acc 0.715245\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19700\n",
      "\n",
      "2018-05-23T15:37:00.727792: step 19701, loss 0.0293292, acc 0.984375\n",
      "2018-05-23T15:37:01.354116: step 19702, loss 0.0688859, acc 0.984375\n",
      "2018-05-23T15:37:01.903646: step 19703, loss 0.197942, acc 0.953125\n",
      "2018-05-23T15:37:02.671592: step 19704, loss 0.0590861, acc 0.984375\n",
      "2018-05-23T15:37:03.420589: step 19705, loss 0.087846, acc 0.96875\n",
      "2018-05-23T15:37:04.031953: step 19706, loss 0.0419269, acc 1\n",
      "2018-05-23T15:37:04.508678: step 19707, loss 0.0562661, acc 0.984375\n",
      "2018-05-23T15:37:05.017317: step 19708, loss 0.10564, acc 0.953125\n",
      "2018-05-23T15:37:05.452153: step 19709, loss 0.0744826, acc 0.96875\n",
      "2018-05-23T15:37:05.924888: step 19710, loss 0.0638242, acc 0.984375\n",
      "2018-05-23T15:37:06.432531: step 19711, loss 0.0503246, acc 0.984375\n",
      "2018-05-23T15:37:07.270290: step 19712, loss 0.035817, acc 1\n",
      "2018-05-23T15:37:08.200800: step 19713, loss 0.0191435, acc 1\n",
      "2018-05-23T15:37:08.730382: step 19714, loss 0.19477, acc 0.9375\n",
      "2018-05-23T15:37:09.195140: step 19715, loss 0.0133391, acc 1\n",
      "2018-05-23T15:37:09.699802: step 19716, loss 0.0545783, acc 0.984375\n",
      "2018-05-23T15:37:10.523587: step 19717, loss 0.0938935, acc 0.953125\n",
      "2018-05-23T15:37:11.228701: step 19718, loss 0.161176, acc 0.953125\n",
      "2018-05-23T15:37:12.050502: step 19719, loss 0.199724, acc 0.953125\n",
      "2018-05-23T15:37:13.675156: step 19720, loss 0.0705113, acc 0.96875\n",
      "2018-05-23T15:37:14.928802: step 19721, loss 0.0518463, acc 0.96875\n",
      "2018-05-23T15:37:15.975004: step 19722, loss 0.0501727, acc 0.984375\n",
      "2018-05-23T15:37:17.084044: step 19723, loss 0.0563922, acc 0.984375\n",
      "2018-05-23T15:37:17.952713: step 19724, loss 0.0497059, acc 0.984375\n",
      "2018-05-23T15:37:19.097649: step 19725, loss 0.0422249, acc 0.984375\n",
      "2018-05-23T15:37:19.882550: step 19726, loss 0.0282567, acc 0.984375\n",
      "2018-05-23T15:37:20.563727: step 19727, loss 0.0659969, acc 0.953125\n",
      "2018-05-23T15:37:21.206009: step 19728, loss 0.0612673, acc 0.96875\n",
      "2018-05-23T15:37:21.861256: step 19729, loss 0.105167, acc 0.984375\n",
      "2018-05-23T15:37:22.521490: step 19730, loss 0.113069, acc 0.984375\n",
      "2018-05-23T15:37:23.399143: step 19731, loss 0.0423035, acc 1\n",
      "2018-05-23T15:37:24.011503: step 19732, loss 0.0745329, acc 0.96875\n",
      "2018-05-23T15:37:24.723599: step 19733, loss 0.102277, acc 0.96875\n",
      "2018-05-23T15:37:25.469604: step 19734, loss 0.0433391, acc 0.984375\n",
      "2018-05-23T15:37:26.335294: step 19735, loss 0.152181, acc 0.921875\n",
      "2018-05-23T15:37:27.136146: step 19736, loss 0.0436873, acc 0.984375\n",
      "2018-05-23T15:37:28.262133: step 19737, loss 0.117626, acc 0.953125\n",
      "2018-05-23T15:37:29.010132: step 19738, loss 0.156218, acc 0.953125\n",
      "2018-05-23T15:37:29.784061: step 19739, loss 0.0581753, acc 0.953125\n",
      "2018-05-23T15:37:30.557991: step 19740, loss 0.104974, acc 0.953125\n",
      "2018-05-23T15:37:31.272082: step 19741, loss 0.0442396, acc 0.984375\n",
      "2018-05-23T15:37:31.936304: step 19742, loss 0.0546676, acc 0.96875\n",
      "2018-05-23T15:37:32.666351: step 19743, loss 0.0399405, acc 1\n",
      "2018-05-23T15:37:33.405374: step 19744, loss 0.089045, acc 0.96875\n",
      "2018-05-23T15:37:34.036685: step 19745, loss 0.116293, acc 0.921875\n",
      "2018-05-23T15:37:34.624113: step 19746, loss 0.0358308, acc 1\n",
      "2018-05-23T15:37:35.255425: step 19747, loss 0.058538, acc 0.96875\n",
      "2018-05-23T15:37:35.886735: step 19748, loss 0.131113, acc 0.9375\n",
      "2018-05-23T15:37:36.469177: step 19749, loss 0.0276583, acc 1\n",
      "2018-05-23T15:37:37.130409: step 19750, loss 0.0973009, acc 0.953125\n",
      "2018-05-23T15:37:37.784657: step 19751, loss 0.0401215, acc 0.984375\n",
      "2018-05-23T15:37:38.463841: step 19752, loss 0.096546, acc 0.984375\n",
      "2018-05-23T15:37:39.091163: step 19753, loss 0.121627, acc 0.953125\n",
      "2018-05-23T15:37:39.717486: step 19754, loss 0.0439587, acc 0.984375\n",
      "2018-05-23T15:37:40.291951: step 19755, loss 0.0264513, acc 0.984375\n",
      "2018-05-23T15:37:40.864421: step 19756, loss 0.0424332, acc 0.984375\n",
      "2018-05-23T15:37:41.311222: step 19757, loss 0.110474, acc 0.96875\n",
      "2018-05-23T15:37:41.740077: step 19758, loss 0.0277017, acc 1\n",
      "2018-05-23T15:37:42.172919: step 19759, loss 0.0770067, acc 0.953125\n",
      "2018-05-23T15:37:42.684549: step 19760, loss 0.0177152, acc 1\n",
      "2018-05-23T15:37:43.131353: step 19761, loss 0.0661554, acc 0.9375\n",
      "2018-05-23T15:37:43.560207: step 19762, loss 0.0263498, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:37:43.998035: step 19763, loss 0.124673, acc 0.953125\n",
      "2018-05-23T15:37:44.431875: step 19764, loss 0.0864089, acc 0.953125\n",
      "2018-05-23T15:37:44.827816: step 19765, loss 0.01159, acc 1\n",
      "2018-05-23T15:37:45.238720: step 19766, loss 0.0781083, acc 0.953125\n",
      "2018-05-23T15:37:45.667569: step 19767, loss 0.052965, acc 1\n",
      "2018-05-23T15:37:46.138310: step 19768, loss 0.1492, acc 0.953125\n",
      "2018-05-23T15:37:46.754662: step 19769, loss 0.0751742, acc 0.96875\n",
      "2018-05-23T15:37:47.329124: step 19770, loss 0.0620062, acc 0.96875\n",
      "2018-05-23T15:37:47.779917: step 19771, loss 0.0790604, acc 0.96875\n",
      "2018-05-23T15:37:48.189822: step 19772, loss 0.013956, acc 1\n",
      "2018-05-23T15:37:48.624660: step 19773, loss 0.162072, acc 0.90625\n",
      "2018-05-23T15:37:49.093404: step 19774, loss 0.0991384, acc 0.953125\n",
      "2018-05-23T15:37:49.570129: step 19775, loss 0.0410551, acc 0.96875\n",
      "2018-05-23T15:37:50.045859: step 19776, loss 0.0540444, acc 0.984375\n",
      "2018-05-23T15:37:50.533551: step 19777, loss 0.0549608, acc 0.96875\n",
      "2018-05-23T15:37:50.978362: step 19778, loss 0.0427941, acc 0.984375\n",
      "2018-05-23T15:37:51.498969: step 19779, loss 0.075852, acc 0.96875\n",
      "2018-05-23T15:37:51.969711: step 19780, loss 0.0279487, acc 0.984375\n",
      "2018-05-23T15:37:52.447431: step 19781, loss 0.0337082, acc 0.984375\n",
      "2018-05-23T15:37:53.124621: step 19782, loss 0.0580857, acc 0.984375\n",
      "2018-05-23T15:37:53.761915: step 19783, loss 0.229474, acc 0.921875\n",
      "2018-05-23T15:37:54.476005: step 19784, loss 0.137513, acc 0.921875\n",
      "2018-05-23T15:37:55.561102: step 19785, loss 0.0664074, acc 0.984375\n",
      "2018-05-23T15:37:56.720002: step 19786, loss 0.0601029, acc 0.96875\n",
      "2018-05-23T15:37:57.541803: step 19787, loss 0.0840519, acc 0.96875\n",
      "2018-05-23T15:37:58.414470: step 19788, loss 0.0727564, acc 0.96875\n",
      "2018-05-23T15:37:59.137535: step 19789, loss 0.0515656, acc 0.984375\n",
      "2018-05-23T15:37:59.743913: step 19790, loss 0.0623726, acc 0.96875\n",
      "2018-05-23T15:38:00.350290: step 19791, loss 0.109985, acc 0.953125\n",
      "2018-05-23T15:38:00.856934: step 19792, loss 0.0356745, acc 1\n",
      "2018-05-23T15:38:01.425415: step 19793, loss 0.0316963, acc 1\n",
      "2018-05-23T15:38:01.991898: step 19794, loss 0.0322116, acc 1\n",
      "2018-05-23T15:38:02.539434: step 19795, loss 0.0524026, acc 1\n",
      "2018-05-23T15:38:03.075001: step 19796, loss 0.0351088, acc 1\n",
      "2018-05-23T15:38:03.721272: step 19797, loss 0.0501882, acc 0.96875\n",
      "2018-05-23T15:38:04.220936: step 19798, loss 0.155783, acc 0.921875\n",
      "2018-05-23T15:38:04.722593: step 19799, loss 0.0500377, acc 0.953125\n",
      "2018-05-23T15:38:05.221259: step 19800, loss 0.115526, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:38:11.089561: step 19800, loss 2.11952, acc 0.711959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19800\n",
      "\n",
      "2018-05-23T15:38:15.673298: step 19801, loss 0.14799, acc 0.921875\n",
      "2018-05-23T15:38:16.440247: step 19802, loss 0.0704337, acc 0.984375\n",
      "2018-05-23T15:38:17.230134: step 19803, loss 0.0503128, acc 0.96875\n",
      "2018-05-23T15:38:18.038971: step 19804, loss 0.0573998, acc 0.984375\n",
      "2018-05-23T15:38:18.977459: step 19805, loss 0.121215, acc 0.9375\n",
      "2018-05-23T15:38:20.196199: step 19806, loss 0.0592794, acc 0.984375\n",
      "2018-05-23T15:38:21.146656: step 19807, loss 0.119021, acc 0.953125\n",
      "2018-05-23T15:38:21.919589: step 19808, loss 0.0750331, acc 0.96875\n",
      "2018-05-23T15:38:22.997705: step 19809, loss 0.0960939, acc 0.953125\n",
      "2018-05-23T15:38:23.756674: step 19810, loss 0.0668478, acc 0.96875\n",
      "2018-05-23T15:38:24.730070: step 19811, loss 0.103909, acc 0.984375\n",
      "2018-05-23T15:38:25.632655: step 19812, loss 0.0827315, acc 0.9375\n",
      "2018-05-23T15:38:26.461439: step 19813, loss 0.0252598, acc 0.984375\n",
      "2018-05-23T15:38:27.536563: step 19814, loss 0.0965253, acc 0.984375\n",
      "2018-05-23T15:38:28.581766: step 19815, loss 0.117545, acc 0.9375\n",
      "2018-05-23T15:38:29.632955: step 19816, loss 0.0446288, acc 0.984375\n",
      "2018-05-23T15:38:30.421843: step 19817, loss 0.101206, acc 0.9375\n",
      "2018-05-23T15:38:31.170840: step 19818, loss 0.0686453, acc 0.953125\n",
      "2018-05-23T15:38:31.881937: step 19819, loss 0.0439091, acc 0.96875\n",
      "2018-05-23T15:38:32.698753: step 19820, loss 0.0462226, acc 0.984375\n",
      "2018-05-23T15:38:33.341034: step 19821, loss 0.0844176, acc 0.953125\n",
      "2018-05-23T15:38:34.036176: step 19822, loss 0.125916, acc 0.953125\n",
      "2018-05-23T15:38:34.674466: step 19823, loss 0.0474685, acc 0.984375\n",
      "2018-05-23T15:38:35.257905: step 19824, loss 0.161041, acc 0.875\n",
      "2018-05-23T15:38:35.937089: step 19825, loss 0.110288, acc 0.953125\n",
      "2018-05-23T15:38:36.533494: step 19826, loss 0.0545377, acc 0.984375\n",
      "2018-05-23T15:38:37.160815: step 19827, loss 0.108533, acc 0.9375\n",
      "2018-05-23T15:38:37.754228: step 19828, loss 0.0433809, acc 0.984375\n",
      "2018-05-23T15:38:38.756547: step 19829, loss 0.0358362, acc 0.984375\n",
      "2018-05-23T15:38:39.511526: step 19830, loss 0.114547, acc 0.96875\n",
      "2018-05-23T15:38:40.256533: step 19831, loss 0.15281, acc 0.96875\n",
      "2018-05-23T15:38:40.919760: step 19832, loss 0.0427594, acc 0.984375\n",
      "2018-05-23T15:38:41.591960: step 19833, loss 0.103713, acc 0.953125\n",
      "2018-05-23T15:38:42.249202: step 19834, loss 0.38741, acc 0.90625\n",
      "2018-05-23T15:38:42.965288: step 19835, loss 0.114219, acc 0.953125\n",
      "2018-05-23T15:38:43.615548: step 19836, loss 0.0677067, acc 0.96875\n",
      "2018-05-23T15:38:44.451312: step 19837, loss 0.0367835, acc 1\n",
      "2018-05-23T15:38:45.271119: step 19838, loss 0.0977466, acc 0.984375\n",
      "2018-05-23T15:38:46.008147: step 19839, loss 0.0743322, acc 0.953125\n",
      "2018-05-23T15:38:46.777090: step 19840, loss 0.110787, acc 0.9375\n",
      "2018-05-23T15:38:47.360530: step 19841, loss 0.0635696, acc 0.984375\n",
      "2018-05-23T15:38:47.934992: step 19842, loss 0.107484, acc 0.96875\n",
      "2018-05-23T15:38:48.529401: step 19843, loss 0.103414, acc 0.96875\n",
      "2018-05-23T15:38:49.114835: step 19844, loss 0.1005, acc 0.9375\n",
      "2018-05-23T15:38:49.688302: step 19845, loss 0.267809, acc 0.9375\n",
      "2018-05-23T15:38:50.216889: step 19846, loss 0.100341, acc 0.96875\n",
      "2018-05-23T15:38:50.724529: step 19847, loss 0.149249, acc 0.9375\n",
      "2018-05-23T15:38:51.338887: step 19848, loss 0.133943, acc 0.9375\n",
      "2018-05-23T15:38:51.936287: step 19849, loss 0.0473852, acc 0.984375\n",
      "2018-05-23T15:38:52.621456: step 19850, loss 0.129624, acc 0.96875\n",
      "2018-05-23T15:38:53.317593: step 19851, loss 0.0404044, acc 0.984375\n",
      "2018-05-23T15:38:53.943919: step 19852, loss 0.0874921, acc 0.96875\n",
      "2018-05-23T15:38:54.478486: step 19853, loss 0.0572417, acc 0.96875\n",
      "2018-05-23T15:38:54.941248: step 19854, loss 0.0311532, acc 0.984375\n",
      "2018-05-23T15:38:55.514715: step 19855, loss 0.0365129, acc 0.984375\n",
      "2018-05-23T15:38:56.134059: step 19856, loss 0.0847627, acc 0.96875\n",
      "2018-05-23T15:38:56.815236: step 19857, loss 0.0461921, acc 0.984375\n",
      "2018-05-23T15:38:57.410643: step 19858, loss 0.0754315, acc 0.984375\n",
      "2018-05-23T15:38:57.901330: step 19859, loss 0.0641268, acc 0.984375\n",
      "2018-05-23T15:38:58.505713: step 19860, loss 0.101908, acc 0.953125\n",
      "2018-05-23T15:38:59.121068: step 19861, loss 0.0643485, acc 0.96875\n",
      "2018-05-23T15:38:59.626714: step 19862, loss 0.107386, acc 0.953125\n",
      "2018-05-23T15:39:00.116404: step 19863, loss 0.0386134, acc 0.984375\n",
      "2018-05-23T15:39:00.632025: step 19864, loss 0.104334, acc 0.953125\n",
      "2018-05-23T15:39:01.141662: step 19865, loss 0.0525185, acc 0.984375\n",
      "2018-05-23T15:39:01.663266: step 19866, loss 0.0751869, acc 0.984375\n",
      "2018-05-23T15:39:02.192849: step 19867, loss 0.0919027, acc 0.96875\n",
      "2018-05-23T15:39:02.750359: step 19868, loss 0.0768222, acc 0.953125\n",
      "2018-05-23T15:39:03.628010: step 19869, loss 0.0641264, acc 0.96875\n",
      "2018-05-23T15:39:04.569492: step 19870, loss 0.113535, acc 0.9375\n",
      "2018-05-23T15:39:05.140962: step 19871, loss 0.0664585, acc 0.96875\n",
      "2018-05-23T15:39:05.738365: step 19872, loss 0.12941, acc 0.9375\n",
      "2018-05-23T15:39:06.637958: step 19873, loss 0.10513, acc 0.984375\n",
      "2018-05-23T15:39:07.324122: step 19874, loss 0.0548202, acc 0.984375\n",
      "2018-05-23T15:39:08.149913: step 19875, loss 0.0630297, acc 0.96875\n",
      "2018-05-23T15:39:08.965731: step 19876, loss 0.0441028, acc 0.96875\n",
      "2018-05-23T15:39:09.649901: step 19877, loss 0.178705, acc 0.984375\n",
      "2018-05-23T15:39:10.230353: step 19878, loss 0.127448, acc 0.96875\n",
      "2018-05-23T15:39:10.893574: step 19879, loss 0.0773176, acc 0.984375\n",
      "2018-05-23T15:39:11.455072: step 19880, loss 0.0624376, acc 0.984375\n",
      "2018-05-23T15:39:11.981663: step 19881, loss 0.0611978, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:39:12.516233: step 19882, loss 0.0330131, acc 0.984375\n",
      "2018-05-23T15:39:13.446744: step 19883, loss 0.100018, acc 0.921875\n",
      "2018-05-23T15:39:14.334369: step 19884, loss 0.155755, acc 0.9375\n",
      "2018-05-23T15:39:14.974657: step 19885, loss 0.0846325, acc 0.953125\n",
      "2018-05-23T15:39:15.817402: step 19886, loss 0.168112, acc 0.921875\n",
      "2018-05-23T15:39:16.463673: step 19887, loss 0.0296524, acc 1\n",
      "2018-05-23T15:39:17.077032: step 19888, loss 0.107696, acc 0.9375\n",
      "2018-05-23T15:39:17.624568: step 19889, loss 0.0604971, acc 0.984375\n",
      "2018-05-23T15:39:18.211999: step 19890, loss 0.0557151, acc 0.96875\n",
      "2018-05-23T15:39:18.945037: step 19891, loss 0.0991529, acc 0.96875\n",
      "2018-05-23T15:39:19.680068: step 19892, loss 0.0348242, acc 0.984375\n",
      "2018-05-23T15:39:20.342297: step 19893, loss 0.0567224, acc 0.984375\n",
      "2018-05-23T15:39:20.977597: step 19894, loss 0.0838901, acc 0.96875\n",
      "2018-05-23T15:39:21.654786: step 19895, loss 0.0627832, acc 0.984375\n",
      "2018-05-23T15:39:22.289090: step 19896, loss 0.0489005, acc 0.96875\n",
      "2018-05-23T15:39:22.814682: step 19897, loss 0.100456, acc 0.96875\n",
      "2018-05-23T15:39:23.329305: step 19898, loss 0.130418, acc 0.9375\n",
      "2018-05-23T15:39:23.817997: step 19899, loss 0.118413, acc 0.953125\n",
      "2018-05-23T15:39:24.336611: step 19900, loss 0.111774, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:39:30.593871: step 19900, loss 2.12987, acc 0.708387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-19900\n",
      "\n",
      "2018-05-23T15:39:32.910673: step 19901, loss 0.0658612, acc 0.984375\n",
      "2018-05-23T15:39:33.692583: step 19902, loss 0.0419658, acc 1\n",
      "2018-05-23T15:39:34.426620: step 19903, loss 0.0655072, acc 0.984375\n",
      "2018-05-23T15:39:35.320228: step 19904, loss 0.123158, acc 0.9375\n",
      "2018-05-23T15:39:36.095154: step 19905, loss 0.10314, acc 0.9375\n",
      "2018-05-23T15:39:36.853127: step 19906, loss 0.135531, acc 0.953125\n",
      "2018-05-23T15:39:37.611098: step 19907, loss 0.0238322, acc 0.984375\n",
      "2018-05-23T15:39:38.349125: step 19908, loss 0.0639303, acc 0.96875\n",
      "2018-05-23T15:39:39.028308: step 19909, loss 0.0434131, acc 0.984375\n",
      "2018-05-23T15:39:39.749380: step 19910, loss 0.0363292, acc 0.984375\n",
      "2018-05-23T15:39:40.542258: step 19911, loss 0.0540238, acc 0.984375\n",
      "2018-05-23T15:39:41.296241: step 19912, loss 0.131286, acc 0.921875\n",
      "2018-05-23T15:39:41.938523: step 19913, loss 0.154758, acc 0.953125\n",
      "2018-05-23T15:39:42.654607: step 19914, loss 0.0562277, acc 0.984375\n",
      "2018-05-23T15:39:43.392633: step 19915, loss 0.101175, acc 0.953125\n",
      "2018-05-23T15:39:44.193490: step 19916, loss 0.084123, acc 0.953125\n",
      "2018-05-23T15:39:44.877660: step 19917, loss 0.0736926, acc 0.96875\n",
      "2018-05-23T15:39:45.633640: step 19918, loss 0.0653932, acc 0.953125\n",
      "2018-05-23T15:39:46.304842: step 19919, loss 0.18024, acc 0.96875\n",
      "2018-05-23T15:39:46.964078: step 19920, loss 0.0590559, acc 0.984375\n",
      "2018-05-23T15:39:47.572451: step 19921, loss 0.0603419, acc 0.984375\n",
      "2018-05-23T15:39:48.387271: step 19922, loss 0.131907, acc 0.90625\n",
      "2018-05-23T15:39:49.065456: step 19923, loss 0.0870825, acc 0.984375\n",
      "2018-05-23T15:39:49.735664: step 19924, loss 0.0605086, acc 0.953125\n",
      "2018-05-23T15:39:50.393903: step 19925, loss 0.101852, acc 0.984375\n",
      "2018-05-23T15:39:50.957395: step 19926, loss 0.0570228, acc 0.984375\n",
      "2018-05-23T15:39:51.533853: step 19927, loss 0.0316851, acc 1\n",
      "2018-05-23T15:39:52.058454: step 19928, loss 0.0399369, acc 0.984375\n",
      "2018-05-23T15:39:52.707712: step 19929, loss 0.0942644, acc 0.953125\n",
      "2018-05-23T15:39:53.339024: step 19930, loss 0.108186, acc 0.9375\n",
      "2018-05-23T15:39:54.067076: step 19931, loss 0.127404, acc 0.953125\n",
      "2018-05-23T15:39:54.787400: step 19932, loss 0.120339, acc 0.921875\n",
      "2018-05-23T15:39:55.368844: step 19933, loss 0.0465649, acc 0.96875\n",
      "2018-05-23T15:39:55.890449: step 19934, loss 0.0806222, acc 0.953125\n",
      "2018-05-23T15:39:56.383131: step 19935, loss 0.0358647, acc 0.984375\n",
      "2018-05-23T15:39:56.977541: step 19936, loss 0.045036, acc 0.984375\n",
      "2018-05-23T15:39:57.741497: step 19937, loss 0.0568831, acc 0.984375\n",
      "2018-05-23T15:39:58.370813: step 19938, loss 0.222612, acc 0.9375\n",
      "2018-05-23T15:39:59.030050: step 19939, loss 0.0335946, acc 0.984375\n",
      "2018-05-23T15:39:59.770070: step 19940, loss 0.0162638, acc 1\n",
      "2018-05-23T15:40:00.474187: step 19941, loss 0.12667, acc 0.9375\n",
      "2018-05-23T15:40:01.107492: step 19942, loss 0.0705453, acc 0.96875\n",
      "2018-05-23T15:40:01.780692: step 19943, loss 0.224022, acc 0.90625\n",
      "2018-05-23T15:40:02.418984: step 19944, loss 0.380977, acc 0.921875\n",
      "2018-05-23T15:40:03.219842: step 19945, loss 0.095136, acc 0.96875\n",
      "2018-05-23T15:40:04.046630: step 19946, loss 0.106797, acc 0.9375\n",
      "2018-05-23T15:40:04.842501: step 19947, loss 0.218341, acc 0.953125\n",
      "2018-05-23T15:40:05.521685: step 19948, loss 0.0668975, acc 0.96875\n",
      "2018-05-23T15:40:06.176930: step 19949, loss 0.0724261, acc 0.953125\n",
      "2018-05-23T15:40:07.146337: step 19950, loss 0.112645, acc 0.9375\n",
      "2018-05-23T15:40:07.822528: step 19951, loss 0.0253826, acc 0.984375\n",
      "2018-05-23T15:40:08.739076: step 19952, loss 0.105749, acc 0.953125\n",
      "2018-05-23T15:40:09.434218: step 19953, loss 0.0322307, acc 1\n",
      "2018-05-23T15:40:10.112403: step 19954, loss 0.15235, acc 0.921875\n",
      "2018-05-23T15:40:10.937199: step 19955, loss 0.0693627, acc 0.984375\n",
      "2018-05-23T15:40:11.544572: step 19956, loss 0.0498264, acc 0.984375\n",
      "2018-05-23T15:40:12.099090: step 19957, loss 0.0617414, acc 0.96875\n",
      "2018-05-23T15:40:12.598750: step 19958, loss 0.0884755, acc 0.953125\n",
      "2018-05-23T15:40:13.089439: step 19959, loss 0.0746472, acc 0.984375\n",
      "2018-05-23T15:40:13.595086: step 19960, loss 0.0851126, acc 0.953125\n",
      "2018-05-23T15:40:14.206450: step 19961, loss 0.0382565, acc 0.984375\n",
      "2018-05-23T15:40:15.047201: step 19962, loss 0.119555, acc 0.953125\n",
      "2018-05-23T15:40:16.552176: step 19963, loss 0.0476433, acc 0.96875\n",
      "2018-05-23T15:40:17.380958: step 19964, loss 0.0606204, acc 0.96875\n",
      "2018-05-23T15:40:18.490999: step 19965, loss 0.0239221, acc 1\n",
      "2018-05-23T15:40:19.523226: step 19966, loss 0.0470704, acc 0.96875\n",
      "2018-05-23T15:40:20.141573: step 19967, loss 0.0781143, acc 0.96875\n",
      "2018-05-23T15:40:21.298500: step 19968, loss 0.049664, acc 0.96875\n",
      "2018-05-23T15:40:22.162188: step 19969, loss 0.106998, acc 0.984375\n",
      "2018-05-23T15:40:23.267233: step 19970, loss 0.0344645, acc 0.984375\n",
      "2018-05-23T15:40:24.395215: step 19971, loss 0.0780937, acc 0.96875\n",
      "2018-05-23T15:40:25.087365: step 19972, loss 0.0336442, acc 0.96875\n",
      "2018-05-23T15:40:25.763555: step 19973, loss 0.0924086, acc 0.984375\n",
      "2018-05-23T15:40:26.467671: step 19974, loss 0.0907609, acc 0.9375\n",
      "2018-05-23T15:40:27.103969: step 19975, loss 0.10174, acc 0.9375\n",
      "2018-05-23T15:40:27.920783: step 19976, loss 0.0547946, acc 0.953125\n",
      "2018-05-23T15:40:28.697706: step 19977, loss 0.082335, acc 0.96875\n",
      "2018-05-23T15:40:29.439721: step 19978, loss 0.097988, acc 0.953125\n",
      "2018-05-23T15:40:30.488274: step 19979, loss 0.04242, acc 0.96875\n",
      "2018-05-23T15:40:31.192390: step 19980, loss 0.0377421, acc 0.96875\n",
      "2018-05-23T15:40:31.802758: step 19981, loss 0.0850465, acc 0.953125\n",
      "2018-05-23T15:40:32.371237: step 19982, loss 0.100736, acc 0.9375\n",
      "2018-05-23T15:40:32.922762: step 19983, loss 0.0436949, acc 0.984375\n",
      "2018-05-23T15:40:33.463316: step 19984, loss 0.0842552, acc 0.953125\n",
      "2018-05-23T15:40:34.039773: step 19985, loss 0.0801155, acc 0.953125\n",
      "2018-05-23T15:40:34.554397: step 19986, loss 0.0444155, acc 0.984375\n",
      "2018-05-23T15:40:35.049073: step 19987, loss 0.0259464, acc 1\n",
      "2018-05-23T15:40:35.543750: step 19988, loss 0.145802, acc 0.9375\n",
      "2018-05-23T15:40:36.041419: step 19989, loss 0.0245788, acc 1\n",
      "2018-05-23T15:40:36.533103: step 19990, loss 0.0684763, acc 0.96875\n",
      "2018-05-23T15:40:37.036755: step 19991, loss 0.0551693, acc 0.984375\n",
      "2018-05-23T15:40:37.526446: step 19992, loss 0.0349884, acc 1\n",
      "2018-05-23T15:40:38.012146: step 19993, loss 0.0633681, acc 0.953125\n",
      "2018-05-23T15:40:38.517793: step 19994, loss 0.042339, acc 1\n",
      "2018-05-23T15:40:38.982550: step 19995, loss 0.043339, acc 0.984375\n",
      "2018-05-23T15:40:39.496178: step 19996, loss 0.0497879, acc 0.984375\n",
      "2018-05-23T15:40:40.052688: step 19997, loss 0.0463664, acc 0.984375\n",
      "2018-05-23T15:40:40.558335: step 19998, loss 0.11985, acc 0.9375\n",
      "2018-05-23T15:40:41.046030: step 19999, loss 0.0735674, acc 0.984375\n",
      "2018-05-23T15:40:41.499816: step 20000, loss 0.0434599, acc 0.984375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:40:46.833548: step 20000, loss 2.12235, acc 0.711102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20000\n",
      "\n",
      "2018-05-23T15:40:48.567908: step 20001, loss 0.0582686, acc 0.953125\n",
      "2018-05-23T15:40:49.057597: step 20002, loss 0.0415406, acc 0.984375\n",
      "2018-05-23T15:40:49.522354: step 20003, loss 0.165753, acc 0.921875\n",
      "2018-05-23T15:40:49.987112: step 20004, loss 0.10308, acc 0.9375\n",
      "2018-05-23T15:40:50.411975: step 20005, loss 0.058101, acc 0.984375\n",
      "2018-05-23T15:40:50.824869: step 20006, loss 0.037627, acc 1\n",
      "2018-05-23T15:40:51.248739: step 20007, loss 0.0384527, acc 0.984375\n",
      "2018-05-23T15:40:51.673599: step 20008, loss 0.0619301, acc 0.984375\n",
      "2018-05-23T15:40:52.089488: step 20009, loss 0.0787757, acc 0.953125\n",
      "2018-05-23T15:40:52.506373: step 20010, loss 0.0287057, acc 0.984375\n",
      "2018-05-23T15:40:52.897326: step 20011, loss 0.0674157, acc 0.984375\n",
      "2018-05-23T15:40:53.325181: step 20012, loss 0.0905171, acc 0.953125\n",
      "2018-05-23T15:40:53.769991: step 20013, loss 0.133432, acc 0.9375\n",
      "2018-05-23T15:40:54.287608: step 20014, loss 0.078414, acc 0.96875\n",
      "2018-05-23T15:40:54.850103: step 20015, loss 0.0637964, acc 0.96875\n",
      "2018-05-23T15:40:55.427558: step 20016, loss 0.071659, acc 0.96875\n",
      "2018-05-23T15:40:55.974095: step 20017, loss 0.253086, acc 0.90625\n",
      "2018-05-23T15:40:56.474756: step 20018, loss 0.0433699, acc 0.984375\n",
      "2018-05-23T15:40:56.961453: step 20019, loss 0.0582222, acc 0.96875\n",
      "2018-05-23T15:40:57.425213: step 20020, loss 0.118889, acc 0.953125\n",
      "2018-05-23T15:40:57.884983: step 20021, loss 0.0632375, acc 0.984375\n",
      "2018-05-23T15:40:58.355724: step 20022, loss 0.176777, acc 0.9375\n",
      "2018-05-23T15:40:58.861372: step 20023, loss 0.107618, acc 0.953125\n",
      "2018-05-23T15:40:59.433840: step 20024, loss 0.0972656, acc 0.984375\n",
      "2018-05-23T15:41:00.002319: step 20025, loss 0.143043, acc 0.953125\n",
      "2018-05-23T15:41:00.641609: step 20026, loss 0.0616368, acc 0.96875\n",
      "2018-05-23T15:41:01.184157: step 20027, loss 0.0283367, acc 0.984375\n",
      "2018-05-23T15:41:01.749645: step 20028, loss 0.0360511, acc 0.984375\n",
      "2018-05-23T15:41:02.365996: step 20029, loss 0.0396085, acc 0.984375\n",
      "2018-05-23T15:41:03.067120: step 20030, loss 0.0418937, acc 0.984375\n",
      "2018-05-23T15:41:03.859998: step 20031, loss 0.0490946, acc 0.984375\n",
      "2018-05-23T15:41:04.693769: step 20032, loss 0.118386, acc 0.984375\n",
      "2018-05-23T15:41:05.395890: step 20033, loss 0.151676, acc 0.9375\n",
      "2018-05-23T15:41:06.059115: step 20034, loss 0.147323, acc 0.9375\n",
      "2018-05-23T15:41:06.628594: step 20035, loss 0.118949, acc 0.953125\n",
      "2018-05-23T15:41:07.434436: step 20036, loss 0.0607831, acc 0.96875\n",
      "2018-05-23T15:41:08.414814: step 20037, loss 0.0797885, acc 0.9375\n",
      "2018-05-23T15:41:09.055101: step 20038, loss 0.0528025, acc 0.96875\n",
      "2018-05-23T15:41:09.701372: step 20039, loss 0.200629, acc 0.953125\n",
      "2018-05-23T15:41:10.356621: step 20040, loss 0.0971898, acc 0.953125\n",
      "2018-05-23T15:41:10.999899: step 20041, loss 0.0173654, acc 1\n",
      "2018-05-23T15:41:11.745902: step 20042, loss 0.0650513, acc 0.984375\n",
      "2018-05-23T15:41:12.455006: step 20043, loss 0.150131, acc 0.90625\n",
      "2018-05-23T15:41:12.991570: step 20044, loss 0.0662343, acc 0.96875\n",
      "2018-05-23T15:41:13.572017: step 20045, loss 0.0825246, acc 0.953125\n",
      "2018-05-23T15:41:14.158449: step 20046, loss 0.0844311, acc 0.96875\n",
      "2018-05-23T15:41:14.817684: step 20047, loss 0.0953681, acc 0.953125\n",
      "2018-05-23T15:41:15.447999: step 20048, loss 0.0563402, acc 0.96875\n",
      "2018-05-23T15:41:16.018472: step 20049, loss 0.0441241, acc 1\n",
      "2018-05-23T15:41:16.548056: step 20050, loss 0.105647, acc 0.96875\n",
      "2018-05-23T15:41:17.050711: step 20051, loss 0.0700014, acc 0.96875\n",
      "2018-05-23T15:41:17.574310: step 20052, loss 0.198848, acc 0.953125\n",
      "2018-05-23T15:41:18.091926: step 20053, loss 0.0816149, acc 0.96875\n",
      "2018-05-23T15:41:18.672372: step 20054, loss 0.0783549, acc 0.96875\n",
      "2018-05-23T15:41:19.281743: step 20055, loss 0.0740374, acc 0.984375\n",
      "2018-05-23T15:41:19.864185: step 20056, loss 0.0531954, acc 0.96875\n",
      "2018-05-23T15:41:20.496493: step 20057, loss 0.106122, acc 0.953125\n",
      "2018-05-23T15:41:21.093895: step 20058, loss 0.129159, acc 0.953125\n",
      "2018-05-23T15:41:21.625472: step 20059, loss 0.0328287, acc 1\n",
      "2018-05-23T15:41:22.117156: step 20060, loss 0.0478993, acc 1\n",
      "2018-05-23T15:41:22.567952: step 20061, loss 0.0504576, acc 0.984375\n",
      "2018-05-23T15:41:22.991817: step 20062, loss 0.0629488, acc 0.96875\n",
      "2018-05-23T15:41:23.405711: step 20063, loss 0.00950742, acc 1\n",
      "2018-05-23T15:41:23.799658: step 20064, loss 0.0579002, acc 0.96875\n",
      "2018-05-23T15:41:24.157700: step 20065, loss 0.105233, acc 0.96875\n",
      "2018-05-23T15:41:24.513746: step 20066, loss 0.0412619, acc 0.984375\n",
      "2018-05-23T15:41:24.861816: step 20067, loss 0.21885, acc 0.9375\n",
      "2018-05-23T15:41:25.213874: step 20068, loss 0.0734113, acc 0.984375\n",
      "2018-05-23T15:41:25.561757: step 20069, loss 0.143076, acc 0.96875\n",
      "2018-05-23T15:41:25.959692: step 20070, loss 0.0484582, acc 0.984375\n",
      "2018-05-23T15:41:26.338678: step 20071, loss 0.0646541, acc 0.953125\n",
      "2018-05-23T15:41:26.691735: step 20072, loss 0.0571558, acc 0.984375\n",
      "2018-05-23T15:41:27.049777: step 20073, loss 0.091138, acc 0.96875\n",
      "2018-05-23T15:41:27.461675: step 20074, loss 0.0900949, acc 0.921875\n",
      "2018-05-23T15:41:27.825704: step 20075, loss 0.0599873, acc 0.96875\n",
      "2018-05-23T15:41:28.225631: step 20076, loss 0.0698878, acc 0.984375\n",
      "2018-05-23T15:41:28.583673: step 20077, loss 0.0746331, acc 0.984375\n",
      "2018-05-23T15:41:28.941718: step 20078, loss 0.264552, acc 0.921875\n",
      "2018-05-23T15:41:29.330677: step 20079, loss 0.104497, acc 0.953125\n",
      "2018-05-23T15:41:29.732599: step 20080, loss 0.0601492, acc 0.96875\n",
      "2018-05-23T15:41:30.118568: step 20081, loss 0.150734, acc 0.953125\n",
      "2018-05-23T15:41:30.537447: step 20082, loss 0.0879541, acc 0.96875\n",
      "2018-05-23T15:41:30.976273: step 20083, loss 0.0731728, acc 0.96875\n",
      "2018-05-23T15:41:31.442030: step 20084, loss 0.0603417, acc 0.96875\n",
      "2018-05-23T15:41:31.885840: step 20085, loss 0.0773838, acc 0.953125\n",
      "2018-05-23T15:41:32.375530: step 20086, loss 0.039452, acc 0.96875\n",
      "2018-05-23T15:41:32.898134: step 20087, loss 0.111012, acc 0.96875\n",
      "2018-05-23T15:41:33.400787: step 20088, loss 0.0466844, acc 0.984375\n",
      "2018-05-23T15:41:33.921398: step 20089, loss 0.0632768, acc 0.984375\n",
      "2018-05-23T15:41:34.385156: step 20090, loss 0.0960251, acc 0.96875\n",
      "2018-05-23T15:41:35.007491: step 20091, loss 0.0784346, acc 0.96875\n",
      "2018-05-23T15:41:35.618854: step 20092, loss 0.0868539, acc 0.953125\n",
      "2018-05-23T15:41:36.136468: step 20093, loss 0.0994122, acc 0.953125\n",
      "2018-05-23T15:41:36.638127: step 20094, loss 0.0318195, acc 1\n",
      "2018-05-23T15:41:37.137789: step 20095, loss 0.101454, acc 0.9375\n",
      "2018-05-23T15:41:37.632467: step 20096, loss 0.0867496, acc 0.953125\n",
      "2018-05-23T15:41:38.191970: step 20097, loss 0.0510766, acc 0.96875\n",
      "2018-05-23T15:41:38.727536: step 20098, loss 0.0699242, acc 0.984375\n",
      "2018-05-23T15:41:39.347877: step 20099, loss 0.0336743, acc 1\n",
      "2018-05-23T15:41:40.026064: step 20100, loss 0.25435, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:41:46.750076: step 20100, loss 2.1189, acc 0.709387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20100\n",
      "\n",
      "2018-05-23T15:41:48.749725: step 20101, loss 0.0945477, acc 0.921875\n",
      "2018-05-23T15:41:49.304243: step 20102, loss 0.140156, acc 0.921875\n",
      "2018-05-23T15:41:49.861751: step 20103, loss 0.0521359, acc 0.984375\n",
      "2018-05-23T15:41:50.427238: step 20104, loss 0.106883, acc 0.984375\n",
      "2018-05-23T15:41:50.998709: step 20105, loss 0.103518, acc 0.953125\n",
      "2018-05-23T15:41:51.561205: step 20106, loss 0.125961, acc 0.96875\n",
      "2018-05-23T15:41:52.097770: step 20107, loss 0.0936166, acc 0.984375\n",
      "2018-05-23T15:41:52.635331: step 20108, loss 0.0504998, acc 0.96875\n",
      "2018-05-23T15:41:53.156935: step 20109, loss 0.0805595, acc 0.953125\n",
      "2018-05-23T15:41:53.679537: step 20110, loss 0.0709286, acc 0.96875\n",
      "2018-05-23T15:41:54.190171: step 20111, loss 0.0387492, acc 0.984375\n",
      "2018-05-23T15:41:54.675874: step 20112, loss 0.0649844, acc 0.984375\n",
      "2018-05-23T15:41:55.162572: step 20113, loss 0.0257177, acc 0.984375\n",
      "2018-05-23T15:41:55.655253: step 20114, loss 0.147336, acc 0.921875\n",
      "2018-05-23T15:41:56.128987: step 20115, loss 0.0559189, acc 0.984375\n",
      "2018-05-23T15:41:56.613687: step 20116, loss 0.0328456, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:41:57.104375: step 20117, loss 0.08563, acc 0.953125\n",
      "2018-05-23T15:41:57.592071: step 20118, loss 0.0779326, acc 0.984375\n",
      "2018-05-23T15:41:58.063809: step 20119, loss 0.0575698, acc 0.96875\n",
      "2018-05-23T15:41:58.528566: step 20120, loss 0.0642981, acc 0.984375\n",
      "2018-05-23T15:41:58.996315: step 20121, loss 0.10795, acc 0.953125\n",
      "2018-05-23T15:41:59.482015: step 20122, loss 0.0967232, acc 0.953125\n",
      "2018-05-23T15:41:59.972703: step 20123, loss 0.0585516, acc 0.984375\n",
      "2018-05-23T15:42:00.469375: step 20124, loss 0.0982219, acc 0.96875\n",
      "2018-05-23T15:42:00.975020: step 20125, loss 0.0388597, acc 0.984375\n",
      "2018-05-23T15:42:01.495629: step 20126, loss 0.0761018, acc 0.953125\n",
      "2018-05-23T15:42:02.046156: step 20127, loss 0.26367, acc 0.953125\n",
      "2018-05-23T15:42:02.572747: step 20128, loss 0.126721, acc 0.9375\n",
      "2018-05-23T15:42:03.084378: step 20129, loss 0.151223, acc 0.953125\n",
      "2018-05-23T15:42:03.578057: step 20130, loss 0.0956206, acc 0.9375\n",
      "2018-05-23T15:42:04.117614: step 20131, loss 0.0882528, acc 0.953125\n",
      "2018-05-23T15:42:04.590349: step 20132, loss 0.0803894, acc 0.953125\n",
      "2018-05-23T15:42:05.071064: step 20133, loss 0.138764, acc 0.953125\n",
      "2018-05-23T15:42:05.557762: step 20134, loss 0.137582, acc 0.96875\n",
      "2018-05-23T15:42:06.046453: step 20135, loss 0.0433883, acc 0.984375\n",
      "2018-05-23T15:42:06.550106: step 20136, loss 0.0359723, acc 1\n",
      "2018-05-23T15:42:07.063732: step 20137, loss 0.139692, acc 0.9375\n",
      "2018-05-23T15:42:07.540457: step 20138, loss 0.0762285, acc 0.96875\n",
      "2018-05-23T15:42:08.037128: step 20139, loss 0.0846166, acc 0.953125\n",
      "2018-05-23T15:42:08.527815: step 20140, loss 0.0295261, acc 1\n",
      "2018-05-23T15:42:09.020498: step 20141, loss 0.0576422, acc 0.96875\n",
      "2018-05-23T15:42:09.501211: step 20142, loss 0.0853692, acc 0.96875\n",
      "2018-05-23T15:42:09.977936: step 20143, loss 0.244551, acc 0.921875\n",
      "2018-05-23T15:42:10.478596: step 20144, loss 0.074565, acc 0.953125\n",
      "2018-05-23T15:42:10.969284: step 20145, loss 0.0547916, acc 0.96875\n",
      "2018-05-23T15:42:11.433043: step 20146, loss 0.121216, acc 0.96875\n",
      "2018-05-23T15:42:11.947666: step 20147, loss 0.055562, acc 0.953125\n",
      "2018-05-23T15:42:12.450322: step 20148, loss 0.0842479, acc 0.96875\n",
      "2018-05-23T15:42:12.935026: step 20149, loss 0.12614, acc 0.953125\n",
      "2018-05-23T15:42:13.423718: step 20150, loss 0.100476, acc 0.953125\n",
      "2018-05-23T15:42:13.916400: step 20151, loss 0.0482805, acc 0.984375\n",
      "2018-05-23T15:42:14.381157: step 20152, loss 0.0637808, acc 0.984375\n",
      "2018-05-23T15:42:14.854890: step 20153, loss 0.175254, acc 0.953125\n",
      "2018-05-23T15:42:15.336601: step 20154, loss 0.0423812, acc 0.984375\n",
      "2018-05-23T15:42:15.835267: step 20155, loss 0.0582766, acc 0.984375\n",
      "2018-05-23T15:42:16.330941: step 20156, loss 0.0683641, acc 0.96875\n",
      "2018-05-23T15:42:16.812652: step 20157, loss 0.0321574, acc 1\n",
      "2018-05-23T15:42:17.318299: step 20158, loss 0.201236, acc 0.953125\n",
      "2018-05-23T15:42:17.826939: step 20159, loss 0.0327458, acc 0.984375\n",
      "2018-05-23T15:42:18.312640: step 20160, loss 0.0456587, acc 1\n",
      "2018-05-23T15:42:18.782383: step 20161, loss 0.086397, acc 0.96875\n",
      "2018-05-23T15:42:19.291023: step 20162, loss 0.0460936, acc 0.984375\n",
      "2018-05-23T15:42:19.776723: step 20163, loss 0.065112, acc 0.984375\n",
      "2018-05-23T15:42:20.246468: step 20164, loss 0.0216022, acc 1\n",
      "2018-05-23T15:42:20.739149: step 20165, loss 0.0866247, acc 0.953125\n",
      "2018-05-23T15:42:21.242801: step 20166, loss 0.0825448, acc 0.953125\n",
      "2018-05-23T15:42:21.723514: step 20167, loss 0.0687511, acc 0.953125\n",
      "2018-05-23T15:42:22.221183: step 20168, loss 0.0443603, acc 0.96875\n",
      "2018-05-23T15:42:22.709876: step 20169, loss 0.0485004, acc 0.984375\n",
      "2018-05-23T15:42:23.209539: step 20170, loss 0.299843, acc 0.953125\n",
      "2018-05-23T15:42:23.698231: step 20171, loss 0.0860297, acc 0.96875\n",
      "2018-05-23T15:42:24.159997: step 20172, loss 0.115194, acc 0.921875\n",
      "2018-05-23T15:42:24.649686: step 20173, loss 0.0332593, acc 0.96875\n",
      "2018-05-23T15:42:25.157328: step 20174, loss 0.0855025, acc 0.9375\n",
      "2018-05-23T15:42:25.652006: step 20175, loss 0.069939, acc 0.96875\n",
      "2018-05-23T15:42:26.143690: step 20176, loss 0.194362, acc 0.90625\n",
      "2018-05-23T15:42:26.607449: step 20177, loss 0.0811961, acc 0.96875\n",
      "2018-05-23T15:42:27.097139: step 20178, loss 0.0523513, acc 0.984375\n",
      "2018-05-23T15:42:27.558904: step 20179, loss 0.130999, acc 0.96875\n",
      "2018-05-23T15:42:28.038620: step 20180, loss 0.0616457, acc 0.96875\n",
      "2018-05-23T15:42:28.518337: step 20181, loss 0.0489265, acc 1\n",
      "2018-05-23T15:42:28.985089: step 20182, loss 0.133607, acc 0.921875\n",
      "2018-05-23T15:42:29.468793: step 20183, loss 0.0890598, acc 0.953125\n",
      "2018-05-23T15:42:29.964469: step 20184, loss 0.045967, acc 1\n",
      "2018-05-23T15:42:30.432217: step 20185, loss 0.0817393, acc 0.96875\n",
      "2018-05-23T15:42:30.917918: step 20186, loss 0.054919, acc 0.953125\n",
      "2018-05-23T15:42:31.397635: step 20187, loss 0.0133326, acc 1\n",
      "2018-05-23T15:42:31.881341: step 20188, loss 0.133383, acc 0.96875\n",
      "2018-05-23T15:42:32.361057: step 20189, loss 0.0633054, acc 0.96875\n",
      "2018-05-23T15:42:32.829802: step 20190, loss 0.0883626, acc 0.953125\n",
      "2018-05-23T15:42:33.337445: step 20191, loss 0.0260707, acc 1\n",
      "2018-05-23T15:42:33.861044: step 20192, loss 0.0609768, acc 0.96875\n",
      "2018-05-23T15:42:34.372675: step 20193, loss 0.0738784, acc 0.984375\n",
      "2018-05-23T15:42:34.856381: step 20194, loss 0.0244545, acc 1\n",
      "2018-05-23T15:42:35.340088: step 20195, loss 0.0945456, acc 0.9375\n",
      "2018-05-23T15:42:35.820801: step 20196, loss 0.0405706, acc 1\n",
      "2018-05-23T15:42:36.316476: step 20197, loss 0.0490078, acc 1\n",
      "2018-05-23T15:42:36.794197: step 20198, loss 0.290465, acc 0.9375\n",
      "2018-05-23T15:42:37.303834: step 20199, loss 0.0919811, acc 0.953125\n",
      "2018-05-23T15:42:37.778564: step 20200, loss 0.144586, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:42:43.812423: step 20200, loss 2.12579, acc 0.706101\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20200\n",
      "\n",
      "2018-05-23T15:42:45.624575: step 20201, loss 0.114007, acc 0.96875\n",
      "2018-05-23T15:42:46.157150: step 20202, loss 0.120241, acc 0.953125\n",
      "2018-05-23T15:42:46.677757: step 20203, loss 0.0764941, acc 0.96875\n",
      "2018-05-23T15:42:47.178418: step 20204, loss 0.0398072, acc 0.984375\n",
      "2018-05-23T15:42:47.686060: step 20205, loss 0.103031, acc 0.96875\n",
      "2018-05-23T15:42:48.210657: step 20206, loss 0.0690822, acc 0.96875\n",
      "2018-05-23T15:42:48.709324: step 20207, loss 0.122281, acc 0.90625\n",
      "2018-05-23T15:42:49.270821: step 20208, loss 0.04732, acc 0.953125\n",
      "2018-05-23T15:42:49.787438: step 20209, loss 0.0620362, acc 0.984375\n",
      "2018-05-23T15:42:50.285107: step 20210, loss 0.100551, acc 0.953125\n",
      "2018-05-23T15:42:50.769811: step 20211, loss 0.0400822, acc 1\n",
      "2018-05-23T15:42:51.266482: step 20212, loss 0.0976869, acc 0.9375\n",
      "2018-05-23T15:42:51.757169: step 20213, loss 0.0892365, acc 0.96875\n",
      "2018-05-23T15:42:52.238882: step 20214, loss 0.0497375, acc 1\n",
      "2018-05-23T15:42:52.729567: step 20215, loss 0.0949482, acc 0.984375\n",
      "2018-05-23T15:42:53.226240: step 20216, loss 0.151747, acc 0.96875\n",
      "2018-05-23T15:42:53.687005: step 20217, loss 0.0210995, acc 0.984375\n",
      "2018-05-23T15:42:54.159742: step 20218, loss 0.163174, acc 0.9375\n",
      "2018-05-23T15:42:54.621505: step 20219, loss 0.111973, acc 0.953125\n",
      "2018-05-23T15:42:55.093244: step 20220, loss 0.104387, acc 0.953125\n",
      "2018-05-23T15:42:55.495170: step 20221, loss 0.0969355, acc 0.953125\n",
      "2018-05-23T15:42:55.913051: step 20222, loss 0.0388319, acc 0.984375\n",
      "2018-05-23T15:42:56.327943: step 20223, loss 0.0653262, acc 0.96875\n",
      "2018-05-23T15:42:56.744826: step 20224, loss 0.0839086, acc 0.9375\n",
      "2018-05-23T15:42:57.152734: step 20225, loss 0.0725791, acc 0.96875\n",
      "2018-05-23T15:42:57.561641: step 20226, loss 0.0897451, acc 0.953125\n",
      "2018-05-23T15:42:57.961572: step 20227, loss 0.0852038, acc 0.984375\n",
      "2018-05-23T15:42:58.367484: step 20228, loss 0.105618, acc 0.921875\n",
      "2018-05-23T15:42:58.780380: step 20229, loss 0.109621, acc 0.953125\n",
      "2018-05-23T15:42:59.193275: step 20230, loss 0.103547, acc 0.96875\n",
      "2018-05-23T15:42:59.619136: step 20231, loss 0.160159, acc 0.96875\n",
      "2018-05-23T15:43:00.033029: step 20232, loss 0.0210911, acc 1\n",
      "2018-05-23T15:43:00.469860: step 20233, loss 0.0686037, acc 0.96875\n",
      "2018-05-23T15:43:00.909683: step 20234, loss 0.0761372, acc 0.96875\n",
      "2018-05-23T15:43:01.281689: step 20235, loss 0.0831774, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:43:01.633746: step 20236, loss 0.134913, acc 0.953125\n",
      "2018-05-23T15:43:01.978823: step 20237, loss 0.0987991, acc 0.984375\n",
      "2018-05-23T15:43:02.324897: step 20238, loss 0.0250673, acc 1\n",
      "2018-05-23T15:43:02.663990: step 20239, loss 0.0820691, acc 0.984375\n",
      "2018-05-23T15:43:03.008867: step 20240, loss 0.0768345, acc 0.96875\n",
      "2018-05-23T15:43:03.357934: step 20241, loss 0.041244, acc 0.984375\n",
      "2018-05-23T15:43:03.699019: step 20242, loss 0.086346, acc 0.96875\n",
      "2018-05-23T15:43:04.037116: step 20243, loss 0.111535, acc 0.96875\n",
      "2018-05-23T15:43:04.397153: step 20244, loss 0.0512926, acc 0.984375\n",
      "2018-05-23T15:43:04.732257: step 20245, loss 0.0718742, acc 0.984375\n",
      "2018-05-23T15:43:05.083357: step 20246, loss 0.0707919, acc 0.984375\n",
      "2018-05-23T15:43:05.443392: step 20247, loss 0.0852624, acc 0.984375\n",
      "2018-05-23T15:43:05.781190: step 20248, loss 0.0878143, acc 0.953125\n",
      "2018-05-23T15:43:06.119288: step 20249, loss 0.139701, acc 0.921875\n",
      "2018-05-23T15:43:06.469350: step 20250, loss 0.0683808, acc 0.953125\n",
      "2018-05-23T15:43:06.810437: step 20251, loss 0.0826628, acc 0.96875\n",
      "2018-05-23T15:43:07.154518: step 20252, loss 0.104147, acc 0.953125\n",
      "2018-05-23T15:43:07.506575: step 20253, loss 0.101086, acc 0.96875\n",
      "2018-05-23T15:43:07.865615: step 20254, loss 0.0840548, acc 0.953125\n",
      "2018-05-23T15:43:08.227646: step 20255, loss 0.051014, acc 0.984375\n",
      "2018-05-23T15:43:08.573719: step 20256, loss 0.0825911, acc 0.96875\n",
      "2018-05-23T15:43:08.915805: step 20257, loss 0.130527, acc 0.9375\n",
      "2018-05-23T15:43:09.253902: step 20258, loss 0.0440992, acc 0.96875\n",
      "2018-05-23T15:43:09.603964: step 20259, loss 0.0887227, acc 0.96875\n",
      "2018-05-23T15:43:09.950039: step 20260, loss 0.12437, acc 0.953125\n",
      "2018-05-23T15:43:10.295115: step 20261, loss 0.0601842, acc 0.984375\n",
      "2018-05-23T15:43:10.638197: step 20262, loss 0.138297, acc 0.921875\n",
      "2018-05-23T15:43:10.987265: step 20263, loss 0.0711923, acc 0.953125\n",
      "2018-05-23T15:43:11.346304: step 20264, loss 0.0648709, acc 0.984375\n",
      "2018-05-23T15:43:11.692376: step 20265, loss 0.0841143, acc 0.96875\n",
      "2018-05-23T15:43:12.039451: step 20266, loss 0.0695187, acc 0.96875\n",
      "2018-05-23T15:43:12.377543: step 20267, loss 0.0863058, acc 0.9375\n",
      "2018-05-23T15:43:12.719630: step 20268, loss 0.104092, acc 0.96875\n",
      "2018-05-23T15:43:13.065743: step 20269, loss 0.091604, acc 0.96875\n",
      "2018-05-23T15:43:13.408825: step 20270, loss 0.185034, acc 0.9375\n",
      "2018-05-23T15:43:13.753899: step 20271, loss 0.124125, acc 0.96875\n",
      "2018-05-23T15:43:14.096984: step 20272, loss 0.134531, acc 0.90625\n",
      "2018-05-23T15:43:14.443057: step 20273, loss 0.0711869, acc 0.9375\n",
      "2018-05-23T15:43:14.789131: step 20274, loss 0.0741488, acc 0.96875\n",
      "2018-05-23T15:43:15.127225: step 20275, loss 0.0886256, acc 0.953125\n",
      "2018-05-23T15:43:15.467318: step 20276, loss 0.137004, acc 0.921875\n",
      "2018-05-23T15:43:15.810400: step 20277, loss 0.0997111, acc 0.953125\n",
      "2018-05-23T15:43:16.153480: step 20278, loss 0.0644619, acc 0.953125\n",
      "2018-05-23T15:43:16.511911: step 20279, loss 0.14679, acc 0.96875\n",
      "2018-05-23T15:43:16.859983: step 20280, loss 0.0500613, acc 0.984375\n",
      "2018-05-23T15:43:17.212038: step 20281, loss 0.284207, acc 0.96875\n",
      "2018-05-23T15:43:17.566093: step 20282, loss 0.118075, acc 0.9375\n",
      "2018-05-23T15:43:17.927125: step 20283, loss 0.0881952, acc 0.96875\n",
      "2018-05-23T15:43:18.271204: step 20284, loss 0.199449, acc 0.90625\n",
      "2018-05-23T15:43:18.616282: step 20285, loss 0.0912433, acc 0.96875\n",
      "2018-05-23T15:43:18.995268: step 20286, loss 0.118353, acc 0.953125\n",
      "2018-05-23T15:43:19.332368: step 20287, loss 0.0464574, acc 0.953125\n",
      "2018-05-23T15:43:19.673456: step 20288, loss 0.111393, acc 0.96875\n",
      "2018-05-23T15:43:20.020525: step 20289, loss 0.196381, acc 0.9375\n",
      "2018-05-23T15:43:20.361615: step 20290, loss 0.0722764, acc 0.96875\n",
      "2018-05-23T15:43:20.700706: step 20291, loss 0.0555864, acc 0.984375\n",
      "2018-05-23T15:43:21.045782: step 20292, loss 0.0545, acc 0.984375\n",
      "2018-05-23T15:43:21.385873: step 20293, loss 0.0624111, acc 0.984375\n",
      "2018-05-23T15:43:21.729952: step 20294, loss 0.125049, acc 0.953125\n",
      "2018-05-23T15:43:22.085002: step 20295, loss 0.0635424, acc 0.953125\n",
      "2018-05-23T15:43:22.433073: step 20296, loss 0.0457161, acc 0.96875\n",
      "2018-05-23T15:43:22.773925: step 20297, loss 0.216023, acc 0.90625\n",
      "2018-05-23T15:43:23.132966: step 20298, loss 0.069811, acc 0.953125\n",
      "2018-05-23T15:43:23.481651: step 20299, loss 0.110468, acc 0.96875\n",
      "2018-05-23T15:43:23.826725: step 20300, loss 0.0513609, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:43:28.363183: step 20300, loss 2.14956, acc 0.709244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20300\n",
      "\n",
      "2018-05-23T15:43:29.657720: step 20301, loss 0.0626804, acc 1\n",
      "2018-05-23T15:43:30.022745: step 20302, loss 0.0494728, acc 0.984375\n",
      "2018-05-23T15:43:30.420679: step 20303, loss 0.13613, acc 0.953125\n",
      "2018-05-23T15:43:30.794678: step 20304, loss 0.0592699, acc 0.96875\n",
      "2018-05-23T15:43:31.145739: step 20305, loss 0.0262883, acc 1\n",
      "2018-05-23T15:43:31.495804: step 20306, loss 0.102159, acc 0.984375\n",
      "2018-05-23T15:43:31.848857: step 20307, loss 0.0768469, acc 0.96875\n",
      "2018-05-23T15:43:32.190943: step 20308, loss 0.252346, acc 0.96875\n",
      "2018-05-23T15:43:32.541007: step 20309, loss 0.0516403, acc 1\n",
      "2018-05-23T15:43:32.874115: step 20310, loss 0.124715, acc 0.953125\n",
      "2018-05-23T15:43:33.225177: step 20311, loss 0.0791707, acc 0.9375\n",
      "2018-05-23T15:43:33.577235: step 20312, loss 0.0306217, acc 1\n",
      "2018-05-23T15:43:33.933281: step 20313, loss 0.0290589, acc 1\n",
      "2018-05-23T15:43:34.271377: step 20314, loss 0.0384894, acc 0.96875\n",
      "2018-05-23T15:43:34.616454: step 20315, loss 0.0829676, acc 0.984375\n",
      "2018-05-23T15:43:34.958539: step 20316, loss 0.0354335, acc 0.984375\n",
      "2018-05-23T15:43:35.301658: step 20317, loss 0.0493169, acc 0.984375\n",
      "2018-05-23T15:43:35.649691: step 20318, loss 0.145427, acc 0.90625\n",
      "2018-05-23T15:43:35.994769: step 20319, loss 0.130139, acc 0.96875\n",
      "2018-05-23T15:43:36.338846: step 20320, loss 0.0986947, acc 0.96875\n",
      "2018-05-23T15:43:36.681928: step 20321, loss 0.0275108, acc 1\n",
      "2018-05-23T15:43:37.025010: step 20322, loss 0.0488278, acc 1\n",
      "2018-05-23T15:43:37.380061: step 20323, loss 0.0694604, acc 0.96875\n",
      "2018-05-23T15:43:37.742095: step 20324, loss 0.0483695, acc 0.984375\n",
      "2018-05-23T15:43:38.083182: step 20325, loss 0.0404325, acc 1\n",
      "2018-05-23T15:43:38.468151: step 20326, loss 0.0889378, acc 0.96875\n",
      "2018-05-23T15:43:38.821208: step 20327, loss 0.0178978, acc 1\n",
      "2018-05-23T15:43:39.175258: step 20328, loss 0.0827989, acc 0.96875\n",
      "2018-05-23T15:43:39.529311: step 20329, loss 0.0819116, acc 0.9375\n",
      "2018-05-23T15:43:39.883364: step 20330, loss 0.313153, acc 0.890625\n",
      "2018-05-23T15:43:40.222457: step 20331, loss 0.0616096, acc 0.96875\n",
      "2018-05-23T15:43:40.564542: step 20332, loss 0.0581109, acc 0.984375\n",
      "2018-05-23T15:43:40.910616: step 20333, loss 0.0922548, acc 0.953125\n",
      "2018-05-23T15:43:41.252701: step 20334, loss 0.0764292, acc 0.96875\n",
      "2018-05-23T15:43:41.592791: step 20335, loss 0.0793368, acc 0.96875\n",
      "2018-05-23T15:43:41.941859: step 20336, loss 0.072792, acc 0.984375\n",
      "2018-05-23T15:43:42.283942: step 20337, loss 0.0345779, acc 1\n",
      "2018-05-23T15:43:42.626027: step 20338, loss 0.137508, acc 0.96875\n",
      "2018-05-23T15:43:42.976126: step 20339, loss 0.092385, acc 0.921875\n",
      "2018-05-23T15:43:43.320170: step 20340, loss 0.0900911, acc 0.96875\n",
      "2018-05-23T15:43:43.662255: step 20341, loss 0.0643496, acc 0.96875\n",
      "2018-05-23T15:43:44.012318: step 20342, loss 0.0951035, acc 0.953125\n",
      "2018-05-23T15:43:44.370397: step 20343, loss 0.0422389, acc 0.984375\n",
      "2018-05-23T15:43:44.713442: step 20344, loss 0.164348, acc 0.921875\n",
      "2018-05-23T15:43:45.058521: step 20345, loss 0.0555675, acc 0.984375\n",
      "2018-05-23T15:43:45.395617: step 20346, loss 0.085843, acc 0.96875\n",
      "2018-05-23T15:43:45.732718: step 20347, loss 0.119219, acc 0.921875\n",
      "2018-05-23T15:43:46.079788: step 20348, loss 0.0568259, acc 0.984375\n",
      "2018-05-23T15:43:46.421873: step 20349, loss 0.100704, acc 0.984375\n",
      "2018-05-23T15:43:46.760966: step 20350, loss 0.0721449, acc 0.984375\n",
      "2018-05-23T15:43:47.121003: step 20351, loss 0.089673, acc 0.96875\n",
      "2018-05-23T15:43:47.469070: step 20352, loss 0.0656318, acc 0.953125\n",
      "2018-05-23T15:43:47.818137: step 20353, loss 0.0470162, acc 0.984375\n",
      "2018-05-23T15:43:48.183160: step 20354, loss 0.0211419, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:43:48.527243: step 20355, loss 0.0785485, acc 0.96875\n",
      "2018-05-23T15:43:48.882292: step 20356, loss 0.0668567, acc 0.96875\n",
      "2018-05-23T15:43:49.228368: step 20357, loss 0.0616888, acc 0.96875\n",
      "2018-05-23T15:43:49.568454: step 20358, loss 0.107845, acc 0.96875\n",
      "2018-05-23T15:43:49.918520: step 20359, loss 0.0981905, acc 0.96875\n",
      "2018-05-23T15:43:50.273570: step 20360, loss 0.0429781, acc 0.984375\n",
      "2018-05-23T15:43:50.609672: step 20361, loss 0.0561367, acc 0.984375\n",
      "2018-05-23T15:43:50.957737: step 20362, loss 0.133428, acc 0.9375\n",
      "2018-05-23T15:43:51.306805: step 20363, loss 0.0693235, acc 0.984375\n",
      "2018-05-23T15:43:51.662852: step 20364, loss 0.0827629, acc 0.953125\n",
      "2018-05-23T15:43:52.018933: step 20365, loss 0.0375303, acc 1\n",
      "2018-05-23T15:43:52.374946: step 20366, loss 0.0512594, acc 0.96875\n",
      "2018-05-23T15:43:52.728003: step 20367, loss 0.104375, acc 0.9375\n",
      "2018-05-23T15:43:53.076073: step 20368, loss 0.094525, acc 0.96875\n",
      "2018-05-23T15:43:53.482982: step 20369, loss 0.0192608, acc 1\n",
      "2018-05-23T15:43:53.819902: step 20370, loss 0.0164269, acc 1\n",
      "2018-05-23T15:43:54.167969: step 20371, loss 0.0718267, acc 0.96875\n",
      "2018-05-23T15:43:54.519032: step 20372, loss 0.0554091, acc 0.984375\n",
      "2018-05-23T15:43:54.864109: step 20373, loss 0.136427, acc 0.953125\n",
      "2018-05-23T15:43:55.205196: step 20374, loss 0.196667, acc 0.96875\n",
      "2018-05-23T15:43:55.550271: step 20375, loss 0.0291628, acc 1\n",
      "2018-05-23T15:43:55.888367: step 20376, loss 0.0587118, acc 0.984375\n",
      "2018-05-23T15:43:56.233446: step 20377, loss 0.139808, acc 0.9375\n",
      "2018-05-23T15:43:56.592484: step 20378, loss 0.043095, acc 0.984375\n",
      "2018-05-23T15:43:56.933571: step 20379, loss 0.131437, acc 0.90625\n",
      "2018-05-23T15:43:57.273663: step 20380, loss 0.0542779, acc 0.984375\n",
      "2018-05-23T15:43:57.620735: step 20381, loss 0.0642634, acc 0.96875\n",
      "2018-05-23T15:43:57.960823: step 20382, loss 0.0369877, acc 0.96875\n",
      "2018-05-23T15:43:58.310888: step 20383, loss 0.0891678, acc 0.96875\n",
      "2018-05-23T15:43:58.657960: step 20384, loss 0.0301191, acc 0.984375\n",
      "2018-05-23T15:43:59.001043: step 20385, loss 0.0715608, acc 0.953125\n",
      "2018-05-23T15:43:59.350108: step 20386, loss 0.0558643, acc 0.953125\n",
      "2018-05-23T15:43:59.701167: step 20387, loss 0.134176, acc 0.96875\n",
      "2018-05-23T15:44:00.060208: step 20388, loss 0.133221, acc 0.953125\n",
      "2018-05-23T15:44:00.408595: step 20389, loss 0.0584639, acc 0.984375\n",
      "2018-05-23T15:44:00.760653: step 20390, loss 0.0131985, acc 1\n",
      "2018-05-23T15:44:01.107725: step 20391, loss 0.115061, acc 0.9375\n",
      "2018-05-23T15:44:01.506657: step 20392, loss 0.102832, acc 0.9375\n",
      "2018-05-23T15:44:01.903595: step 20393, loss 0.152437, acc 0.90625\n",
      "2018-05-23T15:44:02.258608: step 20394, loss 0.0442694, acc 1\n",
      "2018-05-23T15:44:02.633607: step 20395, loss 0.104742, acc 0.953125\n",
      "2018-05-23T15:44:02.990650: step 20396, loss 0.0345982, acc 0.984375\n",
      "2018-05-23T15:44:03.332737: step 20397, loss 0.0758365, acc 0.984375\n",
      "2018-05-23T15:44:03.675819: step 20398, loss 0.121337, acc 0.953125\n",
      "2018-05-23T15:44:04.073754: step 20399, loss 0.092724, acc 0.953125\n",
      "2018-05-23T15:44:04.411850: step 20400, loss 0.0734227, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:44:08.940735: step 20400, loss 2.19228, acc 0.711816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20400\n",
      "\n",
      "2018-05-23T15:44:10.251227: step 20401, loss 0.0911192, acc 0.96875\n",
      "2018-05-23T15:44:10.608274: step 20402, loss 0.108474, acc 0.96875\n",
      "2018-05-23T15:44:10.972297: step 20403, loss 0.0931316, acc 0.953125\n",
      "2018-05-23T15:44:11.318371: step 20404, loss 0.0948073, acc 0.96875\n",
      "2018-05-23T15:44:11.658464: step 20405, loss 0.0530245, acc 0.96875\n",
      "2018-05-23T15:44:12.007530: step 20406, loss 0.0742119, acc 0.96875\n",
      "2018-05-23T15:44:12.348616: step 20407, loss 0.0539967, acc 0.96875\n",
      "2018-05-23T15:44:12.695687: step 20408, loss 0.0474362, acc 0.984375\n",
      "2018-05-23T15:44:13.039804: step 20409, loss 0.0635386, acc 0.984375\n",
      "2018-05-23T15:44:13.385881: step 20410, loss 0.0478872, acc 0.984375\n",
      "2018-05-23T15:44:13.734946: step 20411, loss 0.0817043, acc 0.96875\n",
      "2018-05-23T15:44:14.078027: step 20412, loss 0.200518, acc 0.953125\n",
      "2018-05-23T15:44:14.423104: step 20413, loss 0.139336, acc 0.953125\n",
      "2018-05-23T15:44:14.770175: step 20414, loss 0.189201, acc 0.96875\n",
      "2018-05-23T15:44:15.114254: step 20415, loss 0.230494, acc 0.921875\n",
      "2018-05-23T15:44:15.451353: step 20416, loss 0.124239, acc 0.96875\n",
      "2018-05-23T15:44:15.790449: step 20417, loss 0.131256, acc 0.9375\n",
      "2018-05-23T15:44:16.137518: step 20418, loss 0.0988336, acc 0.9375\n",
      "2018-05-23T15:44:16.473618: step 20419, loss 0.0993049, acc 0.96875\n",
      "2018-05-23T15:44:16.823682: step 20420, loss 0.0403904, acc 0.984375\n",
      "2018-05-23T15:44:17.224813: step 20421, loss 0.0823898, acc 0.953125\n",
      "2018-05-23T15:44:17.698546: step 20422, loss 0.115095, acc 0.921875\n",
      "2018-05-23T15:44:18.139367: step 20423, loss 0.153915, acc 0.96875\n",
      "2018-05-23T15:44:18.620080: step 20424, loss 0.0797538, acc 0.96875\n",
      "2018-05-23T15:44:19.100794: step 20425, loss 0.173334, acc 0.921875\n",
      "2018-05-23T15:44:19.506709: step 20426, loss 0.13602, acc 0.96875\n",
      "2018-05-23T15:44:19.911626: step 20427, loss 0.161939, acc 0.9375\n",
      "2018-05-23T15:44:20.302579: step 20428, loss 0.0456977, acc 0.984375\n",
      "2018-05-23T15:44:20.691540: step 20429, loss 0.137226, acc 0.921875\n",
      "2018-05-23T15:44:21.085485: step 20430, loss 0.0816492, acc 0.9375\n",
      "2018-05-23T15:44:21.496386: step 20431, loss 0.266432, acc 0.90625\n",
      "2018-05-23T15:44:21.887339: step 20432, loss 0.131163, acc 0.953125\n",
      "2018-05-23T15:44:22.285274: step 20433, loss 0.0771366, acc 0.953125\n",
      "2018-05-23T15:44:22.684207: step 20434, loss 0.089861, acc 0.9375\n",
      "2018-05-23T15:44:23.072169: step 20435, loss 0.0666928, acc 1\n",
      "2018-05-23T15:44:23.493043: step 20436, loss 0.0525202, acc 0.96875\n",
      "2018-05-23T15:44:23.883999: step 20437, loss 0.176551, acc 0.921875\n",
      "2018-05-23T15:44:24.268970: step 20438, loss 0.0640679, acc 0.953125\n",
      "2018-05-23T15:44:24.684856: step 20439, loss 0.129033, acc 0.90625\n",
      "2018-05-23T15:44:25.083790: step 20440, loss 0.0327866, acc 0.984375\n",
      "2018-05-23T15:44:25.491698: step 20441, loss 0.175384, acc 0.90625\n",
      "2018-05-23T15:44:25.894620: step 20442, loss 0.0579519, acc 0.96875\n",
      "2018-05-23T15:44:26.286573: step 20443, loss 0.147298, acc 0.9375\n",
      "2018-05-23T15:44:26.664559: step 20444, loss 0.0237607, acc 1\n",
      "2018-05-23T15:44:27.035566: step 20445, loss 0.0842559, acc 0.953125\n",
      "2018-05-23T15:44:27.389619: step 20446, loss 0.0263395, acc 1\n",
      "2018-05-23T15:44:27.746664: step 20447, loss 0.0767083, acc 0.984375\n",
      "2018-05-23T15:44:28.127647: step 20448, loss 0.057998, acc 0.96875\n",
      "2018-05-23T15:44:28.509624: step 20449, loss 0.0889493, acc 0.953125\n",
      "2018-05-23T15:44:28.878638: step 20450, loss 0.0676347, acc 0.96875\n",
      "2018-05-23T15:44:29.266397: step 20451, loss 0.0644054, acc 0.96875\n",
      "2018-05-23T15:44:29.639399: step 20452, loss 0.0452374, acc 0.984375\n",
      "2018-05-23T15:44:30.053292: step 20453, loss 0.0556152, acc 0.96875\n",
      "2018-05-23T15:44:30.429286: step 20454, loss 0.0695796, acc 0.9375\n",
      "2018-05-23T15:44:30.833205: step 20455, loss 0.0458204, acc 0.96875\n",
      "2018-05-23T15:44:31.191247: step 20456, loss 0.0309291, acc 0.984375\n",
      "2018-05-23T15:44:31.532337: step 20457, loss 0.0936976, acc 0.953125\n",
      "2018-05-23T15:44:31.918302: step 20458, loss 0.0350568, acc 1\n",
      "2018-05-23T15:44:32.295295: step 20459, loss 0.029054, acc 0.984375\n",
      "2018-05-23T15:44:32.649350: step 20460, loss 0.0363667, acc 0.984375\n",
      "2018-05-23T15:44:33.034318: step 20461, loss 0.0578318, acc 0.984375\n",
      "2018-05-23T15:44:33.421282: step 20462, loss 0.0685006, acc 0.96875\n",
      "2018-05-23T15:44:33.817225: step 20463, loss 0.216551, acc 0.921875\n",
      "2018-05-23T15:44:34.241088: step 20464, loss 0.0583466, acc 0.984375\n",
      "2018-05-23T15:44:34.624064: step 20465, loss 0.0729368, acc 0.953125\n",
      "2018-05-23T15:44:35.006042: step 20466, loss 0.097013, acc 0.953125\n",
      "2018-05-23T15:44:35.391012: step 20467, loss 0.0520683, acc 0.96875\n",
      "2018-05-23T15:44:35.858761: step 20468, loss 0.0468476, acc 1\n",
      "2018-05-23T15:44:36.277641: step 20469, loss 0.0364899, acc 0.984375\n",
      "2018-05-23T15:44:36.689539: step 20470, loss 0.0913571, acc 0.984375\n",
      "2018-05-23T15:44:37.087476: step 20471, loss 0.114149, acc 0.9375\n",
      "2018-05-23T15:44:37.498375: step 20472, loss 0.124483, acc 0.96875\n",
      "2018-05-23T15:44:37.879355: step 20473, loss 0.0975662, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:44:38.274300: step 20474, loss 0.0320486, acc 1\n",
      "2018-05-23T15:44:38.664257: step 20475, loss 0.0687563, acc 0.984375\n",
      "2018-05-23T15:44:39.118041: step 20476, loss 0.0589965, acc 0.96875\n",
      "2018-05-23T15:44:39.514980: step 20477, loss 0.0972336, acc 0.984375\n",
      "2018-05-23T15:44:39.928875: step 20478, loss 0.113722, acc 0.953125\n",
      "2018-05-23T15:44:40.311849: step 20479, loss 0.122376, acc 0.96875\n",
      "2018-05-23T15:44:40.708789: step 20480, loss 0.108283, acc 0.953125\n",
      "2018-05-23T15:44:41.111709: step 20481, loss 0.0807331, acc 0.953125\n",
      "2018-05-23T15:44:41.528593: step 20482, loss 0.0383707, acc 1\n",
      "2018-05-23T15:44:41.956450: step 20483, loss 0.102536, acc 0.921875\n",
      "2018-05-23T15:44:42.378321: step 20484, loss 0.0302531, acc 1\n",
      "2018-05-23T15:44:42.779247: step 20485, loss 0.0448402, acc 0.984375\n",
      "2018-05-23T15:44:43.263951: step 20486, loss 0.0209945, acc 1\n",
      "2018-05-23T15:44:43.765609: step 20487, loss 0.077881, acc 0.96875\n",
      "2018-05-23T15:44:44.335086: step 20488, loss 0.0746942, acc 0.984375\n",
      "2018-05-23T15:44:44.847715: step 20489, loss 0.0288434, acc 0.984375\n",
      "2018-05-23T15:44:45.319453: step 20490, loss 0.0765792, acc 0.984375\n",
      "2018-05-23T15:44:45.782214: step 20491, loss 0.061842, acc 0.96875\n",
      "2018-05-23T15:44:46.231014: step 20492, loss 0.0780377, acc 0.96875\n",
      "2018-05-23T15:44:46.676820: step 20493, loss 0.0566346, acc 0.96875\n",
      "2018-05-23T15:44:47.124623: step 20494, loss 0.0880306, acc 0.953125\n",
      "2018-05-23T15:44:47.581403: step 20495, loss 0.0513274, acc 0.96875\n",
      "2018-05-23T15:44:48.013248: step 20496, loss 0.100554, acc 0.953125\n",
      "2018-05-23T15:44:48.509918: step 20497, loss 0.214881, acc 0.921875\n",
      "2018-05-23T15:44:49.045486: step 20498, loss 0.0813083, acc 0.96875\n",
      "2018-05-23T15:44:49.491292: step 20499, loss 0.0443431, acc 0.984375\n",
      "2018-05-23T15:44:49.965024: step 20500, loss 0.123665, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:44:54.741247: step 20500, loss 2.18234, acc 0.709959\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20500\n",
      "\n",
      "2018-05-23T15:44:56.457656: step 20501, loss 0.0457637, acc 0.984375\n",
      "2018-05-23T15:44:56.918423: step 20502, loss 0.067528, acc 0.984375\n",
      "2018-05-23T15:44:57.389163: step 20503, loss 0.0771965, acc 0.96875\n",
      "2018-05-23T15:44:57.845942: step 20504, loss 0.138886, acc 0.953125\n",
      "2018-05-23T15:44:58.318678: step 20505, loss 0.0561452, acc 0.984375\n",
      "2018-05-23T15:44:58.758500: step 20506, loss 0.0703976, acc 0.96875\n",
      "2018-05-23T15:44:59.310026: step 20507, loss 0.0488881, acc 0.984375\n",
      "2018-05-23T15:44:59.865539: step 20508, loss 0.0334381, acc 1\n",
      "2018-05-23T15:45:00.435016: step 20509, loss 0.0348089, acc 0.984375\n",
      "2018-05-23T15:45:00.964598: step 20510, loss 0.0820783, acc 0.9375\n",
      "2018-05-23T15:45:01.450299: step 20511, loss 0.125512, acc 0.953125\n",
      "2018-05-23T15:45:02.034736: step 20512, loss 0.18637, acc 0.921875\n",
      "2018-05-23T15:45:02.651087: step 20513, loss 0.123452, acc 0.90625\n",
      "2018-05-23T15:45:03.199619: step 20514, loss 0.10416, acc 0.96875\n",
      "2018-05-23T15:45:03.743165: step 20515, loss 0.0785787, acc 0.953125\n",
      "2018-05-23T15:45:04.213906: step 20516, loss 0.0291313, acc 0.984375\n",
      "2018-05-23T15:45:04.709580: step 20517, loss 0.0785987, acc 0.953125\n",
      "2018-05-23T15:45:05.202264: step 20518, loss 0.121183, acc 0.953125\n",
      "2018-05-23T15:45:05.634108: step 20519, loss 0.220632, acc 0.921875\n",
      "2018-05-23T15:45:06.111829: step 20520, loss 0.0430134, acc 0.984375\n",
      "2018-05-23T15:45:06.575588: step 20521, loss 0.0548811, acc 0.96875\n",
      "2018-05-23T15:45:07.141075: step 20522, loss 0.0462481, acc 0.984375\n",
      "2018-05-23T15:45:07.639741: step 20523, loss 0.0310038, acc 0.984375\n",
      "2018-05-23T15:45:08.148382: step 20524, loss 0.0754268, acc 0.953125\n",
      "2018-05-23T15:45:08.647048: step 20525, loss 0.0725926, acc 0.96875\n",
      "2018-05-23T15:45:09.192587: step 20526, loss 0.0285767, acc 0.984375\n",
      "2018-05-23T15:45:09.701226: step 20527, loss 0.0629192, acc 0.96875\n",
      "2018-05-23T15:45:10.239785: step 20528, loss 0.114198, acc 0.953125\n",
      "2018-05-23T15:45:10.981801: step 20529, loss 0.228115, acc 0.921875\n",
      "2018-05-23T15:45:11.882391: step 20530, loss 0.164682, acc 0.921875\n",
      "2018-05-23T15:45:12.545617: step 20531, loss 0.0569372, acc 0.984375\n",
      "2018-05-23T15:45:13.482111: step 20532, loss 0.0894875, acc 0.96875\n",
      "2018-05-23T15:45:14.079514: step 20533, loss 0.166671, acc 0.953125\n",
      "2018-05-23T15:45:14.693871: step 20534, loss 0.0559945, acc 0.96875\n",
      "2018-05-23T15:45:15.256365: step 20535, loss 0.113874, acc 0.953125\n",
      "2018-05-23T15:45:15.771986: step 20536, loss 0.238132, acc 0.9375\n",
      "2018-05-23T15:45:16.349441: step 20537, loss 0.10523, acc 0.96875\n",
      "2018-05-23T15:45:17.188198: step 20538, loss 0.285867, acc 0.90625\n",
      "2018-05-23T15:45:17.898297: step 20539, loss 0.13437, acc 0.96875\n",
      "2018-05-23T15:45:18.466777: step 20540, loss 0.0815112, acc 0.953125\n",
      "2018-05-23T15:45:19.066173: step 20541, loss 0.0527296, acc 0.984375\n",
      "2018-05-23T15:45:19.659586: step 20542, loss 0.0650042, acc 0.984375\n",
      "2018-05-23T15:45:20.187175: step 20543, loss 0.039514, acc 0.984375\n",
      "2018-05-23T15:45:20.738699: step 20544, loss 0.111736, acc 0.953125\n",
      "2018-05-23T15:45:21.273269: step 20545, loss 0.113296, acc 0.921875\n",
      "2018-05-23T15:45:21.845737: step 20546, loss 0.125898, acc 0.96875\n",
      "2018-05-23T15:45:22.350389: step 20547, loss 0.129842, acc 0.96875\n",
      "2018-05-23T15:45:22.827112: step 20548, loss 0.0873232, acc 0.96875\n",
      "2018-05-23T15:45:23.294861: step 20549, loss 0.09036, acc 0.953125\n",
      "2018-05-23T15:45:23.760615: step 20550, loss 0.0757398, acc 0.96875\n",
      "2018-05-23T15:45:24.231357: step 20551, loss 0.197943, acc 0.9375\n",
      "2018-05-23T15:45:24.694117: step 20552, loss 0.09955, acc 0.96875\n",
      "2018-05-23T15:45:25.185803: step 20553, loss 0.067001, acc 0.96875\n",
      "2018-05-23T15:45:25.672501: step 20554, loss 0.0372017, acc 1\n",
      "2018-05-23T15:45:26.153217: step 20555, loss 0.0717361, acc 0.984375\n",
      "2018-05-23T15:45:26.734660: step 20556, loss 0.0870355, acc 0.984375\n",
      "2018-05-23T15:45:27.329069: step 20557, loss 0.135027, acc 0.953125\n",
      "2018-05-23T15:45:27.877638: step 20558, loss 0.177682, acc 0.9375\n",
      "2018-05-23T15:45:28.403195: step 20559, loss 0.0357582, acc 0.984375\n",
      "2018-05-23T15:45:28.930785: step 20560, loss 0.102866, acc 0.953125\n",
      "2018-05-23T15:45:29.446406: step 20561, loss 0.0479541, acc 0.984375\n",
      "2018-05-23T15:45:30.028846: step 20562, loss 0.19379, acc 0.953125\n",
      "2018-05-23T15:45:30.576382: step 20563, loss 0.112976, acc 0.90625\n",
      "2018-05-23T15:45:31.123918: step 20564, loss 0.0904022, acc 0.953125\n",
      "2018-05-23T15:45:31.716333: step 20565, loss 0.103575, acc 0.9375\n",
      "2018-05-23T15:45:32.305755: step 20566, loss 0.0433797, acc 0.984375\n",
      "2018-05-23T15:45:32.829355: step 20567, loss 0.042254, acc 0.96875\n",
      "2018-05-23T15:45:33.371903: step 20568, loss 0.0636111, acc 0.96875\n",
      "2018-05-23T15:45:33.962326: step 20569, loss 0.105465, acc 0.96875\n",
      "2018-05-23T15:45:34.501880: step 20570, loss 0.0724964, acc 0.953125\n",
      "2018-05-23T15:45:35.005532: step 20571, loss 0.0577767, acc 0.953125\n",
      "2018-05-23T15:45:35.578001: step 20572, loss 0.205086, acc 0.9375\n",
      "2018-05-23T15:45:36.139500: step 20573, loss 0.178771, acc 0.90625\n",
      "2018-05-23T15:45:36.719946: step 20574, loss 0.0751061, acc 0.953125\n",
      "2018-05-23T15:45:37.267482: step 20575, loss 0.0743315, acc 0.96875\n",
      "2018-05-23T15:45:37.850921: step 20576, loss 0.0723107, acc 0.96875\n",
      "2018-05-23T15:45:38.374521: step 20577, loss 0.0776977, acc 0.96875\n",
      "2018-05-23T15:45:38.922056: step 20578, loss 0.204401, acc 0.921875\n",
      "2018-05-23T15:45:39.434683: step 20579, loss 0.0978779, acc 0.953125\n",
      "2018-05-23T15:45:39.960278: step 20580, loss 0.13518, acc 0.9375\n",
      "2018-05-23T15:45:40.479887: step 20581, loss 0.058783, acc 0.984375\n",
      "2018-05-23T15:45:40.974571: step 20582, loss 0.0419789, acc 1\n",
      "2018-05-23T15:45:41.500158: step 20583, loss 0.0500477, acc 0.984375\n",
      "2018-05-23T15:45:41.979875: step 20584, loss 0.0420307, acc 1\n",
      "2018-05-23T15:45:42.425683: step 20585, loss 0.0860126, acc 0.96875\n",
      "2018-05-23T15:45:42.855533: step 20586, loss 0.0695555, acc 0.96875\n",
      "2018-05-23T15:45:43.312310: step 20587, loss 0.132291, acc 0.953125\n",
      "2018-05-23T15:45:44.093222: step 20588, loss 0.0502342, acc 0.984375\n",
      "2018-05-23T15:45:44.607844: step 20589, loss 0.112336, acc 0.984375\n",
      "2018-05-23T15:45:45.182308: step 20590, loss 0.0583626, acc 0.96875\n",
      "2018-05-23T15:45:45.675988: step 20591, loss 0.0808598, acc 0.984375\n",
      "2018-05-23T15:45:46.208562: step 20592, loss 0.0375038, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:45:46.762082: step 20593, loss 0.1121, acc 0.96875\n",
      "2018-05-23T15:45:47.319590: step 20594, loss 0.0228301, acc 1\n",
      "2018-05-23T15:45:47.843189: step 20595, loss 0.0722531, acc 0.96875\n",
      "2018-05-23T15:45:48.376762: step 20596, loss 0.142082, acc 0.9375\n",
      "2018-05-23T15:45:48.876426: step 20597, loss 0.0697533, acc 0.953125\n",
      "2018-05-23T15:45:49.478815: step 20598, loss 0.100046, acc 0.953125\n",
      "2018-05-23T15:45:50.055272: step 20599, loss 0.0624157, acc 0.96875\n",
      "2018-05-23T15:45:50.612781: step 20600, loss 0.0544296, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:45:57.570168: step 20600, loss 2.19171, acc 0.706815\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20600\n",
      "\n",
      "2018-05-23T15:45:59.857050: step 20601, loss 0.0548154, acc 0.96875\n",
      "2018-05-23T15:46:00.570143: step 20602, loss 0.167674, acc 0.9375\n",
      "2018-05-23T15:46:01.267279: step 20603, loss 0.106664, acc 0.984375\n",
      "2018-05-23T15:46:02.115009: step 20604, loss 0.0869016, acc 0.953125\n",
      "2018-05-23T15:46:02.914870: step 20605, loss 0.140226, acc 0.9375\n",
      "2018-05-23T15:46:03.716726: step 20606, loss 0.0632607, acc 0.96875\n",
      "2018-05-23T15:46:04.454752: step 20607, loss 0.0348672, acc 1\n",
      "2018-05-23T15:46:05.135929: step 20608, loss 0.107532, acc 0.953125\n",
      "2018-05-23T15:46:05.789181: step 20609, loss 0.0581346, acc 0.96875\n",
      "2018-05-23T15:46:06.397553: step 20610, loss 0.0664208, acc 0.984375\n",
      "2018-05-23T15:46:06.966032: step 20611, loss 0.245912, acc 0.921875\n",
      "2018-05-23T15:46:07.522544: step 20612, loss 0.0615836, acc 0.96875\n",
      "2018-05-23T15:46:08.124932: step 20613, loss 0.150345, acc 0.90625\n",
      "2018-05-23T15:46:08.737294: step 20614, loss 0.13845, acc 0.9375\n",
      "2018-05-23T15:46:09.264883: step 20615, loss 0.193342, acc 0.921875\n",
      "2018-05-23T15:46:09.825383: step 20616, loss 0.18005, acc 0.90625\n",
      "2018-05-23T15:46:10.332028: step 20617, loss 0.140597, acc 0.90625\n",
      "2018-05-23T15:46:10.836677: step 20618, loss 0.0243994, acc 1\n",
      "2018-05-23T15:46:11.353296: step 20619, loss 0.369099, acc 0.953125\n",
      "2018-05-23T15:46:11.859941: step 20620, loss 0.126721, acc 0.9375\n",
      "2018-05-23T15:46:12.411465: step 20621, loss 0.0292347, acc 1\n",
      "2018-05-23T15:46:12.944040: step 20622, loss 0.132327, acc 0.9375\n",
      "2018-05-23T15:46:13.423757: step 20623, loss 0.186302, acc 0.9375\n",
      "2018-05-23T15:46:14.060055: step 20624, loss 0.11244, acc 0.953125\n",
      "2018-05-23T15:46:14.737242: step 20625, loss 0.174618, acc 0.953125\n",
      "2018-05-23T15:46:15.351600: step 20626, loss 0.0515612, acc 1\n",
      "2018-05-23T15:46:15.823338: step 20627, loss 0.122989, acc 0.96875\n",
      "2018-05-23T15:46:16.437693: step 20628, loss 0.108354, acc 0.953125\n",
      "2018-05-23T15:46:16.896466: step 20629, loss 0.0962629, acc 0.96875\n",
      "2018-05-23T15:46:17.380173: step 20630, loss 0.0449719, acc 0.984375\n",
      "2018-05-23T15:46:17.871858: step 20631, loss 0.134121, acc 0.921875\n",
      "2018-05-23T15:46:18.412411: step 20632, loss 0.0425168, acc 0.984375\n",
      "2018-05-23T15:46:19.050706: step 20633, loss 0.0590679, acc 0.984375\n",
      "2018-05-23T15:46:19.764794: step 20634, loss 0.113621, acc 0.953125\n",
      "2018-05-23T15:46:20.371171: step 20635, loss 0.132535, acc 0.96875\n",
      "2018-05-23T15:46:20.947629: step 20636, loss 0.0623562, acc 0.96875\n",
      "2018-05-23T15:46:21.480204: step 20637, loss 0.167993, acc 0.9375\n",
      "2018-05-23T15:46:21.993830: step 20638, loss 0.122241, acc 0.9375\n",
      "2018-05-23T15:46:22.499476: step 20639, loss 0.0685489, acc 0.984375\n",
      "2018-05-23T15:46:23.100868: step 20640, loss 0.030592, acc 1\n",
      "2018-05-23T15:46:23.637433: step 20641, loss 0.02889, acc 1\n",
      "2018-05-23T15:46:24.176993: step 20642, loss 0.173918, acc 0.953125\n",
      "2018-05-23T15:46:24.797331: step 20643, loss 0.0408328, acc 1\n",
      "2018-05-23T15:46:25.427644: step 20644, loss 0.0638607, acc 0.984375\n",
      "2018-05-23T15:46:26.051975: step 20645, loss 0.0799261, acc 0.953125\n",
      "2018-05-23T15:46:26.696251: step 20646, loss 0.1317, acc 0.921875\n",
      "2018-05-23T15:46:27.455219: step 20647, loss 0.0511993, acc 0.96875\n",
      "2018-05-23T15:46:28.266051: step 20648, loss 0.0614826, acc 0.96875\n",
      "2018-05-23T15:46:28.989117: step 20649, loss 0.039301, acc 0.96875\n",
      "2018-05-23T15:46:29.678273: step 20650, loss 0.0449815, acc 1\n",
      "2018-05-23T15:46:30.434250: step 20651, loss 0.0388801, acc 1\n",
      "2018-05-23T15:46:31.179258: step 20652, loss 0.0445667, acc 0.96875\n",
      "2018-05-23T15:46:31.904318: step 20653, loss 0.0029505, acc 1\n",
      "2018-05-23T15:46:32.559565: step 20654, loss 0.0433148, acc 0.984375\n",
      "2018-05-23T15:46:33.201848: step 20655, loss 0.136975, acc 0.953125\n",
      "2018-05-23T15:46:33.875046: step 20656, loss 0.100416, acc 0.953125\n",
      "2018-05-23T15:46:34.440534: step 20657, loss 0.0548854, acc 0.984375\n",
      "2018-05-23T15:46:35.015993: step 20658, loss 0.0315107, acc 1\n",
      "2018-05-23T15:46:35.579486: step 20659, loss 0.100121, acc 0.953125\n",
      "2018-05-23T15:46:36.109070: step 20660, loss 0.0610426, acc 0.96875\n",
      "2018-05-23T15:46:36.656604: step 20661, loss 0.0998396, acc 0.953125\n",
      "2018-05-23T15:46:37.165244: step 20662, loss 0.0575846, acc 0.96875\n",
      "2018-05-23T15:46:37.676875: step 20663, loss 0.123817, acc 0.984375\n",
      "2018-05-23T15:46:38.173546: step 20664, loss 0.0293905, acc 1\n",
      "2018-05-23T15:46:38.686175: step 20665, loss 0.0974711, acc 0.953125\n",
      "2018-05-23T15:46:39.150932: step 20666, loss 0.400626, acc 0.96875\n",
      "2018-05-23T15:46:39.602723: step 20667, loss 0.101847, acc 0.96875\n",
      "2018-05-23T15:46:40.058504: step 20668, loss 0.2067, acc 0.953125\n",
      "2018-05-23T15:46:40.499324: step 20669, loss 0.150725, acc 0.921875\n",
      "2018-05-23T15:46:40.933164: step 20670, loss 0.0367991, acc 0.984375\n",
      "2018-05-23T15:46:41.368001: step 20671, loss 0.0766579, acc 0.953125\n",
      "2018-05-23T15:46:41.805829: step 20672, loss 0.057892, acc 0.96875\n",
      "2018-05-23T15:46:42.248645: step 20673, loss 0.0772108, acc 0.953125\n",
      "2018-05-23T15:46:42.750302: step 20674, loss 0.0862233, acc 0.96875\n",
      "2018-05-23T15:46:43.306814: step 20675, loss 0.0488446, acc 1\n",
      "2018-05-23T15:46:43.846370: step 20676, loss 0.0546686, acc 0.96875\n",
      "2018-05-23T15:46:44.330077: step 20677, loss 0.197665, acc 0.921875\n",
      "2018-05-23T15:46:44.781869: step 20678, loss 0.0539387, acc 0.96875\n",
      "2018-05-23T15:46:45.274550: step 20679, loss 0.261865, acc 0.875\n",
      "2018-05-23T15:46:45.734320: step 20680, loss 0.143767, acc 0.921875\n",
      "2018-05-23T15:46:46.172149: step 20681, loss 0.0519276, acc 0.96875\n",
      "2018-05-23T15:46:46.658847: step 20682, loss 0.0392386, acc 0.96875\n",
      "2018-05-23T15:46:47.232315: step 20683, loss 0.11626, acc 0.9375\n",
      "2018-05-23T15:46:47.766882: step 20684, loss 0.13296, acc 0.9375\n",
      "2018-05-23T15:46:48.217676: step 20685, loss 0.117267, acc 0.941176\n",
      "2018-05-23T15:46:48.750253: step 20686, loss 0.19394, acc 0.9375\n",
      "2018-05-23T15:46:49.250913: step 20687, loss 0.115125, acc 0.953125\n",
      "2018-05-23T15:46:49.716666: step 20688, loss 0.134475, acc 0.953125\n",
      "2018-05-23T15:46:50.224308: step 20689, loss 0.127876, acc 0.9375\n",
      "2018-05-23T15:46:50.704026: step 20690, loss 0.0771707, acc 0.953125\n",
      "2018-05-23T15:46:51.189726: step 20691, loss 0.0420872, acc 0.984375\n",
      "2018-05-23T15:46:51.684402: step 20692, loss 0.0218375, acc 1\n",
      "2018-05-23T15:46:52.133202: step 20693, loss 0.0724518, acc 1\n",
      "2018-05-23T15:46:52.600950: step 20694, loss 0.0625539, acc 0.984375\n",
      "2018-05-23T15:46:53.131531: step 20695, loss 0.171299, acc 0.921875\n",
      "2018-05-23T15:46:53.627205: step 20696, loss 0.0278732, acc 1\n",
      "2018-05-23T15:46:54.070019: step 20697, loss 0.0257548, acc 0.984375\n",
      "2018-05-23T15:46:54.585641: step 20698, loss 0.118852, acc 0.96875\n",
      "2018-05-23T15:46:55.081316: step 20699, loss 0.10536, acc 0.9375\n",
      "2018-05-23T15:46:55.575992: step 20700, loss 0.0916597, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:47:01.937971: step 20700, loss 2.19673, acc 0.713102\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20700\n",
      "\n",
      "2018-05-23T15:47:03.954577: step 20701, loss 0.0537702, acc 0.984375\n",
      "2018-05-23T15:47:04.497129: step 20702, loss 0.0495941, acc 0.984375\n",
      "2018-05-23T15:47:05.004767: step 20703, loss 0.0488701, acc 0.984375\n",
      "2018-05-23T15:47:05.537343: step 20704, loss 0.0889869, acc 0.984375\n",
      "2018-05-23T15:47:06.049971: step 20705, loss 0.0522784, acc 0.984375\n",
      "2018-05-23T15:47:06.542654: step 20706, loss 0.0958539, acc 0.953125\n",
      "2018-05-23T15:47:07.018380: step 20707, loss 0.128489, acc 0.9375\n",
      "2018-05-23T15:47:07.600823: step 20708, loss 0.0328088, acc 0.984375\n",
      "2018-05-23T15:47:08.138386: step 20709, loss 0.0525454, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:47:08.650016: step 20710, loss 0.131436, acc 0.953125\n",
      "2018-05-23T15:47:09.203536: step 20711, loss 0.0682617, acc 0.953125\n",
      "2018-05-23T15:47:09.802931: step 20712, loss 0.12444, acc 0.953125\n",
      "2018-05-23T15:47:10.382382: step 20713, loss 0.032657, acc 1\n",
      "2018-05-23T15:47:10.991752: step 20714, loss 0.0457469, acc 0.984375\n",
      "2018-05-23T15:47:11.599127: step 20715, loss 0.103479, acc 0.953125\n",
      "2018-05-23T15:47:12.232432: step 20716, loss 0.062566, acc 0.984375\n",
      "2018-05-23T15:47:12.818862: step 20717, loss 0.128732, acc 0.953125\n",
      "2018-05-23T15:47:13.362410: step 20718, loss 0.0628237, acc 0.953125\n",
      "2018-05-23T15:47:13.891993: step 20719, loss 0.0456248, acc 0.984375\n",
      "2018-05-23T15:47:14.459474: step 20720, loss 0.0710291, acc 0.96875\n",
      "2018-05-23T15:47:14.911265: step 20721, loss 0.0681125, acc 0.953125\n",
      "2018-05-23T15:47:15.383005: step 20722, loss 0.102525, acc 0.953125\n",
      "2018-05-23T15:47:15.899622: step 20723, loss 0.0647998, acc 0.96875\n",
      "2018-05-23T15:47:16.354405: step 20724, loss 0.0204315, acc 1\n",
      "2018-05-23T15:47:16.795228: step 20725, loss 0.059493, acc 0.984375\n",
      "2018-05-23T15:47:17.320821: step 20726, loss 0.0620089, acc 1\n",
      "2018-05-23T15:47:17.884311: step 20727, loss 0.0683503, acc 0.984375\n",
      "2018-05-23T15:47:18.479725: step 20728, loss 0.0376491, acc 0.984375\n",
      "2018-05-23T15:47:19.203785: step 20729, loss 0.0719437, acc 0.984375\n",
      "2018-05-23T15:47:19.757303: step 20730, loss 0.0938115, acc 0.96875\n",
      "2018-05-23T15:47:20.209095: step 20731, loss 0.0653778, acc 0.96875\n",
      "2018-05-23T15:47:20.658890: step 20732, loss 0.159486, acc 0.953125\n",
      "2018-05-23T15:47:21.084749: step 20733, loss 0.0766862, acc 0.96875\n",
      "2018-05-23T15:47:21.566466: step 20734, loss 0.147182, acc 0.9375\n",
      "2018-05-23T15:47:22.036207: step 20735, loss 0.0819766, acc 0.953125\n",
      "2018-05-23T15:47:22.696440: step 20736, loss 0.125135, acc 0.96875\n",
      "2018-05-23T15:47:23.186130: step 20737, loss 0.0334669, acc 0.984375\n",
      "2018-05-23T15:47:23.697763: step 20738, loss 0.00886843, acc 1\n",
      "2018-05-23T15:47:24.139578: step 20739, loss 0.126573, acc 0.96875\n",
      "2018-05-23T15:47:24.725013: step 20740, loss 0.0539884, acc 0.984375\n",
      "2018-05-23T15:47:25.277533: step 20741, loss 0.076531, acc 0.96875\n",
      "2018-05-23T15:47:25.815098: step 20742, loss 0.122535, acc 0.953125\n",
      "2018-05-23T15:47:26.291822: step 20743, loss 0.0910001, acc 0.953125\n",
      "2018-05-23T15:47:26.825394: step 20744, loss 0.0703279, acc 0.96875\n",
      "2018-05-23T15:47:27.446732: step 20745, loss 0.0379187, acc 1\n",
      "2018-05-23T15:47:28.149850: step 20746, loss 0.0401829, acc 0.96875\n",
      "2018-05-23T15:47:28.849978: step 20747, loss 0.0348248, acc 1\n",
      "2018-05-23T15:47:29.414466: step 20748, loss 0.0517898, acc 0.96875\n",
      "2018-05-23T15:47:29.822375: step 20749, loss 0.0322809, acc 0.984375\n",
      "2018-05-23T15:47:30.384873: step 20750, loss 0.0467927, acc 0.984375\n",
      "2018-05-23T15:47:30.859600: step 20751, loss 0.0670619, acc 0.96875\n",
      "2018-05-23T15:47:31.340315: step 20752, loss 0.0995782, acc 0.953125\n",
      "2018-05-23T15:47:31.742240: step 20753, loss 0.0327393, acc 1\n",
      "2018-05-23T15:47:32.145163: step 20754, loss 0.0833266, acc 0.9375\n",
      "2018-05-23T15:47:32.559054: step 20755, loss 0.0328445, acc 1\n",
      "2018-05-23T15:47:32.984915: step 20756, loss 0.0466798, acc 0.984375\n",
      "2018-05-23T15:47:33.392824: step 20757, loss 0.0811058, acc 0.96875\n",
      "2018-05-23T15:47:33.861571: step 20758, loss 0.086641, acc 0.953125\n",
      "2018-05-23T15:47:34.498866: step 20759, loss 0.0996446, acc 0.921875\n",
      "2018-05-23T15:47:35.030444: step 20760, loss 0.103965, acc 0.96875\n",
      "2018-05-23T15:47:35.579974: step 20761, loss 0.0733124, acc 0.953125\n",
      "2018-05-23T15:47:36.048720: step 20762, loss 0.0901849, acc 0.953125\n",
      "2018-05-23T15:47:36.461615: step 20763, loss 0.0995055, acc 0.9375\n",
      "2018-05-23T15:47:36.852569: step 20764, loss 0.0783731, acc 0.9375\n",
      "2018-05-23T15:47:37.313336: step 20765, loss 0.166048, acc 0.90625\n",
      "2018-05-23T15:47:37.773107: step 20766, loss 0.0284737, acc 1\n",
      "2018-05-23T15:47:38.174034: step 20767, loss 0.0889676, acc 0.9375\n",
      "2018-05-23T15:47:38.592915: step 20768, loss 0.0890265, acc 0.953125\n",
      "2018-05-23T15:47:38.973894: step 20769, loss 0.0660613, acc 0.9375\n",
      "2018-05-23T15:47:39.386790: step 20770, loss 0.0802943, acc 0.96875\n",
      "2018-05-23T15:47:39.795695: step 20771, loss 0.0874187, acc 0.96875\n",
      "2018-05-23T15:47:40.295361: step 20772, loss 0.0711329, acc 0.9375\n",
      "2018-05-23T15:47:40.724212: step 20773, loss 0.128703, acc 0.96875\n",
      "2018-05-23T15:47:41.101203: step 20774, loss 0.0834023, acc 0.984375\n",
      "2018-05-23T15:47:41.500135: step 20775, loss 0.0598942, acc 0.96875\n",
      "2018-05-23T15:47:41.890092: step 20776, loss 0.107952, acc 0.953125\n",
      "2018-05-23T15:47:42.295008: step 20777, loss 0.090229, acc 0.953125\n",
      "2018-05-23T15:47:42.703915: step 20778, loss 0.0326851, acc 1\n",
      "2018-05-23T15:47:43.133765: step 20779, loss 0.0318706, acc 1\n",
      "2018-05-23T15:47:43.601513: step 20780, loss 0.0639925, acc 0.96875\n",
      "2018-05-23T15:47:44.031364: step 20781, loss 0.0478992, acc 0.953125\n",
      "2018-05-23T15:47:44.720521: step 20782, loss 0.110094, acc 0.9375\n",
      "2018-05-23T15:47:45.258083: step 20783, loss 0.134168, acc 0.921875\n",
      "2018-05-23T15:47:45.662999: step 20784, loss 0.17929, acc 0.953125\n",
      "2018-05-23T15:47:46.160667: step 20785, loss 0.0110902, acc 1\n",
      "2018-05-23T15:47:46.667312: step 20786, loss 0.0871138, acc 0.953125\n",
      "2018-05-23T15:47:47.119106: step 20787, loss 0.035271, acc 0.984375\n",
      "2018-05-23T15:47:47.535989: step 20788, loss 0.123577, acc 0.953125\n",
      "2018-05-23T15:47:47.951875: step 20789, loss 0.04244, acc 0.96875\n",
      "2018-05-23T15:47:48.389704: step 20790, loss 0.0683215, acc 0.953125\n",
      "2018-05-23T15:47:48.831522: step 20791, loss 0.0333513, acc 1\n",
      "2018-05-23T15:47:49.311239: step 20792, loss 0.12861, acc 0.953125\n",
      "2018-05-23T15:47:49.730118: step 20793, loss 0.0375172, acc 0.984375\n",
      "2018-05-23T15:47:50.144011: step 20794, loss 0.117605, acc 0.984375\n",
      "2018-05-23T15:47:50.561894: step 20795, loss 0.0838108, acc 0.96875\n",
      "2018-05-23T15:47:50.988754: step 20796, loss 0.0356515, acc 0.984375\n",
      "2018-05-23T15:47:51.398654: step 20797, loss 0.0282632, acc 0.984375\n",
      "2018-05-23T15:47:51.817537: step 20798, loss 0.125941, acc 0.96875\n",
      "2018-05-23T15:47:52.228435: step 20799, loss 0.0937033, acc 0.921875\n",
      "2018-05-23T15:47:52.640336: step 20800, loss 0.0602525, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:47:58.054849: step 20800, loss 2.23114, acc 0.706672\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20800\n",
      "\n",
      "2018-05-23T15:47:59.776243: step 20801, loss 0.0452337, acc 0.96875\n",
      "2018-05-23T15:48:00.189138: step 20802, loss 0.133108, acc 0.96875\n",
      "2018-05-23T15:48:00.558154: step 20803, loss 0.0303138, acc 1\n",
      "2018-05-23T15:48:00.903228: step 20804, loss 0.044256, acc 0.96875\n",
      "2018-05-23T15:48:01.293462: step 20805, loss 0.0879005, acc 0.96875\n",
      "2018-05-23T15:48:01.700374: step 20806, loss 0.018839, acc 1\n",
      "2018-05-23T15:48:02.073377: step 20807, loss 0.0491187, acc 0.953125\n",
      "2018-05-23T15:48:02.467321: step 20808, loss 0.0598771, acc 0.984375\n",
      "2018-05-23T15:48:02.839327: step 20809, loss 0.0283582, acc 1\n",
      "2018-05-23T15:48:03.204353: step 20810, loss 0.0613687, acc 0.96875\n",
      "2018-05-23T15:48:03.576355: step 20811, loss 0.106274, acc 0.953125\n",
      "2018-05-23T15:48:03.928412: step 20812, loss 0.0367439, acc 0.984375\n",
      "2018-05-23T15:48:04.372225: step 20813, loss 0.128548, acc 0.96875\n",
      "2018-05-23T15:48:04.814043: step 20814, loss 0.0336619, acc 1\n",
      "2018-05-23T15:48:05.196021: step 20815, loss 0.0628768, acc 0.96875\n",
      "2018-05-23T15:48:05.574010: step 20816, loss 0.0883431, acc 0.96875\n",
      "2018-05-23T15:48:05.926068: step 20817, loss 0.0866801, acc 0.984375\n",
      "2018-05-23T15:48:06.276132: step 20818, loss 0.140197, acc 0.953125\n",
      "2018-05-23T15:48:06.632179: step 20819, loss 0.0485955, acc 0.96875\n",
      "2018-05-23T15:48:06.976263: step 20820, loss 0.108259, acc 0.953125\n",
      "2018-05-23T15:48:07.322369: step 20821, loss 0.0214068, acc 1\n",
      "2018-05-23T15:48:07.677383: step 20822, loss 0.204031, acc 0.96875\n",
      "2018-05-23T15:48:08.026452: step 20823, loss 0.0559437, acc 1\n",
      "2018-05-23T15:48:08.363547: step 20824, loss 0.0389665, acc 0.984375\n",
      "2018-05-23T15:48:08.744528: step 20825, loss 0.071115, acc 0.96875\n",
      "2018-05-23T15:48:09.094593: step 20826, loss 0.0729462, acc 0.953125\n",
      "2018-05-23T15:48:09.444656: step 20827, loss 0.0771707, acc 0.9375\n",
      "2018-05-23T15:48:09.797710: step 20828, loss 0.0436882, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:48:10.146779: step 20829, loss 0.190354, acc 0.921875\n",
      "2018-05-23T15:48:10.491854: step 20830, loss 0.0603096, acc 0.953125\n",
      "2018-05-23T15:48:10.846904: step 20831, loss 0.0526809, acc 0.984375\n",
      "2018-05-23T15:48:11.199000: step 20832, loss 0.0357281, acc 1\n",
      "2018-05-23T15:48:11.542044: step 20833, loss 0.0499912, acc 0.96875\n",
      "2018-05-23T15:48:11.888119: step 20834, loss 0.050643, acc 1\n",
      "2018-05-23T15:48:12.230987: step 20835, loss 0.130322, acc 0.953125\n",
      "2018-05-23T15:48:12.582053: step 20836, loss 0.0730121, acc 0.96875\n",
      "2018-05-23T15:48:12.932112: step 20837, loss 0.0262166, acc 0.984375\n",
      "2018-05-23T15:48:13.286162: step 20838, loss 0.0682738, acc 0.984375\n",
      "2018-05-23T15:48:13.633235: step 20839, loss 0.0936145, acc 0.9375\n",
      "2018-05-23T15:48:13.985293: step 20840, loss 0.0608836, acc 0.984375\n",
      "2018-05-23T15:48:14.334358: step 20841, loss 0.072292, acc 0.984375\n",
      "2018-05-23T15:48:14.678440: step 20842, loss 0.0715959, acc 0.953125\n",
      "2018-05-23T15:48:15.034487: step 20843, loss 0.0768576, acc 0.96875\n",
      "2018-05-23T15:48:15.371586: step 20844, loss 0.114331, acc 0.96875\n",
      "2018-05-23T15:48:15.716660: step 20845, loss 0.115971, acc 0.96875\n",
      "2018-05-23T15:48:16.065726: step 20846, loss 0.123726, acc 0.96875\n",
      "2018-05-23T15:48:16.410804: step 20847, loss 0.0587986, acc 0.96875\n",
      "2018-05-23T15:48:16.750896: step 20848, loss 0.0896106, acc 0.953125\n",
      "2018-05-23T15:48:17.101954: step 20849, loss 0.141523, acc 0.921875\n",
      "2018-05-23T15:48:17.479060: step 20850, loss 0.0678943, acc 0.984375\n",
      "2018-05-23T15:48:17.826130: step 20851, loss 0.0920647, acc 0.921875\n",
      "2018-05-23T15:48:18.181179: step 20852, loss 0.20061, acc 0.90625\n",
      "2018-05-23T15:48:18.530248: step 20853, loss 0.0248719, acc 1\n",
      "2018-05-23T15:48:18.872331: step 20854, loss 0.0131581, acc 1\n",
      "2018-05-23T15:48:19.245336: step 20855, loss 0.124506, acc 0.921875\n",
      "2018-05-23T15:48:19.588416: step 20856, loss 0.075014, acc 0.984375\n",
      "2018-05-23T15:48:19.933494: step 20857, loss 0.127231, acc 0.9375\n",
      "2018-05-23T15:48:20.284554: step 20858, loss 0.0274382, acc 0.984375\n",
      "2018-05-23T15:48:20.625643: step 20859, loss 0.081116, acc 0.96875\n",
      "2018-05-23T15:48:20.965732: step 20860, loss 0.0403491, acc 1\n",
      "2018-05-23T15:48:21.332751: step 20861, loss 0.137124, acc 0.953125\n",
      "2018-05-23T15:48:21.681815: step 20862, loss 0.0349981, acc 0.96875\n",
      "2018-05-23T15:48:22.018913: step 20863, loss 0.0477116, acc 0.984375\n",
      "2018-05-23T15:48:22.370971: step 20864, loss 0.0810341, acc 0.96875\n",
      "2018-05-23T15:48:22.717045: step 20865, loss 0.102608, acc 0.96875\n",
      "2018-05-23T15:48:23.060127: step 20866, loss 0.0366959, acc 1\n",
      "2018-05-23T15:48:23.409196: step 20867, loss 0.120124, acc 0.921875\n",
      "2018-05-23T15:48:23.756267: step 20868, loss 0.0562264, acc 0.953125\n",
      "2018-05-23T15:48:24.104337: step 20869, loss 0.094678, acc 0.953125\n",
      "2018-05-23T15:48:24.454398: step 20870, loss 0.0765198, acc 0.96875\n",
      "2018-05-23T15:48:24.802469: step 20871, loss 0.108948, acc 0.9375\n",
      "2018-05-23T15:48:25.159513: step 20872, loss 0.0664188, acc 0.984375\n",
      "2018-05-23T15:48:25.526532: step 20873, loss 0.0772136, acc 0.96875\n",
      "2018-05-23T15:48:25.877590: step 20874, loss 0.0849343, acc 0.953125\n",
      "2018-05-23T15:48:26.270545: step 20875, loss 0.0367182, acc 1\n",
      "2018-05-23T15:48:26.638555: step 20876, loss 0.055766, acc 0.96875\n",
      "2018-05-23T15:48:27.002583: step 20877, loss 0.0302348, acc 0.984375\n",
      "2018-05-23T15:48:27.350650: step 20878, loss 0.0324183, acc 1\n",
      "2018-05-23T15:48:27.707696: step 20879, loss 0.0529679, acc 0.953125\n",
      "2018-05-23T15:48:28.060752: step 20880, loss 0.093265, acc 0.953125\n",
      "2018-05-23T15:48:28.405829: step 20881, loss 0.0241206, acc 0.984375\n",
      "2018-05-23T15:48:28.762873: step 20882, loss 0.136727, acc 0.921875\n",
      "2018-05-23T15:48:29.122909: step 20883, loss 0.0428576, acc 0.984375\n",
      "2018-05-23T15:48:29.469980: step 20884, loss 0.252362, acc 0.90625\n",
      "2018-05-23T15:48:29.827026: step 20885, loss 0.0505789, acc 0.96875\n",
      "2018-05-23T15:48:30.181080: step 20886, loss 0.118922, acc 0.9375\n",
      "2018-05-23T15:48:30.539154: step 20887, loss 0.0614526, acc 0.96875\n",
      "2018-05-23T15:48:30.885196: step 20888, loss 0.155706, acc 0.890625\n",
      "2018-05-23T15:48:31.241244: step 20889, loss 0.132789, acc 0.953125\n",
      "2018-05-23T15:48:31.586320: step 20890, loss 0.0526998, acc 0.953125\n",
      "2018-05-23T15:48:31.935384: step 20891, loss 0.0395962, acc 1\n",
      "2018-05-23T15:48:32.297418: step 20892, loss 0.0371456, acc 0.984375\n",
      "2018-05-23T15:48:32.647481: step 20893, loss 0.145718, acc 0.953125\n",
      "2018-05-23T15:48:33.000535: step 20894, loss 0.0618573, acc 0.96875\n",
      "2018-05-23T15:48:33.352594: step 20895, loss 0.127008, acc 0.9375\n",
      "2018-05-23T15:48:33.701657: step 20896, loss 0.0580491, acc 0.953125\n",
      "2018-05-23T15:48:34.059699: step 20897, loss 0.0590285, acc 0.984375\n",
      "2018-05-23T15:48:34.406770: step 20898, loss 0.0788905, acc 0.96875\n",
      "2018-05-23T15:48:34.758826: step 20899, loss 0.0496093, acc 0.96875\n",
      "2018-05-23T15:48:35.109888: step 20900, loss 0.0744434, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:48:40.011773: step 20900, loss 2.23527, acc 0.707244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-20900\n",
      "\n",
      "2018-05-23T15:48:41.880774: step 20901, loss 0.0741849, acc 0.96875\n",
      "2018-05-23T15:48:42.327615: step 20902, loss 0.0632356, acc 0.96875\n",
      "2018-05-23T15:48:42.949913: step 20903, loss 0.107091, acc 0.90625\n",
      "2018-05-23T15:48:43.420654: step 20904, loss 0.0317667, acc 0.984375\n",
      "2018-05-23T15:48:43.983081: step 20905, loss 0.0527725, acc 0.984375\n",
      "2018-05-23T15:48:44.562663: step 20906, loss 0.147708, acc 0.953125\n",
      "2018-05-23T15:48:44.972567: step 20907, loss 0.0494827, acc 0.96875\n",
      "2018-05-23T15:48:45.359531: step 20908, loss 0.140392, acc 0.921875\n",
      "2018-05-23T15:48:45.760268: step 20909, loss 0.138601, acc 0.96875\n",
      "2018-05-23T15:48:46.520788: step 20910, loss 0.0902014, acc 0.984375\n",
      "2018-05-23T15:48:47.033414: step 20911, loss 0.0321259, acc 0.984375\n",
      "2018-05-23T15:48:47.501170: step 20912, loss 0.0205262, acc 0.984375\n",
      "2018-05-23T15:48:47.975893: step 20913, loss 0.112437, acc 0.921875\n",
      "2018-05-23T15:48:48.622164: step 20914, loss 0.0459426, acc 0.984375\n",
      "2018-05-23T15:48:49.588580: step 20915, loss 0.175481, acc 0.953125\n",
      "2018-05-23T15:48:50.678696: step 20916, loss 0.40591, acc 0.96875\n",
      "2018-05-23T15:48:51.429260: step 20917, loss 0.102187, acc 0.96875\n",
      "2018-05-23T15:48:52.032643: step 20918, loss 0.0680158, acc 0.96875\n",
      "2018-05-23T15:48:52.485434: step 20919, loss 0.069539, acc 0.96875\n",
      "2018-05-23T15:48:52.890348: step 20920, loss 0.050502, acc 1\n",
      "2018-05-23T15:48:53.296261: step 20921, loss 0.199193, acc 0.9375\n",
      "2018-05-23T15:48:53.690208: step 20922, loss 0.0977471, acc 0.96875\n",
      "2018-05-23T15:48:54.207823: step 20923, loss 0.0989201, acc 0.96875\n",
      "2018-05-23T15:48:54.713473: step 20924, loss 0.0831231, acc 0.984375\n",
      "2018-05-23T15:48:55.144318: step 20925, loss 0.0875235, acc 0.9375\n",
      "2018-05-23T15:48:55.555221: step 20926, loss 0.151458, acc 0.9375\n",
      "2018-05-23T15:48:56.203485: step 20927, loss 0.157122, acc 0.96875\n",
      "2018-05-23T15:48:56.789915: step 20928, loss 0.0384504, acc 0.984375\n",
      "2018-05-23T15:48:57.335456: step 20929, loss 0.039109, acc 1\n",
      "2018-05-23T15:48:57.841103: step 20930, loss 0.0977212, acc 0.953125\n",
      "2018-05-23T15:48:58.319823: step 20931, loss 0.151247, acc 0.953125\n",
      "2018-05-23T15:48:58.927201: step 20932, loss 0.0732428, acc 0.953125\n",
      "2018-05-23T15:48:59.443816: step 20933, loss 0.0665488, acc 0.96875\n",
      "2018-05-23T15:48:59.825794: step 20934, loss 0.0518457, acc 0.984375\n",
      "2018-05-23T15:49:00.219742: step 20935, loss 0.112903, acc 0.9375\n",
      "2018-05-23T15:49:00.599725: step 20936, loss 0.150776, acc 0.96875\n",
      "2018-05-23T15:49:01.001650: step 20937, loss 0.0722699, acc 0.96875\n",
      "2018-05-23T15:49:01.403573: step 20938, loss 0.0365817, acc 1\n",
      "2018-05-23T15:49:01.759623: step 20939, loss 0.107489, acc 0.953125\n",
      "2018-05-23T15:49:02.153567: step 20940, loss 0.0403153, acc 0.984375\n",
      "2018-05-23T15:49:02.511611: step 20941, loss 0.0692865, acc 0.953125\n",
      "2018-05-23T15:49:02.858681: step 20942, loss 0.0759705, acc 0.984375\n",
      "2018-05-23T15:49:03.228691: step 20943, loss 0.099519, acc 0.9375\n",
      "2018-05-23T15:49:03.587732: step 20944, loss 0.0600457, acc 0.984375\n",
      "2018-05-23T15:49:03.991650: step 20945, loss 0.0504796, acc 1\n",
      "2018-05-23T15:49:04.400557: step 20946, loss 0.133801, acc 0.921875\n",
      "2018-05-23T15:49:04.779894: step 20947, loss 0.0505965, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:49:05.219714: step 20948, loss 0.0774631, acc 0.96875\n",
      "2018-05-23T15:49:05.595710: step 20949, loss 0.131872, acc 0.96875\n",
      "2018-05-23T15:49:05.947769: step 20950, loss 0.0725499, acc 0.96875\n",
      "2018-05-23T15:49:06.329744: step 20951, loss 0.0624186, acc 0.984375\n",
      "2018-05-23T15:49:06.713718: step 20952, loss 0.0366429, acc 1\n",
      "2018-05-23T15:49:07.081735: step 20953, loss 0.0832356, acc 0.984375\n",
      "2018-05-23T15:49:07.455735: step 20954, loss 0.0510568, acc 0.984375\n",
      "2018-05-23T15:49:07.881596: step 20955, loss 0.0821924, acc 0.953125\n",
      "2018-05-23T15:49:08.240637: step 20956, loss 0.0849943, acc 0.96875\n",
      "2018-05-23T15:49:08.628595: step 20957, loss 0.130599, acc 0.921875\n",
      "2018-05-23T15:49:08.995612: step 20958, loss 0.058444, acc 0.96875\n",
      "2018-05-23T15:49:09.366620: step 20959, loss 0.140256, acc 0.984375\n",
      "2018-05-23T15:49:09.782509: step 20960, loss 0.0952922, acc 0.953125\n",
      "2018-05-23T15:49:10.155886: step 20961, loss 0.0359279, acc 0.984375\n",
      "2018-05-23T15:49:10.529884: step 20962, loss 0.182366, acc 0.9375\n",
      "2018-05-23T15:49:10.886249: step 20963, loss 0.0909502, acc 0.953125\n",
      "2018-05-23T15:49:11.245288: step 20964, loss 0.119714, acc 0.953125\n",
      "2018-05-23T15:49:11.608322: step 20965, loss 0.0851895, acc 0.96875\n",
      "2018-05-23T15:49:12.000271: step 20966, loss 0.0919103, acc 0.984375\n",
      "2018-05-23T15:49:12.364296: step 20967, loss 0.0571919, acc 0.96875\n",
      "2018-05-23T15:49:12.728323: step 20968, loss 0.10026, acc 0.953125\n",
      "2018-05-23T15:49:13.079381: step 20969, loss 0.077459, acc 0.96875\n",
      "2018-05-23T15:49:13.425457: step 20970, loss 0.0370168, acc 1\n",
      "2018-05-23T15:49:13.816368: step 20971, loss 0.0259297, acc 0.984375\n",
      "2018-05-23T15:49:14.191364: step 20972, loss 0.059867, acc 0.984375\n",
      "2018-05-23T15:49:14.560378: step 20973, loss 0.0247307, acc 1\n",
      "2018-05-23T15:49:14.985242: step 20974, loss 0.0537979, acc 0.984375\n",
      "2018-05-23T15:49:15.353259: step 20975, loss 0.0980681, acc 0.9375\n",
      "2018-05-23T15:49:15.715288: step 20976, loss 0.107287, acc 0.953125\n",
      "2018-05-23T15:49:16.074328: step 20977, loss 0.0363589, acc 0.984375\n",
      "2018-05-23T15:49:16.450324: step 20978, loss 0.0506662, acc 0.984375\n",
      "2018-05-23T15:49:16.825318: step 20979, loss 0.0206977, acc 1\n",
      "2018-05-23T15:49:17.353904: step 20980, loss 0.0338465, acc 0.984375\n",
      "2018-05-23T15:49:17.842597: step 20981, loss 0.0612084, acc 0.953125\n",
      "2018-05-23T15:49:18.253498: step 20982, loss 0.0499467, acc 1\n",
      "2018-05-23T15:49:18.622512: step 20983, loss 0.0224046, acc 1\n",
      "2018-05-23T15:49:19.008479: step 20984, loss 0.103358, acc 0.9375\n",
      "2018-05-23T15:49:19.371540: step 20985, loss 0.312625, acc 0.90625\n",
      "2018-05-23T15:49:19.738527: step 20986, loss 0.0431336, acc 1\n",
      "2018-05-23T15:49:20.124494: step 20987, loss 0.0939815, acc 0.953125\n",
      "2018-05-23T15:49:20.482536: step 20988, loss 0.0563479, acc 0.984375\n",
      "2018-05-23T15:49:20.869995: step 20989, loss 0.0836325, acc 0.96875\n",
      "2018-05-23T15:49:21.252973: step 20990, loss 0.159833, acc 0.90625\n",
      "2018-05-23T15:49:21.613008: step 20991, loss 0.0269597, acc 1\n",
      "2018-05-23T15:49:21.986011: step 20992, loss 0.127785, acc 0.9375\n",
      "2018-05-23T15:49:22.363998: step 20993, loss 0.0656452, acc 0.96875\n",
      "2018-05-23T15:49:22.735007: step 20994, loss 0.0830912, acc 0.9375\n",
      "2018-05-23T15:49:23.100029: step 20995, loss 0.0581559, acc 0.96875\n",
      "2018-05-23T15:49:23.508938: step 20996, loss 0.0135913, acc 1\n",
      "2018-05-23T15:49:23.883932: step 20997, loss 0.0668712, acc 0.96875\n",
      "2018-05-23T15:49:24.239980: step 20998, loss 0.079331, acc 0.96875\n",
      "2018-05-23T15:49:24.598023: step 20999, loss 0.0766363, acc 0.96875\n",
      "2018-05-23T15:49:24.973019: step 21000, loss 0.0544691, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:49:30.392521: step 21000, loss 2.23172, acc 0.709101\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21000\n",
      "\n",
      "2018-05-23T15:49:32.082998: step 21001, loss 0.0883173, acc 0.96875\n",
      "2018-05-23T15:49:32.516838: step 21002, loss 0.081665, acc 0.9375\n",
      "2018-05-23T15:49:32.928737: step 21003, loss 0.0810784, acc 0.984375\n",
      "2018-05-23T15:49:33.320687: step 21004, loss 0.223791, acc 0.9375\n",
      "2018-05-23T15:49:33.684713: step 21005, loss 0.0862929, acc 0.984375\n",
      "2018-05-23T15:49:34.055720: step 21006, loss 0.101703, acc 0.96875\n",
      "2018-05-23T15:49:34.418752: step 21007, loss 0.0643639, acc 0.984375\n",
      "2018-05-23T15:49:34.777789: step 21008, loss 0.0386249, acc 0.984375\n",
      "2018-05-23T15:49:35.132839: step 21009, loss 0.0228248, acc 1\n",
      "2018-05-23T15:49:35.488887: step 21010, loss 0.123966, acc 0.953125\n",
      "2018-05-23T15:49:35.849923: step 21011, loss 0.099802, acc 0.953125\n",
      "2018-05-23T15:49:36.205020: step 21012, loss 0.121543, acc 0.921875\n",
      "2018-05-23T15:49:36.565055: step 21013, loss 0.0594977, acc 0.96875\n",
      "2018-05-23T15:49:36.923099: step 21014, loss 0.0401737, acc 0.984375\n",
      "2018-05-23T15:49:37.270171: step 21015, loss 0.0116043, acc 1\n",
      "2018-05-23T15:49:37.629210: step 21016, loss 0.209636, acc 0.9375\n",
      "2018-05-23T15:49:37.985211: step 21017, loss 0.111698, acc 0.953125\n",
      "2018-05-23T15:49:38.343254: step 21018, loss 0.113442, acc 0.953125\n",
      "2018-05-23T15:49:38.720690: step 21019, loss 0.0629279, acc 0.984375\n",
      "2018-05-23T15:49:39.076735: step 21020, loss 0.0715324, acc 0.953125\n",
      "2018-05-23T15:49:39.428796: step 21021, loss 0.119863, acc 0.96875\n",
      "2018-05-23T15:49:39.789830: step 21022, loss 0.132003, acc 0.953125\n",
      "2018-05-23T15:49:40.146875: step 21023, loss 0.0379506, acc 0.984375\n",
      "2018-05-23T15:49:40.504917: step 21024, loss 0.0834026, acc 0.96875\n",
      "2018-05-23T15:49:40.873928: step 21025, loss 0.0429816, acc 0.984375\n",
      "2018-05-23T15:49:41.263884: step 21026, loss 0.0799518, acc 0.953125\n",
      "2018-05-23T15:49:41.695728: step 21027, loss 0.0396571, acc 0.984375\n",
      "2018-05-23T15:49:42.104636: step 21028, loss 0.0479628, acc 0.984375\n",
      "2018-05-23T15:49:42.562413: step 21029, loss 0.0709357, acc 0.984375\n",
      "2018-05-23T15:49:42.996251: step 21030, loss 0.105534, acc 0.96875\n",
      "2018-05-23T15:49:43.374239: step 21031, loss 0.0484522, acc 0.984375\n",
      "2018-05-23T15:49:43.766190: step 21032, loss 0.101615, acc 0.953125\n",
      "2018-05-23T15:49:44.185069: step 21033, loss 0.0518195, acc 0.984375\n",
      "2018-05-23T15:49:44.563060: step 21034, loss 0.0734079, acc 0.9375\n",
      "2018-05-23T15:49:44.928084: step 21035, loss 0.11109, acc 0.96875\n",
      "2018-05-23T15:49:45.286125: step 21036, loss 0.0320986, acc 0.984375\n",
      "2018-05-23T15:49:45.665113: step 21037, loss 0.0775704, acc 0.984375\n",
      "2018-05-23T15:49:46.031131: step 21038, loss 0.125809, acc 0.890625\n",
      "2018-05-23T15:49:46.382194: step 21039, loss 0.164473, acc 0.921875\n",
      "2018-05-23T15:49:46.735249: step 21040, loss 0.0865202, acc 0.984375\n",
      "2018-05-23T15:49:47.098279: step 21041, loss 0.0825673, acc 0.984375\n",
      "2018-05-23T15:49:47.502195: step 21042, loss 0.014124, acc 1\n",
      "2018-05-23T15:49:47.909110: step 21043, loss 0.0441456, acc 1\n",
      "2018-05-23T15:49:48.315023: step 21044, loss 0.0712306, acc 0.96875\n",
      "2018-05-23T15:49:48.704980: step 21045, loss 0.094365, acc 0.96875\n",
      "2018-05-23T15:49:49.158773: step 21046, loss 0.244674, acc 0.90625\n",
      "2018-05-23T15:49:49.582631: step 21047, loss 0.0837805, acc 0.953125\n",
      "2018-05-23T15:49:50.004501: step 21048, loss 0.0663838, acc 0.953125\n",
      "2018-05-23T15:49:50.442330: step 21049, loss 0.0712864, acc 0.984375\n",
      "2018-05-23T15:49:50.846249: step 21050, loss 0.206478, acc 0.953125\n",
      "2018-05-23T15:49:51.230222: step 21051, loss 0.117689, acc 0.953125\n",
      "2018-05-23T15:49:51.592256: step 21052, loss 0.240978, acc 0.90625\n",
      "2018-05-23T15:49:51.943314: step 21053, loss 0.0931655, acc 0.9375\n",
      "2018-05-23T15:49:52.310333: step 21054, loss 0.0599348, acc 0.96875\n",
      "2018-05-23T15:49:52.727218: step 21055, loss 0.122173, acc 0.921875\n",
      "2018-05-23T15:49:53.111190: step 21056, loss 0.0293002, acc 0.984375\n",
      "2018-05-23T15:49:53.509128: step 21057, loss 0.0553386, acc 0.984375\n",
      "2018-05-23T15:49:53.899082: step 21058, loss 0.113228, acc 0.921875\n",
      "2018-05-23T15:49:54.262111: step 21059, loss 0.0389867, acc 1\n",
      "2018-05-23T15:49:54.622149: step 21060, loss 0.0529721, acc 0.984375\n",
      "2018-05-23T15:49:54.977201: step 21061, loss 0.157573, acc 0.96875\n",
      "2018-05-23T15:49:55.345998: step 21062, loss 0.0735039, acc 0.984375\n",
      "2018-05-23T15:49:55.698056: step 21063, loss 0.202847, acc 0.953125\n",
      "2018-05-23T15:49:56.086393: step 21064, loss 0.130714, acc 0.9375\n",
      "2018-05-23T15:49:56.498294: step 21065, loss 0.0599856, acc 1\n",
      "2018-05-23T15:49:56.868303: step 21066, loss 0.0659217, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:49:57.254269: step 21067, loss 0.170302, acc 0.921875\n",
      "2018-05-23T15:49:57.663175: step 21068, loss 0.0465875, acc 0.984375\n",
      "2018-05-23T15:49:58.104994: step 21069, loss 0.0469544, acc 0.96875\n",
      "2018-05-23T15:49:58.492958: step 21070, loss 0.124007, acc 0.953125\n",
      "2018-05-23T15:49:58.888899: step 21071, loss 0.112476, acc 0.953125\n",
      "2018-05-23T15:49:59.238963: step 21072, loss 0.129264, acc 0.96875\n",
      "2018-05-23T15:49:59.621937: step 21073, loss 0.173206, acc 0.953125\n",
      "2018-05-23T15:49:59.971563: step 21074, loss 0.0793499, acc 0.96875\n",
      "2018-05-23T15:50:00.334593: step 21075, loss 0.141825, acc 0.953125\n",
      "2018-05-23T15:50:00.691638: step 21076, loss 0.0664368, acc 0.984375\n",
      "2018-05-23T15:50:01.039738: step 21077, loss 0.0252873, acc 1\n",
      "2018-05-23T15:50:01.424674: step 21078, loss 0.141615, acc 0.953125\n",
      "2018-05-23T15:50:01.814630: step 21079, loss 0.0273189, acc 1\n",
      "2018-05-23T15:50:02.179655: step 21080, loss 0.0707384, acc 0.96875\n",
      "2018-05-23T15:50:02.548666: step 21081, loss 0.116296, acc 0.953125\n",
      "2018-05-23T15:50:02.900725: step 21082, loss 0.0499113, acc 0.984375\n",
      "2018-05-23T15:50:03.254814: step 21083, loss 0.0500498, acc 1\n",
      "2018-05-23T15:50:03.601849: step 21084, loss 0.120178, acc 0.953125\n",
      "2018-05-23T15:50:03.951745: step 21085, loss 0.043496, acc 0.96875\n",
      "2018-05-23T15:50:04.318764: step 21086, loss 0.097055, acc 0.953125\n",
      "2018-05-23T15:50:04.661848: step 21087, loss 0.121281, acc 0.9375\n",
      "2018-05-23T15:50:05.028865: step 21088, loss 0.0582012, acc 0.96875\n",
      "2018-05-23T15:50:05.379926: step 21089, loss 0.0566273, acc 0.984375\n",
      "2018-05-23T15:50:05.725001: step 21090, loss 0.0992906, acc 0.953125\n",
      "2018-05-23T15:50:06.072072: step 21091, loss 0.08724, acc 0.96875\n",
      "2018-05-23T15:50:06.421139: step 21092, loss 0.0215381, acc 1\n",
      "2018-05-23T15:50:06.822069: step 21093, loss 0.0880611, acc 0.96875\n",
      "2018-05-23T15:50:07.189084: step 21094, loss 0.0833113, acc 0.953125\n",
      "2018-05-23T15:50:07.563085: step 21095, loss 0.104029, acc 0.96875\n",
      "2018-05-23T15:50:07.939081: step 21096, loss 0.0763693, acc 0.96875\n",
      "2018-05-23T15:50:08.306978: step 21097, loss 0.0293778, acc 1\n",
      "2018-05-23T15:50:08.670007: step 21098, loss 0.0925443, acc 0.96875\n",
      "2018-05-23T15:50:09.100854: step 21099, loss 0.129324, acc 0.9375\n",
      "2018-05-23T15:50:09.511757: step 21100, loss 0.0337099, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:50:14.356795: step 21100, loss 2.27041, acc 0.707244\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21100\n",
      "\n",
      "2018-05-23T15:50:15.765187: step 21101, loss 0.0631366, acc 0.96875\n",
      "2018-05-23T15:50:16.165120: step 21102, loss 0.067312, acc 0.96875\n",
      "2018-05-23T15:50:16.557069: step 21103, loss 0.135506, acc 0.984375\n",
      "2018-05-23T15:50:16.929075: step 21104, loss 0.0946343, acc 0.9375\n",
      "2018-05-23T15:50:17.291107: step 21105, loss 0.0285924, acc 0.984375\n",
      "2018-05-23T15:50:17.674083: step 21106, loss 0.0196526, acc 1\n",
      "2018-05-23T15:50:18.032123: step 21107, loss 0.074525, acc 0.96875\n",
      "2018-05-23T15:50:18.407121: step 21108, loss 0.0175723, acc 1\n",
      "2018-05-23T15:50:18.784114: step 21109, loss 0.0867964, acc 0.96875\n",
      "2018-05-23T15:50:19.202990: step 21110, loss 0.0443937, acc 0.96875\n",
      "2018-05-23T15:50:19.564025: step 21111, loss 0.0915279, acc 0.96875\n",
      "2018-05-23T15:50:19.931043: step 21112, loss 0.0571579, acc 0.96875\n",
      "2018-05-23T15:50:20.301053: step 21113, loss 0.092548, acc 0.921875\n",
      "2018-05-23T15:50:20.660093: step 21114, loss 0.126031, acc 0.96875\n",
      "2018-05-23T15:50:21.015144: step 21115, loss 0.0642409, acc 0.984375\n",
      "2018-05-23T15:50:21.382161: step 21116, loss 0.126501, acc 0.96875\n",
      "2018-05-23T15:50:21.741200: step 21117, loss 0.0490053, acc 0.984375\n",
      "2018-05-23T15:50:22.096250: step 21118, loss 0.0339678, acc 0.984375\n",
      "2018-05-23T15:50:22.482218: step 21119, loss 0.119692, acc 0.96875\n",
      "2018-05-23T15:50:22.831285: step 21120, loss 0.0692292, acc 0.984375\n",
      "2018-05-23T15:50:23.181348: step 21121, loss 0.0985669, acc 0.953125\n",
      "2018-05-23T15:50:23.594243: step 21122, loss 0.0993393, acc 0.953125\n",
      "2018-05-23T15:50:24.000157: step 21123, loss 0.0250174, acc 0.984375\n",
      "2018-05-23T15:50:24.446963: step 21124, loss 0.0842288, acc 0.984375\n",
      "2018-05-23T15:50:24.899752: step 21125, loss 0.0303238, acc 0.984375\n",
      "2018-05-23T15:50:25.288711: step 21126, loss 0.0435603, acc 0.984375\n",
      "2018-05-23T15:50:25.687644: step 21127, loss 0.0629463, acc 0.984375\n",
      "2018-05-23T15:50:26.045686: step 21128, loss 0.0658603, acc 0.984375\n",
      "2018-05-23T15:50:26.428660: step 21129, loss 0.218496, acc 0.96875\n",
      "2018-05-23T15:50:26.841369: step 21130, loss 0.0506682, acc 0.96875\n",
      "2018-05-23T15:50:27.199408: step 21131, loss 0.090525, acc 0.96875\n",
      "2018-05-23T15:50:27.578395: step 21132, loss 0.0520715, acc 0.984375\n",
      "2018-05-23T15:50:27.936439: step 21133, loss 0.0508733, acc 0.96875\n",
      "2018-05-23T15:50:28.290490: step 21134, loss 0.00915542, acc 1\n",
      "2018-05-23T15:50:28.651525: step 21135, loss 0.207464, acc 0.953125\n",
      "2018-05-23T15:50:29.019540: step 21136, loss 0.0518214, acc 1\n",
      "2018-05-23T15:50:29.380574: step 21137, loss 0.0566496, acc 0.984375\n",
      "2018-05-23T15:50:29.744599: step 21138, loss 0.186208, acc 0.953125\n",
      "2018-05-23T15:50:30.117603: step 21139, loss 0.0315805, acc 1\n",
      "2018-05-23T15:50:30.493599: step 21140, loss 0.10743, acc 0.953125\n",
      "2018-05-23T15:50:30.861611: step 21141, loss 0.138581, acc 0.90625\n",
      "2018-05-23T15:50:31.226634: step 21142, loss 0.0450921, acc 0.96875\n",
      "2018-05-23T15:50:31.590662: step 21143, loss 0.0193017, acc 1\n",
      "2018-05-23T15:50:32.017519: step 21144, loss 0.158095, acc 0.9375\n",
      "2018-05-23T15:50:32.453353: step 21145, loss 0.0412897, acc 0.96875\n",
      "2018-05-23T15:50:32.880212: step 21146, loss 0.0930729, acc 0.953125\n",
      "2018-05-23T15:50:33.267177: step 21147, loss 0.0633156, acc 0.984375\n",
      "2018-05-23T15:50:33.648191: step 21148, loss 0.0860755, acc 0.9375\n",
      "2018-05-23T15:50:34.036119: step 21149, loss 0.227265, acc 0.9375\n",
      "2018-05-23T15:50:34.410118: step 21150, loss 0.0523076, acc 0.984375\n",
      "2018-05-23T15:50:34.814038: step 21151, loss 0.0545255, acc 0.984375\n",
      "2018-05-23T15:50:35.285776: step 21152, loss 0.0539604, acc 0.96875\n",
      "2018-05-23T15:50:35.661771: step 21153, loss 0.0292331, acc 1\n",
      "2018-05-23T15:50:36.040758: step 21154, loss 0.0858756, acc 0.953125\n",
      "2018-05-23T15:50:36.390822: step 21155, loss 0.0164936, acc 1\n",
      "2018-05-23T15:50:36.739886: step 21156, loss 0.0259894, acc 1\n",
      "2018-05-23T15:50:37.097662: step 21157, loss 0.0492586, acc 0.984375\n",
      "2018-05-23T15:50:37.440746: step 21158, loss 0.0274648, acc 0.984375\n",
      "2018-05-23T15:50:37.832697: step 21159, loss 0.055882, acc 0.984375\n",
      "2018-05-23T15:50:38.247592: step 21160, loss 0.12207, acc 0.96875\n",
      "2018-05-23T15:50:38.639538: step 21161, loss 0.0526308, acc 0.984375\n",
      "2018-05-23T15:50:39.010547: step 21162, loss 0.060658, acc 0.96875\n",
      "2018-05-23T15:50:39.367864: step 21163, loss 0.0983916, acc 0.96875\n",
      "2018-05-23T15:50:39.721917: step 21164, loss 0.0720707, acc 0.984375\n",
      "2018-05-23T15:50:40.072978: step 21165, loss 0.13542, acc 0.953125\n",
      "2018-05-23T15:50:40.435009: step 21166, loss 0.108401, acc 0.96875\n",
      "2018-05-23T15:50:40.777094: step 21167, loss 0.0766426, acc 0.984375\n",
      "2018-05-23T15:50:41.129152: step 21168, loss 0.0704029, acc 0.96875\n",
      "2018-05-23T15:50:41.482207: step 21169, loss 0.0623393, acc 0.953125\n",
      "2018-05-23T15:50:41.825321: step 21170, loss 0.106258, acc 0.96875\n",
      "2018-05-23T15:50:42.181335: step 21171, loss 0.0313582, acc 1\n",
      "2018-05-23T15:50:42.533396: step 21172, loss 0.0437634, acc 0.984375\n",
      "2018-05-23T15:50:42.881464: step 21173, loss 0.0190364, acc 1\n",
      "2018-05-23T15:50:43.226539: step 21174, loss 0.135365, acc 0.9375\n",
      "2018-05-23T15:50:43.583587: step 21175, loss 0.0842606, acc 0.96875\n",
      "2018-05-23T15:50:43.943623: step 21176, loss 0.15406, acc 0.9375\n",
      "2018-05-23T15:50:44.295681: step 21177, loss 0.0444434, acc 0.984375\n",
      "2018-05-23T15:50:44.697606: step 21178, loss 0.065359, acc 0.984375\n",
      "2018-05-23T15:50:45.081576: step 21179, loss 0.0545911, acc 0.96875\n",
      "2018-05-23T15:50:45.499459: step 21180, loss 0.187692, acc 0.953125\n",
      "2018-05-23T15:50:45.886425: step 21181, loss 0.0542503, acc 0.984375\n",
      "2018-05-23T15:50:46.248457: step 21182, loss 0.128512, acc 0.953125\n",
      "2018-05-23T15:50:46.607497: step 21183, loss 0.0627988, acc 1\n",
      "2018-05-23T15:50:46.952572: step 21184, loss 0.102558, acc 0.984375\n",
      "2018-05-23T15:50:47.296400: step 21185, loss 0.095432, acc 0.9375\n",
      "2018-05-23T15:50:47.645469: step 21186, loss 0.0877957, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:50:47.998525: step 21187, loss 0.0706198, acc 0.96875\n",
      "2018-05-23T15:50:48.353605: step 21188, loss 0.0721022, acc 0.953125\n",
      "2018-05-23T15:50:48.704635: step 21189, loss 0.0418684, acc 0.984375\n",
      "2018-05-23T15:50:49.166399: step 21190, loss 0.122082, acc 0.953125\n",
      "2018-05-23T15:50:49.615198: step 21191, loss 0.0472178, acc 0.984375\n",
      "2018-05-23T15:50:49.973240: step 21192, loss 0.102992, acc 0.953125\n",
      "2018-05-23T15:50:50.327295: step 21193, loss 0.04026, acc 0.96875\n",
      "2018-05-23T15:50:50.682345: step 21194, loss 0.0755119, acc 0.96875\n",
      "2018-05-23T15:50:51.050357: step 21195, loss 0.20227, acc 0.953125\n",
      "2018-05-23T15:50:51.417377: step 21196, loss 0.026283, acc 1\n",
      "2018-05-23T15:50:51.827280: step 21197, loss 0.108129, acc 0.96875\n",
      "2018-05-23T15:50:52.214246: step 21198, loss 0.113517, acc 0.9375\n",
      "2018-05-23T15:50:52.618164: step 21199, loss 0.00709372, acc 1\n",
      "2018-05-23T15:50:53.004133: step 21200, loss 0.0768181, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:50:58.147375: step 21200, loss 2.30343, acc 0.711387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21200\n",
      "\n",
      "2018-05-23T15:50:59.604474: step 21201, loss 0.10439, acc 0.953125\n",
      "2018-05-23T15:51:00.104139: step 21202, loss 0.134632, acc 0.9375\n",
      "2018-05-23T15:51:00.705529: step 21203, loss 0.111807, acc 0.953125\n",
      "2018-05-23T15:51:01.224140: step 21204, loss 0.0557316, acc 0.984375\n",
      "2018-05-23T15:51:01.649038: step 21205, loss 0.0196317, acc 0.984375\n",
      "2018-05-23T15:51:02.069882: step 21206, loss 0.062622, acc 0.96875\n",
      "2018-05-23T15:51:02.482774: step 21207, loss 0.0400404, acc 1\n",
      "2018-05-23T15:51:02.911627: step 21208, loss 0.0957166, acc 0.953125\n",
      "2018-05-23T15:51:03.314549: step 21209, loss 0.0789284, acc 0.953125\n",
      "2018-05-23T15:51:03.709494: step 21210, loss 0.0631815, acc 0.96875\n",
      "2018-05-23T15:51:04.146327: step 21211, loss 0.0463941, acc 0.96875\n",
      "2018-05-23T15:51:04.537280: step 21212, loss 0.107532, acc 0.953125\n",
      "2018-05-23T15:51:04.952168: step 21213, loss 0.0997072, acc 0.953125\n",
      "2018-05-23T15:51:05.338136: step 21214, loss 0.0552529, acc 0.984375\n",
      "2018-05-23T15:51:05.719116: step 21215, loss 0.0433079, acc 1\n",
      "2018-05-23T15:51:06.117051: step 21216, loss 0.0559831, acc 0.96875\n",
      "2018-05-23T15:51:06.511995: step 21217, loss 0.246911, acc 0.953125\n",
      "2018-05-23T15:51:06.894971: step 21218, loss 0.0501017, acc 0.984375\n",
      "2018-05-23T15:51:07.280941: step 21219, loss 0.103066, acc 0.953125\n",
      "2018-05-23T15:51:07.660924: step 21220, loss 0.0451098, acc 0.984375\n",
      "2018-05-23T15:51:08.017966: step 21221, loss 0.0418183, acc 0.984375\n",
      "2018-05-23T15:51:08.372019: step 21222, loss 0.0616733, acc 0.96875\n",
      "2018-05-23T15:51:08.760978: step 21223, loss 0.0758905, acc 0.984375\n",
      "2018-05-23T15:51:09.124007: step 21224, loss 0.0766111, acc 0.984375\n",
      "2018-05-23T15:51:09.502526: step 21225, loss 0.0725265, acc 0.96875\n",
      "2018-05-23T15:51:09.893479: step 21226, loss 0.0319238, acc 0.984375\n",
      "2018-05-23T15:51:10.282439: step 21227, loss 0.0415029, acc 0.984375\n",
      "2018-05-23T15:51:10.685359: step 21228, loss 0.151602, acc 0.9375\n",
      "2018-05-23T15:51:11.075318: step 21229, loss 0.119158, acc 0.921875\n",
      "2018-05-23T15:51:11.435354: step 21230, loss 0.0187142, acc 1\n",
      "2018-05-23T15:51:11.848250: step 21231, loss 0.0657193, acc 0.96875\n",
      "2018-05-23T15:51:12.239202: step 21232, loss 0.0972444, acc 0.953125\n",
      "2018-05-23T15:51:12.642125: step 21233, loss 0.0601686, acc 0.96875\n",
      "2018-05-23T15:51:13.030086: step 21234, loss 0.118409, acc 0.96875\n",
      "2018-05-23T15:51:13.492848: step 21235, loss 0.167061, acc 0.9375\n",
      "2018-05-23T15:51:13.894773: step 21236, loss 0.049422, acc 0.984375\n",
      "2018-05-23T15:51:14.322630: step 21237, loss 0.0859106, acc 0.953125\n",
      "2018-05-23T15:51:14.745500: step 21238, loss 0.211734, acc 0.9375\n",
      "2018-05-23T15:51:15.176346: step 21239, loss 0.0226073, acc 1\n",
      "2018-05-23T15:51:15.573319: step 21240, loss 0.0665289, acc 0.96875\n",
      "2018-05-23T15:51:16.006040: step 21241, loss 0.0399227, acc 1\n",
      "2018-05-23T15:51:16.442873: step 21242, loss 0.0860052, acc 0.953125\n",
      "2018-05-23T15:51:16.877707: step 21243, loss 0.265454, acc 0.921875\n",
      "2018-05-23T15:51:17.295590: step 21244, loss 0.0737848, acc 0.96875\n",
      "2018-05-23T15:51:17.716362: step 21245, loss 0.0322774, acc 0.984375\n",
      "2018-05-23T15:51:18.136239: step 21246, loss 0.0911309, acc 0.96875\n",
      "2018-05-23T15:51:18.539162: step 21247, loss 0.0660242, acc 0.984375\n",
      "2018-05-23T15:51:18.958043: step 21248, loss 0.107806, acc 0.96875\n",
      "2018-05-23T15:51:19.447730: step 21249, loss 0.156925, acc 0.96875\n",
      "2018-05-23T15:51:19.882568: step 21250, loss 0.0983265, acc 0.96875\n",
      "2018-05-23T15:51:20.299454: step 21251, loss 0.224101, acc 0.953125\n",
      "2018-05-23T15:51:20.690409: step 21252, loss 0.143535, acc 0.890625\n",
      "2018-05-23T15:51:21.096320: step 21253, loss 0.0667273, acc 0.9375\n",
      "2018-05-23T15:51:21.492261: step 21254, loss 0.105823, acc 0.984375\n",
      "2018-05-23T15:51:21.896181: step 21255, loss 0.117946, acc 0.921875\n",
      "2018-05-23T15:51:22.303789: step 21256, loss 0.0757116, acc 0.96875\n",
      "2018-05-23T15:51:22.695266: step 21257, loss 0.264085, acc 0.953125\n",
      "2018-05-23T15:51:23.106037: step 21258, loss 0.051609, acc 0.96875\n",
      "2018-05-23T15:51:23.651575: step 21259, loss 0.101924, acc 0.953125\n",
      "2018-05-23T15:51:24.189171: step 21260, loss 0.0413955, acc 1\n",
      "2018-05-23T15:51:24.624971: step 21261, loss 0.048756, acc 0.984375\n",
      "2018-05-23T15:51:25.057813: step 21262, loss 0.0377625, acc 0.96875\n",
      "2018-05-23T15:51:25.461736: step 21263, loss 0.121667, acc 0.953125\n",
      "2018-05-23T15:51:25.854227: step 21264, loss 0.0294383, acc 1\n",
      "2018-05-23T15:51:26.254157: step 21265, loss 0.0882087, acc 0.96875\n",
      "2018-05-23T15:51:26.621173: step 21266, loss 0.0920598, acc 0.953125\n",
      "2018-05-23T15:51:26.999164: step 21267, loss 0.0527065, acc 0.96875\n",
      "2018-05-23T15:51:27.384134: step 21268, loss 0.170666, acc 0.9375\n",
      "2018-05-23T15:51:27.746165: step 21269, loss 0.148648, acc 0.96875\n",
      "2018-05-23T15:51:28.099220: step 21270, loss 0.102524, acc 0.953125\n",
      "2018-05-23T15:51:28.465242: step 21271, loss 0.0903367, acc 0.96875\n",
      "2018-05-23T15:51:28.846222: step 21272, loss 0.103211, acc 0.96875\n",
      "2018-05-23T15:51:29.208254: step 21273, loss 0.125856, acc 0.953125\n",
      "2018-05-23T15:51:29.565299: step 21274, loss 0.209879, acc 0.984375\n",
      "2018-05-23T15:51:29.927362: step 21275, loss 0.121637, acc 0.96875\n",
      "2018-05-23T15:51:30.276397: step 21276, loss 0.0700247, acc 0.96875\n",
      "2018-05-23T15:51:30.625462: step 21277, loss 0.114687, acc 0.953125\n",
      "2018-05-23T15:51:30.973529: step 21278, loss 0.0305158, acc 0.984375\n",
      "2018-05-23T15:51:31.346533: step 21279, loss 0.0928566, acc 0.953125\n",
      "2018-05-23T15:51:31.777381: step 21280, loss 0.04302, acc 0.984375\n",
      "2018-05-23T15:51:32.130434: step 21281, loss 0.104673, acc 0.953125\n",
      "2018-05-23T15:51:32.495459: step 21282, loss 0.0654443, acc 0.984375\n",
      "2018-05-23T15:51:32.845524: step 21283, loss 0.0588115, acc 0.96875\n",
      "2018-05-23T15:51:33.193593: step 21284, loss 0.0815918, acc 0.953125\n",
      "2018-05-23T15:51:33.564600: step 21285, loss 0.136131, acc 0.953125\n",
      "2018-05-23T15:51:33.918651: step 21286, loss 0.133515, acc 0.9375\n",
      "2018-05-23T15:51:34.302626: step 21287, loss 0.0908929, acc 0.9375\n",
      "2018-05-23T15:51:34.727489: step 21288, loss 0.0721474, acc 0.96875\n",
      "2018-05-23T15:51:35.128414: step 21289, loss 0.148406, acc 0.9375\n",
      "2018-05-23T15:51:35.600154: step 21290, loss 0.103471, acc 0.96875\n",
      "2018-05-23T15:51:36.017038: step 21291, loss 0.0662425, acc 0.953125\n",
      "2018-05-23T15:51:36.388045: step 21292, loss 0.0250059, acc 1\n",
      "2018-05-23T15:51:36.750077: step 21293, loss 0.0616213, acc 0.96875\n",
      "2018-05-23T15:51:37.127070: step 21294, loss 0.0225942, acc 1\n",
      "2018-05-23T15:51:37.499073: step 21295, loss 0.0505457, acc 0.96875\n",
      "2018-05-23T15:51:37.912965: step 21296, loss 0.0804005, acc 0.96875\n",
      "2018-05-23T15:51:38.285969: step 21297, loss 0.0553069, acc 0.96875\n",
      "2018-05-23T15:51:38.744741: step 21298, loss 0.23903, acc 0.90625\n",
      "2018-05-23T15:51:39.175591: step 21299, loss 0.040601, acc 0.984375\n",
      "2018-05-23T15:51:39.629373: step 21300, loss 0.10944, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:51:44.567164: step 21300, loss 2.24042, acc 0.708958\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21300\n",
      "\n",
      "2018-05-23T15:51:46.005316: step 21301, loss 0.193604, acc 0.921875\n",
      "2018-05-23T15:51:46.394278: step 21302, loss 0.0514415, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:51:46.799195: step 21303, loss 0.122671, acc 0.921875\n",
      "2018-05-23T15:51:47.211093: step 21304, loss 0.15857, acc 0.921875\n",
      "2018-05-23T15:51:47.601050: step 21305, loss 0.0521141, acc 0.984375\n",
      "2018-05-23T15:51:47.988014: step 21306, loss 0.107143, acc 0.9375\n",
      "2018-05-23T15:51:48.368996: step 21307, loss 0.118048, acc 0.953125\n",
      "2018-05-23T15:51:48.756955: step 21308, loss 0.0357705, acc 0.984375\n",
      "2018-05-23T15:51:49.142923: step 21309, loss 0.0738889, acc 0.984375\n",
      "2018-05-23T15:51:49.494984: step 21310, loss 0.106051, acc 0.96875\n",
      "2018-05-23T15:51:49.853026: step 21311, loss 0.101549, acc 0.953125\n",
      "2018-05-23T15:51:50.229018: step 21312, loss 0.0452921, acc 0.984375\n",
      "2018-05-23T15:51:50.585067: step 21313, loss 0.0629564, acc 0.96875\n",
      "2018-05-23T15:51:50.934133: step 21314, loss 0.212724, acc 0.875\n",
      "2018-05-23T15:51:51.295166: step 21315, loss 0.0684226, acc 0.96875\n",
      "2018-05-23T15:51:51.656201: step 21316, loss 0.0557086, acc 0.953125\n",
      "2018-05-23T15:51:52.006263: step 21317, loss 0.0448656, acc 0.96875\n",
      "2018-05-23T15:51:52.370289: step 21318, loss 0.170035, acc 0.953125\n",
      "2018-05-23T15:51:52.722349: step 21319, loss 0.04072, acc 0.984375\n",
      "2018-05-23T15:51:53.075404: step 21320, loss 0.118543, acc 0.9375\n",
      "2018-05-23T15:51:53.433447: step 21321, loss 0.0431863, acc 0.984375\n",
      "2018-05-23T15:51:53.856314: step 21322, loss 0.0556736, acc 0.984375\n",
      "2018-05-23T15:51:54.211366: step 21323, loss 0.119289, acc 0.9375\n",
      "2018-05-23T15:51:54.571403: step 21324, loss 0.136954, acc 0.890625\n",
      "2018-05-23T15:51:54.939418: step 21325, loss 0.054142, acc 0.984375\n",
      "2018-05-23T15:51:55.292513: step 21326, loss 0.0622787, acc 0.953125\n",
      "2018-05-23T15:51:55.656498: step 21327, loss 0.0755457, acc 0.984375\n",
      "2018-05-23T15:51:56.016879: step 21328, loss 0.104651, acc 0.9375\n",
      "2018-05-23T15:51:56.359962: step 21329, loss 0.0584217, acc 0.984375\n",
      "2018-05-23T15:51:56.729972: step 21330, loss 0.0851306, acc 0.96875\n",
      "2018-05-23T15:51:57.087015: step 21331, loss 0.256224, acc 0.90625\n",
      "2018-05-23T15:51:57.439212: step 21332, loss 0.2216, acc 0.9375\n",
      "2018-05-23T15:51:57.800246: step 21333, loss 0.0450546, acc 1\n",
      "2018-05-23T15:51:58.153299: step 21334, loss 0.0853787, acc 0.96875\n",
      "2018-05-23T15:51:58.508352: step 21335, loss 0.292908, acc 0.921875\n",
      "2018-05-23T15:51:58.859413: step 21336, loss 0.11167, acc 0.9375\n",
      "2018-05-23T15:51:59.216457: step 21337, loss 0.066647, acc 0.96875\n",
      "2018-05-23T15:51:59.561535: step 21338, loss 0.0663966, acc 0.984375\n",
      "2018-05-23T15:51:59.908604: step 21339, loss 0.186603, acc 0.953125\n",
      "2018-05-23T15:52:00.273628: step 21340, loss 0.0631537, acc 0.984375\n",
      "2018-05-23T15:52:00.631673: step 21341, loss 0.0716358, acc 0.96875\n",
      "2018-05-23T15:52:00.983727: step 21342, loss 0.0804272, acc 0.9375\n",
      "2018-05-23T15:52:01.344762: step 21343, loss 0.121144, acc 0.921875\n",
      "2018-05-23T15:52:01.773615: step 21344, loss 0.0867058, acc 0.984375\n",
      "2018-05-23T15:52:02.174544: step 21345, loss 0.116974, acc 0.953125\n",
      "2018-05-23T15:52:02.614368: step 21346, loss 0.0304391, acc 0.984375\n",
      "2018-05-23T15:52:02.998338: step 21347, loss 0.115333, acc 0.9375\n",
      "2018-05-23T15:52:03.362364: step 21348, loss 0.0626714, acc 0.984375\n",
      "2018-05-23T15:52:03.717415: step 21349, loss 0.0374346, acc 0.984375\n",
      "2018-05-23T15:52:04.078451: step 21350, loss 0.257075, acc 0.9375\n",
      "2018-05-23T15:52:04.428513: step 21351, loss 0.0926192, acc 0.921875\n",
      "2018-05-23T15:52:04.769403: step 21352, loss 0.0652888, acc 0.96875\n",
      "2018-05-23T15:52:05.137420: step 21353, loss 0.139144, acc 0.96875\n",
      "2018-05-23T15:52:05.481500: step 21354, loss 0.270212, acc 0.90625\n",
      "2018-05-23T15:52:05.890405: step 21355, loss 0.100384, acc 0.953125\n",
      "2018-05-23T15:52:06.249831: step 21356, loss 0.080734, acc 0.9375\n",
      "2018-05-23T15:52:06.605877: step 21357, loss 0.111245, acc 0.9375\n",
      "2018-05-23T15:52:06.959929: step 21358, loss 0.0494083, acc 0.984375\n",
      "2018-05-23T15:52:07.308000: step 21359, loss 0.0436945, acc 0.984375\n",
      "2018-05-23T15:52:07.661054: step 21360, loss 0.1298, acc 0.9375\n",
      "2018-05-23T15:52:08.010119: step 21361, loss 0.0964339, acc 0.984375\n",
      "2018-05-23T15:52:08.358189: step 21362, loss 0.0869989, acc 0.96875\n",
      "2018-05-23T15:52:08.773081: step 21363, loss 0.0676863, acc 0.96875\n",
      "2018-05-23T15:52:09.191959: step 21364, loss 0.0324645, acc 1\n",
      "2018-05-23T15:52:09.558975: step 21365, loss 0.0647043, acc 0.984375\n",
      "2018-05-23T15:52:09.912030: step 21366, loss 0.0905588, acc 0.984375\n",
      "2018-05-23T15:52:10.269076: step 21367, loss 0.0760352, acc 0.96875\n",
      "2018-05-23T15:52:10.622164: step 21368, loss 0.0297278, acc 1\n",
      "2018-05-23T15:52:10.993141: step 21369, loss 0.0602811, acc 0.953125\n",
      "2018-05-23T15:52:11.351183: step 21370, loss 0.0684562, acc 0.96875\n",
      "2018-05-23T15:52:11.701890: step 21371, loss 0.0686244, acc 0.984375\n",
      "2018-05-23T15:52:12.064921: step 21372, loss 0.10478, acc 0.953125\n",
      "2018-05-23T15:52:12.421967: step 21373, loss 0.0777382, acc 0.953125\n",
      "2018-05-23T15:52:12.765046: step 21374, loss 0.0498244, acc 1\n",
      "2018-05-23T15:52:13.119101: step 21375, loss 0.106226, acc 0.953125\n",
      "2018-05-23T15:52:13.474152: step 21376, loss 0.0681968, acc 0.96875\n",
      "2018-05-23T15:52:13.825210: step 21377, loss 0.0814376, acc 0.96875\n",
      "2018-05-23T15:52:14.182255: step 21378, loss 0.099436, acc 0.9375\n",
      "2018-05-23T15:52:14.547278: step 21379, loss 0.0806142, acc 0.953125\n",
      "2018-05-23T15:52:14.894349: step 21380, loss 0.103189, acc 0.96875\n",
      "2018-05-23T15:52:15.253391: step 21381, loss 0.0491207, acc 0.984375\n",
      "2018-05-23T15:52:15.609437: step 21382, loss 0.0449285, acc 0.984375\n",
      "2018-05-23T15:52:15.957505: step 21383, loss 0.107518, acc 0.953125\n",
      "2018-05-23T15:52:16.314550: step 21384, loss 0.0469252, acc 0.984375\n",
      "2018-05-23T15:52:16.676583: step 21385, loss 0.04342, acc 0.984375\n",
      "2018-05-23T15:52:17.032632: step 21386, loss 0.090017, acc 0.984375\n",
      "2018-05-23T15:52:17.388677: step 21387, loss 0.0948279, acc 0.984375\n",
      "2018-05-23T15:52:17.756378: step 21388, loss 0.0988427, acc 0.921875\n",
      "2018-05-23T15:52:18.118412: step 21389, loss 0.0700276, acc 0.984375\n",
      "2018-05-23T15:52:18.488422: step 21390, loss 0.0580203, acc 0.984375\n",
      "2018-05-23T15:52:18.844468: step 21391, loss 0.0216301, acc 1\n",
      "2018-05-23T15:52:19.239413: step 21392, loss 0.078364, acc 0.96875\n",
      "2018-05-23T15:52:19.598452: step 21393, loss 0.119402, acc 0.96875\n",
      "2018-05-23T15:52:19.944524: step 21394, loss 0.0785906, acc 0.984375\n",
      "2018-05-23T15:52:20.301569: step 21395, loss 0.124199, acc 0.9375\n",
      "2018-05-23T15:52:20.660610: step 21396, loss 0.0550315, acc 0.984375\n",
      "2018-05-23T15:52:21.017654: step 21397, loss 0.116445, acc 0.9375\n",
      "2018-05-23T15:52:21.372703: step 21398, loss 0.121652, acc 0.984375\n",
      "2018-05-23T15:52:21.726757: step 21399, loss 0.0778354, acc 0.9375\n",
      "2018-05-23T15:52:22.080809: step 21400, loss 0.0762019, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:52:26.770266: step 21400, loss 2.26289, acc 0.711816\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21400\n",
      "\n",
      "2018-05-23T15:52:28.088208: step 21401, loss 0.0422761, acc 0.984375\n",
      "2018-05-23T15:52:28.460212: step 21402, loss 0.0427743, acc 0.984375\n",
      "2018-05-23T15:52:28.831223: step 21403, loss 0.0722955, acc 0.96875\n",
      "2018-05-23T15:52:29.176296: step 21404, loss 0.106187, acc 0.921875\n",
      "2018-05-23T15:52:29.532344: step 21405, loss 0.0350648, acc 1\n",
      "2018-05-23T15:52:29.890386: step 21406, loss 0.0924637, acc 0.9375\n",
      "2018-05-23T15:52:30.240449: step 21407, loss 0.0263106, acc 0.984375\n",
      "2018-05-23T15:52:30.591320: step 21408, loss 0.13805, acc 0.9375\n",
      "2018-05-23T15:52:30.939391: step 21409, loss 0.0906902, acc 0.9375\n",
      "2018-05-23T15:52:31.283468: step 21410, loss 0.0935494, acc 0.984375\n",
      "2018-05-23T15:52:31.635529: step 21411, loss 0.0486331, acc 0.984375\n",
      "2018-05-23T15:52:31.990576: step 21412, loss 0.101447, acc 0.96875\n",
      "2018-05-23T15:52:32.338648: step 21413, loss 0.0445262, acc 0.984375\n",
      "2018-05-23T15:52:32.686714: step 21414, loss 0.0559918, acc 0.96875\n",
      "2018-05-23T15:52:33.035089: step 21415, loss 0.10136, acc 0.953125\n",
      "2018-05-23T15:52:33.380783: step 21416, loss 0.0522444, acc 0.96875\n",
      "2018-05-23T15:52:33.734809: step 21417, loss 0.0340478, acc 1\n",
      "2018-05-23T15:52:34.092854: step 21418, loss 0.119575, acc 0.96875\n",
      "2018-05-23T15:52:34.437338: step 21419, loss 0.0636769, acc 0.96875\n",
      "2018-05-23T15:52:34.776433: step 21420, loss 0.232546, acc 0.953125\n",
      "2018-05-23T15:52:35.142452: step 21421, loss 0.198038, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:52:35.490523: step 21422, loss 0.233937, acc 0.9375\n",
      "2018-05-23T15:52:35.847568: step 21423, loss 0.220587, acc 0.96875\n",
      "2018-05-23T15:52:36.207604: step 21424, loss 0.0471034, acc 0.984375\n",
      "2018-05-23T15:52:36.556671: step 21425, loss 0.121594, acc 0.9375\n",
      "2018-05-23T15:52:36.913715: step 21426, loss 0.091778, acc 0.953125\n",
      "2018-05-23T15:52:37.268763: step 21427, loss 0.0856089, acc 0.96875\n",
      "2018-05-23T15:52:37.623813: step 21428, loss 0.0247047, acc 1\n",
      "2018-05-23T15:52:37.972882: step 21429, loss 0.0456582, acc 0.984375\n",
      "2018-05-23T15:52:38.319952: step 21430, loss 0.212848, acc 0.90625\n",
      "2018-05-23T15:52:38.675001: step 21431, loss 0.0439225, acc 1\n",
      "2018-05-23T15:52:39.026062: step 21432, loss 0.0418482, acc 0.984375\n",
      "2018-05-23T15:52:39.384107: step 21433, loss 0.114608, acc 0.953125\n",
      "2018-05-23T15:52:39.748130: step 21434, loss 0.191033, acc 0.953125\n",
      "2018-05-23T15:52:40.101187: step 21435, loss 0.0946207, acc 0.96875\n",
      "2018-05-23T15:52:40.459228: step 21436, loss 0.205025, acc 0.9375\n",
      "2018-05-23T15:52:40.811287: step 21437, loss 0.0726471, acc 0.953125\n",
      "2018-05-23T15:52:41.158360: step 21438, loss 0.0396176, acc 1\n",
      "2018-05-23T15:52:41.508421: step 21439, loss 0.0738028, acc 0.984375\n",
      "2018-05-23T15:52:41.852537: step 21440, loss 0.024499, acc 1\n",
      "2018-05-23T15:52:42.203564: step 21441, loss 0.113743, acc 0.9375\n",
      "2018-05-23T15:52:42.557614: step 21442, loss 0.0285701, acc 0.984375\n",
      "2018-05-23T15:52:42.907681: step 21443, loss 0.0710486, acc 0.953125\n",
      "2018-05-23T15:52:43.258741: step 21444, loss 0.248962, acc 0.90625\n",
      "2018-05-23T15:52:43.616784: step 21445, loss 0.123191, acc 0.9375\n",
      "2018-05-23T15:52:43.964850: step 21446, loss 0.11437, acc 0.96875\n",
      "2018-05-23T15:52:44.306970: step 21447, loss 0.102236, acc 0.953125\n",
      "2018-05-23T15:52:44.663979: step 21448, loss 0.0621089, acc 0.96875\n",
      "2018-05-23T15:52:45.008061: step 21449, loss 0.103424, acc 0.96875\n",
      "2018-05-23T15:52:45.356168: step 21450, loss 0.0412634, acc 0.984375\n",
      "2018-05-23T15:52:45.709224: step 21451, loss 0.0816624, acc 0.96875\n",
      "2018-05-23T15:52:46.060287: step 21452, loss 0.00880665, acc 1\n",
      "2018-05-23T15:52:46.411346: step 21453, loss 0.0868818, acc 0.9375\n",
      "2018-05-23T15:52:46.754883: step 21454, loss 0.283731, acc 0.90625\n",
      "2018-05-23T15:52:47.095970: step 21455, loss 0.101909, acc 0.96875\n",
      "2018-05-23T15:52:47.446034: step 21456, loss 0.2119, acc 0.90625\n",
      "2018-05-23T15:52:47.814048: step 21457, loss 0.138907, acc 0.90625\n",
      "2018-05-23T15:52:48.168102: step 21458, loss 0.0595224, acc 0.96875\n",
      "2018-05-23T15:52:48.527165: step 21459, loss 0.0813333, acc 0.953125\n",
      "2018-05-23T15:52:48.899169: step 21460, loss 0.0591246, acc 0.96875\n",
      "2018-05-23T15:52:49.322037: step 21461, loss 0.0681129, acc 0.96875\n",
      "2018-05-23T15:52:49.692080: step 21462, loss 0.108723, acc 0.9375\n",
      "2018-05-23T15:52:50.082463: step 21463, loss 0.0379902, acc 1\n",
      "2018-05-23T15:52:50.441501: step 21464, loss 0.10037, acc 0.953125\n",
      "2018-05-23T15:52:50.798545: step 21465, loss 0.160297, acc 0.96875\n",
      "2018-05-23T15:52:51.171549: step 21466, loss 0.110846, acc 0.984375\n",
      "2018-05-23T15:52:51.526600: step 21467, loss 0.049427, acc 0.984375\n",
      "2018-05-23T15:52:51.871677: step 21468, loss 0.0670672, acc 0.953125\n",
      "2018-05-23T15:52:52.242683: step 21469, loss 0.0224242, acc 0.984375\n",
      "2018-05-23T15:52:52.624659: step 21470, loss 0.0792898, acc 0.953125\n",
      "2018-05-23T15:52:52.975720: step 21471, loss 0.200532, acc 0.9375\n",
      "2018-05-23T15:52:53.333764: step 21472, loss 0.0196725, acc 1\n",
      "2018-05-23T15:52:53.712749: step 21473, loss 0.0261195, acc 0.984375\n",
      "2018-05-23T15:52:54.066803: step 21474, loss 0.330806, acc 0.921875\n",
      "2018-05-23T15:52:54.434852: step 21475, loss 0.0982458, acc 0.953125\n",
      "2018-05-23T15:52:54.785878: step 21476, loss 0.0975128, acc 0.953125\n",
      "2018-05-23T15:52:55.139932: step 21477, loss 0.0227959, acc 1\n",
      "2018-05-23T15:52:55.499970: step 21478, loss 0.0852907, acc 0.953125\n",
      "2018-05-23T15:52:55.869978: step 21479, loss 0.153058, acc 0.96875\n",
      "2018-05-23T15:52:56.260935: step 21480, loss 0.0587226, acc 0.96875\n",
      "2018-05-23T15:52:56.736660: step 21481, loss 0.0923312, acc 0.984375\n",
      "2018-05-23T15:52:57.150552: step 21482, loss 0.0450294, acc 0.984375\n",
      "2018-05-23T15:52:57.539515: step 21483, loss 0.0618639, acc 0.96875\n",
      "2018-05-23T15:52:57.933457: step 21484, loss 0.0224668, acc 1\n",
      "2018-05-23T15:52:58.331427: step 21485, loss 0.0235275, acc 1\n",
      "2018-05-23T15:52:58.729329: step 21486, loss 0.0568439, acc 0.96875\n",
      "2018-05-23T15:52:59.127264: step 21487, loss 0.103562, acc 0.9375\n",
      "2018-05-23T15:52:59.513231: step 21488, loss 0.133377, acc 0.921875\n",
      "2018-05-23T15:52:59.902191: step 21489, loss 0.119871, acc 0.90625\n",
      "2018-05-23T15:53:00.513557: step 21490, loss 0.0932692, acc 0.953125\n",
      "2018-05-23T15:53:01.040149: step 21491, loss 0.170067, acc 0.90625\n",
      "2018-05-23T15:53:01.468004: step 21492, loss 0.166123, acc 0.953125\n",
      "2018-05-23T15:53:01.915805: step 21493, loss 0.0796122, acc 0.96875\n",
      "2018-05-23T15:53:02.337678: step 21494, loss 0.132379, acc 0.96875\n",
      "2018-05-23T15:53:02.735611: step 21495, loss 0.0743012, acc 0.96875\n",
      "2018-05-23T15:53:03.127564: step 21496, loss 0.0224142, acc 0.984375\n",
      "2018-05-23T15:53:03.616259: step 21497, loss 0.0890306, acc 0.953125\n",
      "2018-05-23T15:53:04.053089: step 21498, loss 0.121976, acc 0.96875\n",
      "2018-05-23T15:53:04.622565: step 21499, loss 0.0634283, acc 0.96875\n",
      "2018-05-23T15:53:05.006577: step 21500, loss 0.0629046, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:53:10.767165: step 21500, loss 2.23408, acc 0.70953\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21500\n",
      "\n",
      "2018-05-23T15:53:12.310036: step 21501, loss 0.0796242, acc 0.96875\n",
      "2018-05-23T15:53:12.721937: step 21502, loss 0.0370189, acc 0.984375\n",
      "2018-05-23T15:53:13.120867: step 21503, loss 0.120522, acc 0.921875\n",
      "2018-05-23T15:53:13.481902: step 21504, loss 0.0478188, acc 0.96875\n",
      "2018-05-23T15:53:13.834956: step 21505, loss 0.156118, acc 0.9375\n",
      "2018-05-23T15:53:14.219927: step 21506, loss 0.161091, acc 0.953125\n",
      "2018-05-23T15:53:14.589936: step 21507, loss 0.0532939, acc 0.96875\n",
      "2018-05-23T15:53:14.943992: step 21508, loss 0.0589613, acc 0.984375\n",
      "2018-05-23T15:53:15.304028: step 21509, loss 0.0686736, acc 0.96875\n",
      "2018-05-23T15:53:15.691989: step 21510, loss 0.0703374, acc 0.96875\n",
      "2018-05-23T15:53:16.058010: step 21511, loss 0.0838069, acc 0.953125\n",
      "2018-05-23T15:53:16.450201: step 21512, loss 0.121432, acc 0.953125\n",
      "2018-05-23T15:53:16.809240: step 21513, loss 0.132133, acc 0.953125\n",
      "2018-05-23T15:53:17.162298: step 21514, loss 0.267823, acc 0.9375\n",
      "2018-05-23T15:53:17.531309: step 21515, loss 0.0782058, acc 0.953125\n",
      "2018-05-23T15:53:17.894339: step 21516, loss 0.0239993, acc 0.984375\n",
      "2018-05-23T15:53:18.261356: step 21517, loss 0.0460577, acc 0.96875\n",
      "2018-05-23T15:53:18.625381: step 21518, loss 0.0606395, acc 0.96875\n",
      "2018-05-23T15:53:19.005372: step 21519, loss 0.0574552, acc 0.984375\n",
      "2018-05-23T15:53:19.400169: step 21520, loss 0.0626837, acc 0.96875\n",
      "2018-05-23T15:53:19.762203: step 21521, loss 0.0824472, acc 0.953125\n",
      "2018-05-23T15:53:20.126227: step 21522, loss 0.0483705, acc 0.984375\n",
      "2018-05-23T15:53:20.484269: step 21523, loss 0.0726982, acc 0.953125\n",
      "2018-05-23T15:53:20.877217: step 21524, loss 0.0567839, acc 0.984375\n",
      "2018-05-23T15:53:21.229276: step 21525, loss 0.0661948, acc 0.96875\n",
      "2018-05-23T15:53:21.594302: step 21526, loss 0.071486, acc 0.96875\n",
      "2018-05-23T15:53:21.959325: step 21527, loss 0.140793, acc 0.9375\n",
      "2018-05-23T15:53:22.306394: step 21528, loss 0.0318153, acc 0.984375\n",
      "2018-05-23T15:53:22.655460: step 21529, loss 0.157509, acc 0.984375\n",
      "2018-05-23T15:53:23.004526: step 21530, loss 0.0709612, acc 0.953125\n",
      "2018-05-23T15:53:23.373571: step 21531, loss 0.169033, acc 0.953125\n",
      "2018-05-23T15:53:23.748536: step 21532, loss 0.077086, acc 0.96875\n",
      "2018-05-23T15:53:24.097603: step 21533, loss 0.207127, acc 0.90625\n",
      "2018-05-23T15:53:24.446669: step 21534, loss 0.0822812, acc 0.9375\n",
      "2018-05-23T15:53:24.799724: step 21535, loss 0.0541916, acc 0.953125\n",
      "2018-05-23T15:53:25.145802: step 21536, loss 0.174157, acc 0.953125\n",
      "2018-05-23T15:53:25.505835: step 21537, loss 0.066401, acc 0.96875\n",
      "2018-05-23T15:53:25.865873: step 21538, loss 0.213546, acc 0.9375\n",
      "2018-05-23T15:53:26.252870: step 21539, loss 0.0971559, acc 0.953125\n",
      "2018-05-23T15:53:26.640798: step 21540, loss 0.0662553, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:53:26.993856: step 21541, loss 0.0291996, acc 0.984375\n",
      "2018-05-23T15:53:27.353893: step 21542, loss 0.131458, acc 0.921875\n",
      "2018-05-23T15:53:27.742850: step 21543, loss 0.104547, acc 0.984375\n",
      "2018-05-23T15:53:28.119877: step 21544, loss 0.134623, acc 0.96875\n",
      "2018-05-23T15:53:28.475924: step 21545, loss 0.117771, acc 0.953125\n",
      "2018-05-23T15:53:28.869868: step 21546, loss 0.0424899, acc 0.984375\n",
      "2018-05-23T15:53:29.231901: step 21547, loss 0.044943, acc 0.984375\n",
      "2018-05-23T15:53:29.609891: step 21548, loss 0.103874, acc 0.953125\n",
      "2018-05-23T15:53:29.974914: step 21549, loss 0.0636703, acc 0.984375\n",
      "2018-05-23T15:53:30.353900: step 21550, loss 0.0567886, acc 0.984375\n",
      "2018-05-23T15:53:30.731889: step 21551, loss 0.179997, acc 0.9375\n",
      "2018-05-23T15:53:31.090929: step 21552, loss 0.089835, acc 0.96875\n",
      "2018-05-23T15:53:31.455951: step 21553, loss 0.114601, acc 0.953125\n",
      "2018-05-23T15:53:31.845909: step 21554, loss 0.1515, acc 0.9375\n",
      "2018-05-23T15:53:32.225891: step 21555, loss 0.0593821, acc 0.96875\n",
      "2018-05-23T15:53:32.590915: step 21556, loss 0.0614121, acc 0.96875\n",
      "2018-05-23T15:53:32.946960: step 21557, loss 0.110973, acc 0.96875\n",
      "2018-05-23T15:53:33.339910: step 21558, loss 0.108083, acc 0.921875\n",
      "2018-05-23T15:53:33.733858: step 21559, loss 0.0862825, acc 0.96875\n",
      "2018-05-23T15:53:34.129799: step 21560, loss 0.0710405, acc 0.984375\n",
      "2018-05-23T15:53:34.556689: step 21561, loss 0.216128, acc 0.96875\n",
      "2018-05-23T15:53:34.948607: step 21562, loss 0.152034, acc 0.96875\n",
      "2018-05-23T15:53:35.315624: step 21563, loss 0.0745767, acc 0.96875\n",
      "2018-05-23T15:53:35.691621: step 21564, loss 0.127772, acc 0.9375\n",
      "2018-05-23T15:53:36.066616: step 21565, loss 0.0601171, acc 0.984375\n",
      "2018-05-23T15:53:36.491481: step 21566, loss 0.163387, acc 0.921875\n",
      "2018-05-23T15:53:36.920331: step 21567, loss 0.122347, acc 0.953125\n",
      "2018-05-23T15:53:37.316272: step 21568, loss 0.0790331, acc 0.96875\n",
      "2018-05-23T15:53:37.696257: step 21569, loss 0.157683, acc 0.953125\n",
      "2018-05-23T15:53:38.089206: step 21570, loss 0.0636213, acc 0.953125\n",
      "2018-05-23T15:53:38.443258: step 21571, loss 0.0940262, acc 0.953125\n",
      "2018-05-23T15:53:38.896046: step 21572, loss 0.108746, acc 0.953125\n",
      "2018-05-23T15:53:39.291989: step 21573, loss 0.0589597, acc 0.96875\n",
      "2018-05-23T15:53:39.657012: step 21574, loss 0.070619, acc 0.984375\n",
      "2018-05-23T15:53:40.075890: step 21575, loss 0.0809757, acc 0.953125\n",
      "2018-05-23T15:53:40.484797: step 21576, loss 0.0434728, acc 1\n",
      "2018-05-23T15:53:40.849820: step 21577, loss 0.0573524, acc 0.984375\n",
      "2018-05-23T15:53:41.222823: step 21578, loss 0.0614566, acc 0.953125\n",
      "2018-05-23T15:53:41.588844: step 21579, loss 0.0835208, acc 0.984375\n",
      "2018-05-23T15:53:41.992804: step 21580, loss 0.0996919, acc 0.953125\n",
      "2018-05-23T15:53:42.369794: step 21581, loss 0.0536489, acc 0.953125\n",
      "2018-05-23T15:53:42.736815: step 21582, loss 0.0658333, acc 0.96875\n",
      "2018-05-23T15:53:43.146717: step 21583, loss 0.212109, acc 0.921875\n",
      "2018-05-23T15:53:43.529694: step 21584, loss 0.032039, acc 0.984375\n",
      "2018-05-23T15:53:43.895748: step 21585, loss 0.0510928, acc 0.984375\n",
      "2018-05-23T15:53:44.294651: step 21586, loss 0.10782, acc 0.953125\n",
      "2018-05-23T15:53:44.676624: step 21587, loss 0.0878667, acc 0.96875\n",
      "2018-05-23T15:53:45.065585: step 21588, loss 0.0772943, acc 0.96875\n",
      "2018-05-23T15:53:45.474489: step 21589, loss 0.109458, acc 0.96875\n",
      "2018-05-23T15:53:45.884425: step 21590, loss 0.122867, acc 0.953125\n",
      "2018-05-23T15:53:46.262382: step 21591, loss 0.0452856, acc 1\n",
      "2018-05-23T15:53:46.632395: step 21592, loss 0.0699829, acc 0.953125\n",
      "2018-05-23T15:53:47.005395: step 21593, loss 0.0809339, acc 0.96875\n",
      "2018-05-23T15:53:47.398343: step 21594, loss 0.246877, acc 0.9375\n",
      "2018-05-23T15:53:47.794283: step 21595, loss 0.0316276, acc 1\n",
      "2018-05-23T15:53:48.179254: step 21596, loss 0.16707, acc 0.96875\n",
      "2018-05-23T15:53:48.582311: step 21597, loss 0.0423181, acc 0.984375\n",
      "2018-05-23T15:53:48.942347: step 21598, loss 0.0452426, acc 0.96875\n",
      "2018-05-23T15:53:49.334299: step 21599, loss 0.146475, acc 0.96875\n",
      "2018-05-23T15:53:49.710293: step 21600, loss 0.0477769, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:53:55.012108: step 21600, loss 2.25047, acc 0.707815\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21600\n",
      "\n",
      "2018-05-23T15:53:56.400394: step 21601, loss 0.0375684, acc 0.984375\n",
      "2018-05-23T15:53:56.792345: step 21602, loss 0.0611453, acc 0.953125\n",
      "2018-05-23T15:53:57.217210: step 21603, loss 0.042151, acc 0.984375\n",
      "2018-05-23T15:53:57.613154: step 21604, loss 0.0601775, acc 0.96875\n",
      "2018-05-23T15:53:57.987151: step 21605, loss 0.0669162, acc 0.96875\n",
      "2018-05-23T15:53:58.442931: step 21606, loss 0.139349, acc 0.921875\n",
      "2018-05-23T15:53:59.007419: step 21607, loss 0.146027, acc 0.9375\n",
      "2018-05-23T15:53:59.576898: step 21608, loss 0.130464, acc 0.921875\n",
      "2018-05-23T15:54:00.044647: step 21609, loss 0.105912, acc 0.953125\n",
      "2018-05-23T15:54:00.473500: step 21610, loss 0.0534949, acc 0.96875\n",
      "2018-05-23T15:54:00.878417: step 21611, loss 0.0493658, acc 0.984375\n",
      "2018-05-23T15:54:01.360125: step 21612, loss 0.118753, acc 0.9375\n",
      "2018-05-23T15:54:01.994429: step 21613, loss 0.0410385, acc 0.984375\n",
      "2018-05-23T15:54:02.469159: step 21614, loss 0.133313, acc 0.96875\n",
      "2018-05-23T15:54:02.853132: step 21615, loss 0.169547, acc 0.921875\n",
      "2018-05-23T15:54:03.233115: step 21616, loss 0.226591, acc 0.90625\n",
      "2018-05-23T15:54:03.604125: step 21617, loss 0.074575, acc 0.953125\n",
      "2018-05-23T15:54:03.976127: step 21618, loss 0.0974102, acc 0.96875\n",
      "2018-05-23T15:54:04.348132: step 21619, loss 0.0675682, acc 0.96875\n",
      "2018-05-23T15:54:04.708171: step 21620, loss 0.0984275, acc 0.96875\n",
      "2018-05-23T15:54:05.125055: step 21621, loss 0.160685, acc 0.96875\n",
      "2018-05-23T15:54:05.633693: step 21622, loss 0.121095, acc 0.9375\n",
      "2018-05-23T15:54:06.040604: step 21623, loss 0.148512, acc 0.96875\n",
      "2018-05-23T15:54:06.417595: step 21624, loss 0.0516271, acc 0.96875\n",
      "2018-05-23T15:54:06.783618: step 21625, loss 0.0284204, acc 1\n",
      "2018-05-23T15:54:07.175569: step 21626, loss 0.253293, acc 0.921875\n",
      "2018-05-23T15:54:07.551915: step 21627, loss 0.104274, acc 0.953125\n",
      "2018-05-23T15:54:07.961819: step 21628, loss 0.0929488, acc 0.96875\n",
      "2018-05-23T15:54:08.351775: step 21629, loss 0.114369, acc 0.953125\n",
      "2018-05-23T15:54:08.738740: step 21630, loss 0.179177, acc 0.953125\n",
      "2018-05-23T15:54:09.159612: step 21631, loss 0.0172873, acc 1\n",
      "2018-05-23T15:54:09.522643: step 21632, loss 0.199768, acc 0.9375\n",
      "2018-05-23T15:54:09.931547: step 21633, loss 0.0495561, acc 0.984375\n",
      "2018-05-23T15:54:10.415254: step 21634, loss 0.128248, acc 0.921875\n",
      "2018-05-23T15:54:10.883001: step 21635, loss 0.0620528, acc 0.984375\n",
      "2018-05-23T15:54:11.383662: step 21636, loss 0.0724328, acc 0.953125\n",
      "2018-05-23T15:54:11.879337: step 21637, loss 0.0831159, acc 0.9375\n",
      "2018-05-23T15:54:12.363045: step 21638, loss 0.136249, acc 0.96875\n",
      "2018-05-23T15:54:12.844754: step 21639, loss 0.0800481, acc 0.96875\n",
      "2018-05-23T15:54:13.324472: step 21640, loss 0.0869265, acc 0.921875\n",
      "2018-05-23T15:54:13.761302: step 21641, loss 0.160804, acc 0.953125\n",
      "2018-05-23T15:54:14.209103: step 21642, loss 0.0624496, acc 0.984375\n",
      "2018-05-23T15:54:14.616017: step 21643, loss 0.112868, acc 0.953125\n",
      "2018-05-23T15:54:15.021932: step 21644, loss 0.1504, acc 0.90625\n",
      "2018-05-23T15:54:15.434825: step 21645, loss 0.0821768, acc 0.96875\n",
      "2018-05-23T15:54:15.821789: step 21646, loss 0.0438635, acc 0.984375\n",
      "2018-05-23T15:54:16.259618: step 21647, loss 0.0972861, acc 0.984375\n",
      "2018-05-23T15:54:16.662540: step 21648, loss 0.0512906, acc 0.984375\n",
      "2018-05-23T15:54:17.044519: step 21649, loss 0.230691, acc 0.9375\n",
      "2018-05-23T15:54:17.585073: step 21650, loss 0.033208, acc 0.984375\n",
      "2018-05-23T15:54:18.056812: step 21651, loss 0.116549, acc 0.984375\n",
      "2018-05-23T15:54:18.561460: step 21652, loss 0.0950177, acc 0.96875\n",
      "2018-05-23T15:54:19.098025: step 21653, loss 0.0722086, acc 0.953125\n",
      "2018-05-23T15:54:19.635587: step 21654, loss 0.0658581, acc 0.96875\n",
      "2018-05-23T15:54:20.096354: step 21655, loss 0.0830879, acc 0.9375\n",
      "2018-05-23T15:54:20.481324: step 21656, loss 0.0667269, acc 0.984375\n",
      "2018-05-23T15:54:20.863302: step 21657, loss 0.0347497, acc 1\n",
      "2018-05-23T15:54:21.249323: step 21658, loss 0.127921, acc 0.9375\n",
      "2018-05-23T15:54:21.665211: step 21659, loss 0.122742, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:54:22.058882: step 21660, loss 0.0786952, acc 0.953125\n",
      "2018-05-23T15:54:22.512671: step 21661, loss 0.0830527, acc 0.96875\n",
      "2018-05-23T15:54:22.896640: step 21662, loss 0.130244, acc 0.90625\n",
      "2018-05-23T15:54:23.274629: step 21663, loss 0.039654, acc 0.984375\n",
      "2018-05-23T15:54:23.664587: step 21664, loss 0.0878117, acc 0.96875\n",
      "2018-05-23T15:54:24.033600: step 21665, loss 0.178126, acc 0.953125\n",
      "2018-05-23T15:54:24.397627: step 21666, loss 0.140838, acc 0.984375\n",
      "2018-05-23T15:54:24.765643: step 21667, loss 0.0983624, acc 0.96875\n",
      "2018-05-23T15:54:25.115706: step 21668, loss 0.0507125, acc 0.984375\n",
      "2018-05-23T15:54:25.462777: step 21669, loss 0.0570123, acc 0.984375\n",
      "2018-05-23T15:54:25.721087: step 21670, loss 0.0427228, acc 1\n",
      "2018-05-23T15:54:26.081122: step 21671, loss 0.198152, acc 0.96875\n",
      "2018-05-23T15:54:26.431187: step 21672, loss 0.0823895, acc 0.96875\n",
      "2018-05-23T15:54:26.800200: step 21673, loss 0.105944, acc 0.953125\n",
      "2018-05-23T15:54:27.147269: step 21674, loss 0.117253, acc 0.96875\n",
      "2018-05-23T15:54:27.508306: step 21675, loss 0.0985773, acc 0.953125\n",
      "2018-05-23T15:54:27.869337: step 21676, loss 0.0435134, acc 0.984375\n",
      "2018-05-23T15:54:28.215412: step 21677, loss 0.0664038, acc 0.96875\n",
      "2018-05-23T15:54:28.571460: step 21678, loss 0.0741587, acc 0.96875\n",
      "2018-05-23T15:54:29.025245: step 21679, loss 0.139364, acc 0.96875\n",
      "2018-05-23T15:54:29.425178: step 21680, loss 0.133198, acc 0.953125\n",
      "2018-05-23T15:54:29.850039: step 21681, loss 0.0711033, acc 0.984375\n",
      "2018-05-23T15:54:30.308813: step 21682, loss 0.100314, acc 0.953125\n",
      "2018-05-23T15:54:30.774566: step 21683, loss 0.0638444, acc 0.96875\n",
      "2018-05-23T15:54:31.248324: step 21684, loss 0.163119, acc 0.953125\n",
      "2018-05-23T15:54:31.703110: step 21685, loss 0.0129499, acc 1\n",
      "2018-05-23T15:54:32.192798: step 21686, loss 0.0514566, acc 0.96875\n",
      "2018-05-23T15:54:32.755294: step 21687, loss 0.0607388, acc 0.96875\n",
      "2018-05-23T15:54:33.273906: step 21688, loss 0.0309628, acc 1\n",
      "2018-05-23T15:54:33.711735: step 21689, loss 0.0698662, acc 0.9375\n",
      "2018-05-23T15:54:34.132609: step 21690, loss 0.0698895, acc 0.984375\n",
      "2018-05-23T15:54:34.556475: step 21691, loss 0.180668, acc 0.953125\n",
      "2018-05-23T15:54:34.975354: step 21692, loss 0.0500293, acc 0.96875\n",
      "2018-05-23T15:54:35.421164: step 21693, loss 0.0697589, acc 0.9375\n",
      "2018-05-23T15:54:35.857992: step 21694, loss 0.103938, acc 0.96875\n",
      "2018-05-23T15:54:36.330728: step 21695, loss 0.106523, acc 0.953125\n",
      "2018-05-23T15:54:36.789502: step 21696, loss 0.0605574, acc 0.96875\n",
      "2018-05-23T15:54:37.196412: step 21697, loss 0.0398572, acc 1\n",
      "2018-05-23T15:54:37.653191: step 21698, loss 0.0411568, acc 0.984375\n",
      "2018-05-23T15:54:38.092019: step 21699, loss 0.0460765, acc 0.984375\n",
      "2018-05-23T15:54:38.517878: step 21700, loss 0.0956529, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:54:45.376529: step 21700, loss 2.26017, acc 0.704958\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21700\n",
      "\n",
      "2018-05-23T15:54:47.328309: step 21701, loss 0.045291, acc 1\n",
      "2018-05-23T15:54:47.923715: step 21702, loss 0.0370792, acc 1\n",
      "2018-05-23T15:54:48.585943: step 21703, loss 0.042162, acc 0.984375\n",
      "2018-05-23T15:54:49.110542: step 21704, loss 0.297212, acc 0.921875\n",
      "2018-05-23T15:54:49.634139: step 21705, loss 0.166769, acc 0.90625\n",
      "2018-05-23T15:54:50.051025: step 21706, loss 0.0701802, acc 1\n",
      "2018-05-23T15:54:50.464920: step 21707, loss 0.114155, acc 0.953125\n",
      "2018-05-23T15:54:50.845897: step 21708, loss 0.021661, acc 1\n",
      "2018-05-23T15:54:51.229871: step 21709, loss 0.0400116, acc 0.984375\n",
      "2018-05-23T15:54:51.688643: step 21710, loss 0.0418615, acc 0.984375\n",
      "2018-05-23T15:54:52.084583: step 21711, loss 0.066581, acc 0.984375\n",
      "2018-05-23T15:54:52.477535: step 21712, loss 0.0342401, acc 0.984375\n",
      "2018-05-23T15:54:52.849540: step 21713, loss 0.0851905, acc 0.9375\n",
      "2018-05-23T15:54:53.248470: step 21714, loss 0.0926307, acc 0.96875\n",
      "2018-05-23T15:54:53.708240: step 21715, loss 0.0702321, acc 0.96875\n",
      "2018-05-23T15:54:54.119141: step 21716, loss 0.114872, acc 0.921875\n",
      "2018-05-23T15:54:54.518076: step 21717, loss 0.114569, acc 0.921875\n",
      "2018-05-23T15:54:54.898057: step 21718, loss 0.133747, acc 0.953125\n",
      "2018-05-23T15:54:55.282032: step 21719, loss 0.158102, acc 0.921875\n",
      "2018-05-23T15:54:55.653037: step 21720, loss 0.0536379, acc 0.984375\n",
      "2018-05-23T15:54:56.009088: step 21721, loss 0.0860059, acc 0.953125\n",
      "2018-05-23T15:54:56.374110: step 21722, loss 0.054744, acc 0.953125\n",
      "2018-05-23T15:54:56.756087: step 21723, loss 0.0745677, acc 0.984375\n",
      "2018-05-23T15:54:57.115126: step 21724, loss 0.0647377, acc 0.96875\n",
      "2018-05-23T15:54:57.550961: step 21725, loss 0.0539957, acc 0.96875\n",
      "2018-05-23T15:54:57.932938: step 21726, loss 0.0751644, acc 0.953125\n",
      "2018-05-23T15:54:58.354845: step 21727, loss 0.0275205, acc 1\n",
      "2018-05-23T15:54:58.774719: step 21728, loss 0.214265, acc 0.953125\n",
      "2018-05-23T15:54:59.244240: step 21729, loss 0.0676652, acc 0.953125\n",
      "2018-05-23T15:54:59.797761: step 21730, loss 0.201856, acc 0.90625\n",
      "2018-05-23T15:55:00.333330: step 21731, loss 0.0573926, acc 0.96875\n",
      "2018-05-23T15:55:00.817036: step 21732, loss 0.0549227, acc 0.96875\n",
      "2018-05-23T15:55:01.292760: step 21733, loss 0.146994, acc 0.90625\n",
      "2018-05-23T15:55:01.896146: step 21734, loss 0.0612193, acc 0.984375\n",
      "2018-05-23T15:55:02.346939: step 21735, loss 0.035568, acc 1\n",
      "2018-05-23T15:55:02.868545: step 21736, loss 0.0912523, acc 0.984375\n",
      "2018-05-23T15:55:03.760160: step 21737, loss 0.0667638, acc 0.953125\n",
      "2018-05-23T15:55:04.299716: step 21738, loss 0.169781, acc 0.90625\n",
      "2018-05-23T15:55:04.758490: step 21739, loss 0.0426169, acc 0.984375\n",
      "2018-05-23T15:55:05.252168: step 21740, loss 0.0821966, acc 0.96875\n",
      "2018-05-23T15:55:05.718919: step 21741, loss 0.0601043, acc 0.984375\n",
      "2018-05-23T15:55:06.150764: step 21742, loss 0.0784836, acc 0.953125\n",
      "2018-05-23T15:55:06.643446: step 21743, loss 0.0788847, acc 0.96875\n",
      "2018-05-23T15:55:07.142112: step 21744, loss 0.104369, acc 0.953125\n",
      "2018-05-23T15:55:07.568970: step 21745, loss 0.0502252, acc 0.984375\n",
      "2018-05-23T15:55:08.011785: step 21746, loss 0.0608816, acc 0.96875\n",
      "2018-05-23T15:55:08.448620: step 21747, loss 0.0844786, acc 0.96875\n",
      "2018-05-23T15:55:08.934317: step 21748, loss 0.111936, acc 0.9375\n",
      "2018-05-23T15:55:09.353197: step 21749, loss 0.0476077, acc 1\n",
      "2018-05-23T15:55:09.846877: step 21750, loss 0.040966, acc 0.984375\n",
      "2018-05-23T15:55:10.273735: step 21751, loss 0.0478808, acc 0.96875\n",
      "2018-05-23T15:55:10.744476: step 21752, loss 0.197448, acc 0.90625\n",
      "2018-05-23T15:55:11.174326: step 21753, loss 0.0988039, acc 0.96875\n",
      "2018-05-23T15:55:11.598197: step 21754, loss 0.123123, acc 0.953125\n",
      "2018-05-23T15:55:12.185619: step 21755, loss 0.0649604, acc 0.953125\n",
      "2018-05-23T15:55:12.653368: step 21756, loss 0.0693184, acc 0.96875\n",
      "2018-05-23T15:55:13.101171: step 21757, loss 0.056074, acc 0.984375\n",
      "2018-05-23T15:55:13.496114: step 21758, loss 0.0959687, acc 0.953125\n",
      "2018-05-23T15:55:13.967852: step 21759, loss 0.0396628, acc 1\n",
      "2018-05-23T15:55:14.415656: step 21760, loss 0.163806, acc 0.90625\n",
      "2018-05-23T15:55:14.892380: step 21761, loss 0.0499941, acc 0.984375\n",
      "2018-05-23T15:55:15.352150: step 21762, loss 0.143927, acc 0.921875\n",
      "2018-05-23T15:55:15.762053: step 21763, loss 0.0630831, acc 0.984375\n",
      "2018-05-23T15:55:16.182926: step 21764, loss 0.0946448, acc 0.953125\n",
      "2018-05-23T15:55:16.568893: step 21765, loss 0.083844, acc 0.984375\n",
      "2018-05-23T15:55:17.023677: step 21766, loss 0.0827592, acc 0.96875\n",
      "2018-05-23T15:55:17.454525: step 21767, loss 0.0838138, acc 0.96875\n",
      "2018-05-23T15:55:17.857450: step 21768, loss 0.0321127, acc 0.984375\n",
      "2018-05-23T15:55:18.334171: step 21769, loss 0.0561197, acc 0.953125\n",
      "2018-05-23T15:55:18.834832: step 21770, loss 0.102595, acc 0.953125\n",
      "2018-05-23T15:55:19.328513: step 21771, loss 0.106285, acc 0.96875\n",
      "2018-05-23T15:55:19.745397: step 21772, loss 0.121373, acc 0.9375\n",
      "2018-05-23T15:55:20.160287: step 21773, loss 0.0527192, acc 0.984375\n",
      "2018-05-23T15:55:20.579166: step 21774, loss 0.104405, acc 0.9375\n",
      "2018-05-23T15:55:21.061876: step 21775, loss 0.253765, acc 0.96875\n",
      "2018-05-23T15:55:21.540593: step 21776, loss 0.0861412, acc 0.953125\n",
      "2018-05-23T15:55:22.311532: step 21777, loss 0.0671699, acc 0.984375\n",
      "2018-05-23T15:55:22.994704: step 21778, loss 0.0483685, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:55:23.554206: step 21779, loss 0.0833476, acc 0.953125\n",
      "2018-05-23T15:55:24.401942: step 21780, loss 0.109042, acc 0.9375\n",
      "2018-05-23T15:55:25.369351: step 21781, loss 0.0345961, acc 0.984375\n",
      "2018-05-23T15:55:26.100396: step 21782, loss 0.117871, acc 0.953125\n",
      "2018-05-23T15:55:26.800522: step 21783, loss 0.116057, acc 0.953125\n",
      "2018-05-23T15:55:27.363017: step 21784, loss 0.0382121, acc 0.984375\n",
      "2018-05-23T15:55:28.034226: step 21785, loss 0.0503995, acc 0.984375\n",
      "2018-05-23T15:55:28.831089: step 21786, loss 0.161465, acc 0.96875\n",
      "2018-05-23T15:55:29.346710: step 21787, loss 0.0813787, acc 0.96875\n",
      "2018-05-23T15:55:30.089727: step 21788, loss 0.0650921, acc 0.96875\n",
      "2018-05-23T15:55:30.798826: step 21789, loss 0.0392119, acc 1\n",
      "2018-05-23T15:55:31.372294: step 21790, loss 0.375777, acc 0.9375\n",
      "2018-05-23T15:55:31.855000: step 21791, loss 0.0449014, acc 0.984375\n",
      "2018-05-23T15:55:32.574076: step 21792, loss 0.0801963, acc 0.953125\n",
      "2018-05-23T15:55:33.332051: step 21793, loss 0.0579427, acc 0.96875\n",
      "2018-05-23T15:55:33.933440: step 21794, loss 0.030896, acc 0.984375\n",
      "2018-05-23T15:55:34.436097: step 21795, loss 0.0756719, acc 0.9375\n",
      "2018-05-23T15:55:34.879908: step 21796, loss 0.0965615, acc 0.9375\n",
      "2018-05-23T15:55:35.329705: step 21797, loss 0.15526, acc 0.984375\n",
      "2018-05-23T15:55:35.852307: step 21798, loss 0.0893645, acc 0.96875\n",
      "2018-05-23T15:55:36.461677: step 21799, loss 0.148894, acc 0.9375\n",
      "2018-05-23T15:55:36.906507: step 21800, loss 0.0471232, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:55:42.669071: step 21800, loss 2.24754, acc 0.710101\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21800\n",
      "\n",
      "2018-05-23T15:55:44.730555: step 21801, loss 0.0476131, acc 0.96875\n",
      "2018-05-23T15:55:45.176363: step 21802, loss 0.0616027, acc 0.984375\n",
      "2018-05-23T15:55:45.605217: step 21803, loss 0.134386, acc 0.953125\n",
      "2018-05-23T15:55:46.049031: step 21804, loss 0.0982782, acc 0.953125\n",
      "2018-05-23T15:55:46.818969: step 21805, loss 0.0863884, acc 0.921875\n",
      "2018-05-23T15:55:47.551010: step 21806, loss 0.0718358, acc 0.984375\n",
      "2018-05-23T15:55:48.139438: step 21807, loss 0.14311, acc 0.9375\n",
      "2018-05-23T15:55:48.679990: step 21808, loss 0.0799171, acc 0.953125\n",
      "2018-05-23T15:55:49.164693: step 21809, loss 0.0644759, acc 0.96875\n",
      "2018-05-23T15:55:49.627456: step 21810, loss 0.0993312, acc 0.96875\n",
      "2018-05-23T15:55:50.059303: step 21811, loss 0.0528112, acc 0.984375\n",
      "2018-05-23T15:55:50.481171: step 21812, loss 0.039251, acc 0.984375\n",
      "2018-05-23T15:55:50.897060: step 21813, loss 0.032368, acc 1\n",
      "2018-05-23T15:55:51.320925: step 21814, loss 0.0481945, acc 1\n",
      "2018-05-23T15:55:51.748781: step 21815, loss 0.0629227, acc 0.96875\n",
      "2018-05-23T15:55:52.165666: step 21816, loss 0.0273714, acc 0.984375\n",
      "2018-05-23T15:55:52.603495: step 21817, loss 0.12981, acc 0.9375\n",
      "2018-05-23T15:55:53.035341: step 21818, loss 0.0440482, acc 1\n",
      "2018-05-23T15:55:53.461200: step 21819, loss 0.153035, acc 0.953125\n",
      "2018-05-23T15:55:53.936926: step 21820, loss 0.093717, acc 0.9375\n",
      "2018-05-23T15:55:54.349823: step 21821, loss 0.0468689, acc 0.984375\n",
      "2018-05-23T15:55:54.759728: step 21822, loss 0.050546, acc 0.984375\n",
      "2018-05-23T15:55:55.196557: step 21823, loss 0.0221306, acc 1\n",
      "2018-05-23T15:55:55.631395: step 21824, loss 0.0813016, acc 0.96875\n",
      "2018-05-23T15:55:56.077204: step 21825, loss 0.0927415, acc 0.953125\n",
      "2018-05-23T15:55:56.508050: step 21826, loss 0.0881805, acc 0.9375\n",
      "2018-05-23T15:55:56.939893: step 21827, loss 0.0982728, acc 0.921875\n",
      "2018-05-23T15:55:57.453519: step 21828, loss 0.0799194, acc 0.96875\n",
      "2018-05-23T15:55:58.058901: step 21829, loss 0.0241599, acc 1\n",
      "2018-05-23T15:55:58.804906: step 21830, loss 0.0633823, acc 0.984375\n",
      "2018-05-23T15:55:59.353436: step 21831, loss 0.0619004, acc 0.953125\n",
      "2018-05-23T15:55:59.819192: step 21832, loss 0.0459849, acc 0.984375\n",
      "2018-05-23T15:56:00.267989: step 21833, loss 0.0612015, acc 0.984375\n",
      "2018-05-23T15:56:00.770646: step 21834, loss 0.0464224, acc 1\n",
      "2018-05-23T15:56:01.314191: step 21835, loss 0.0786794, acc 0.953125\n",
      "2018-05-23T15:56:01.816847: step 21836, loss 0.113544, acc 0.96875\n",
      "2018-05-23T15:56:02.260660: step 21837, loss 0.044614, acc 0.96875\n",
      "2018-05-23T15:56:02.756333: step 21838, loss 0.0856146, acc 0.953125\n",
      "2018-05-23T15:56:03.219096: step 21839, loss 0.0798781, acc 0.984375\n",
      "2018-05-23T15:56:03.677868: step 21840, loss 0.109688, acc 0.9375\n",
      "2018-05-23T15:56:04.122678: step 21841, loss 0.113559, acc 0.9375\n",
      "2018-05-23T15:56:04.561504: step 21842, loss 0.040785, acc 0.984375\n",
      "2018-05-23T15:56:04.977393: step 21843, loss 0.0514781, acc 0.96875\n",
      "2018-05-23T15:56:05.384305: step 21844, loss 0.0327601, acc 0.984375\n",
      "2018-05-23T15:56:05.802186: step 21845, loss 0.0906988, acc 0.96875\n",
      "2018-05-23T15:56:06.245997: step 21846, loss 0.104593, acc 0.921875\n",
      "2018-05-23T15:56:06.656898: step 21847, loss 0.0291215, acc 0.984375\n",
      "2018-05-23T15:56:07.073784: step 21848, loss 0.0437267, acc 0.984375\n",
      "2018-05-23T15:56:07.513606: step 21849, loss 0.0611072, acc 0.953125\n",
      "2018-05-23T15:56:07.945453: step 21850, loss 0.0702211, acc 0.96875\n",
      "2018-05-23T15:56:08.391259: step 21851, loss 0.065268, acc 0.96875\n",
      "2018-05-23T15:56:08.813132: step 21852, loss 0.0564726, acc 0.984375\n",
      "2018-05-23T15:56:09.224034: step 21853, loss 0.0569561, acc 0.96875\n",
      "2018-05-23T15:56:09.628950: step 21854, loss 0.0890128, acc 0.9375\n",
      "2018-05-23T15:56:10.040846: step 21855, loss 0.0406524, acc 0.984375\n",
      "2018-05-23T15:56:10.451747: step 21856, loss 0.136704, acc 0.9375\n",
      "2018-05-23T15:56:10.844696: step 21857, loss 0.0277677, acc 0.984375\n",
      "2018-05-23T15:56:11.244626: step 21858, loss 0.0859527, acc 0.96875\n",
      "2018-05-23T15:56:11.652534: step 21859, loss 0.040344, acc 0.984375\n",
      "2018-05-23T15:56:12.290828: step 21860, loss 0.104966, acc 0.953125\n",
      "2018-05-23T15:56:13.250263: step 21861, loss 0.108331, acc 0.953125\n",
      "2018-05-23T15:56:13.809763: step 21862, loss 0.15662, acc 0.9375\n",
      "2018-05-23T15:56:14.481965: step 21863, loss 0.0283166, acc 1\n",
      "2018-05-23T15:56:15.114274: step 21864, loss 0.0179778, acc 1\n",
      "2018-05-23T15:56:15.714667: step 21865, loss 0.0555822, acc 0.984375\n",
      "2018-05-23T15:56:16.322043: step 21866, loss 0.0940342, acc 0.953125\n",
      "2018-05-23T15:56:16.844646: step 21867, loss 0.158994, acc 0.9375\n",
      "2018-05-23T15:56:17.313390: step 21868, loss 0.0453697, acc 1\n",
      "2018-05-23T15:56:17.846963: step 21869, loss 0.0810432, acc 0.96875\n",
      "2018-05-23T15:56:18.416440: step 21870, loss 0.0270702, acc 1\n",
      "2018-05-23T15:56:18.887181: step 21871, loss 0.0233876, acc 1\n",
      "2018-05-23T15:56:19.513505: step 21872, loss 0.0866132, acc 0.953125\n",
      "2018-05-23T15:56:20.172743: step 21873, loss 0.133122, acc 0.953125\n",
      "2018-05-23T15:56:20.825996: step 21874, loss 0.0630277, acc 0.953125\n",
      "2018-05-23T15:56:21.284766: step 21875, loss 0.183402, acc 0.96875\n",
      "2018-05-23T15:56:21.706637: step 21876, loss 0.091288, acc 0.953125\n",
      "2018-05-23T15:56:22.196328: step 21877, loss 0.133016, acc 0.96875\n",
      "2018-05-23T15:56:22.644129: step 21878, loss 0.0660353, acc 0.984375\n",
      "2018-05-23T15:56:23.130828: step 21879, loss 0.19309, acc 0.953125\n",
      "2018-05-23T15:56:23.671385: step 21880, loss 0.131948, acc 0.984375\n",
      "2018-05-23T15:56:24.177029: step 21881, loss 0.127564, acc 0.9375\n",
      "2018-05-23T15:56:24.566985: step 21882, loss 0.0310396, acc 1\n",
      "2018-05-23T15:56:24.945974: step 21883, loss 0.139633, acc 0.953125\n",
      "2018-05-23T15:56:25.344905: step 21884, loss 0.0636515, acc 0.96875\n",
      "2018-05-23T15:56:25.736859: step 21885, loss 0.125341, acc 0.96875\n",
      "2018-05-23T15:56:26.103874: step 21886, loss 0.0530913, acc 0.984375\n",
      "2018-05-23T15:56:26.489842: step 21887, loss 0.0811839, acc 0.953125\n",
      "2018-05-23T15:56:26.890769: step 21888, loss 0.113395, acc 0.953125\n",
      "2018-05-23T15:56:27.276736: step 21889, loss 0.144663, acc 0.921875\n",
      "2018-05-23T15:56:27.701601: step 21890, loss 0.0573276, acc 0.984375\n",
      "2018-05-23T15:56:28.129458: step 21891, loss 0.0696463, acc 0.984375\n",
      "2018-05-23T15:56:28.552324: step 21892, loss 0.062499, acc 0.984375\n",
      "2018-05-23T15:56:28.944275: step 21893, loss 0.0951916, acc 0.953125\n",
      "2018-05-23T15:56:29.451917: step 21894, loss 0.0334789, acc 1\n",
      "2018-05-23T15:56:29.959559: step 21895, loss 0.0890731, acc 0.9375\n",
      "2018-05-23T15:56:30.329570: step 21896, loss 0.0448983, acc 0.984375\n",
      "2018-05-23T15:56:30.703569: step 21897, loss 0.0465502, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23T15:56:31.129429: step 21898, loss 0.0424014, acc 0.984375\n",
      "2018-05-23T15:56:31.622113: step 21899, loss 0.108572, acc 0.9375\n",
      "2018-05-23T15:56:32.160815: step 21900, loss 0.0974062, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-05-23T15:56:37.687993: step 21900, loss 2.30302, acc 0.709387\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Ung Lik Teng\\Desktop\\CodenData\\Machine Learning\\NLP\\runs\\1527070647\\checkpoints\\model-21900\n",
      "\n",
      "2018-05-23T15:56:39.192967: step 21901, loss 0.0646335, acc 0.96875\n",
      "2018-05-23T15:56:39.598883: step 21902, loss 0.0488974, acc 0.96875\n",
      "2018-05-23T15:56:40.065633: step 21903, loss 0.0604542, acc 0.96875\n",
      "2018-05-23T15:56:40.557316: step 21904, loss 0.079999, acc 0.96875\n",
      "2018-05-23T15:56:41.046012: step 21905, loss 0.111937, acc 0.953125\n",
      "2018-05-23T15:56:41.555648: step 21906, loss 0.07285, acc 0.96875\n",
      "2018-05-23T15:56:41.932637: step 21907, loss 0.0948688, acc 0.96875\n",
      "2018-05-23T15:56:42.395399: step 21908, loss 0.0718235, acc 0.96875\n",
      "2018-05-23T15:56:42.898055: step 21909, loss 0.062144, acc 0.96875\n",
      "2018-05-23T15:56:43.284025: step 21910, loss 0.0653616, acc 0.96875\n",
      "2018-05-23T15:56:43.669992: step 21911, loss 0.0868807, acc 0.984375\n",
      "2018-05-23T15:56:44.040999: step 21912, loss 0.0365591, acc 0.984375\n",
      "2018-05-23T15:56:44.391060: step 21913, loss 0.0756652, acc 0.96875\n",
      "2018-05-23T15:56:44.743118: step 21914, loss 0.0875432, acc 0.96875\n",
      "2018-05-23T15:56:45.102158: step 21915, loss 0.078803, acc 0.96875\n",
      "2018-05-23T15:56:45.482142: step 21916, loss 0.0695341, acc 0.9375\n",
      "2018-05-23T15:56:45.841181: step 21917, loss 0.0830699, acc 0.9375\n",
      "2018-05-23T15:56:46.199223: step 21918, loss 0.0967416, acc 0.9375\n",
      "2018-05-23T15:56:46.568236: step 21919, loss 0.0981032, acc 0.96875\n",
      "2018-05-23T15:56:46.920294: step 21920, loss 0.0504679, acc 0.984375\n",
      "2018-05-23T15:56:47.271355: step 21921, loss 0.110164, acc 0.953125\n",
      "2018-05-23T15:56:47.631392: step 21922, loss 0.0307943, acc 1\n",
      "2018-05-23T15:56:47.986442: step 21923, loss 0.0670719, acc 0.953125\n",
      "2018-05-23T15:56:48.342168: step 21924, loss 0.0724456, acc 0.96875\n",
      "2018-05-23T15:56:48.707743: step 21925, loss 0.0498487, acc 0.984375\n",
      "2018-05-23T15:56:49.062793: step 21926, loss 0.0296691, acc 1\n",
      "2018-05-23T15:56:49.419841: step 21927, loss 0.0458704, acc 0.96875\n",
      "2018-05-23T15:56:49.776886: step 21928, loss 0.034932, acc 1\n",
      "2018-05-23T15:56:50.144898: step 21929, loss 0.0676503, acc 0.96875\n",
      "2018-05-23T15:56:50.500946: step 21930, loss 0.0909647, acc 0.96875\n",
      "2018-05-23T15:56:50.858991: step 21931, loss 0.118512, acc 0.96875\n",
      "2018-05-23T15:56:51.214038: step 21932, loss 0.169564, acc 0.96875\n",
      "2018-05-23T15:56:51.572081: step 21933, loss 0.0466543, acc 0.984375\n",
      "2018-05-23T15:56:51.953061: step 21934, loss 0.0532098, acc 0.984375\n",
      "2018-05-23T15:56:52.338034: step 21935, loss 0.0402668, acc 0.984375\n",
      "2018-05-23T15:56:52.686101: step 21936, loss 0.0610298, acc 0.96875\n",
      "2018-05-23T15:56:53.101989: step 21937, loss 0.0523946, acc 0.984375\n",
      "2018-05-23T15:56:53.504911: step 21938, loss 0.0282095, acc 1\n",
      "2018-05-23T15:56:53.925786: step 21939, loss 0.0374091, acc 0.984375\n",
      "2018-05-23T15:56:54.321727: step 21940, loss 0.0601909, acc 0.96875\n",
      "2018-05-23T15:56:54.781496: step 21941, loss 0.0207943, acc 1\n",
      "2018-05-23T15:56:55.211345: step 21942, loss 0.050178, acc 1\n",
      "2018-05-23T15:56:55.790796: step 21943, loss 0.0679802, acc 0.953125\n",
      "2018-05-23T15:56:56.267519: step 21944, loss 0.0898215, acc 0.96875\n",
      "2018-05-23T15:56:56.661466: step 21945, loss 0.0984419, acc 0.984375\n",
      "2018-05-23T15:56:57.025493: step 21946, loss 0.0686075, acc 0.96875\n",
      "2018-05-23T15:56:57.472299: step 21947, loss 0.0830847, acc 0.953125\n",
      "2018-05-23T15:56:57.869237: step 21948, loss 0.0405635, acc 0.96875\n",
      "2018-05-23T15:56:58.286122: step 21949, loss 0.112884, acc 0.9375\n",
      "2018-05-23T15:56:58.705998: step 21950, loss 0.0344306, acc 0.984375\n",
      "2018-05-23T15:56:59.127868: step 21951, loss 0.069892, acc 0.96875\n",
      "2018-05-23T15:56:59.590629: step 21952, loss 0.0728059, acc 0.953125\n",
      "2018-05-23T15:56:59.974605: step 21953, loss 0.0553715, acc 0.953125\n",
      "2018-05-23T15:57:00.448335: step 21954, loss 0.0139589, acc 1\n",
      "2018-05-23T15:57:00.830314: step 21955, loss 0.107497, acc 0.96875\n",
      "2018-05-23T15:57:01.195336: step 21956, loss 0.147127, acc 0.9375\n",
      "2018-05-23T15:57:01.559365: step 21957, loss 0.0609364, acc 0.984375\n",
      "2018-05-23T15:57:01.915410: step 21958, loss 0.0962403, acc 0.96875\n",
      "2018-05-23T15:57:02.283426: step 21959, loss 0.107513, acc 0.96875\n",
      "2018-05-23T15:57:02.667398: step 21960, loss 0.141034, acc 0.9375\n",
      "2018-05-23T15:57:03.023445: step 21961, loss 0.0809824, acc 0.96875\n",
      "2018-05-23T15:57:03.411411: step 21962, loss 0.0405857, acc 0.984375\n",
      "2018-05-23T15:57:03.797376: step 21963, loss 0.0573377, acc 1\n",
      "2018-05-23T15:57:04.221252: step 21964, loss 0.0696099, acc 0.984375\n",
      "2018-05-23T15:57:04.631155: step 21965, loss 0.0752354, acc 0.953125\n",
      "2018-05-23T15:57:05.000098: step 21966, loss 0.140093, acc 0.953125\n",
      "2018-05-23T15:57:05.356145: step 21967, loss 0.0559994, acc 0.96875\n",
      "2018-05-23T15:57:05.716180: step 21968, loss 0.0335065, acc 0.984375\n",
      "2018-05-23T15:57:06.206867: step 21969, loss 0.0631289, acc 0.984375\n",
      "2018-05-23T15:57:06.787319: step 21970, loss 0.102015, acc 0.921875\n",
      "2018-05-23T15:57:07.294957: step 21971, loss 0.0367007, acc 1\n",
      "2018-05-23T15:57:07.789633: step 21972, loss 0.0451885, acc 0.984375\n",
      "2018-05-23T15:57:08.185575: step 21973, loss 0.1161, acc 0.953125\n",
      "2018-05-23T15:57:08.555587: step 21974, loss 0.0343965, acc 0.984375\n",
      "2018-05-23T15:57:08.984437: step 21975, loss 0.0492009, acc 0.984375\n",
      "2018-05-23T15:57:09.339489: step 21976, loss 0.0698487, acc 0.984375\n",
      "2018-05-23T15:57:09.699526: step 21977, loss 0.0821742, acc 0.984375\n",
      "2018-05-23T15:57:10.056570: step 21978, loss 0.103486, acc 0.96875\n",
      "2018-05-23T15:57:10.459491: step 21979, loss 0.0869696, acc 0.96875\n",
      "2018-05-23T15:57:11.022985: step 21980, loss 0.0860031, acc 0.953125\n",
      "2018-05-23T15:57:11.657287: step 21981, loss 0.077626, acc 0.96875\n",
      "2018-05-23T15:57:12.275636: step 21982, loss 0.0200699, acc 1\n",
      "2018-05-23T15:57:12.937862: step 21983, loss 0.0541784, acc 0.953125\n",
      "2018-05-23T15:57:13.579145: step 21984, loss 0.0573885, acc 0.96875\n",
      "2018-05-23T15:57:14.150616: step 21985, loss 0.0751621, acc 0.9375\n",
      "2018-05-23T15:57:14.661253: step 21986, loss 0.0736644, acc 0.984375\n",
      "2018-05-23T15:57:15.082126: step 21987, loss 0.052814, acc 0.96875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-3a7be1b324f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-5dcb14833547>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, y_batch)\u001b[0m\n\u001b[0;32m     12\u001b[0m     _, step, summaries, loss, accuracy = sess.run(\n\u001b[0;32m     13\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         feed_dict)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtime_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement=allow_soft_placement,\n",
    "    log_device_placement=log_device_placement)\n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        cnn = TweetsCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=filter_sizes,\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
